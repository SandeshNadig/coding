{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3c504a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b79da03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minfo\u001b[49m(stats)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "info(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991b5016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.stats in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.stats - .. _statsrefmanual:\n",
      "\n",
      "DESCRIPTION\n",
      "    ==========================================\n",
      "    Statistical functions (:mod:`scipy.stats`)\n",
      "    ==========================================\n",
      "    \n",
      "    .. currentmodule:: scipy.stats\n",
      "    \n",
      "    This module contains a large number of probability distributions,\n",
      "    summary and frequency statistics, correlation functions and statistical\n",
      "    tests, masked statistics, kernel density estimation, quasi-Monte Carlo\n",
      "    functionality, and more.\n",
      "    \n",
      "    Statistics is a very large area, and there are topics that are out of scope\n",
      "    for SciPy and are covered by other packages. Some of the most important ones\n",
      "    are:\n",
      "    \n",
      "    - `statsmodels <https://www.statsmodels.org/stable/index.html>`__:\n",
      "      regression, linear models, time series analysis, extensions to topics\n",
      "      also covered by ``scipy.stats``.\n",
      "    - `Pandas <https://pandas.pydata.org/>`__: tabular data, time series\n",
      "      functionality, interfaces to other statistical languages.\n",
      "    - `PyMC3 <https://docs.pymc.io/>`__: Bayesian statistical\n",
      "      modeling, probabilistic machine learning.\n",
      "    - `scikit-learn <https://scikit-learn.org/>`__: classification, regression,\n",
      "      model selection.\n",
      "    - `Seaborn <https://seaborn.pydata.org/>`__: statistical data visualization.\n",
      "    - `rpy2 <https://rpy2.github.io/>`__: Python to R bridge.\n",
      "    \n",
      "    \n",
      "    Probability distributions\n",
      "    =========================\n",
      "    \n",
      "    Each univariate distribution is an instance of a subclass of `rv_continuous`\n",
      "    (`rv_discrete` for discrete distributions):\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rv_continuous\n",
      "       rv_discrete\n",
      "       rv_histogram\n",
      "    \n",
      "    Continuous distributions\n",
      "    ------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       alpha             -- Alpha\n",
      "       anglit            -- Anglit\n",
      "       arcsine           -- Arcsine\n",
      "       argus             -- Argus\n",
      "       beta              -- Beta\n",
      "       betaprime         -- Beta Prime\n",
      "       bradford          -- Bradford\n",
      "       burr              -- Burr (Type III)\n",
      "       burr12            -- Burr (Type XII)\n",
      "       cauchy            -- Cauchy\n",
      "       chi               -- Chi\n",
      "       chi2              -- Chi-squared\n",
      "       cosine            -- Cosine\n",
      "       crystalball       -- Crystalball\n",
      "       dgamma            -- Double Gamma\n",
      "       dweibull          -- Double Weibull\n",
      "       erlang            -- Erlang\n",
      "       expon             -- Exponential\n",
      "       exponnorm         -- Exponentially Modified Normal\n",
      "       exponweib         -- Exponentiated Weibull\n",
      "       exponpow          -- Exponential Power\n",
      "       f                 -- F (Snecdor F)\n",
      "       fatiguelife       -- Fatigue Life (Birnbaum-Saunders)\n",
      "       fisk              -- Fisk\n",
      "       foldcauchy        -- Folded Cauchy\n",
      "       foldnorm          -- Folded Normal\n",
      "       genlogistic       -- Generalized Logistic\n",
      "       gennorm           -- Generalized normal\n",
      "       genpareto         -- Generalized Pareto\n",
      "       genexpon          -- Generalized Exponential\n",
      "       genextreme        -- Generalized Extreme Value\n",
      "       gausshyper        -- Gauss Hypergeometric\n",
      "       gamma             -- Gamma\n",
      "       gengamma          -- Generalized gamma\n",
      "       genhalflogistic   -- Generalized Half Logistic\n",
      "       genhyperbolic     -- Generalized Hyperbolic\n",
      "       geninvgauss       -- Generalized Inverse Gaussian\n",
      "       gilbrat           -- Gilbrat\n",
      "       gompertz          -- Gompertz (Truncated Gumbel)\n",
      "       gumbel_r          -- Right Sided Gumbel, Log-Weibull, Fisher-Tippett, Extreme Value Type I\n",
      "       gumbel_l          -- Left Sided Gumbel, etc.\n",
      "       halfcauchy        -- Half Cauchy\n",
      "       halflogistic      -- Half Logistic\n",
      "       halfnorm          -- Half Normal\n",
      "       halfgennorm       -- Generalized Half Normal\n",
      "       hypsecant         -- Hyperbolic Secant\n",
      "       invgamma          -- Inverse Gamma\n",
      "       invgauss          -- Inverse Gaussian\n",
      "       invweibull        -- Inverse Weibull\n",
      "       johnsonsb         -- Johnson SB\n",
      "       johnsonsu         -- Johnson SU\n",
      "       kappa4            -- Kappa 4 parameter\n",
      "       kappa3            -- Kappa 3 parameter\n",
      "       ksone             -- Distribution of Kolmogorov-Smirnov one-sided test statistic\n",
      "       kstwo             -- Distribution of Kolmogorov-Smirnov two-sided test statistic\n",
      "       kstwobign         -- Limiting Distribution of scaled Kolmogorov-Smirnov two-sided test statistic.\n",
      "       laplace           -- Laplace\n",
      "       laplace_asymmetric    -- Asymmetric Laplace\n",
      "       levy              -- Levy\n",
      "       levy_l\n",
      "       levy_stable\n",
      "       logistic          -- Logistic\n",
      "       loggamma          -- Log-Gamma\n",
      "       loglaplace        -- Log-Laplace (Log Double Exponential)\n",
      "       lognorm           -- Log-Normal\n",
      "       loguniform        -- Log-Uniform\n",
      "       lomax             -- Lomax (Pareto of the second kind)\n",
      "       maxwell           -- Maxwell\n",
      "       mielke            -- Mielke's Beta-Kappa\n",
      "       moyal             -- Moyal\n",
      "       nakagami          -- Nakagami\n",
      "       ncx2              -- Non-central chi-squared\n",
      "       ncf               -- Non-central F\n",
      "       nct               -- Non-central Student's T\n",
      "       norm              -- Normal (Gaussian)\n",
      "       norminvgauss      -- Normal Inverse Gaussian\n",
      "       pareto            -- Pareto\n",
      "       pearson3          -- Pearson type III\n",
      "       powerlaw          -- Power-function\n",
      "       powerlognorm      -- Power log normal\n",
      "       powernorm         -- Power normal\n",
      "       rdist             -- R-distribution\n",
      "       rayleigh          -- Rayleigh\n",
      "       rice              -- Rice\n",
      "       recipinvgauss     -- Reciprocal Inverse Gaussian\n",
      "       semicircular      -- Semicircular\n",
      "       skewcauchy        -- Skew Cauchy\n",
      "       skewnorm          -- Skew normal\n",
      "       studentized_range    -- Studentized Range\n",
      "       t                 -- Student's T\n",
      "       trapezoid         -- Trapezoidal\n",
      "       triang            -- Triangular\n",
      "       truncexpon        -- Truncated Exponential\n",
      "       truncnorm         -- Truncated Normal\n",
      "       tukeylambda       -- Tukey-Lambda\n",
      "       uniform           -- Uniform\n",
      "       vonmises          -- Von-Mises (Circular)\n",
      "       vonmises_line     -- Von-Mises (Line)\n",
      "       wald              -- Wald\n",
      "       weibull_min       -- Minimum Weibull (see Frechet)\n",
      "       weibull_max       -- Maximum Weibull (see Frechet)\n",
      "       wrapcauchy        -- Wrapped Cauchy\n",
      "    \n",
      "    Multivariate distributions\n",
      "    --------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       multivariate_normal    -- Multivariate normal distribution\n",
      "       matrix_normal          -- Matrix normal distribution\n",
      "       dirichlet              -- Dirichlet\n",
      "       wishart                -- Wishart\n",
      "       invwishart             -- Inverse Wishart\n",
      "       multinomial            -- Multinomial distribution\n",
      "       special_ortho_group    -- SO(N) group\n",
      "       ortho_group            -- O(N) group\n",
      "       unitary_group          -- U(N) group\n",
      "       random_correlation     -- random correlation matrices\n",
      "       multivariate_t         -- Multivariate t-distribution\n",
      "       multivariate_hypergeom -- Multivariate hypergeometric distribution\n",
      "    \n",
      "    Discrete distributions\n",
      "    ----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       bernoulli                -- Bernoulli\n",
      "       betabinom                -- Beta-Binomial\n",
      "       binom                    -- Binomial\n",
      "       boltzmann                -- Boltzmann (Truncated Discrete Exponential)\n",
      "       dlaplace                 -- Discrete Laplacian\n",
      "       geom                     -- Geometric\n",
      "       hypergeom                -- Hypergeometric\n",
      "       logser                   -- Logarithmic (Log-Series, Series)\n",
      "       nbinom                   -- Negative Binomial\n",
      "       nchypergeom_fisher       -- Fisher's Noncentral Hypergeometric\n",
      "       nchypergeom_wallenius    -- Wallenius's Noncentral Hypergeometric\n",
      "       nhypergeom               -- Negative Hypergeometric\n",
      "       planck                   -- Planck (Discrete Exponential)\n",
      "       poisson                  -- Poisson\n",
      "       randint                  -- Discrete Uniform\n",
      "       skellam                  -- Skellam\n",
      "       yulesimon                -- Yule-Simon\n",
      "       zipf                     -- Zipf (Zeta)\n",
      "       zipfian                  -- Zipfian\n",
      "    \n",
      "    An overview of statistical functions is given below.  Many of these functions\n",
      "    have a similar version in `scipy.stats.mstats` which work for masked arrays.\n",
      "    \n",
      "    Summary statistics\n",
      "    ==================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       describe          -- Descriptive statistics\n",
      "       gmean             -- Geometric mean\n",
      "       hmean             -- Harmonic mean\n",
      "       kurtosis          -- Fisher or Pearson kurtosis\n",
      "       mode              -- Modal value\n",
      "       moment            -- Central moment\n",
      "       skew              -- Skewness\n",
      "       kstat             --\n",
      "       kstatvar          --\n",
      "       tmean             -- Truncated arithmetic mean\n",
      "       tvar              -- Truncated variance\n",
      "       tmin              --\n",
      "       tmax              --\n",
      "       tstd              --\n",
      "       tsem              --\n",
      "       variation         -- Coefficient of variation\n",
      "       find_repeats\n",
      "       trim_mean\n",
      "       gstd              -- Geometric Standard Deviation\n",
      "       iqr\n",
      "       sem\n",
      "       bayes_mvs\n",
      "       mvsdist\n",
      "       entropy\n",
      "       differential_entropy\n",
      "       median_absolute_deviation\n",
      "       median_abs_deviation\n",
      "       bootstrap\n",
      "    \n",
      "    Frequency statistics\n",
      "    ====================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       cumfreq\n",
      "       itemfreq\n",
      "       percentileofscore\n",
      "       scoreatpercentile\n",
      "       relfreq\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       binned_statistic     -- Compute a binned statistic for a set of data.\n",
      "       binned_statistic_2d  -- Compute a 2-D binned statistic for a set of data.\n",
      "       binned_statistic_dd  -- Compute a d-D binned statistic for a set of data.\n",
      "    \n",
      "    Correlation functions\n",
      "    =====================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       f_oneway\n",
      "       alexandergovern\n",
      "       pearsonr\n",
      "       spearmanr\n",
      "       pointbiserialr\n",
      "       kendalltau\n",
      "       weightedtau\n",
      "       somersd\n",
      "       linregress\n",
      "       siegelslopes\n",
      "       theilslopes\n",
      "       multiscale_graphcorr\n",
      "    \n",
      "    Statistical tests\n",
      "    =================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ttest_1samp\n",
      "       ttest_ind\n",
      "       ttest_ind_from_stats\n",
      "       ttest_rel\n",
      "       chisquare\n",
      "       cramervonmises\n",
      "       cramervonmises_2samp\n",
      "       power_divergence\n",
      "       kstest\n",
      "       ks_1samp\n",
      "       ks_2samp\n",
      "       epps_singleton_2samp\n",
      "       mannwhitneyu\n",
      "       tiecorrect\n",
      "       rankdata\n",
      "       ranksums\n",
      "       wilcoxon\n",
      "       kruskal\n",
      "       friedmanchisquare\n",
      "       brunnermunzel\n",
      "       combine_pvalues\n",
      "       jarque_bera\n",
      "       page_trend_test\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ansari\n",
      "       bartlett\n",
      "       levene\n",
      "       shapiro\n",
      "       anderson\n",
      "       anderson_ksamp\n",
      "       binom_test\n",
      "       binomtest\n",
      "       fligner\n",
      "       median_test\n",
      "       mood\n",
      "       skewtest\n",
      "       kurtosistest\n",
      "       normaltest\n",
      "    \n",
      "    \n",
      "    Quasi-Monte Carlo\n",
      "    =================\n",
      "    \n",
      "    .. toctree::\n",
      "       :maxdepth: 4\n",
      "    \n",
      "       stats.qmc\n",
      "    \n",
      "    \n",
      "    Masked statistics functions\n",
      "    ===========================\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       stats.mstats\n",
      "    \n",
      "    \n",
      "    Other statistical functionality\n",
      "    ===============================\n",
      "    \n",
      "    Transformations\n",
      "    ---------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       boxcox\n",
      "       boxcox_normmax\n",
      "       boxcox_llf\n",
      "       yeojohnson\n",
      "       yeojohnson_normmax\n",
      "       yeojohnson_llf\n",
      "       obrientransform\n",
      "       sigmaclip\n",
      "       trimboth\n",
      "       trim1\n",
      "       zmap\n",
      "       zscore\n",
      "    \n",
      "    Statistical distances\n",
      "    ---------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       wasserstein_distance\n",
      "       energy_distance\n",
      "    \n",
      "    Random variate generation / CDF Inversion\n",
      "    =========================================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rvs_ratio_uniforms\n",
      "       NumericalInverseHermite\n",
      "    \n",
      "    Circular statistical functions\n",
      "    ------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       circmean\n",
      "       circvar\n",
      "       circstd\n",
      "    \n",
      "    Contingency table functions\n",
      "    ---------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       chi2_contingency\n",
      "       contingency.crosstab\n",
      "       contingency.expected_freq\n",
      "       contingency.margins\n",
      "       contingency.relative_risk\n",
      "       contingency.association\n",
      "       fisher_exact\n",
      "       barnard_exact\n",
      "       boschloo_exact\n",
      "    \n",
      "    Plot-tests\n",
      "    ----------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ppcc_max\n",
      "       ppcc_plot\n",
      "       probplot\n",
      "       boxcox_normplot\n",
      "       yeojohnson_normplot\n",
      "    \n",
      "    Univariate and multivariate kernel density estimation\n",
      "    -----------------------------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       gaussian_kde\n",
      "    \n",
      "    Warnings used in :mod:`scipy.stats`\n",
      "    -----------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       F_onewayConstantInputWarning\n",
      "       F_onewayBadInputSizesWarning\n",
      "       PearsonRConstantInputWarning\n",
      "       PearsonRNearConstantInputWarning\n",
      "       SpearmanRConstantInputWarning\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _binned_statistic\n",
      "    _binomtest\n",
      "    _boost (package)\n",
      "    _bootstrap\n",
      "    _common\n",
      "    _constants\n",
      "    _continuous_distns\n",
      "    _crosstab\n",
      "    _discrete_distns\n",
      "    _distn_infrastructure\n",
      "    _distr_params\n",
      "    _entropy\n",
      "    _generate_pyx\n",
      "    _hypotests\n",
      "    _ksstats\n",
      "    _mannwhitneyu\n",
      "    _multivariate\n",
      "    _page_trend_test\n",
      "    _qmc\n",
      "    _qmc_cy\n",
      "    _relative_risk\n",
      "    _result_classes\n",
      "    _rvs_sampling\n",
      "    _sobol\n",
      "    _stats\n",
      "    _stats_mstats_common\n",
      "    _tukeylambda_stats\n",
      "    _wilcoxon_data\n",
      "    biasedurn\n",
      "    contingency\n",
      "    distributions\n",
      "    kde\n",
      "    morestats\n",
      "    mstats\n",
      "    mstats_basic\n",
      "    mstats_extras\n",
      "    mvn\n",
      "    qmc\n",
      "    setup\n",
      "    statlib\n",
      "    stats\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.RuntimeWarning(builtins.Warning)\n",
      "        scipy.stats.stats.F_onewayBadInputSizesWarning\n",
      "        scipy.stats.stats.F_onewayConstantInputWarning\n",
      "        scipy.stats.stats.PearsonRConstantInputWarning\n",
      "        scipy.stats.stats.PearsonRNearConstantInputWarning\n",
      "        scipy.stats.stats.SpearmanRConstantInputWarning\n",
      "    builtins.object\n",
      "        scipy.stats._rvs_sampling.NumericalInverseHermite\n",
      "        scipy.stats.kde.gaussian_kde\n",
      "    scipy.stats._distn_infrastructure.rv_generic(builtins.object)\n",
      "        scipy.stats._distn_infrastructure.rv_continuous\n",
      "            scipy.stats._continuous_distns.rv_histogram\n",
      "        scipy.stats._distn_infrastructure.rv_discrete\n",
      "    \n",
      "    class F_onewayBadInputSizesWarning(builtins.RuntimeWarning)\n",
      "     |  Warning generated by `f_oneway` when an input has length 0,\n",
      "     |  or if all the inputs have length 1.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      F_onewayBadInputSizesWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class F_onewayConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  F_onewayConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `f_oneway` when an input is constant, e.g.\n",
      "     |  each of the samples provided is a constant array.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      F_onewayConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class NumericalInverseHermite(builtins.object)\n",
      "     |  NumericalInverseHermite(dist, *, tol=1e-12, max_intervals=100000)\n",
      "     |  \n",
      "     |  A Hermite spline fast numerical inverse of a probability distribution.\n",
      "     |  \n",
      "     |  The initializer of `NumericalInverseHermite` accepts `dist`, an object\n",
      "     |  representing a continuous distribution, and provides an object with methods\n",
      "     |  that approximate `dist.ppf` and `dist.rvs`. For most distributions,\n",
      "     |  these methods are faster than those of `dist` itself.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  dist : object\n",
      "     |      Object representing the distribution for which a fast numerical inverse\n",
      "     |      is desired; for instance, a frozen instance of a `scipy.stats`\n",
      "     |      continuous distribution. See Notes and Examples for details.\n",
      "     |  tol : float, optional\n",
      "     |      u-error tolerance (see Notes). The default is 1e-12.\n",
      "     |  max_intervals : int, optional\n",
      "     |      Maximum number of intervals in the cubic Hermite spline used to\n",
      "     |      approximate the percent point function. The default is 100000.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  intervals : int\n",
      "     |      The number of intervals of the interpolant.\n",
      "     |  midpoint_error : float\n",
      "     |      The maximum u-error at an interpolant interval midpoint.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  `NumericalInverseHermite` approximates the inverse of a continuous\n",
      "     |  statistical distribution's CDF with a cubic Hermite spline.\n",
      "     |  \n",
      "     |  As described in [1]_, it begins by evaluating the distribution's PDF and\n",
      "     |  CDF at a mesh of quantiles ``x`` within the distribution's support.\n",
      "     |  It uses the results to fit a cubic Hermite spline ``H`` such that\n",
      "     |  ``H(p) == x``, where ``p`` is the array of percentiles corresponding\n",
      "     |  with the quantiles ``x``. Therefore, the spline approximates the inverse\n",
      "     |  of the distribution's CDF to machine precision at the percentiles ``p``,\n",
      "     |  but typically, the spline will not be as accurate at the midpoints between\n",
      "     |  the percentile points::\n",
      "     |  \n",
      "     |      p_mid = (p[:-1] + p[1:])/2\n",
      "     |  \n",
      "     |  so the mesh of quantiles is refined as needed to reduce the maximum\n",
      "     |  \"u-error\"::\n",
      "     |  \n",
      "     |      u_error = np.max(np.abs(dist.cdf(H(p_mid)) - p_mid))\n",
      "     |  \n",
      "     |  below the specified tolerance `tol`. Refinement stops when the required\n",
      "     |  tolerance is achieved or when the number of mesh intervals after the next\n",
      "     |  refinement could exceed the maximum allowed number `max_intervals`.\n",
      "     |  \n",
      "     |  The object `dist` must have methods ``pdf``, ``cdf``, and ``ppf`` that\n",
      "     |  behave like those of a *frozen* instance of `scipy.stats.rv_continuous`.\n",
      "     |  Specifically, it must have methods ``pdf`` and ``cdf`` that accept exactly\n",
      "     |  one ndarray argument ``x`` and return the probability density function and\n",
      "     |  cumulative density function (respectively) at ``x``. The object must also\n",
      "     |  have a method ``ppf`` that accepts a float ``p`` and returns the percentile\n",
      "     |  point function at ``p``. The object may also have a method ``isf`` that\n",
      "     |  accepts a float ``p`` and returns the inverse survival function at ``p``;\n",
      "     |  if it does not, it will be assigned an attribute ``isf`` that calculates\n",
      "     |  the inverse survival function using ``ppf``. The ``ppf`` and\n",
      "     |  ``isf` methods will each be evaluated at a small positive float ``p``\n",
      "     |  (e.g. ``p = utol/10``), and the domain over which the approximate numerical\n",
      "     |  inverse is defined will be ``ppf(p)`` to ``isf(p)``. The approximation will\n",
      "     |  not be accurate in the extreme tails beyond this domain.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Hörmann, Wolfgang, and Josef Leydold. \"Continuous random variate\n",
      "     |         generation by fast numerical inversion.\" ACM Transactions on\n",
      "     |         Modeling and Computer Simulation (TOMACS) 13.4 (2003): 347-362.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  For some distributions, ``dist.ppf`` and ``dist.rvs`` are quite slow.\n",
      "     |  For instance, consider `scipy.stats.genexpon`. We freeze the distribution\n",
      "     |  by passing all shape parameters into its initializer and time the resulting\n",
      "     |  object's ``ppf`` and ``rvs`` functions.\n",
      "     |  \n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from scipy import stats\n",
      "     |  >>> from timeit import timeit\n",
      "     |  >>> time_once = lambda f: f\"{timeit(f, number=1)*1000:.6} ms\"\n",
      "     |  >>> dist = stats.genexpon(9, 16, 3)  # freeze the distribution\n",
      "     |  >>> p = np.linspace(0.01, 0.99, 99)  # percentiles from 1% to 99%\n",
      "     |  >>> time_once(lambda: dist.ppf(p))\n",
      "     |  '154.565 ms'  # may vary\n",
      "     |  \n",
      "     |  >>> time_once(lambda: dist.rvs(size=100))\n",
      "     |  '148.979 ms'  # may vary\n",
      "     |  \n",
      "     |  The `NumericalInverseHermite` has a method that approximates ``dist.ppf``.\n",
      "     |  \n",
      "     |  >>> from scipy.stats import NumericalInverseHermite\n",
      "     |  >>> fni = NumericalInverseHermite(dist)\n",
      "     |  >>> np.allclose(fni.ppf(p), dist.ppf(p))\n",
      "     |  True\n",
      "     |  \n",
      "     |  In some cases, it is faster to both generate the fast numerical inverse\n",
      "     |  and use it than to call ``dist.ppf``.\n",
      "     |  \n",
      "     |  >>> def time_me():\n",
      "     |  ...     fni = NumericalInverseHermite(dist)\n",
      "     |  ...     fni.ppf(p)\n",
      "     |  >>> time_once(time_me)\n",
      "     |  '11.9222 ms'  # may vary\n",
      "     |  \n",
      "     |  After generating the fast numerical inverse, subsequent calls to its\n",
      "     |  methods are much faster.\n",
      "     |  >>> time_once(lambda: fni.ppf(p))\n",
      "     |  '0.0819 ms'  # may vary\n",
      "     |  \n",
      "     |  The fast numerical inverse can also be used to generate random variates\n",
      "     |  using inverse transform sampling.\n",
      "     |  \n",
      "     |  >>> time_once(lambda: fni.rvs(size=100))\n",
      "     |  '0.0911 ms'  # may vary\n",
      "     |  \n",
      "     |  Depending on the implementation of the distribution's random sampling\n",
      "     |  method, the random variates generated may be nearly identical, given\n",
      "     |  the same random state.\n",
      "     |  \n",
      "     |  >>> # `seed` ensures identical random streams are used by each `rvs` method\n",
      "     |  >>> seed = 500072020\n",
      "     |  >>> rvs1 = dist.rvs(size=100, random_state=np.random.default_rng(seed))\n",
      "     |  >>> rvs2 = fni.rvs(size=100, random_state=np.random.default_rng(seed))\n",
      "     |  >>> np.allclose(rvs1, rvs2)\n",
      "     |  True\n",
      "     |  \n",
      "     |  To use `NumericalInverseHermite` with a custom distribution, users may\n",
      "     |  subclass  `scipy.stats.rv_continuous` and initialize a frozen instance or\n",
      "     |  create an object with equivalent ``pdf``, ``cdf``, and ``ppf`` methods.\n",
      "     |  For instance, the following object represents the standard normal\n",
      "     |  distribution. For simplicity, we use `scipy.special.ndtr` and\n",
      "     |  `scipy.special.ndtri` to compute the ``cdf`` and ``ppf``, respectively.\n",
      "     |  \n",
      "     |  >>> from scipy.special import ndtr, ndtri\n",
      "     |  >>>\n",
      "     |  >>> class MyNormal:\n",
      "     |  ...\n",
      "     |  ...     def pdf(self, x):\n",
      "     |  ...        return 1/np.sqrt(2*np.pi) * np.exp(-x**2 / 2)\n",
      "     |  ...\n",
      "     |  ...     def cdf(self, x):\n",
      "     |  ...        return ndtr(x)\n",
      "     |  ...\n",
      "     |  ...     def ppf(self, x):\n",
      "     |  ...        return ndtri(x)\n",
      "     |  ...\n",
      "     |  >>> dist1 = MyNormal()\n",
      "     |  >>> fni1 = NumericalInverseHermite(dist1)\n",
      "     |  >>>\n",
      "     |  >>> dist2 = stats.norm()\n",
      "     |  >>> fni2 = NumericalInverseHermite(dist2)\n",
      "     |  >>>\n",
      "     |  >>> print(fni1.rvs(random_state=seed), fni2.rvs(random_state=seed))\n",
      "     |  -1.9603810921759424 -1.9603810921747074\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dist, *, tol=1e-12, max_intervals=100000)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ppf(self, q)\n",
      "     |      Approximate percent point function (inverse `cdf`) of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          lower tail probability.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : array_like\n",
      "     |          quantile corresponding to the lower tail probability `q`.\n",
      "     |  \n",
      "     |  qrvs(self, size=None, d=None, qmc_engine=None)\n",
      "     |      Quasi-random variates of the given RV.\n",
      "     |      \n",
      "     |      The `qmc_engine` is used to draw uniform quasi-random variates, and\n",
      "     |      these are converted to quasi-random variates of the given RV using\n",
      "     |      inverse transform sampling.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      size : int, tuple of ints, or None; optional\n",
      "     |          Defines shape of random variates array. Default is ``None``.\n",
      "     |      d : int or None, optional\n",
      "     |          Defines dimension of uniform quasi-random variates to be\n",
      "     |          transformed. Default is ``None``.\n",
      "     |      qmc_engine : scipy.stats.qmc.QMCEngine(d=1), optional\n",
      "     |          Defines the object to use for drawing\n",
      "     |          quasi-random variates. Default is ``None``, which uses\n",
      "     |          `scipy.stats.qmc.Halton(1)`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Quasi-random variates. See Notes for shape information.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The shape of the output array depends on `size`, `d`, and `qmc_engine`.\n",
      "     |      The intent is for the interface to be natural, but the detailed rules\n",
      "     |      to achieve this are complicated.\n",
      "     |      \n",
      "     |      - If `qmc_engine` is ``None``, a `scipy.stats.qmc.Halton` instance is\n",
      "     |        created with dimension `d`. If `d` is not provided, ``d=1``.\n",
      "     |      - If `qmc_engine` is not ``None`` and `d` is ``None``, `d` is\n",
      "     |        determined from the dimension of the `qmc_engine`.\n",
      "     |      - If `qmc_engine` is not ``None`` and `d` is not ``None`` but the\n",
      "     |        dimensions are inconsistent, a ``ValueError`` is raised.\n",
      "     |      - After `d` is determined according to the rules above, the output\n",
      "     |        shape is ``tuple_shape + d_shape``, where:\n",
      "     |      \n",
      "     |            - ``tuple_shape = tuple()`` if `size` is ``None``,\n",
      "     |            - ``tuple_shape = (size,)`` if `size` is an ``int``,\n",
      "     |            - ``tuple_shape = size`` if `size` is a sequence,\n",
      "     |            - ``d_shape = tuple()`` if `d` is ``None`` or `d` is 1, and\n",
      "     |            - ``d_shape = (d,)`` if `d` is greater than 1.\n",
      "     |      \n",
      "     |      The elements of the returned array are part of a low-discrepancy\n",
      "     |      sequence. If `d` is 1, this means that none of the samples are truly\n",
      "     |      independent. If `d` > 1, each slice ``rvs[..., i]`` will be of a\n",
      "     |      quasi-independent sequence; see `scipy.stats.qmc.QMCEngine` for\n",
      "     |      details. Note that when `d` > 1, the samples returned are still those\n",
      "     |      of the provided univariate distribution, not a multivariate\n",
      "     |      generalization of that distribution.\n",
      "     |  \n",
      "     |  rvs(self, size=None, random_state=None)\n",
      "     |      Random variates of the given RV.\n",
      "     |      \n",
      "     |      The `random_state` is used to draw uniform pseudo-random variates, and\n",
      "     |      these are converted to pseudo-random variates of the given RV using\n",
      "     |      inverse transform sampling.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      size : int, tuple of ints, or None; optional\n",
      "     |          Defines shape of array of random variates. Default is ``None``.\n",
      "     |      random_state : {None, int, `numpy.random.Generator`,\n",
      "     |                      `numpy.random.RandomState`}, optional\n",
      "     |      \n",
      "     |          Defines the object to use for drawing pseudorandom variates.\n",
      "     |          If `random_state` is ``None`` the `np.random.RandomState`\n",
      "     |          singleton is used.\n",
      "     |          If `random_state` is an ``int``, a new ``RandomState`` instance is\n",
      "     |          used, seeded with `random_state`.\n",
      "     |          If `random_state` is already a ``RandomState`` or ``Generator``\n",
      "     |          instance, then that object is used.\n",
      "     |          Default is None.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Random variates of given `size`. If `size` is ``None``, a scalar\n",
      "     |          is returned.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PearsonRConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  PearsonRConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `pearsonr` when an input is constant.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PearsonRConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class PearsonRNearConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  PearsonRNearConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `pearsonr` when an input is nearly constant.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PearsonRNearConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class SpearmanRConstantInputWarning(builtins.RuntimeWarning)\n",
      "     |  SpearmanRConstantInputWarning(msg=None)\n",
      "     |  \n",
      "     |  Warning generated by `spearmanr` when an input is constant.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpearmanRConstantInputWarning\n",
      "     |      builtins.RuntimeWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, msg=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.RuntimeWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class gaussian_kde(builtins.object)\n",
      "     |  gaussian_kde(dataset, bw_method=None, weights=None)\n",
      "     |  \n",
      "     |  Representation of a kernel-density estimate using Gaussian kernels.\n",
      "     |  \n",
      "     |  Kernel density estimation is a way to estimate the probability density\n",
      "     |  function (PDF) of a random variable in a non-parametric way.\n",
      "     |  `gaussian_kde` works for both uni-variate and multi-variate data.   It\n",
      "     |  includes automatic bandwidth determination.  The estimation works best for\n",
      "     |  a unimodal distribution; bimodal or multi-modal distributions tend to be\n",
      "     |  oversmoothed.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  dataset : array_like\n",
      "     |      Datapoints to estimate from. In case of univariate data this is a 1-D\n",
      "     |      array, otherwise a 2-D array with shape (# of dims, # of data).\n",
      "     |  bw_method : str, scalar or callable, optional\n",
      "     |      The method used to calculate the estimator bandwidth.  This can be\n",
      "     |      'scott', 'silverman', a scalar constant or a callable.  If a scalar,\n",
      "     |      this will be used directly as `kde.factor`.  If a callable, it should\n",
      "     |      take a `gaussian_kde` instance as only parameter and return a scalar.\n",
      "     |      If None (default), 'scott' is used.  See Notes for more details.\n",
      "     |  weights : array_like, optional\n",
      "     |      weights of datapoints. This must be the same shape as dataset.\n",
      "     |      If None (default), the samples are assumed to be equally weighted\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  dataset : ndarray\n",
      "     |      The dataset with which `gaussian_kde` was initialized.\n",
      "     |  d : int\n",
      "     |      Number of dimensions.\n",
      "     |  n : int\n",
      "     |      Number of datapoints.\n",
      "     |  neff : int\n",
      "     |      Effective number of datapoints.\n",
      "     |  \n",
      "     |      .. versionadded:: 1.2.0\n",
      "     |  factor : float\n",
      "     |      The bandwidth factor, obtained from `kde.covariance_factor`, with which\n",
      "     |      the covariance matrix is multiplied.\n",
      "     |  covariance : ndarray\n",
      "     |      The covariance matrix of `dataset`, scaled by the calculated bandwidth\n",
      "     |      (`kde.factor`).\n",
      "     |  inv_cov : ndarray\n",
      "     |      The inverse of `covariance`.\n",
      "     |  \n",
      "     |  Methods\n",
      "     |  -------\n",
      "     |  evaluate\n",
      "     |  __call__\n",
      "     |  integrate_gaussian\n",
      "     |  integrate_box_1d\n",
      "     |  integrate_box\n",
      "     |  integrate_kde\n",
      "     |  pdf\n",
      "     |  logpdf\n",
      "     |  resample\n",
      "     |  set_bandwidth\n",
      "     |  covariance_factor\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Bandwidth selection strongly influences the estimate obtained from the KDE\n",
      "     |  (much more so than the actual shape of the kernel).  Bandwidth selection\n",
      "     |  can be done by a \"rule of thumb\", by cross-validation, by \"plug-in\n",
      "     |  methods\" or by other means; see [3]_, [4]_ for reviews.  `gaussian_kde`\n",
      "     |  uses a rule of thumb, the default is Scott's Rule.\n",
      "     |  \n",
      "     |  Scott's Rule [1]_, implemented as `scotts_factor`, is::\n",
      "     |  \n",
      "     |      n**(-1./(d+4)),\n",
      "     |  \n",
      "     |  with ``n`` the number of data points and ``d`` the number of dimensions.\n",
      "     |  In the case of unequally weighted points, `scotts_factor` becomes::\n",
      "     |  \n",
      "     |      neff**(-1./(d+4)),\n",
      "     |  \n",
      "     |  with ``neff`` the effective number of datapoints.\n",
      "     |  Silverman's Rule [2]_, implemented as `silverman_factor`, is::\n",
      "     |  \n",
      "     |      (n * (d + 2) / 4.)**(-1. / (d + 4)).\n",
      "     |  \n",
      "     |  or in the case of unequally weighted points::\n",
      "     |  \n",
      "     |      (neff * (d + 2) / 4.)**(-1. / (d + 4)).\n",
      "     |  \n",
      "     |  Good general descriptions of kernel density estimation can be found in [1]_\n",
      "     |  and [2]_, the mathematics for this multi-dimensional implementation can be\n",
      "     |  found in [1]_.\n",
      "     |  \n",
      "     |  With a set of weighted samples, the effective number of datapoints ``neff``\n",
      "     |  is defined by::\n",
      "     |  \n",
      "     |      neff = sum(weights)^2 / sum(weights^2)\n",
      "     |  \n",
      "     |  as detailed in [5]_.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] D.W. Scott, \"Multivariate Density Estimation: Theory, Practice, and\n",
      "     |         Visualization\", John Wiley & Sons, New York, Chicester, 1992.\n",
      "     |  .. [2] B.W. Silverman, \"Density Estimation for Statistics and Data\n",
      "     |         Analysis\", Vol. 26, Monographs on Statistics and Applied Probability,\n",
      "     |         Chapman and Hall, London, 1986.\n",
      "     |  .. [3] B.A. Turlach, \"Bandwidth Selection in Kernel Density Estimation: A\n",
      "     |         Review\", CORE and Institut de Statistique, Vol. 19, pp. 1-33, 1993.\n",
      "     |  .. [4] D.M. Bashtannyk and R.J. Hyndman, \"Bandwidth selection for kernel\n",
      "     |         conditional density estimation\", Computational Statistics & Data\n",
      "     |         Analysis, Vol. 36, pp. 279-298, 2001.\n",
      "     |  .. [5] Gray P. G., 1969, Journal of the Royal Statistical Society.\n",
      "     |         Series A (General), 132, 272\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Generate some random two-dimensional data:\n",
      "     |  \n",
      "     |  >>> from scipy import stats\n",
      "     |  >>> def measure(n):\n",
      "     |  ...     \"Measurement model, return two coupled measurements.\"\n",
      "     |  ...     m1 = np.random.normal(size=n)\n",
      "     |  ...     m2 = np.random.normal(scale=0.5, size=n)\n",
      "     |  ...     return m1+m2, m1-m2\n",
      "     |  \n",
      "     |  >>> m1, m2 = measure(2000)\n",
      "     |  >>> xmin = m1.min()\n",
      "     |  >>> xmax = m1.max()\n",
      "     |  >>> ymin = m2.min()\n",
      "     |  >>> ymax = m2.max()\n",
      "     |  \n",
      "     |  Perform a kernel density estimate on the data:\n",
      "     |  \n",
      "     |  >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
      "     |  >>> positions = np.vstack([X.ravel(), Y.ravel()])\n",
      "     |  >>> values = np.vstack([m1, m2])\n",
      "     |  >>> kernel = stats.gaussian_kde(values)\n",
      "     |  >>> Z = np.reshape(kernel(positions).T, X.shape)\n",
      "     |  \n",
      "     |  Plot the results:\n",
      "     |  \n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> fig, ax = plt.subplots()\n",
      "     |  >>> ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n",
      "     |  ...           extent=[xmin, xmax, ymin, ymax])\n",
      "     |  >>> ax.plot(m1, m2, 'k.', markersize=2)\n",
      "     |  >>> ax.set_xlim([xmin, xmax])\n",
      "     |  >>> ax.set_ylim([ymin, ymax])\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__ = evaluate(self, points)\n",
      "     |  \n",
      "     |  __init__(self, dataset, bw_method=None, weights=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  covariance_factor = scotts_factor(self)\n",
      "     |  \n",
      "     |  evaluate(self, points)\n",
      "     |      Evaluate the estimated pdf on a set of points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      points : (# of dimensions, # of points)-array\n",
      "     |          Alternatively, a (# of dimensions,) vector can be passed in and\n",
      "     |          treated as a single point.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      values : (# of points,)-array\n",
      "     |          The values at each point.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError : if the dimensionality of the input points is different than\n",
      "     |                   the dimensionality of the KDE.\n",
      "     |  \n",
      "     |  integrate_box(self, low_bounds, high_bounds, maxpts=None)\n",
      "     |      Computes the integral of a pdf over a rectangular interval.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      low_bounds : array_like\n",
      "     |          A 1-D array containing the lower bounds of integration.\n",
      "     |      high_bounds : array_like\n",
      "     |          A 1-D array containing the upper bounds of integration.\n",
      "     |      maxpts : int, optional\n",
      "     |          The maximum number of points to use for integration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : scalar\n",
      "     |          The result of the integral.\n",
      "     |  \n",
      "     |  integrate_box_1d(self, low, high)\n",
      "     |      Computes the integral of a 1D pdf between two bounds.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      low : scalar\n",
      "     |          Lower bound of integration.\n",
      "     |      high : scalar\n",
      "     |          Upper bound of integration.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : scalar\n",
      "     |          The result of the integral.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If the KDE is over more than one dimension.\n",
      "     |  \n",
      "     |  integrate_gaussian(self, mean, cov)\n",
      "     |      Multiply estimated density by a multivariate Gaussian and integrate\n",
      "     |      over the whole space.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      mean : aray_like\n",
      "     |          A 1-D array, specifying the mean of the Gaussian.\n",
      "     |      cov : array_like\n",
      "     |          A 2-D array, specifying the covariance matrix of the Gaussian.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : scalar\n",
      "     |          The value of the integral.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If the mean or covariance of the input Gaussian differs from\n",
      "     |          the KDE's dimensionality.\n",
      "     |  \n",
      "     |  integrate_kde(self, other)\n",
      "     |      Computes the integral of the product of this  kernel density estimate\n",
      "     |      with another.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      other : gaussian_kde instance\n",
      "     |          The other kde.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : scalar\n",
      "     |          The result of the integral.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      ValueError\n",
      "     |          If the KDEs have different dimensionality.\n",
      "     |  \n",
      "     |  logpdf(self, x)\n",
      "     |      Evaluate the log of the estimated pdf on a provided set of points.\n",
      "     |  \n",
      "     |  pdf(self, x)\n",
      "     |      Evaluate the estimated pdf on a provided set of points.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is an alias for `gaussian_kde.evaluate`.  See the ``evaluate``\n",
      "     |      docstring for more details.\n",
      "     |  \n",
      "     |  resample(self, size=None, seed=None)\n",
      "     |      Randomly sample a dataset from the estimated pdf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      size : int, optional\n",
      "     |          The number of samples to draw.  If not provided, then the size is\n",
      "     |          the same as the effective number of samples in the underlying\n",
      "     |          dataset.\n",
      "     |      seed : {None, int, `numpy.random.Generator`,\n",
      "     |              `numpy.random.RandomState`}, optional\n",
      "     |      \n",
      "     |          If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |          singleton is used.\n",
      "     |          If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |          seeded with `seed`.\n",
      "     |          If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "     |          that instance is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      resample : (self.d, `size`) ndarray\n",
      "     |          The sampled dataset.\n",
      "     |  \n",
      "     |  scotts_factor(self)\n",
      "     |      Computes the coefficient (`kde.factor`) that\n",
      "     |      multiplies the data covariance matrix to obtain the kernel covariance\n",
      "     |      matrix. The default is `scotts_factor`.  A subclass can overwrite this\n",
      "     |      method to provide a different method, or set it through a call to\n",
      "     |      `kde.set_bandwidth`.\n",
      "     |  \n",
      "     |  set_bandwidth(self, bw_method=None)\n",
      "     |      Compute the estimator bandwidth with given method.\n",
      "     |      \n",
      "     |      The new bandwidth calculated after a call to `set_bandwidth` is used\n",
      "     |      for subsequent evaluations of the estimated density.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      bw_method : str, scalar or callable, optional\n",
      "     |          The method used to calculate the estimator bandwidth.  This can be\n",
      "     |          'scott', 'silverman', a scalar constant or a callable.  If a\n",
      "     |          scalar, this will be used directly as `kde.factor`.  If a callable,\n",
      "     |          it should take a `gaussian_kde` instance as only parameter and\n",
      "     |          return a scalar.  If None (default), nothing happens; the current\n",
      "     |          `kde.covariance_factor` method is kept.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      .. versionadded:: 0.11\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      >>> import scipy.stats as stats\n",
      "     |      >>> x1 = np.array([-7, -5, 1, 4, 5.])\n",
      "     |      >>> kde = stats.gaussian_kde(x1)\n",
      "     |      >>> xs = np.linspace(-10, 10, num=50)\n",
      "     |      >>> y1 = kde(xs)\n",
      "     |      >>> kde.set_bandwidth(bw_method='silverman')\n",
      "     |      >>> y2 = kde(xs)\n",
      "     |      >>> kde.set_bandwidth(bw_method=kde.factor / 3.)\n",
      "     |      >>> y3 = kde(xs)\n",
      "     |      \n",
      "     |      >>> import matplotlib.pyplot as plt\n",
      "     |      >>> fig, ax = plt.subplots()\n",
      "     |      >>> ax.plot(x1, np.full(x1.shape, 1 / (4. * x1.size)), 'bo',\n",
      "     |      ...         label='Data points (rescaled)')\n",
      "     |      >>> ax.plot(xs, y1, label='Scott (default)')\n",
      "     |      >>> ax.plot(xs, y2, label='Silverman')\n",
      "     |      >>> ax.plot(xs, y3, label='Const (1/3 * Silverman)')\n",
      "     |      >>> ax.legend()\n",
      "     |      >>> plt.show()\n",
      "     |  \n",
      "     |  silverman_factor(self)\n",
      "     |      Compute the Silverman factor.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      s : float\n",
      "     |          The silverman factor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  neff\n",
      "     |  \n",
      "     |  weights\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class rv_continuous(rv_generic)\n",
      "     |  rv_continuous(momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |  \n",
      "     |  A generic continuous random variable class meant for subclassing.\n",
      "     |  \n",
      "     |  `rv_continuous` is a base class to construct specific distribution classes\n",
      "     |  and instances for continuous random variables. It cannot be used\n",
      "     |  directly as a distribution.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  momtype : int, optional\n",
      "     |      The type of generic moment calculation to use: 0 for pdf, 1 (default)\n",
      "     |      for ppf.\n",
      "     |  a : float, optional\n",
      "     |      Lower bound of the support of the distribution, default is minus\n",
      "     |      infinity.\n",
      "     |  b : float, optional\n",
      "     |      Upper bound of the support of the distribution, default is plus\n",
      "     |      infinity.\n",
      "     |  xtol : float, optional\n",
      "     |      The tolerance for fixed point calculation for generic ppf.\n",
      "     |  badvalue : float, optional\n",
      "     |      The value in a result arrays that indicates a value that for which\n",
      "     |      some argument restriction is violated, default is np.nan.\n",
      "     |  name : str, optional\n",
      "     |      The name of the instance. This string is used to construct the default\n",
      "     |      example for distributions.\n",
      "     |  longname : str, optional\n",
      "     |      This string is used as part of the first line of the docstring returned\n",
      "     |      when a subclass has no docstring of its own. Note: `longname` exists\n",
      "     |      for backwards compatibility, do not use for new subclasses.\n",
      "     |  shapes : str, optional\n",
      "     |      The shape of the distribution. For example ``\"m, n\"`` for a\n",
      "     |      distribution that takes two integers as the two shape arguments for all\n",
      "     |      its methods. If not provided, shape parameters will be inferred from\n",
      "     |      the signature of the private methods, ``_pdf`` and ``_cdf`` of the\n",
      "     |      instance.\n",
      "     |  extradoc :  str, optional, deprecated\n",
      "     |      This string is used as the last part of the docstring returned when a\n",
      "     |      subclass has no docstring of its own. Note: `extradoc` exists for\n",
      "     |      backwards compatibility, do not use for new subclasses.\n",
      "     |  seed : {None, int, `numpy.random.Generator`,\n",
      "     |          `numpy.random.RandomState`}, optional\n",
      "     |  \n",
      "     |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |      singleton is used.\n",
      "     |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |      seeded with `seed`.\n",
      "     |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "     |      that instance is used.\n",
      "     |  \n",
      "     |  Methods\n",
      "     |  -------\n",
      "     |  rvs\n",
      "     |  pdf\n",
      "     |  logpdf\n",
      "     |  cdf\n",
      "     |  logcdf\n",
      "     |  sf\n",
      "     |  logsf\n",
      "     |  ppf\n",
      "     |  isf\n",
      "     |  moment\n",
      "     |  stats\n",
      "     |  entropy\n",
      "     |  expect\n",
      "     |  median\n",
      "     |  mean\n",
      "     |  std\n",
      "     |  var\n",
      "     |  interval\n",
      "     |  __call__\n",
      "     |  fit\n",
      "     |  fit_loc_scale\n",
      "     |  nnlf\n",
      "     |  support\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Public methods of an instance of a distribution class (e.g., ``pdf``,\n",
      "     |  ``cdf``) check their arguments and pass valid arguments to private,\n",
      "     |  computational methods (``_pdf``, ``_cdf``). For ``pdf(x)``, ``x`` is valid\n",
      "     |  if it is within the support of the distribution.\n",
      "     |  Whether a shape parameter is valid is decided by an ``_argcheck`` method\n",
      "     |  (which defaults to checking that its arguments are strictly positive.)\n",
      "     |  \n",
      "     |  **Subclassing**\n",
      "     |  \n",
      "     |  New random variables can be defined by subclassing the `rv_continuous` class\n",
      "     |  and re-defining at least the ``_pdf`` or the ``_cdf`` method (normalized\n",
      "     |  to location 0 and scale 1).\n",
      "     |  \n",
      "     |  If positive argument checking is not correct for your RV\n",
      "     |  then you will also need to re-define the ``_argcheck`` method.\n",
      "     |  \n",
      "     |  For most of the scipy.stats distributions, the support interval doesn't\n",
      "     |  depend on the shape parameters. ``x`` being in the support interval is\n",
      "     |  equivalent to ``self.a <= x <= self.b``.  If either of the endpoints of\n",
      "     |  the support do depend on the shape parameters, then\n",
      "     |  i) the distribution must implement the ``_get_support`` method; and\n",
      "     |  ii) those dependent endpoints must be omitted from the distribution's\n",
      "     |  call to the ``rv_continuous`` initializer.\n",
      "     |  \n",
      "     |  Correct, but potentially slow defaults exist for the remaining\n",
      "     |  methods but for speed and/or accuracy you can over-ride::\n",
      "     |  \n",
      "     |    _logpdf, _cdf, _logcdf, _ppf, _rvs, _isf, _sf, _logsf\n",
      "     |  \n",
      "     |  The default method ``_rvs`` relies on the inverse of the cdf, ``_ppf``,\n",
      "     |  applied to a uniform random variate. In order to generate random variates\n",
      "     |  efficiently, either the default ``_ppf`` needs to be overwritten (e.g.\n",
      "     |  if the inverse cdf can expressed in an explicit form) or a sampling\n",
      "     |  method needs to be implemented in a custom ``_rvs`` method.\n",
      "     |  \n",
      "     |  If possible, you should override ``_isf``, ``_sf`` or ``_logsf``.\n",
      "     |  The main reason would be to improve numerical accuracy: for example,\n",
      "     |  the survival function ``_sf`` is computed as ``1 - _cdf`` which can\n",
      "     |  result in loss of precision if ``_cdf(x)`` is close to one.\n",
      "     |  \n",
      "     |  **Methods that can be overwritten by subclasses**\n",
      "     |  ::\n",
      "     |  \n",
      "     |    _rvs\n",
      "     |    _pdf\n",
      "     |    _cdf\n",
      "     |    _sf\n",
      "     |    _ppf\n",
      "     |    _isf\n",
      "     |    _stats\n",
      "     |    _munp\n",
      "     |    _entropy\n",
      "     |    _argcheck\n",
      "     |    _get_support\n",
      "     |  \n",
      "     |  There are additional (internal and private) generic methods that can\n",
      "     |  be useful for cross-checking and for debugging, but might work in all\n",
      "     |  cases when directly called.\n",
      "     |  \n",
      "     |  A note on ``shapes``: subclasses need not specify them explicitly. In this\n",
      "     |  case, `shapes` will be automatically deduced from the signatures of the\n",
      "     |  overridden methods (`pdf`, `cdf` etc).\n",
      "     |  If, for some reason, you prefer to avoid relying on introspection, you can\n",
      "     |  specify ``shapes`` explicitly as an argument to the instance constructor.\n",
      "     |  \n",
      "     |  \n",
      "     |  **Frozen Distributions**\n",
      "     |  \n",
      "     |  Normally, you must provide shape parameters (and, optionally, location and\n",
      "     |  scale parameters to each call of a method of a distribution.\n",
      "     |  \n",
      "     |  Alternatively, the object may be called (as a function) to fix the shape,\n",
      "     |  location, and scale parameters returning a \"frozen\" continuous RV object:\n",
      "     |  \n",
      "     |  rv = generic(<shape(s)>, loc=0, scale=1)\n",
      "     |      `rv_frozen` object with the same methods but holding the given shape,\n",
      "     |      location, and scale fixed\n",
      "     |  \n",
      "     |  **Statistics**\n",
      "     |  \n",
      "     |  Statistics are computed using numerical integration by default.\n",
      "     |  For speed you can redefine this using ``_stats``:\n",
      "     |  \n",
      "     |   - take shape parameters and return mu, mu2, g1, g2\n",
      "     |   - If you can't compute one of these, return it as None\n",
      "     |   - Can also be defined with a keyword argument ``moments``, which is a\n",
      "     |     string composed of \"m\", \"v\", \"s\", and/or \"k\".\n",
      "     |     Only the components appearing in string should be computed and\n",
      "     |     returned in the order \"m\", \"v\", \"s\", or \"k\"  with missing values\n",
      "     |     returned as None.\n",
      "     |  \n",
      "     |  Alternatively, you can override ``_munp``, which takes ``n`` and shape\n",
      "     |  parameters and returns the n-th non-central moment of the distribution.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  To create a new Gaussian distribution, we would do the following:\n",
      "     |  \n",
      "     |  >>> from scipy.stats import rv_continuous\n",
      "     |  >>> class gaussian_gen(rv_continuous):\n",
      "     |  ...     \"Gaussian distribution\"\n",
      "     |  ...     def _pdf(self, x):\n",
      "     |  ...         return np.exp(-x**2 / 2.) / np.sqrt(2.0 * np.pi)\n",
      "     |  >>> gaussian = gaussian_gen(name='gaussian')\n",
      "     |  \n",
      "     |  ``scipy.stats`` distributions are *instances*, so here we subclass\n",
      "     |  `rv_continuous` and create an instance. With this, we now have\n",
      "     |  a fully functional distribution with all relevant methods automagically\n",
      "     |  generated by the framework.\n",
      "     |  \n",
      "     |  Note that above we defined a standard normal distribution, with zero mean\n",
      "     |  and unit variance. Shifting and scaling of the distribution can be done\n",
      "     |  by using ``loc`` and ``scale`` parameters: ``gaussian.pdf(x, loc, scale)``\n",
      "     |  essentially computes ``y = (x - loc) / scale`` and\n",
      "     |  ``gaussian._pdf(y) / scale``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      rv_continuous\n",
      "     |      rv_generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, x, *args, **kwds)\n",
      "     |      Cumulative distribution function of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      cdf : ndarray\n",
      "     |          Cumulative distribution function evaluated at `x`\n",
      "     |  \n",
      "     |  expect(self, func=None, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "     |      Calculate expected value of a function with respect to the\n",
      "     |      distribution by numerical integration.\n",
      "     |      \n",
      "     |      The expected value of a function ``f(x)`` with respect to a\n",
      "     |      distribution ``dist`` is defined as::\n",
      "     |      \n",
      "     |                  ub\n",
      "     |          E[f(x)] = Integral(f(x) * dist.pdf(x)),\n",
      "     |                  lb\n",
      "     |      \n",
      "     |      where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``\n",
      "     |      distribution. If the bounds ``lb`` and ``ub`` correspond to the\n",
      "     |      support of the distribution, e.g. ``[-inf, inf]`` in the default\n",
      "     |      case, then the integral is the unrestricted expectation of ``f(x)``.\n",
      "     |      Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``\n",
      "     |      outside a finite interval in which case the expectation is\n",
      "     |      calculated within the finite range ``[lb, ub]``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : callable, optional\n",
      "     |          Function for which integral is calculated. Takes only one argument.\n",
      "     |          The default is the identity mapping f(x) = x.\n",
      "     |      args : tuple, optional\n",
      "     |          Shape parameters of the distribution.\n",
      "     |      loc : float, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : float, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      lb, ub : scalar, optional\n",
      "     |          Lower and upper bound for integration. Default is set to the\n",
      "     |          support of the distribution.\n",
      "     |      conditional : bool, optional\n",
      "     |          If True, the integral is corrected by the conditional probability\n",
      "     |          of the integration interval.  The return value is the expectation\n",
      "     |          of the function, conditional on being in the given interval.\n",
      "     |          Default is False.\n",
      "     |      \n",
      "     |      Additional keyword arguments are passed to the integration routine.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      expect : float\n",
      "     |          The calculated expected value.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The integration behavior of this function is inherited from\n",
      "     |      `scipy.integrate.quad`. Neither this function nor\n",
      "     |      `scipy.integrate.quad` can verify whether the integral exists or is\n",
      "     |      finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and\n",
      "     |      ``cauchy(0).expect()`` returns ``0.0``.\n",
      "     |      \n",
      "     |      The function is not vectorized.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      To understand the effect of the bounds of integration consider\n",
      "     |      \n",
      "     |      >>> from scipy.stats import expon\n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)\n",
      "     |      0.6321205588285578\n",
      "     |      \n",
      "     |      This is close to\n",
      "     |      \n",
      "     |      >>> expon(1).cdf(2.0) - expon(1).cdf(0.0)\n",
      "     |      0.6321205588285577\n",
      "     |      \n",
      "     |      If ``conditional=True``\n",
      "     |      \n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)\n",
      "     |      1.0000000000000002\n",
      "     |      \n",
      "     |      The slight deviation from 1 is due to numerical integration.\n",
      "     |  \n",
      "     |  fit(self, data, *args, **kwds)\n",
      "     |      Return estimates of shape (if applicable), location, and scale\n",
      "     |      parameters from data. The default estimation method is Maximum\n",
      "     |      Likelihood Estimation (MLE), but Method of Moments (MM)\n",
      "     |      is also available.\n",
      "     |      \n",
      "     |      Starting estimates for\n",
      "     |      the fit are given by input arguments; for any arguments not provided\n",
      "     |      with starting estimates, ``self._fitstart(data)`` is called to generate\n",
      "     |      such.\n",
      "     |      \n",
      "     |      One can hold some parameters fixed to specific values by passing in\n",
      "     |      keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\n",
      "     |      and ``floc`` and ``fscale`` (for location and scale parameters,\n",
      "     |      respectively).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to use in estimating the distribution parameters.\n",
      "     |      arg1, arg2, arg3,... : floats, optional\n",
      "     |          Starting value(s) for any shape-characterizing arguments (those not\n",
      "     |          provided will be determined by a call to ``_fitstart(data)``).\n",
      "     |          No default value.\n",
      "     |      kwds : floats, optional\n",
      "     |          - `loc`: initial guess of the distribution's location parameter.\n",
      "     |          - `scale`: initial guess of the distribution's scale parameter.\n",
      "     |      \n",
      "     |          Special keyword arguments are recognized as holding certain\n",
      "     |          parameters fixed:\n",
      "     |      \n",
      "     |          - f0...fn : hold respective shape parameters fixed.\n",
      "     |            Alternatively, shape parameters to fix can be specified by name.\n",
      "     |            For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n",
      "     |            are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n",
      "     |            equivalent to ``f1``.\n",
      "     |      \n",
      "     |          - floc : hold location parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - fscale : hold scale parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - optimizer : The optimizer to use.\n",
      "     |            The optimizer must take ``func``,\n",
      "     |            and starting position as the first two arguments,\n",
      "     |            plus ``args`` (for extra arguments to pass to the\n",
      "     |            function to be optimized) and ``disp=0`` to suppress\n",
      "     |            output as keyword arguments.\n",
      "     |      \n",
      "     |          - method : The method to use. The default is \"MLE\" (Maximum\n",
      "     |            Likelihood Estimate); \"MM\" (Method of Moments)\n",
      "     |            is also available.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      parameter_tuple : tuple of floats\n",
      "     |          Estimates for any shape parameters (if applicable),\n",
      "     |          followed by those for location and scale.\n",
      "     |          For most random variables, shape statistics\n",
      "     |          will be returned, but there are exceptions (e.g. ``norm``).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      With ``method=\"MLE\"`` (default), the fit is computed by minimizing\n",
      "     |      the negative log-likelihood function. A large, finite penalty\n",
      "     |      (rather than infinite negative log-likelihood) is applied for\n",
      "     |      observations beyond the support of the distribution.\n",
      "     |      \n",
      "     |      With ``method=\"MM\"``, the fit is computed by minimizing the L2 norm\n",
      "     |      of the relative errors between the first *k* raw (about zero) data\n",
      "     |      moments and the corresponding distribution moments, where *k* is the\n",
      "     |      number of non-fixed parameters.\n",
      "     |      More precisely, the objective function is::\n",
      "     |      \n",
      "     |          (((data_moments - dist_moments)\n",
      "     |            / np.maximum(np.abs(data_moments), 1e-8))**2).sum()\n",
      "     |      \n",
      "     |      where the constant ``1e-8`` avoids division by zero in case of\n",
      "     |      vanishing data moments. Typically, this error norm can be reduced to\n",
      "     |      zero.\n",
      "     |      Note that the standard method of moments can produce parameters for\n",
      "     |      which some data are outside the support of the fitted distribution;\n",
      "     |      this implementation does nothing to prevent this.\n",
      "     |      \n",
      "     |      For either method,\n",
      "     |      the returned answer is not guaranteed to be globally optimal; it\n",
      "     |      may only be locally optimal, or the optimization may fail altogether.\n",
      "     |      If the data contain any of ``np.nan``, ``np.inf``, or ``-np.inf``,\n",
      "     |      the `fit` method will raise a ``RuntimeError``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Generate some data to fit: draw random variates from the `beta`\n",
      "     |      distribution\n",
      "     |      \n",
      "     |      >>> from scipy.stats import beta\n",
      "     |      >>> a, b = 1., 2.\n",
      "     |      >>> x = beta.rvs(a, b, size=1000)\n",
      "     |      \n",
      "     |      Now we can fit all four parameters (``a``, ``b``, ``loc``\n",
      "     |      and ``scale``):\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x)\n",
      "     |      \n",
      "     |      We can also use some prior knowledge about the dataset: let's keep\n",
      "     |      ``loc`` and ``scale`` fixed:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0, 1)\n",
      "     |      \n",
      "     |      We can also keep shape parameters fixed by using ``f``-keywords. To\n",
      "     |      keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\n",
      "     |      equivalently, ``fa=1``:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n",
      "     |      >>> a1\n",
      "     |      1\n",
      "     |      \n",
      "     |      Not all distributions return estimates for the shape parameters.\n",
      "     |      ``norm`` for example just returns estimates for location and scale:\n",
      "     |      \n",
      "     |      >>> from scipy.stats import norm\n",
      "     |      >>> x = norm.rvs(a, b, size=1000, random_state=123)\n",
      "     |      >>> loc1, scale1 = norm.fit(x)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0.92087172783841631, 2.0015750750324668)\n",
      "     |  \n",
      "     |  fit_loc_scale(self, data, *args)\n",
      "     |      Estimate loc and scale parameters from data using 1st and 2nd moments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to fit.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Lhat : float\n",
      "     |          Estimated location parameter for the data.\n",
      "     |      Shat : float\n",
      "     |          Estimated scale parameter for the data.\n",
      "     |  \n",
      "     |  isf(self, q, *args, **kwds)\n",
      "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          upper tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : ndarray or scalar\n",
      "     |          Quantile corresponding to the upper tail probability q.\n",
      "     |  \n",
      "     |  logcdf(self, x, *args, **kwds)\n",
      "     |      Log of the cumulative distribution function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logcdf : array_like\n",
      "     |          Log of the cumulative distribution function evaluated at x\n",
      "     |  \n",
      "     |  logpdf(self, x, *args, **kwds)\n",
      "     |      Log of the probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      This uses a more numerically accurate calculation if available.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logpdf : array_like\n",
      "     |          Log of the probability density function evaluated at x\n",
      "     |  \n",
      "     |  logsf(self, x, *args, **kwds)\n",
      "     |      Log of the survival function of the given RV.\n",
      "     |      \n",
      "     |      Returns the log of the \"survival function,\" defined as (1 - `cdf`),\n",
      "     |      evaluated at `x`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logsf : ndarray\n",
      "     |          Log of the survival function evaluated at `x`.\n",
      "     |  \n",
      "     |  nnlf(self, theta, x)\n",
      "     |      Negative loglikelihood function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
      "     |      parameters (including loc and scale).\n",
      "     |  \n",
      "     |  pdf(self, x, *args, **kwds)\n",
      "     |      Probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pdf : ndarray\n",
      "     |          Probability density function evaluated at x\n",
      "     |  \n",
      "     |  ppf(self, q, *args, **kwds)\n",
      "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          lower tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : array_like\n",
      "     |          quantile corresponding to the lower tail probability q.\n",
      "     |  \n",
      "     |  sf(self, x, *args, **kwds)\n",
      "     |      Survival function (1 - `cdf`) at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sf : array_like\n",
      "     |          Survival function evaluated at x\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from rv_generic:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  entropy(self, *args, **kwds)\n",
      "     |      Differential entropy of the RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional  (continuous distributions only).\n",
      "     |          Scale parameter (default=1).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Entropy is defined base `e`:\n",
      "     |      \n",
      "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
      "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
      "     |      True\n",
      "     |  \n",
      "     |  freeze(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  interval(self, alpha, *args, **kwds)\n",
      "     |      Confidence interval with equal areas around the median.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alpha : array_like of float\n",
      "     |          Probability that an rv will be drawn from the returned range.\n",
      "     |          Each value should be in the range [0, 1].\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : ndarray of float\n",
      "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
      "     |          possible values.\n",
      "     |  \n",
      "     |  mean(self, *args, **kwds)\n",
      "     |      Mean of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mean : float\n",
      "     |          the mean of the distribution\n",
      "     |  \n",
      "     |  median(self, *args, **kwds)\n",
      "     |      Median of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      median : float\n",
      "     |          The median of the distribution.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      rv_discrete.ppf\n",
      "     |          Inverse of the CDF\n",
      "     |  \n",
      "     |  moment(self, n, *args, **kwds)\n",
      "     |      n-th order non-central moment of distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, n >= 1\n",
      "     |          Order of moment.\n",
      "     |      arg1, arg2, arg3,... : float\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |  \n",
      "     |  rvs(self, *args, **kwds)\n",
      "     |      Random variates of given type.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      size : int or tuple of ints, optional\n",
      "     |          Defining number of random variates (default is 1).\n",
      "     |      random_state : {None, int, `numpy.random.Generator`,\n",
      "     |                      `numpy.random.RandomState`}, optional\n",
      "     |      \n",
      "     |          If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |          singleton is used.\n",
      "     |          If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |          seeded with `seed`.\n",
      "     |          If `seed` is already a ``Generator`` or ``RandomState`` instance\n",
      "     |          then that instance is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Random variates of given `size`.\n",
      "     |  \n",
      "     |  stats(self, *args, **kwds)\n",
      "     |      Some statistics of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional (continuous RVs only)\n",
      "     |          scale parameter (default=1)\n",
      "     |      moments : str, optional\n",
      "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
      "     |          'm' = mean,\n",
      "     |          'v' = variance,\n",
      "     |          's' = (Fisher's) skew,\n",
      "     |          'k' = (Fisher's) kurtosis.\n",
      "     |          (default is 'mv')\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stats : sequence\n",
      "     |          of requested moments.\n",
      "     |  \n",
      "     |  std(self, *args, **kwds)\n",
      "     |      Standard deviation of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      std : float\n",
      "     |          standard deviation of the distribution\n",
      "     |  \n",
      "     |  support(self, *args, **kwargs)\n",
      "     |      Support of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : array_like\n",
      "     |          end-points of the distribution's support.\n",
      "     |  \n",
      "     |  var(self, *args, **kwds)\n",
      "     |      Variance of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      var : float\n",
      "     |          the variance of the distribution\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from rv_generic:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  random_state\n",
      "     |      Get or set the generator object for generating random variates.\n",
      "     |      \n",
      "     |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |      singleton is used.\n",
      "     |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |      seeded with `seed`.\n",
      "     |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "     |      that instance is used.\n",
      "    \n",
      "    class rv_discrete(rv_generic)\n",
      "     |  rv_discrete(a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |  \n",
      "     |  A generic discrete random variable class meant for subclassing.\n",
      "     |  \n",
      "     |  `rv_discrete` is a base class to construct specific distribution classes\n",
      "     |  and instances for discrete random variables. It can also be used\n",
      "     |  to construct an arbitrary distribution defined by a list of support\n",
      "     |  points and corresponding probabilities.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  a : float, optional\n",
      "     |      Lower bound of the support of the distribution, default: 0\n",
      "     |  b : float, optional\n",
      "     |      Upper bound of the support of the distribution, default: plus infinity\n",
      "     |  moment_tol : float, optional\n",
      "     |      The tolerance for the generic calculation of moments.\n",
      "     |  values : tuple of two array_like, optional\n",
      "     |      ``(xk, pk)`` where ``xk`` are integers and ``pk`` are the non-zero\n",
      "     |      probabilities between 0 and 1 with ``sum(pk) = 1``. ``xk``\n",
      "     |      and ``pk`` must have the same shape.\n",
      "     |  inc : integer, optional\n",
      "     |      Increment for the support of the distribution.\n",
      "     |      Default is 1. (other values have not been tested)\n",
      "     |  badvalue : float, optional\n",
      "     |      The value in a result arrays that indicates a value that for which\n",
      "     |      some argument restriction is violated, default is np.nan.\n",
      "     |  name : str, optional\n",
      "     |      The name of the instance. This string is used to construct the default\n",
      "     |      example for distributions.\n",
      "     |  longname : str, optional\n",
      "     |      This string is used as part of the first line of the docstring returned\n",
      "     |      when a subclass has no docstring of its own. Note: `longname` exists\n",
      "     |      for backwards compatibility, do not use for new subclasses.\n",
      "     |  shapes : str, optional\n",
      "     |      The shape of the distribution. For example \"m, n\" for a distribution\n",
      "     |      that takes two integers as the two shape arguments for all its methods\n",
      "     |      If not provided, shape parameters will be inferred from\n",
      "     |      the signatures of the private methods, ``_pmf`` and ``_cdf`` of\n",
      "     |      the instance.\n",
      "     |  extradoc :  str, optional\n",
      "     |      This string is used as the last part of the docstring returned when a\n",
      "     |      subclass has no docstring of its own. Note: `extradoc` exists for\n",
      "     |      backwards compatibility, do not use for new subclasses.\n",
      "     |  seed : {None, int, `numpy.random.Generator`,\n",
      "     |          `numpy.random.RandomState`}, optional\n",
      "     |  \n",
      "     |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |      singleton is used.\n",
      "     |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |      seeded with `seed`.\n",
      "     |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "     |      that instance is used.\n",
      "     |  \n",
      "     |  Methods\n",
      "     |  -------\n",
      "     |  rvs\n",
      "     |  pmf\n",
      "     |  logpmf\n",
      "     |  cdf\n",
      "     |  logcdf\n",
      "     |  sf\n",
      "     |  logsf\n",
      "     |  ppf\n",
      "     |  isf\n",
      "     |  moment\n",
      "     |  stats\n",
      "     |  entropy\n",
      "     |  expect\n",
      "     |  median\n",
      "     |  mean\n",
      "     |  std\n",
      "     |  var\n",
      "     |  interval\n",
      "     |  __call__\n",
      "     |  support\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  This class is similar to `rv_continuous`. Whether a shape parameter is\n",
      "     |  valid is decided by an ``_argcheck`` method (which defaults to checking\n",
      "     |  that its arguments are strictly positive.)\n",
      "     |  The main differences are:\n",
      "     |  \n",
      "     |  - the support of the distribution is a set of integers\n",
      "     |  - instead of the probability density function, ``pdf`` (and the\n",
      "     |    corresponding private ``_pdf``), this class defines the\n",
      "     |    *probability mass function*, `pmf` (and the corresponding\n",
      "     |    private ``_pmf``.)\n",
      "     |  - scale parameter is not defined.\n",
      "     |  \n",
      "     |  To create a new discrete distribution, we would do the following:\n",
      "     |  \n",
      "     |  >>> from scipy.stats import rv_discrete\n",
      "     |  >>> class poisson_gen(rv_discrete):\n",
      "     |  ...     \"Poisson distribution\"\n",
      "     |  ...     def _pmf(self, k, mu):\n",
      "     |  ...         return exp(-mu) * mu**k / factorial(k)\n",
      "     |  \n",
      "     |  and create an instance::\n",
      "     |  \n",
      "     |  >>> poisson = poisson_gen(name=\"poisson\")\n",
      "     |  \n",
      "     |  Note that above we defined the Poisson distribution in the standard form.\n",
      "     |  Shifting the distribution can be done by providing the ``loc`` parameter\n",
      "     |  to the methods of the instance. For example, ``poisson.pmf(x, mu, loc)``\n",
      "     |  delegates the work to ``poisson._pmf(x-loc, mu)``.\n",
      "     |  \n",
      "     |  **Discrete distributions from a list of probabilities**\n",
      "     |  \n",
      "     |  Alternatively, you can construct an arbitrary discrete rv defined\n",
      "     |  on a finite set of values ``xk`` with ``Prob{X=xk} = pk`` by using the\n",
      "     |  ``values`` keyword argument to the `rv_discrete` constructor.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Custom made discrete distribution:\n",
      "     |  \n",
      "     |  >>> from scipy import stats\n",
      "     |  >>> xk = np.arange(7)\n",
      "     |  >>> pk = (0.1, 0.2, 0.3, 0.1, 0.1, 0.0, 0.2)\n",
      "     |  >>> custm = stats.rv_discrete(name='custm', values=(xk, pk))\n",
      "     |  >>>\n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> fig, ax = plt.subplots(1, 1)\n",
      "     |  >>> ax.plot(xk, custm.pmf(xk), 'ro', ms=12, mec='r')\n",
      "     |  >>> ax.vlines(xk, 0, custm.pmf(xk), colors='r', lw=4)\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Random number generation:\n",
      "     |  \n",
      "     |  >>> R = custm.rvs(size=100)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      rv_discrete\n",
      "     |      rv_generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, k, *args, **kwds)\n",
      "     |      Cumulative distribution function of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like, int\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      cdf : ndarray\n",
      "     |          Cumulative distribution function evaluated at `k`.\n",
      "     |  \n",
      "     |  expect(self, func=None, args=(), loc=0, lb=None, ub=None, conditional=False, maxcount=1000, tolerance=1e-10, chunksize=32)\n",
      "     |      Calculate expected value of a function with respect to the distribution\n",
      "     |      for discrete distribution by numerical summation.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : callable, optional\n",
      "     |          Function for which the expectation value is calculated.\n",
      "     |          Takes only one argument.\n",
      "     |          The default is the identity mapping f(k) = k.\n",
      "     |      args : tuple, optional\n",
      "     |          Shape parameters of the distribution.\n",
      "     |      loc : float, optional\n",
      "     |          Location parameter.\n",
      "     |          Default is 0.\n",
      "     |      lb, ub : int, optional\n",
      "     |          Lower and upper bound for the summation, default is set to the\n",
      "     |          support of the distribution, inclusive (``lb <= k <= ub``).\n",
      "     |      conditional : bool, optional\n",
      "     |          If true then the expectation is corrected by the conditional\n",
      "     |          probability of the summation interval. The return value is the\n",
      "     |          expectation of the function, `func`, conditional on being in\n",
      "     |          the given interval (k such that ``lb <= k <= ub``).\n",
      "     |          Default is False.\n",
      "     |      maxcount : int, optional\n",
      "     |          Maximal number of terms to evaluate (to avoid an endless loop for\n",
      "     |          an infinite sum). Default is 1000.\n",
      "     |      tolerance : float, optional\n",
      "     |          Absolute tolerance for the summation. Default is 1e-10.\n",
      "     |      chunksize : int, optional\n",
      "     |          Iterate over the support of a distributions in chunks of this size.\n",
      "     |          Default is 32.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      expect : float\n",
      "     |          Expected value.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      For heavy-tailed distributions, the expected value may or\n",
      "     |      may not exist,\n",
      "     |      depending on the function, `func`. If it does exist, but the\n",
      "     |      sum converges\n",
      "     |      slowly, the accuracy of the result may be rather low. For instance, for\n",
      "     |      ``zipf(4)``, accuracy for mean, variance in example is only 1e-5.\n",
      "     |      increasing `maxcount` and/or `chunksize` may improve the result,\n",
      "     |      but may also make zipf very slow.\n",
      "     |      \n",
      "     |      The function is not vectorized.\n",
      "     |  \n",
      "     |  isf(self, q, *args, **kwds)\n",
      "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          Upper tail probability.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      k : ndarray or scalar\n",
      "     |          Quantile corresponding to the upper tail probability, q.\n",
      "     |  \n",
      "     |  logcdf(self, k, *args, **kwds)\n",
      "     |      Log of the cumulative distribution function at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like, int\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logcdf : array_like\n",
      "     |          Log of the cumulative distribution function evaluated at k.\n",
      "     |  \n",
      "     |  logpmf(self, k, *args, **kwds)\n",
      "     |      Log of the probability mass function at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter. Default is 0.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logpmf : array_like\n",
      "     |          Log of the probability mass function evaluated at k.\n",
      "     |  \n",
      "     |  logsf(self, k, *args, **kwds)\n",
      "     |      Log of the survival function of the given RV.\n",
      "     |      \n",
      "     |      Returns the log of the \"survival function,\" defined as 1 - `cdf`,\n",
      "     |      evaluated at `k`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logsf : ndarray\n",
      "     |          Log of the survival function evaluated at `k`.\n",
      "     |  \n",
      "     |  pmf(self, k, *args, **kwds)\n",
      "     |      Probability mass function at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pmf : array_like\n",
      "     |          Probability mass function evaluated at k\n",
      "     |  \n",
      "     |  ppf(self, q, *args, **kwds)\n",
      "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          Lower tail probability.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      k : array_like\n",
      "     |          Quantile corresponding to the lower tail probability, q.\n",
      "     |  \n",
      "     |  rvs(self, *args, **kwargs)\n",
      "     |      Random variates of given type.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      size : int or tuple of ints, optional\n",
      "     |          Defining number of random variates (Default is 1). Note that `size`\n",
      "     |          has to be given as keyword, not as positional argument.\n",
      "     |      random_state : {None, int, `numpy.random.Generator`,\n",
      "     |                      `numpy.random.RandomState`}, optional\n",
      "     |      \n",
      "     |          If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |          singleton is used.\n",
      "     |          If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |          seeded with `seed`.\n",
      "     |          If `seed` is already a ``Generator`` or ``RandomState`` instance\n",
      "     |          then that instance is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Random variates of given `size`.\n",
      "     |  \n",
      "     |  sf(self, k, *args, **kwds)\n",
      "     |      Survival function (1 - `cdf`) at k of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      k : array_like\n",
      "     |          Quantiles.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sf : array_like\n",
      "     |          Survival function evaluated at k.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  __new__(cls, a=0, b=inf, name=None, badvalue=None, moment_tol=1e-08, values=None, inc=1, longname=None, shapes=None, extradoc=None, seed=None)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from rv_generic:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  entropy(self, *args, **kwds)\n",
      "     |      Differential entropy of the RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional  (continuous distributions only).\n",
      "     |          Scale parameter (default=1).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Entropy is defined base `e`:\n",
      "     |      \n",
      "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
      "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
      "     |      True\n",
      "     |  \n",
      "     |  freeze(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  interval(self, alpha, *args, **kwds)\n",
      "     |      Confidence interval with equal areas around the median.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alpha : array_like of float\n",
      "     |          Probability that an rv will be drawn from the returned range.\n",
      "     |          Each value should be in the range [0, 1].\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : ndarray of float\n",
      "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
      "     |          possible values.\n",
      "     |  \n",
      "     |  mean(self, *args, **kwds)\n",
      "     |      Mean of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mean : float\n",
      "     |          the mean of the distribution\n",
      "     |  \n",
      "     |  median(self, *args, **kwds)\n",
      "     |      Median of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      median : float\n",
      "     |          The median of the distribution.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      rv_discrete.ppf\n",
      "     |          Inverse of the CDF\n",
      "     |  \n",
      "     |  moment(self, n, *args, **kwds)\n",
      "     |      n-th order non-central moment of distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, n >= 1\n",
      "     |          Order of moment.\n",
      "     |      arg1, arg2, arg3,... : float\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |  \n",
      "     |  stats(self, *args, **kwds)\n",
      "     |      Some statistics of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional (continuous RVs only)\n",
      "     |          scale parameter (default=1)\n",
      "     |      moments : str, optional\n",
      "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
      "     |          'm' = mean,\n",
      "     |          'v' = variance,\n",
      "     |          's' = (Fisher's) skew,\n",
      "     |          'k' = (Fisher's) kurtosis.\n",
      "     |          (default is 'mv')\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stats : sequence\n",
      "     |          of requested moments.\n",
      "     |  \n",
      "     |  std(self, *args, **kwds)\n",
      "     |      Standard deviation of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      std : float\n",
      "     |          standard deviation of the distribution\n",
      "     |  \n",
      "     |  support(self, *args, **kwargs)\n",
      "     |      Support of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : array_like\n",
      "     |          end-points of the distribution's support.\n",
      "     |  \n",
      "     |  var(self, *args, **kwds)\n",
      "     |      Variance of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      var : float\n",
      "     |          the variance of the distribution\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from rv_generic:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  random_state\n",
      "     |      Get or set the generator object for generating random variates.\n",
      "     |      \n",
      "     |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |      singleton is used.\n",
      "     |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |      seeded with `seed`.\n",
      "     |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "     |      that instance is used.\n",
      "    \n",
      "    class rv_histogram(scipy.stats._distn_infrastructure.rv_continuous)\n",
      "     |  rv_histogram(histogram, *args, **kwargs)\n",
      "     |  \n",
      "     |  Generates a distribution given by a histogram.\n",
      "     |  This is useful to generate a template distribution from a binned\n",
      "     |  datasample.\n",
      "     |  \n",
      "     |  As a subclass of the `rv_continuous` class, `rv_histogram` inherits from it\n",
      "     |  a collection of generic methods (see `rv_continuous` for the full list),\n",
      "     |  and implements them based on the properties of the provided binned\n",
      "     |  datasample.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  histogram : tuple of array_like\n",
      "     |    Tuple containing two array_like objects\n",
      "     |    The first containing the content of n bins\n",
      "     |    The second containing the (n+1) bin boundaries\n",
      "     |    In particular the return value np.histogram is accepted\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There are no additional shape parameters except for the loc and scale.\n",
      "     |  The pdf is defined as a stepwise function from the provided histogram\n",
      "     |  The cdf is a linear interpolation of the pdf.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.19.0\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  \n",
      "     |  Create a scipy.stats distribution from a numpy histogram\n",
      "     |  \n",
      "     |  >>> import scipy.stats\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> data = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5, random_state=123)\n",
      "     |  >>> hist = np.histogram(data, bins=100)\n",
      "     |  >>> hist_dist = scipy.stats.rv_histogram(hist)\n",
      "     |  \n",
      "     |  Behaves like an ordinary scipy rv_continuous distribution\n",
      "     |  \n",
      "     |  >>> hist_dist.pdf(1.0)\n",
      "     |  0.20538577847618705\n",
      "     |  >>> hist_dist.cdf(2.0)\n",
      "     |  0.90818568543056499\n",
      "     |  \n",
      "     |  PDF is zero above (below) the highest (lowest) bin of the histogram,\n",
      "     |  defined by the max (min) of the original dataset\n",
      "     |  \n",
      "     |  >>> hist_dist.pdf(np.max(data))\n",
      "     |  0.0\n",
      "     |  >>> hist_dist.cdf(np.max(data))\n",
      "     |  1.0\n",
      "     |  >>> hist_dist.pdf(np.min(data))\n",
      "     |  7.7591907244498314e-05\n",
      "     |  >>> hist_dist.cdf(np.min(data))\n",
      "     |  0.0\n",
      "     |  \n",
      "     |  PDF and CDF follow the histogram\n",
      "     |  \n",
      "     |  >>> import matplotlib.pyplot as plt\n",
      "     |  >>> X = np.linspace(-5.0, 5.0, 100)\n",
      "     |  >>> plt.title(\"PDF from Template\")\n",
      "     |  >>> plt.hist(data, density=True, bins=100)\n",
      "     |  >>> plt.plot(X, hist_dist.pdf(X), label='PDF')\n",
      "     |  >>> plt.plot(X, hist_dist.cdf(X), label='CDF')\n",
      "     |  >>> plt.show()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      rv_histogram\n",
      "     |      scipy.stats._distn_infrastructure.rv_continuous\n",
      "     |      scipy.stats._distn_infrastructure.rv_generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, histogram, *args, **kwargs)\n",
      "     |      Create a new distribution using the given histogram\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      histogram : tuple of array_like\n",
      "     |        Tuple containing two array_like objects\n",
      "     |        The first containing the content of n bins\n",
      "     |        The second containing the (n+1) bin boundaries\n",
      "     |        In particular the return value np.histogram is accepted\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.stats._distn_infrastructure.rv_continuous:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  cdf(self, x, *args, **kwds)\n",
      "     |      Cumulative distribution function of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      cdf : ndarray\n",
      "     |          Cumulative distribution function evaluated at `x`\n",
      "     |  \n",
      "     |  expect(self, func=None, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "     |      Calculate expected value of a function with respect to the\n",
      "     |      distribution by numerical integration.\n",
      "     |      \n",
      "     |      The expected value of a function ``f(x)`` with respect to a\n",
      "     |      distribution ``dist`` is defined as::\n",
      "     |      \n",
      "     |                  ub\n",
      "     |          E[f(x)] = Integral(f(x) * dist.pdf(x)),\n",
      "     |                  lb\n",
      "     |      \n",
      "     |      where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``\n",
      "     |      distribution. If the bounds ``lb`` and ``ub`` correspond to the\n",
      "     |      support of the distribution, e.g. ``[-inf, inf]`` in the default\n",
      "     |      case, then the integral is the unrestricted expectation of ``f(x)``.\n",
      "     |      Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``\n",
      "     |      outside a finite interval in which case the expectation is\n",
      "     |      calculated within the finite range ``[lb, ub]``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      func : callable, optional\n",
      "     |          Function for which integral is calculated. Takes only one argument.\n",
      "     |          The default is the identity mapping f(x) = x.\n",
      "     |      args : tuple, optional\n",
      "     |          Shape parameters of the distribution.\n",
      "     |      loc : float, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : float, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      lb, ub : scalar, optional\n",
      "     |          Lower and upper bound for integration. Default is set to the\n",
      "     |          support of the distribution.\n",
      "     |      conditional : bool, optional\n",
      "     |          If True, the integral is corrected by the conditional probability\n",
      "     |          of the integration interval.  The return value is the expectation\n",
      "     |          of the function, conditional on being in the given interval.\n",
      "     |          Default is False.\n",
      "     |      \n",
      "     |      Additional keyword arguments are passed to the integration routine.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      expect : float\n",
      "     |          The calculated expected value.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The integration behavior of this function is inherited from\n",
      "     |      `scipy.integrate.quad`. Neither this function nor\n",
      "     |      `scipy.integrate.quad` can verify whether the integral exists or is\n",
      "     |      finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and\n",
      "     |      ``cauchy(0).expect()`` returns ``0.0``.\n",
      "     |      \n",
      "     |      The function is not vectorized.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      To understand the effect of the bounds of integration consider\n",
      "     |      \n",
      "     |      >>> from scipy.stats import expon\n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)\n",
      "     |      0.6321205588285578\n",
      "     |      \n",
      "     |      This is close to\n",
      "     |      \n",
      "     |      >>> expon(1).cdf(2.0) - expon(1).cdf(0.0)\n",
      "     |      0.6321205588285577\n",
      "     |      \n",
      "     |      If ``conditional=True``\n",
      "     |      \n",
      "     |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)\n",
      "     |      1.0000000000000002\n",
      "     |      \n",
      "     |      The slight deviation from 1 is due to numerical integration.\n",
      "     |  \n",
      "     |  fit(self, data, *args, **kwds)\n",
      "     |      Return estimates of shape (if applicable), location, and scale\n",
      "     |      parameters from data. The default estimation method is Maximum\n",
      "     |      Likelihood Estimation (MLE), but Method of Moments (MM)\n",
      "     |      is also available.\n",
      "     |      \n",
      "     |      Starting estimates for\n",
      "     |      the fit are given by input arguments; for any arguments not provided\n",
      "     |      with starting estimates, ``self._fitstart(data)`` is called to generate\n",
      "     |      such.\n",
      "     |      \n",
      "     |      One can hold some parameters fixed to specific values by passing in\n",
      "     |      keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\n",
      "     |      and ``floc`` and ``fscale`` (for location and scale parameters,\n",
      "     |      respectively).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to use in estimating the distribution parameters.\n",
      "     |      arg1, arg2, arg3,... : floats, optional\n",
      "     |          Starting value(s) for any shape-characterizing arguments (those not\n",
      "     |          provided will be determined by a call to ``_fitstart(data)``).\n",
      "     |          No default value.\n",
      "     |      kwds : floats, optional\n",
      "     |          - `loc`: initial guess of the distribution's location parameter.\n",
      "     |          - `scale`: initial guess of the distribution's scale parameter.\n",
      "     |      \n",
      "     |          Special keyword arguments are recognized as holding certain\n",
      "     |          parameters fixed:\n",
      "     |      \n",
      "     |          - f0...fn : hold respective shape parameters fixed.\n",
      "     |            Alternatively, shape parameters to fix can be specified by name.\n",
      "     |            For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n",
      "     |            are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n",
      "     |            equivalent to ``f1``.\n",
      "     |      \n",
      "     |          - floc : hold location parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - fscale : hold scale parameter fixed to specified value.\n",
      "     |      \n",
      "     |          - optimizer : The optimizer to use.\n",
      "     |            The optimizer must take ``func``,\n",
      "     |            and starting position as the first two arguments,\n",
      "     |            plus ``args`` (for extra arguments to pass to the\n",
      "     |            function to be optimized) and ``disp=0`` to suppress\n",
      "     |            output as keyword arguments.\n",
      "     |      \n",
      "     |          - method : The method to use. The default is \"MLE\" (Maximum\n",
      "     |            Likelihood Estimate); \"MM\" (Method of Moments)\n",
      "     |            is also available.\n",
      "     |      \n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      parameter_tuple : tuple of floats\n",
      "     |          Estimates for any shape parameters (if applicable),\n",
      "     |          followed by those for location and scale.\n",
      "     |          For most random variables, shape statistics\n",
      "     |          will be returned, but there are exceptions (e.g. ``norm``).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      With ``method=\"MLE\"`` (default), the fit is computed by minimizing\n",
      "     |      the negative log-likelihood function. A large, finite penalty\n",
      "     |      (rather than infinite negative log-likelihood) is applied for\n",
      "     |      observations beyond the support of the distribution.\n",
      "     |      \n",
      "     |      With ``method=\"MM\"``, the fit is computed by minimizing the L2 norm\n",
      "     |      of the relative errors between the first *k* raw (about zero) data\n",
      "     |      moments and the corresponding distribution moments, where *k* is the\n",
      "     |      number of non-fixed parameters.\n",
      "     |      More precisely, the objective function is::\n",
      "     |      \n",
      "     |          (((data_moments - dist_moments)\n",
      "     |            / np.maximum(np.abs(data_moments), 1e-8))**2).sum()\n",
      "     |      \n",
      "     |      where the constant ``1e-8`` avoids division by zero in case of\n",
      "     |      vanishing data moments. Typically, this error norm can be reduced to\n",
      "     |      zero.\n",
      "     |      Note that the standard method of moments can produce parameters for\n",
      "     |      which some data are outside the support of the fitted distribution;\n",
      "     |      this implementation does nothing to prevent this.\n",
      "     |      \n",
      "     |      For either method,\n",
      "     |      the returned answer is not guaranteed to be globally optimal; it\n",
      "     |      may only be locally optimal, or the optimization may fail altogether.\n",
      "     |      If the data contain any of ``np.nan``, ``np.inf``, or ``-np.inf``,\n",
      "     |      the `fit` method will raise a ``RuntimeError``.\n",
      "     |      \n",
      "     |      Examples\n",
      "     |      --------\n",
      "     |      \n",
      "     |      Generate some data to fit: draw random variates from the `beta`\n",
      "     |      distribution\n",
      "     |      \n",
      "     |      >>> from scipy.stats import beta\n",
      "     |      >>> a, b = 1., 2.\n",
      "     |      >>> x = beta.rvs(a, b, size=1000)\n",
      "     |      \n",
      "     |      Now we can fit all four parameters (``a``, ``b``, ``loc``\n",
      "     |      and ``scale``):\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x)\n",
      "     |      \n",
      "     |      We can also use some prior knowledge about the dataset: let's keep\n",
      "     |      ``loc`` and ``scale`` fixed:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0, 1)\n",
      "     |      \n",
      "     |      We can also keep shape parameters fixed by using ``f``-keywords. To\n",
      "     |      keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\n",
      "     |      equivalently, ``fa=1``:\n",
      "     |      \n",
      "     |      >>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n",
      "     |      >>> a1\n",
      "     |      1\n",
      "     |      \n",
      "     |      Not all distributions return estimates for the shape parameters.\n",
      "     |      ``norm`` for example just returns estimates for location and scale:\n",
      "     |      \n",
      "     |      >>> from scipy.stats import norm\n",
      "     |      >>> x = norm.rvs(a, b, size=1000, random_state=123)\n",
      "     |      >>> loc1, scale1 = norm.fit(x)\n",
      "     |      >>> loc1, scale1\n",
      "     |      (0.92087172783841631, 2.0015750750324668)\n",
      "     |  \n",
      "     |  fit_loc_scale(self, data, *args)\n",
      "     |      Estimate loc and scale parameters from data using 1st and 2nd moments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : array_like\n",
      "     |          Data to fit.\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Lhat : float\n",
      "     |          Estimated location parameter for the data.\n",
      "     |      Shat : float\n",
      "     |          Estimated scale parameter for the data.\n",
      "     |  \n",
      "     |  isf(self, q, *args, **kwds)\n",
      "     |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          upper tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : ndarray or scalar\n",
      "     |          Quantile corresponding to the upper tail probability q.\n",
      "     |  \n",
      "     |  logcdf(self, x, *args, **kwds)\n",
      "     |      Log of the cumulative distribution function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logcdf : array_like\n",
      "     |          Log of the cumulative distribution function evaluated at x\n",
      "     |  \n",
      "     |  logpdf(self, x, *args, **kwds)\n",
      "     |      Log of the probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      This uses a more numerically accurate calculation if available.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logpdf : array_like\n",
      "     |          Log of the probability density function evaluated at x\n",
      "     |  \n",
      "     |  logsf(self, x, *args, **kwds)\n",
      "     |      Log of the survival function of the given RV.\n",
      "     |      \n",
      "     |      Returns the log of the \"survival function,\" defined as (1 - `cdf`),\n",
      "     |      evaluated at `x`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logsf : ndarray\n",
      "     |          Log of the survival function evaluated at `x`.\n",
      "     |  \n",
      "     |  nnlf(self, theta, x)\n",
      "     |      Negative loglikelihood function.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
      "     |      parameters (including loc and scale).\n",
      "     |  \n",
      "     |  pdf(self, x, *args, **kwds)\n",
      "     |      Probability density function at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      pdf : ndarray\n",
      "     |          Probability density function evaluated at x\n",
      "     |  \n",
      "     |  ppf(self, q, *args, **kwds)\n",
      "     |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      q : array_like\n",
      "     |          lower tail probability\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      x : array_like\n",
      "     |          quantile corresponding to the lower tail probability q.\n",
      "     |  \n",
      "     |  sf(self, x, *args, **kwds)\n",
      "     |      Survival function (1 - `cdf`) at x of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          quantiles\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      sf : array_like\n",
      "     |          Survival function evaluated at x\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  entropy(self, *args, **kwds)\n",
      "     |      Differential entropy of the RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional  (continuous distributions only).\n",
      "     |          Scale parameter (default=1).\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Entropy is defined base `e`:\n",
      "     |      \n",
      "     |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
      "     |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
      "     |      True\n",
      "     |  \n",
      "     |  freeze(self, *args, **kwds)\n",
      "     |      Freeze the distribution for the given arguments.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution.  Should include all\n",
      "     |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rv_frozen : rv_frozen instance\n",
      "     |          The frozen distribution.\n",
      "     |  \n",
      "     |  interval(self, alpha, *args, **kwds)\n",
      "     |      Confidence interval with equal areas around the median.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      alpha : array_like of float\n",
      "     |          Probability that an rv will be drawn from the returned range.\n",
      "     |          Each value should be in the range [0, 1].\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : ndarray of float\n",
      "     |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
      "     |          possible values.\n",
      "     |  \n",
      "     |  mean(self, *args, **kwds)\n",
      "     |      Mean of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      mean : float\n",
      "     |          the mean of the distribution\n",
      "     |  \n",
      "     |  median(self, *args, **kwds)\n",
      "     |      Median of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      median : float\n",
      "     |          The median of the distribution.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      rv_discrete.ppf\n",
      "     |          Inverse of the CDF\n",
      "     |  \n",
      "     |  moment(self, n, *args, **kwds)\n",
      "     |      n-th order non-central moment of distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int, n >= 1\n",
      "     |          Order of moment.\n",
      "     |      arg1, arg2, arg3,... : float\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |  \n",
      "     |  rvs(self, *args, **kwds)\n",
      "     |      Random variates of given type.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          Location parameter (default=0).\n",
      "     |      scale : array_like, optional\n",
      "     |          Scale parameter (default=1).\n",
      "     |      size : int or tuple of ints, optional\n",
      "     |          Defining number of random variates (default is 1).\n",
      "     |      random_state : {None, int, `numpy.random.Generator`,\n",
      "     |                      `numpy.random.RandomState`}, optional\n",
      "     |      \n",
      "     |          If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |          singleton is used.\n",
      "     |          If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |          seeded with `seed`.\n",
      "     |          If `seed` is already a ``Generator`` or ``RandomState`` instance\n",
      "     |          then that instance is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      rvs : ndarray or scalar\n",
      "     |          Random variates of given `size`.\n",
      "     |  \n",
      "     |  stats(self, *args, **kwds)\n",
      "     |      Some statistics of the given RV.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional (continuous RVs only)\n",
      "     |          scale parameter (default=1)\n",
      "     |      moments : str, optional\n",
      "     |          composed of letters ['mvsk'] defining which moments to compute:\n",
      "     |          'm' = mean,\n",
      "     |          'v' = variance,\n",
      "     |          's' = (Fisher's) skew,\n",
      "     |          'k' = (Fisher's) kurtosis.\n",
      "     |          (default is 'mv')\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      stats : sequence\n",
      "     |          of requested moments.\n",
      "     |  \n",
      "     |  std(self, *args, **kwds)\n",
      "     |      Standard deviation of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      std : float\n",
      "     |          standard deviation of the distribution\n",
      "     |  \n",
      "     |  support(self, *args, **kwargs)\n",
      "     |      Support of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, ... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information).\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter, Default is 0.\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter, Default is 1.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a, b : array_like\n",
      "     |          end-points of the distribution's support.\n",
      "     |  \n",
      "     |  var(self, *args, **kwds)\n",
      "     |      Variance of the distribution.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      arg1, arg2, arg3,... : array_like\n",
      "     |          The shape parameter(s) for the distribution (see docstring of the\n",
      "     |          instance object for more information)\n",
      "     |      loc : array_like, optional\n",
      "     |          location parameter (default=0)\n",
      "     |      scale : array_like, optional\n",
      "     |          scale parameter (default=1)\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      var : float\n",
      "     |          the variance of the distribution\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  random_state\n",
      "     |      Get or set the generator object for generating random variates.\n",
      "     |      \n",
      "     |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "     |      singleton is used.\n",
      "     |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "     |      seeded with `seed`.\n",
      "     |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "     |      that instance is used.\n",
      "\n",
      "FUNCTIONS\n",
      "    alexandergovern(*args, nan_policy='propagate')\n",
      "        Performs the Alexander Govern test.\n",
      "        \n",
      "        The Alexander-Govern approximation tests the equality of k independent\n",
      "        means in the face of heterogeneity of variance. The test is applied to\n",
      "        samples from two or more groups, possibly with differing sizes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            The sample measurements for each group.  There must be at least\n",
      "            two samples.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The computed A statistic of the test.\n",
      "        pvalue : float\n",
      "            The associated p-value from the chi-squared distribution.\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        AlexanderGovernConstantInputWarning\n",
      "            Raised if an input is a constant array.  The statistic is not defined\n",
      "            in this case, so ``np.nan`` is returned.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        f_oneway : one-way ANOVA\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The use of this test relies on several assumptions.\n",
      "        \n",
      "        1. The samples are independent.\n",
      "        2. Each sample is from a normally distributed population.\n",
      "        3. Unlike `f_oneway`, this test does not assume on homoscedasticity,\n",
      "           instead relaxing the assumption of equal variances.\n",
      "        \n",
      "        Input samples must be finite, one dimensional, and with size greater than\n",
      "        one.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Alexander, Ralph A., and Diane M. Govern. \"A New and Simpler\n",
      "               Approximation for ANOVA under Variance Heterogeneity.\" Journal\n",
      "               of Educational Statistics, vol. 19, no. 2, 1994, pp. 91-101.\n",
      "               JSTOR, www.jstor.org/stable/1165140. Accessed 12 Sept. 2020.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import alexandergovern\n",
      "        \n",
      "        Here are some data on annual percentage rate of interest charged on\n",
      "        new car loans at nine of the largest banks in four American cities\n",
      "        taken from the National Institute of Standards and Technology's\n",
      "        ANOVA dataset.\n",
      "        \n",
      "        We use `alexandergovern` to test the null hypothesis that all cities\n",
      "        have the same mean APR against the alternative that the cities do not\n",
      "        all have the same mean APR. We decide that a sigificance level of 5%\n",
      "        is required to reject the null hypothesis in favor of the alternative.\n",
      "        \n",
      "        >>> atlanta = [13.75, 13.75, 13.5, 13.5, 13.0, 13.0, 13.0, 12.75, 12.5]\n",
      "        >>> chicago = [14.25, 13.0, 12.75, 12.5, 12.5, 12.4, 12.3, 11.9, 11.9]\n",
      "        >>> houston = [14.0, 14.0, 13.51, 13.5, 13.5, 13.25, 13.0, 12.5, 12.5]\n",
      "        >>> memphis = [15.0, 14.0, 13.75, 13.59, 13.25, 12.97, 12.5, 12.25,\n",
      "        ...           11.89]\n",
      "        >>> alexandergovern(atlanta, chicago, houston, memphis)\n",
      "        AlexanderGovernResult(statistic=4.65087071883494,\n",
      "                              pvalue=0.19922132490385214)\n",
      "        \n",
      "        The p-value is 0.1992, indicating a nearly 20% chance of observing\n",
      "        such an extreme value of the test statistic under the null hypothesis.\n",
      "        This exceeds 5%, so we do not reject the null hypothesis in favor of\n",
      "        the alternative.\n",
      "    \n",
      "    anderson(x, dist='norm')\n",
      "        Anderson-Darling test for data coming from a particular distribution.\n",
      "        \n",
      "        The Anderson-Darling test tests the null hypothesis that a sample is\n",
      "        drawn from a population that follows a particular distribution.\n",
      "        For the Anderson-Darling test, the critical values depend on\n",
      "        which distribution is being tested against.  This function works\n",
      "        for normal, exponential, logistic, or Gumbel (Extreme Value\n",
      "        Type I) distributions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Array of sample data.\n",
      "        dist : {'norm', 'expon', 'logistic', 'gumbel', 'gumbel_l', 'gumbel_r', 'extreme1'}, optional\n",
      "            The type of distribution to test against.  The default is 'norm'.\n",
      "            The names 'extreme1', 'gumbel_l' and 'gumbel' are synonyms for the\n",
      "            same distribution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The Anderson-Darling test statistic.\n",
      "        critical_values : list\n",
      "            The critical values for this distribution.\n",
      "        significance_level : list\n",
      "            The significance levels for the corresponding critical values\n",
      "            in percents.  The function returns critical values for a\n",
      "            differing set of significance levels depending on the\n",
      "            distribution that is being tested against.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstest : The Kolmogorov-Smirnov test for goodness-of-fit.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Critical values provided are for the following significance levels:\n",
      "        \n",
      "        normal/exponential\n",
      "            15%, 10%, 5%, 2.5%, 1%\n",
      "        logistic\n",
      "            25%, 10%, 5%, 2.5%, 1%, 0.5%\n",
      "        Gumbel\n",
      "            25%, 10%, 5%, 2.5%, 1%\n",
      "        \n",
      "        If the returned statistic is larger than these critical values then\n",
      "        for the corresponding significance level, the null hypothesis that\n",
      "        the data come from the chosen distribution can be rejected.\n",
      "        The returned statistic is referred to as 'A2' in the references.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
      "        .. [2] Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and\n",
      "               Some Comparisons, Journal of the American Statistical Association,\n",
      "               Vol. 69, pp. 730-737.\n",
      "        .. [3] Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit\n",
      "               Statistics with Unknown Parameters, Annals of Statistics, Vol. 4,\n",
      "               pp. 357-369.\n",
      "        .. [4] Stephens, M. A. (1977). Goodness of Fit for the Extreme Value\n",
      "               Distribution, Biometrika, Vol. 64, pp. 583-588.\n",
      "        .. [5] Stephens, M. A. (1977). Goodness of Fit with Special Reference\n",
      "               to Tests for Exponentiality , Technical Report No. 262,\n",
      "               Department of Statistics, Stanford University, Stanford, CA.\n",
      "        .. [6] Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution\n",
      "               Based on the Empirical Distribution Function, Biometrika, Vol. 66,\n",
      "               pp. 591-595.\n",
      "    \n",
      "    anderson_ksamp(samples, midrank=True)\n",
      "        The Anderson-Darling test for k-samples.\n",
      "        \n",
      "        The k-sample Anderson-Darling test is a modification of the\n",
      "        one-sample Anderson-Darling test. It tests the null hypothesis\n",
      "        that k-samples are drawn from the same population without having\n",
      "        to specify the distribution function of that population. The\n",
      "        critical values depend on the number of samples.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : sequence of 1-D array_like\n",
      "            Array of sample data in arrays.\n",
      "        midrank : bool, optional\n",
      "            Type of Anderson-Darling test which is computed. Default\n",
      "            (True) is the midrank test applicable to continuous and\n",
      "            discrete populations. If False, the right side empirical\n",
      "            distribution is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            Normalized k-sample Anderson-Darling test statistic.\n",
      "        critical_values : array\n",
      "            The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%,\n",
      "            0.5%, 0.1%.\n",
      "        significance_level : float\n",
      "            An approximate significance level at which the null hypothesis for the\n",
      "            provided samples can be rejected. The value is floored / capped at\n",
      "            0.1% / 25%.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If less than 2 samples are provided, a sample is empty, or no\n",
      "            distinct observations are in the samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp : 2 sample Kolmogorov-Smirnov test\n",
      "        anderson : 1 sample Anderson-Darling test\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        [1]_ defines three versions of the k-sample Anderson-Darling test:\n",
      "        one for continuous distributions and two for discrete\n",
      "        distributions, in which ties between samples may occur. The\n",
      "        default of this routine is to compute the version based on the\n",
      "        midrank empirical distribution function. This test is applicable\n",
      "        to continuous and discrete data. If midrank is set to False, the\n",
      "        right side empirical distribution is used for a test for discrete\n",
      "        data. According to [1]_, the two discrete test statistics differ\n",
      "        only slightly if a few collisions due to round-off errors occur in\n",
      "        the test not adjusted for ties between samples.\n",
      "        \n",
      "        The critical values corresponding to the significance levels from 0.01\n",
      "        to 0.25 are taken from [1]_. p-values are floored / capped\n",
      "        at 0.1% / 25%. Since the range of critical values might be extended in\n",
      "        future releases, it is recommended not to test ``p == 0.25``, but rather\n",
      "        ``p >= 0.25`` (analogously for the lower bound).\n",
      "        \n",
      "        .. versionadded:: 0.14.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Scholz, F. W and Stephens, M. A. (1987), K-Sample\n",
      "               Anderson-Darling Tests, Journal of the American Statistical\n",
      "               Association, Vol. 82, pp. 918-924.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        The null hypothesis that the two random samples come from the same\n",
      "        distribution can be rejected at the 5% level because the returned\n",
      "        test value is greater than the critical value for 5% (1.961) but\n",
      "        not at the 2.5% level. The interpolation gives an approximate\n",
      "        significance level of 3.2%:\n",
      "        \n",
      "        >>> stats.anderson_ksamp([rng.normal(size=50),\n",
      "        ... rng.normal(loc=0.5, size=30)])\n",
      "        (1.974403288713695,\n",
      "          array([0.325, 1.226, 1.961, 2.718, 3.752, 4.592, 6.546]),\n",
      "          0.04991293614572478)\n",
      "        \n",
      "        \n",
      "        The null hypothesis cannot be rejected for three samples from an\n",
      "        identical distribution. The reported p-value (25%) has been capped and\n",
      "        may not be very accurate (since it corresponds to the value 0.449\n",
      "        whereas the statistic is -0.731):\n",
      "        \n",
      "        >>> stats.anderson_ksamp([rng.normal(size=50),\n",
      "        ... rng.normal(size=30), rng.normal(size=20)])\n",
      "        (-0.29103725200789504,\n",
      "          array([ 0.44925884,  1.3052767 ,  1.9434184 ,  2.57696569,  3.41634856,\n",
      "          4.07210043, 5.56419101]),\n",
      "          0.25)\n",
      "    \n",
      "    ansari(x, y, alternative='two-sided')\n",
      "        Perform the Ansari-Bradley test for equal scale parameters.\n",
      "        \n",
      "        The Ansari-Bradley test ([1]_, [2]_) is a non-parametric test\n",
      "        for the equality of the scale parameter of the distributions\n",
      "        from which two samples were drawn. The null hypothesis states that\n",
      "        the ratio of the scale of the distribution underlying `x` to the scale\n",
      "        of the distribution underlying `y` is 1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of sample data.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
      "            The following options are available:\n",
      "        \n",
      "            * 'two-sided': the ratio of scales is not equal to 1.\n",
      "            * 'less': the ratio of scales is less than 1.\n",
      "            * 'greater': the ratio of scales is greater than 1.\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The Ansari-Bradley test statistic.\n",
      "        pvalue : float\n",
      "            The p-value of the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fligner : A non-parametric test for the equality of k variances\n",
      "        mood : A non-parametric test for the equality of two scale parameters\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The p-value given is exact when the sample sizes are both less than\n",
      "        55 and there are no ties, otherwise a normal approximation for the\n",
      "        p-value is used.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Ansari, A. R. and Bradley, R. A. (1960) Rank-sum tests for\n",
      "               dispersions, Annals of Mathematical Statistics, 31, 1174-1189.\n",
      "        .. [2] Sprent, Peter and N.C. Smeeton.  Applied nonparametric\n",
      "               statistical methods.  3rd ed. Chapman and Hall/CRC. 2001.\n",
      "               Section 5.8.2.\n",
      "        .. [3] Nathaniel E. Helwig \"Nonparametric Dispersion and Equality\n",
      "               Tests\" at http://users.stat.umn.edu/~helwig/notes/npde-Notes.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import ansari\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        For these examples, we'll create three random data sets.  The first\n",
      "        two, with sizes 35 and 25, are drawn from a normal distribution with\n",
      "        mean 0 and standard deviation 2.  The third data set has size 25 and\n",
      "        is drawn from a normal distribution with standard deviation 1.25.\n",
      "        \n",
      "        >>> x1 = rng.normal(loc=0, scale=2, size=35)\n",
      "        >>> x2 = rng.normal(loc=0, scale=2, size=25)\n",
      "        >>> x3 = rng.normal(loc=0, scale=1.25, size=25)\n",
      "        \n",
      "        First we apply `ansari` to `x1` and `x2`.  These samples are drawn\n",
      "        from the same distribution, so we expect the Ansari-Bradley test\n",
      "        should not lead us to conclude that the scales of the distributions\n",
      "        are different.\n",
      "        \n",
      "        >>> ansari(x1, x2)\n",
      "        AnsariResult(statistic=541.0, pvalue=0.9762532927399098)\n",
      "        \n",
      "        With a p-value close to 1, we cannot conclude that there is a\n",
      "        significant difference in the scales (as expected).\n",
      "        \n",
      "        Now apply the test to `x1` and `x3`:\n",
      "        \n",
      "        >>> ansari(x1, x3)\n",
      "        AnsariResult(statistic=425.0, pvalue=0.0003087020407974518)\n",
      "        \n",
      "        The probability of observing such an extreme value of the statistic\n",
      "        under the null hypothesis of equal scales is only 0.03087%. We take this\n",
      "        as evidence against the null hypothesis in favor of the alternative:\n",
      "        the scales of the distributions from which the samples were drawn\n",
      "        are not equal.\n",
      "        \n",
      "        We can use the `alternative` parameter to perform a one-tailed test.\n",
      "        In the above example, the scale of `x1` is greater than `x3` and so\n",
      "        the ratio of scales of `x1` and `x3` is greater than 1. This means\n",
      "        that the p-value when ``alternative='greater'`` should be near 0 and\n",
      "        hence we should be able to reject the null hypothesis:\n",
      "        \n",
      "        >>> ansari(x1, x3, alternative='greater')\n",
      "        AnsariResult(statistic=425.0, pvalue=0.0001543510203987259)\n",
      "        \n",
      "        As we can see, the p-value is indeed quite low. Use of\n",
      "        ``alternative='less'`` should thus yield a large p-value:\n",
      "        \n",
      "        >>> ansari(x1, x3, alternative='less')\n",
      "        AnsariResult(statistic=425.0, pvalue=0.9998643258449039)\n",
      "    \n",
      "    barnard_exact(table, alternative='two-sided', pooled=True, n=32)\n",
      "        Perform a Barnard exact test on a 2x2 contingency table.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        table : array_like of ints\n",
      "            A 2x2 contingency table.  Elements should be non-negative integers.\n",
      "        \n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
      "            Please see explanations in the Notes section below.\n",
      "        \n",
      "        pooled : bool, optional\n",
      "            Whether to compute score statistic with pooled variance (as in\n",
      "            Student's t-test, for example) or unpooled variance (as in Welch's\n",
      "            t-test). Default is ``True``.\n",
      "        \n",
      "        n : int, optional\n",
      "            Number of sampling points used in the construction of the sampling\n",
      "            method. Note that this argument will automatically be converted to\n",
      "            the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to\n",
      "            select sample points. Default is 32. Must be positive. In most cases,\n",
      "            32 points is enough to reach good precision. More points comes at\n",
      "            performance cost.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ber : BarnardExactResult\n",
      "            A result object with the following attributes.\n",
      "        \n",
      "            statistic : float\n",
      "                The Wald statistic with pooled or unpooled variance, depending\n",
      "                on the user choice of `pooled`.\n",
      "        \n",
      "            pvalue : float\n",
      "                P-value, the probability of obtaining a distribution at least as\n",
      "                extreme as the one that was actually observed, assuming that the\n",
      "                null hypothesis is true.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chi2_contingency : Chi-square test of independence of variables in a\n",
      "            contingency table.\n",
      "        fisher_exact : Fisher exact test on a 2x2 contingency table.\n",
      "        boschloo_exact : Boschloo's exact test on a 2x2 contingency table,\n",
      "            which is an uniformly more powerful alternative to Fisher's exact test.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Barnard's test is an exact test used in the analysis of contingency\n",
      "        tables. It examines the association of two categorical variables, and\n",
      "        is a more powerful alternative than Fisher's exact test\n",
      "        for 2x2 contingency tables.\n",
      "        \n",
      "        Let's define :math:`X_0` a 2x2 matrix representing the observed sample,\n",
      "        where each column stores the binomial experiment, as in the example\n",
      "        below. Let's also define :math:`p_1, p_2` the theoretical binomial\n",
      "        probabilities for  :math:`x_{11}` and :math:`x_{12}`. When using\n",
      "        Barnard exact test, we can assert three different null hypotheses :\n",
      "        \n",
      "        - :math:`H_0 : p_1 \\geq p_2` versus :math:`H_1 : p_1 < p_2`,\n",
      "          with `alternative` = \"less\"\n",
      "        \n",
      "        - :math:`H_0 : p_1 \\leq p_2` versus :math:`H_1 : p_1 > p_2`,\n",
      "          with `alternative` = \"greater\"\n",
      "        \n",
      "        - :math:`H_0 : p_1 = p_2` versus :math:`H_1 : p_1 \\neq p_2`,\n",
      "          with `alternative` = \"two-sided\" (default one)\n",
      "        \n",
      "        In order to compute Barnard's exact test, we are using the Wald\n",
      "        statistic [3]_ with pooled or unpooled variance.\n",
      "        Under the default assumption that both variances are equal\n",
      "        (``pooled = True``), the statistic is computed as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T(X) = \\frac{\n",
      "                \\hat{p}_1 - \\hat{p}_2\n",
      "            }{\n",
      "                \\sqrt{\n",
      "                    \\hat{p}(1 - \\hat{p})\n",
      "                    (\\frac{1}{c_1} +\n",
      "                    \\frac{1}{c_2})\n",
      "                }\n",
      "            }\n",
      "        \n",
      "        with :math:`\\hat{p}_1, \\hat{p}_2` and :math:`\\hat{p}` the estimator of\n",
      "        :math:`p_1, p_2` and :math:`p`, the latter being the combined probability,\n",
      "        given the assumption that :math:`p_1 = p_2`.\n",
      "        \n",
      "        If this assumption is invalid (``pooled = False``), the statistic is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T(X) = \\frac{\n",
      "                \\hat{p}_1 - \\hat{p}_2\n",
      "            }{\n",
      "                \\sqrt{\n",
      "                    \\frac{\\hat{p}_1 (1 - \\hat{p}_1)}{c_1} +\n",
      "                    \\frac{\\hat{p}_2 (1 - \\hat{p}_2)}{c_2}\n",
      "                }\n",
      "            }\n",
      "        \n",
      "        The p-value is then computed as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\sum\n",
      "                \\binom{c_1}{x_{11}}\n",
      "                \\binom{c_2}{x_{12}}\n",
      "                \\pi^{x_{11} + x_{12}}\n",
      "                (1 - \\pi)^{t - x_{11} - x_{12}}\n",
      "        \n",
      "        where the sum is over all  2x2 contingency tables :math:`X` such that:\n",
      "        * :math:`T(X) \\leq T(X_0)` when `alternative` = \"less\",\n",
      "        * :math:`T(X) \\geq T(X_0)` when `alternative` = \"greater\", or\n",
      "        * :math:`T(X) \\geq |T(X_0)|` when `alternative` = \"two-sided\".\n",
      "        Above, :math:`c_1, c_2` are the sum of the columns 1 and 2,\n",
      "        and :math:`t` the total (sum of the 4 sample's element).\n",
      "        \n",
      "        The returned p-value is the maximum p-value taken over the nuisance\n",
      "        parameter :math:`\\pi`, where :math:`0 \\leq \\pi \\leq 1`.\n",
      "        \n",
      "        This function's complexity is :math:`O(n c_1 c_2)`, where `n` is the\n",
      "        number of sample points.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barnard, G. A. \"Significance Tests for 2x2 Tables\". *Biometrika*.\n",
      "               34.1/2 (1947): 123-138. :doi:`dpgkg3`\n",
      "        \n",
      "        .. [2] Mehta, Cyrus R., and Pralay Senchaudhuri. \"Conditional versus\n",
      "               unconditional exact tests for comparing two binomials.\"\n",
      "               *Cytel Software Corporation* 675 (2003): 1-5.\n",
      "        \n",
      "        .. [3] \"Wald Test\". *Wikipedia*. https://en.wikipedia.org/wiki/Wald_test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        An example use of Barnard's test is presented in [2]_.\n",
      "        \n",
      "            Consider the following example of a vaccine efficacy study\n",
      "            (Chan, 1998). In a randomized clinical trial of 30 subjects, 15 were\n",
      "            inoculated with a recombinant DNA influenza vaccine and the 15 were\n",
      "            inoculated with a placebo. Twelve of the 15 subjects in the placebo\n",
      "            group (80%) eventually became infected with influenza whereas for the\n",
      "            vaccine group, only 7 of the 15 subjects (47%) became infected. The\n",
      "            data are tabulated as a 2 x 2 table::\n",
      "        \n",
      "                    Vaccine  Placebo\n",
      "                Yes     7        12\n",
      "                No      8        3\n",
      "        \n",
      "        When working with statistical hypothesis testing, we usually use a\n",
      "        threshold probability or significance level upon which we decide\n",
      "        to reject the null hypothesis :math:`H_0`. Suppose we choose the common\n",
      "        significance level of 5%.\n",
      "        \n",
      "        Our alternative hypothesis is that the vaccine will lower the chance of\n",
      "        becoming infected with the virus; that is, the probability :math:`p_1` of\n",
      "        catching the virus with the vaccine will be *less than* the probability\n",
      "        :math:`p_2` of catching the virus without the vaccine.  Therefore, we call\n",
      "        `barnard_exact` with the ``alternative=\"less\"`` option:\n",
      "        \n",
      "        >>> import scipy.stats as stats\n",
      "        >>> res = stats.barnard_exact([[7, 12], [8, 3]], alternative=\"less\")\n",
      "        >>> res.statistic\n",
      "        -1.894...\n",
      "        >>> res.pvalue\n",
      "        0.03407...\n",
      "        \n",
      "        Under the null hypothesis that the vaccine will not lower the chance of\n",
      "        becoming infected, the probability of obtaining test results at least as\n",
      "        extreme as the observed data is approximately 3.4%. Since this p-value is\n",
      "        less than our chosen significance level, we have evidence to reject\n",
      "        :math:`H_0` in favor of the alternative.\n",
      "        \n",
      "        Suppose we had used Fisher's exact test instead:\n",
      "        \n",
      "        >>> _, pvalue = stats.fisher_exact([[7, 12], [8, 3]], alternative=\"less\")\n",
      "        >>> pvalue\n",
      "        0.0640...\n",
      "        \n",
      "        With the same threshold significance of 5%, we would not have been able\n",
      "        to reject the null hypothesis in favor of the alternative. As stated in\n",
      "        [2]_, Barnard's test is uniformly more powerful than Fisher's exact test\n",
      "        because Barnard's test does not condition on any margin. Fisher's test\n",
      "        should only be used when both sets of marginals are fixed.\n",
      "    \n",
      "    bartlett(*args)\n",
      "        Perform Bartlett's test for equal variances.\n",
      "        \n",
      "        Bartlett's test tests the null hypothesis that all input samples\n",
      "        are from populations with equal variances.  For samples\n",
      "        from significantly non-normal populations, Levene's test\n",
      "        `levene` is more robust.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2,... : array_like\n",
      "            arrays of sample data.  Only 1d arrays are accepted, they may have\n",
      "            different lengths.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The p-value of the test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fligner : A non-parametric test for the equality of k variances\n",
      "        levene : A robust parametric test for equality of k variances\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Conover et al. (1981) examine many of the existing parametric and\n",
      "        nonparametric tests by extensive simulations and they conclude that the\n",
      "        tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\n",
      "        superior in terms of robustness of departures from normality and power\n",
      "        ([3]_).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1]  https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm\n",
      "        \n",
      "        .. [2]  Snedecor, George W. and Cochran, William G. (1989), Statistical\n",
      "                  Methods, Eighth Edition, Iowa State University Press.\n",
      "        \n",
      "        .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
      "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
      "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
      "               University.\n",
      "        \n",
      "        .. [4] Bartlett, M. S. (1937). Properties of Sufficiency and Statistical\n",
      "               Tests. Proceedings of the Royal Society of London. Series A,\n",
      "               Mathematical and Physical Sciences, Vol. 160, No.901, pp. 268-282.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Test whether or not the lists `a`, `b` and `c` come from populations\n",
      "        with equal variances.\n",
      "        \n",
      "        >>> from scipy.stats import bartlett\n",
      "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
      "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
      "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
      "        >>> stat, p = bartlett(a, b, c)\n",
      "        >>> p\n",
      "        1.1254782518834628e-05\n",
      "        \n",
      "        The very small p-value suggests that the populations do not have equal\n",
      "        variances.\n",
      "        \n",
      "        This is not surprising, given that the sample variance of `b` is much\n",
      "        larger than that of `a` and `c`:\n",
      "        \n",
      "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
      "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
      "    \n",
      "    bayes_mvs(data, alpha=0.9)\n",
      "        Bayesian confidence intervals for the mean, var, and std.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input data, if multi-dimensional it is flattened to 1-D by `bayes_mvs`.\n",
      "            Requires 2 or more data points.\n",
      "        alpha : float, optional\n",
      "            Probability that the returned confidence interval contains\n",
      "            the true parameter.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mean_cntr, var_cntr, std_cntr : tuple\n",
      "            The three results are for the mean, variance and standard deviation,\n",
      "            respectively.  Each result is a tuple of the form::\n",
      "        \n",
      "                (center, (lower, upper))\n",
      "        \n",
      "            with `center` the mean of the conditional pdf of the value given the\n",
      "            data, and `(lower, upper)` a confidence interval, centered on the\n",
      "            median, containing the estimate to a probability ``alpha``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        mvsdist\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Each tuple of mean, variance, and standard deviation estimates represent\n",
      "        the (center, (lower, upper)) with center the mean of the conditional pdf\n",
      "        of the value given the data and (lower, upper) is a confidence interval\n",
      "        centered on the median, containing the estimate to a probability\n",
      "        ``alpha``.\n",
      "        \n",
      "        Converts data to 1-D and assumes all data has the same mean and variance.\n",
      "        Uses Jeffrey's prior for variance and std.\n",
      "        \n",
      "        Equivalent to ``tuple((x.mean(), x.interval(alpha)) for x in mvsdist(dat))``\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n",
      "        standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n",
      "        2006.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First a basic example to demonstrate the outputs:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> data = [6, 9, 12, 7, 8, 8, 13]\n",
      "        >>> mean, var, std = stats.bayes_mvs(data)\n",
      "        >>> mean\n",
      "        Mean(statistic=9.0, minmax=(7.103650222612533, 10.896349777387467))\n",
      "        >>> var\n",
      "        Variance(statistic=10.0, minmax=(3.176724206..., 24.45910382...))\n",
      "        >>> std\n",
      "        Std_dev(statistic=2.9724954732045084, minmax=(1.7823367265645143, 4.945614605014631))\n",
      "        \n",
      "        Now we generate some normally distributed random data, and get estimates of\n",
      "        mean and standard deviation with 95% confidence intervals for those\n",
      "        estimates:\n",
      "        \n",
      "        >>> n_samples = 100000\n",
      "        >>> data = stats.norm.rvs(size=n_samples)\n",
      "        >>> res_mean, res_var, res_std = stats.bayes_mvs(data, alpha=0.95)\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.hist(data, bins=100, density=True, label='Histogram of data')\n",
      "        >>> ax.vlines(res_mean.statistic, 0, 0.5, colors='r', label='Estimated mean')\n",
      "        >>> ax.axvspan(res_mean.minmax[0],res_mean.minmax[1], facecolor='r',\n",
      "        ...            alpha=0.2, label=r'Estimated mean (95% limits)')\n",
      "        >>> ax.vlines(res_std.statistic, 0, 0.5, colors='g', label='Estimated scale')\n",
      "        >>> ax.axvspan(res_std.minmax[0],res_std.minmax[1], facecolor='g', alpha=0.2,\n",
      "        ...            label=r'Estimated scale (95% limits)')\n",
      "        \n",
      "        >>> ax.legend(fontsize=10)\n",
      "        >>> ax.set_xlim([-4, 4])\n",
      "        >>> ax.set_ylim([0, 0.5])\n",
      "        >>> plt.show()\n",
      "    \n",
      "    binned_statistic(x, values, statistic='mean', bins=10, range=None)\n",
      "        Compute a binned statistic for one or more sets of data.\n",
      "        \n",
      "        This is a generalization of a histogram function.  A histogram divides\n",
      "        the space into bins, and returns the count of the number of points in\n",
      "        each bin.  This function allows the computation of the sum, mean, median,\n",
      "        or other statistic of the values (or set of values) within each bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : (N,) array_like\n",
      "            A sequence of values to be binned.\n",
      "        values : (N,) array_like or list of (N,) array_like\n",
      "            The data on which the statistic will be computed.  This must be\n",
      "            the same shape as `x`, or a set of sequences - each the same shape as\n",
      "            `x`.  If `values` is a set of sequences, the statistic will be computed\n",
      "            on each independently.\n",
      "        statistic : string or callable, optional\n",
      "            The statistic to compute (default is 'mean').\n",
      "            The following statistics are available:\n",
      "        \n",
      "              * 'mean' : compute the mean of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'std' : compute the standard deviation within each bin. This\n",
      "                is implicitly calculated with ddof=0.\n",
      "              * 'median' : compute the median of values for points within each\n",
      "                bin. Empty bins will be represented by NaN.\n",
      "              * 'count' : compute the count of points within each bin.  This is\n",
      "                identical to an unweighted histogram.  `values` array is not\n",
      "                referenced.\n",
      "              * 'sum' : compute the sum of values for points within each bin.\n",
      "                This is identical to a weighted histogram.\n",
      "              * 'min' : compute the minimum of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'max' : compute the maximum of values for point within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * function : a user-defined function which takes a 1D array of\n",
      "                values, and outputs a single numerical statistic. This function\n",
      "                will be called on the values in each bin.  Empty bins will be\n",
      "                represented by function([]), or NaN if this returns an error.\n",
      "        \n",
      "        bins : int or sequence of scalars, optional\n",
      "            If `bins` is an int, it defines the number of equal-width bins in the\n",
      "            given range (10 by default).  If `bins` is a sequence, it defines the\n",
      "            bin edges, including the rightmost edge, allowing for non-uniform bin\n",
      "            widths.  Values in `x` that are smaller than lowest bin edge are\n",
      "            assigned to bin number 0, values beyond the highest bin are assigned to\n",
      "            ``bins[-1]``.  If the bin edges are specified, the number of bins will\n",
      "            be, (nx = len(bins)-1).\n",
      "        range : (float, float) or [(float, float)], optional\n",
      "            The lower and upper range of the bins.  If not provided, range\n",
      "            is simply ``(x.min(), x.max())``.  Values outside the range are\n",
      "            ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : array\n",
      "            The values of the selected statistic in each bin.\n",
      "        bin_edges : array of dtype float\n",
      "            Return the bin edges ``(length(statistic)+1)``.\n",
      "        binnumber: 1-D ndarray of ints\n",
      "            Indices of the bins (corresponding to `bin_edges`) in which each value\n",
      "            of `x` belongs.  Same length as `values`.  A binnumber of `i` means the\n",
      "            corresponding value is between (bin_edges[i-1], bin_edges[i]).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.digitize, numpy.histogram, binned_statistic_2d, binned_statistic_dd\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All but the last (righthand-most) bin is half-open.  In other words, if\n",
      "        `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n",
      "        but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n",
      "        ``[3, 4]``, which *includes* 4.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        First some basic examples:\n",
      "        \n",
      "        Create two evenly spaced bins in the range of the given sample, and sum the\n",
      "        corresponding values in each of those bins:\n",
      "        \n",
      "        >>> values = [1.0, 1.0, 2.0, 1.5, 3.0]\n",
      "        >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n",
      "        BinnedStatisticResult(statistic=array([4. , 4.5]),\n",
      "                bin_edges=array([1., 4., 7.]), binnumber=array([1, 1, 1, 2, 2]))\n",
      "        \n",
      "        Multiple arrays of values can also be passed.  The statistic is calculated\n",
      "        on each set independently:\n",
      "        \n",
      "        >>> values = [[1.0, 1.0, 2.0, 1.5, 3.0], [2.0, 2.0, 4.0, 3.0, 6.0]]\n",
      "        >>> stats.binned_statistic([1, 1, 2, 5, 7], values, 'sum', bins=2)\n",
      "        BinnedStatisticResult(statistic=array([[4. , 4.5],\n",
      "               [8. , 9. ]]), bin_edges=array([1., 4., 7.]),\n",
      "               binnumber=array([1, 1, 1, 2, 2]))\n",
      "        \n",
      "        >>> stats.binned_statistic([1, 2, 1, 2, 4], np.arange(5), statistic='mean',\n",
      "        ...                        bins=3)\n",
      "        BinnedStatisticResult(statistic=array([1., 2., 4.]),\n",
      "                bin_edges=array([1., 2., 3., 4.]),\n",
      "                binnumber=array([1, 2, 1, 2, 3]))\n",
      "        \n",
      "        As a second example, we now generate some random data of sailing boat speed\n",
      "        as a function of wind speed, and then determine how fast our boat is for\n",
      "        certain wind speeds:\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> windspeed = 8 * rng.random(500)\n",
      "        >>> boatspeed = .3 * windspeed**.5 + .2 * rng.random(500)\n",
      "        >>> bin_means, bin_edges, binnumber = stats.binned_statistic(windspeed,\n",
      "        ...                 boatspeed, statistic='median', bins=[1,2,3,4,5,6,7])\n",
      "        >>> plt.figure()\n",
      "        >>> plt.plot(windspeed, boatspeed, 'b.', label='raw data')\n",
      "        >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=5,\n",
      "        ...            label='binned statistic of data')\n",
      "        >>> plt.legend()\n",
      "        \n",
      "        Now we can use ``binnumber`` to select all datapoints with a windspeed\n",
      "        below 1:\n",
      "        \n",
      "        >>> low_boatspeed = boatspeed[binnumber == 0]\n",
      "        \n",
      "        As a final example, we will use ``bin_edges`` and ``binnumber`` to make a\n",
      "        plot of a distribution that shows the mean and distribution around that\n",
      "        mean per bin, on top of a regular histogram and the probability\n",
      "        distribution function:\n",
      "        \n",
      "        >>> x = np.linspace(0, 5, num=500)\n",
      "        >>> x_pdf = stats.maxwell.pdf(x)\n",
      "        >>> samples = stats.maxwell.rvs(size=10000)\n",
      "        \n",
      "        >>> bin_means, bin_edges, binnumber = stats.binned_statistic(x, x_pdf,\n",
      "        ...         statistic='mean', bins=25)\n",
      "        >>> bin_width = (bin_edges[1] - bin_edges[0])\n",
      "        >>> bin_centers = bin_edges[1:] - bin_width/2\n",
      "        \n",
      "        >>> plt.figure()\n",
      "        >>> plt.hist(samples, bins=50, density=True, histtype='stepfilled',\n",
      "        ...          alpha=0.2, label='histogram of data')\n",
      "        >>> plt.plot(x, x_pdf, 'r-', label='analytical pdf')\n",
      "        >>> plt.hlines(bin_means, bin_edges[:-1], bin_edges[1:], colors='g', lw=2,\n",
      "        ...            label='binned statistic of data')\n",
      "        >>> plt.plot((binnumber - 0.5) * bin_width, x_pdf, 'g.', alpha=0.5)\n",
      "        >>> plt.legend(fontsize=10)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    binned_statistic_2d(x, y, values, statistic='mean', bins=10, range=None, expand_binnumbers=False)\n",
      "        Compute a bidimensional binned statistic for one or more sets of data.\n",
      "        \n",
      "        This is a generalization of a histogram2d function.  A histogram divides\n",
      "        the space into bins, and returns the count of the number of points in\n",
      "        each bin.  This function allows the computation of the sum, mean, median,\n",
      "        or other statistic of the values (or set of values) within each bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : (N,) array_like\n",
      "            A sequence of values to be binned along the first dimension.\n",
      "        y : (N,) array_like\n",
      "            A sequence of values to be binned along the second dimension.\n",
      "        values : (N,) array_like or list of (N,) array_like\n",
      "            The data on which the statistic will be computed.  This must be\n",
      "            the same shape as `x`, or a list of sequences - each with the same\n",
      "            shape as `x`.  If `values` is such a list, the statistic will be\n",
      "            computed on each independently.\n",
      "        statistic : string or callable, optional\n",
      "            The statistic to compute (default is 'mean').\n",
      "            The following statistics are available:\n",
      "        \n",
      "              * 'mean' : compute the mean of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'std' : compute the standard deviation within each bin. This\n",
      "                is implicitly calculated with ddof=0.\n",
      "              * 'median' : compute the median of values for points within each\n",
      "                bin. Empty bins will be represented by NaN.\n",
      "              * 'count' : compute the count of points within each bin.  This is\n",
      "                identical to an unweighted histogram.  `values` array is not\n",
      "                referenced.\n",
      "              * 'sum' : compute the sum of values for points within each bin.\n",
      "                This is identical to a weighted histogram.\n",
      "              * 'min' : compute the minimum of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'max' : compute the maximum of values for point within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * function : a user-defined function which takes a 1D array of\n",
      "                values, and outputs a single numerical statistic. This function\n",
      "                will be called on the values in each bin.  Empty bins will be\n",
      "                represented by function([]), or NaN if this returns an error.\n",
      "        \n",
      "        bins : int or [int, int] or array_like or [array, array], optional\n",
      "            The bin specification:\n",
      "        \n",
      "              * the number of bins for the two dimensions (nx = ny = bins),\n",
      "              * the number of bins in each dimension (nx, ny = bins),\n",
      "              * the bin edges for the two dimensions (x_edge = y_edge = bins),\n",
      "              * the bin edges in each dimension (x_edge, y_edge = bins).\n",
      "        \n",
      "            If the bin edges are specified, the number of bins will be,\n",
      "            (nx = len(x_edge)-1, ny = len(y_edge)-1).\n",
      "        \n",
      "        range : (2,2) array_like, optional\n",
      "            The leftmost and rightmost edges of the bins along each dimension\n",
      "            (if not specified explicitly in the `bins` parameters):\n",
      "            [[xmin, xmax], [ymin, ymax]]. All values outside of this range will be\n",
      "            considered outliers and not tallied in the histogram.\n",
      "        expand_binnumbers : bool, optional\n",
      "            'False' (default): the returned `binnumber` is a shape (N,) array of\n",
      "            linearized bin indices.\n",
      "            'True': the returned `binnumber` is 'unraveled' into a shape (2,N)\n",
      "            ndarray, where each row gives the bin numbers in the corresponding\n",
      "            dimension.\n",
      "            See the `binnumber` returned value, and the `Examples` section.\n",
      "        \n",
      "            .. versionadded:: 0.17.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : (nx, ny) ndarray\n",
      "            The values of the selected statistic in each two-dimensional bin.\n",
      "        x_edge : (nx + 1) ndarray\n",
      "            The bin edges along the first dimension.\n",
      "        y_edge : (ny + 1) ndarray\n",
      "            The bin edges along the second dimension.\n",
      "        binnumber : (N,) array of ints or (2,N) ndarray of ints\n",
      "            This assigns to each element of `sample` an integer that represents the\n",
      "            bin in which this observation falls.  The representation depends on the\n",
      "            `expand_binnumbers` argument.  See `Notes` for details.\n",
      "        \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.digitize, numpy.histogram2d, binned_statistic, binned_statistic_dd\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Binedges:\n",
      "        All but the last (righthand-most) bin is half-open.  In other words, if\n",
      "        `bins` is ``[1, 2, 3, 4]``, then the first bin is ``[1, 2)`` (including 1,\n",
      "        but excluding 2) and the second ``[2, 3)``.  The last bin, however, is\n",
      "        ``[3, 4]``, which *includes* 4.\n",
      "        \n",
      "        `binnumber`:\n",
      "        This returned argument assigns to each element of `sample` an integer that\n",
      "        represents the bin in which it belongs.  The representation depends on the\n",
      "        `expand_binnumbers` argument. If 'False' (default): The returned\n",
      "        `binnumber` is a shape (N,) array of linearized indices mapping each\n",
      "        element of `sample` to its corresponding bin (using row-major ordering).\n",
      "        If 'True': The returned `binnumber` is a shape (2,N) ndarray where\n",
      "        each row indicates bin placements for each dimension respectively.  In each\n",
      "        dimension, a binnumber of `i` means the corresponding value is between\n",
      "        (D_edge[i-1], D_edge[i]), where 'D' is either 'x' or 'y'.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        Calculate the counts with explicit bin-edges:\n",
      "        \n",
      "        >>> x = [0.1, 0.1, 0.1, 0.6]\n",
      "        >>> y = [2.1, 2.6, 2.1, 2.1]\n",
      "        >>> binx = [0.0, 0.5, 1.0]\n",
      "        >>> biny = [2.0, 2.5, 3.0]\n",
      "        >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny])\n",
      "        >>> ret.statistic\n",
      "        array([[2., 1.],\n",
      "               [1., 0.]])\n",
      "        \n",
      "        The bin in which each sample is placed is given by the `binnumber`\n",
      "        returned parameter.  By default, these are the linearized bin indices:\n",
      "        \n",
      "        >>> ret.binnumber\n",
      "        array([5, 6, 5, 9])\n",
      "        \n",
      "        The bin indices can also be expanded into separate entries for each\n",
      "        dimension using the `expand_binnumbers` parameter:\n",
      "        \n",
      "        >>> ret = stats.binned_statistic_2d(x, y, None, 'count', bins=[binx, biny],\n",
      "        ...                                 expand_binnumbers=True)\n",
      "        >>> ret.binnumber\n",
      "        array([[1, 1, 1, 2],\n",
      "               [1, 2, 1, 1]])\n",
      "        \n",
      "        Which shows that the first three elements belong in the xbin 1, and the\n",
      "        fourth into xbin 2; and so on for y.\n",
      "    \n",
      "    binned_statistic_dd(sample, values, statistic='mean', bins=10, range=None, expand_binnumbers=False, binned_statistic_result=None)\n",
      "        Compute a multidimensional binned statistic for a set of data.\n",
      "        \n",
      "        This is a generalization of a histogramdd function.  A histogram divides\n",
      "        the space into bins, and returns the count of the number of points in\n",
      "        each bin.  This function allows the computation of the sum, mean, median,\n",
      "        or other statistic of the values within each bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample : array_like\n",
      "            Data to histogram passed as a sequence of N arrays of length D, or\n",
      "            as an (N,D) array.\n",
      "        values : (N,) array_like or list of (N,) array_like\n",
      "            The data on which the statistic will be computed.  This must be\n",
      "            the same shape as `sample`, or a list of sequences - each with the\n",
      "            same shape as `sample`.  If `values` is such a list, the statistic\n",
      "            will be computed on each independently.\n",
      "        statistic : string or callable, optional\n",
      "            The statistic to compute (default is 'mean').\n",
      "            The following statistics are available:\n",
      "        \n",
      "              * 'mean' : compute the mean of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'median' : compute the median of values for points within each\n",
      "                bin. Empty bins will be represented by NaN.\n",
      "              * 'count' : compute the count of points within each bin.  This is\n",
      "                identical to an unweighted histogram.  `values` array is not\n",
      "                referenced.\n",
      "              * 'sum' : compute the sum of values for points within each bin.\n",
      "                This is identical to a weighted histogram.\n",
      "              * 'std' : compute the standard deviation within each bin. This\n",
      "                is implicitly calculated with ddof=0. If the number of values\n",
      "                within a given bin is 0 or 1, the computed standard deviation value\n",
      "                will be 0 for the bin.\n",
      "              * 'min' : compute the minimum of values for points within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * 'max' : compute the maximum of values for point within each bin.\n",
      "                Empty bins will be represented by NaN.\n",
      "              * function : a user-defined function which takes a 1D array of\n",
      "                values, and outputs a single numerical statistic. This function\n",
      "                will be called on the values in each bin.  Empty bins will be\n",
      "                represented by function([]), or NaN if this returns an error.\n",
      "        \n",
      "        bins : sequence or positive int, optional\n",
      "            The bin specification must be in one of the following forms:\n",
      "        \n",
      "              * A sequence of arrays describing the bin edges along each dimension.\n",
      "              * The number of bins for each dimension (nx, ny, ... = bins).\n",
      "              * The number of bins for all dimensions (nx = ny = ... = bins).\n",
      "        range : sequence, optional\n",
      "            A sequence of lower and upper bin edges to be used if the edges are\n",
      "            not given explicitly in `bins`. Defaults to the minimum and maximum\n",
      "            values along each dimension.\n",
      "        expand_binnumbers : bool, optional\n",
      "            'False' (default): the returned `binnumber` is a shape (N,) array of\n",
      "            linearized bin indices.\n",
      "            'True': the returned `binnumber` is 'unraveled' into a shape (D,N)\n",
      "            ndarray, where each row gives the bin numbers in the corresponding\n",
      "            dimension.\n",
      "            See the `binnumber` returned value, and the `Examples` section of\n",
      "            `binned_statistic_2d`.\n",
      "        binned_statistic_result : binnedStatisticddResult\n",
      "            Result of a previous call to the function in order to reuse bin edges\n",
      "            and bin numbers with new values and/or a different statistic.\n",
      "            To reuse bin numbers, `expand_binnumbers` must have been set to False\n",
      "            (the default)\n",
      "        \n",
      "            .. versionadded:: 0.17.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : ndarray, shape(nx1, nx2, nx3,...)\n",
      "            The values of the selected statistic in each two-dimensional bin.\n",
      "        bin_edges : list of ndarrays\n",
      "            A list of D arrays describing the (nxi + 1) bin edges for each\n",
      "            dimension.\n",
      "        binnumber : (N,) array of ints or (D,N) ndarray of ints\n",
      "            This assigns to each element of `sample` an integer that represents the\n",
      "            bin in which this observation falls.  The representation depends on the\n",
      "            `expand_binnumbers` argument.  See `Notes` for details.\n",
      "        \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.digitize, numpy.histogramdd, binned_statistic, binned_statistic_2d\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Binedges:\n",
      "        All but the last (righthand-most) bin is half-open in each dimension.  In\n",
      "        other words, if `bins` is ``[1, 2, 3, 4]``, then the first bin is\n",
      "        ``[1, 2)`` (including 1, but excluding 2) and the second ``[2, 3)``.  The\n",
      "        last bin, however, is ``[3, 4]``, which *includes* 4.\n",
      "        \n",
      "        `binnumber`:\n",
      "        This returned argument assigns to each element of `sample` an integer that\n",
      "        represents the bin in which it belongs.  The representation depends on the\n",
      "        `expand_binnumbers` argument. If 'False' (default): The returned\n",
      "        `binnumber` is a shape (N,) array of linearized indices mapping each\n",
      "        element of `sample` to its corresponding bin (using row-major ordering).\n",
      "        If 'True': The returned `binnumber` is a shape (D,N) ndarray where\n",
      "        each row indicates bin placements for each dimension respectively.  In each\n",
      "        dimension, a binnumber of `i` means the corresponding value is between\n",
      "        (bin_edges[D][i-1], bin_edges[D][i]), for each dimension 'D'.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.mplot3d import Axes3D\n",
      "        \n",
      "        Take an array of 600 (x, y) coordinates as an example.\n",
      "        `binned_statistic_dd` can handle arrays of higher dimension `D`. But a plot\n",
      "        of dimension `D+1` is required.\n",
      "        \n",
      "        >>> mu = np.array([0., 1.])\n",
      "        >>> sigma = np.array([[1., -0.5],[-0.5, 1.5]])\n",
      "        >>> multinormal = stats.multivariate_normal(mu, sigma)\n",
      "        >>> data = multinormal.rvs(size=600, random_state=235412)\n",
      "        >>> data.shape\n",
      "        (600, 2)\n",
      "        \n",
      "        Create bins and count how many arrays fall in each bin:\n",
      "        \n",
      "        >>> N = 60\n",
      "        >>> x = np.linspace(-3, 3, N)\n",
      "        >>> y = np.linspace(-3, 4, N)\n",
      "        >>> ret = stats.binned_statistic_dd(data, np.arange(600), bins=[x, y],\n",
      "        ...                                 statistic='count')\n",
      "        >>> bincounts = ret.statistic\n",
      "        \n",
      "        Set the volume and the location of bars:\n",
      "        \n",
      "        >>> dx = x[1] - x[0]\n",
      "        >>> dy = y[1] - y[0]\n",
      "        >>> x, y = np.meshgrid(x[:-1]+dx/2, y[:-1]+dy/2)\n",
      "        >>> z = 0\n",
      "        \n",
      "        >>> bincounts = bincounts.ravel()\n",
      "        >>> x = x.ravel()\n",
      "        >>> y = y.ravel()\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111, projection='3d')\n",
      "        >>> with np.errstate(divide='ignore'):   # silence random axes3d warning\n",
      "        ...     ax.bar3d(x, y, z, dx, dy, bincounts)\n",
      "        \n",
      "        Reuse bin numbers and bin edges with new values:\n",
      "        \n",
      "        >>> ret2 = stats.binned_statistic_dd(data, -np.arange(600),\n",
      "        ...                                  binned_statistic_result=ret,\n",
      "        ...                                  statistic='mean')\n",
      "    \n",
      "    binom_test(x, n=None, p=0.5, alternative='two-sided')\n",
      "        Perform a test that the probability of success is p.\n",
      "        \n",
      "        Note: `binom_test` is deprecated; it is recommended that `binomtest`\n",
      "        be used instead.\n",
      "        \n",
      "        This is an exact, two-sided test of the null hypothesis\n",
      "        that the probability of success in a Bernoulli experiment\n",
      "        is `p`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : int or array_like\n",
      "            The number of successes, or if x has length 2, it is the\n",
      "            number of successes and the number of failures.\n",
      "        n : int\n",
      "            The number of trials.  This is ignored if x gives both the\n",
      "            number of successes and failures.\n",
      "        p : float, optional\n",
      "            The hypothesized probability of success.  ``0 <= p <= 1``. The\n",
      "            default value is ``p = 0.5``.\n",
      "        alternative : {'two-sided', 'greater', 'less'}, optional\n",
      "            Indicates the alternative hypothesis. The default value is\n",
      "            'two-sided'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        p-value : float\n",
      "            The p-value of the hypothesis test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Binomial_test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        \n",
      "        A car manufacturer claims that no more than 10% of their cars are unsafe.\n",
      "        15 cars are inspected for safety, 3 were found to be unsafe. Test the\n",
      "        manufacturer's claim:\n",
      "        \n",
      "        >>> stats.binom_test(3, n=15, p=0.1, alternative='greater')\n",
      "        0.18406106910639114\n",
      "        \n",
      "        The null hypothesis cannot be rejected at the 5% level of significance\n",
      "        because the returned p-value is greater than the critical value of 5%.\n",
      "    \n",
      "    binomtest(k, n, p=0.5, alternative='two-sided')\n",
      "        Perform a test that the probability of success is p.\n",
      "        \n",
      "        The binomial test [1]_ is a test of the null hypothesis that the\n",
      "        probability of success in a Bernoulli experiment is `p`.\n",
      "        \n",
      "        Details of the test can be found in many texts on statistics, such\n",
      "        as section 24.5 of [2]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : int\n",
      "            The number of successes.\n",
      "        n : int\n",
      "            The number of trials.\n",
      "        p : float, optional\n",
      "            The hypothesized probability of success, i.e. the expected\n",
      "            proportion of successes.  The value must be in the interval\n",
      "            ``0 <= p <= 1``. The default value is ``p = 0.5``.\n",
      "        alternative : {'two-sided', 'greater', 'less'}, optional\n",
      "            Indicates the alternative hypothesis. The default value is\n",
      "            'two-sided'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result : `~scipy.stats._result_classes.BinomTestResult` instance\n",
      "            The return value is an object with the following attributes:\n",
      "        \n",
      "            k : int\n",
      "                The number of successes (copied from `binomtest` input).\n",
      "            n : int\n",
      "                The number of trials (copied from `binomtest` input).\n",
      "            alternative : str\n",
      "                Indicates the alternative hypothesis specified in the input\n",
      "                to `binomtest`.  It will be one of ``'two-sided'``, ``'greater'``,\n",
      "                or ``'less'``.\n",
      "            pvalue : float\n",
      "                The p-value of the hypothesis test.\n",
      "            proportion_estimate : float\n",
      "                The estimate of the proportion of successes.\n",
      "        \n",
      "            The object has the following methods:\n",
      "        \n",
      "            proportion_ci(confidence_level=0.95, method='exact') :\n",
      "                Compute the confidence interval for ``proportion_estimate``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 1.7.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Binomial test, https://en.wikipedia.org/wiki/Binomial_test\n",
      "        .. [2] Jerrold H. Zar, Biostatistical Analysis (fifth edition),\n",
      "               Prentice Hall, Upper Saddle River, New Jersey USA (2010)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import binomtest\n",
      "        \n",
      "        A car manufacturer claims that no more than 10% of their cars are unsafe.\n",
      "        15 cars are inspected for safety, 3 were found to be unsafe. Test the\n",
      "        manufacturer's claim:\n",
      "        \n",
      "        >>> result = binomtest(3, n=15, p=0.1, alternative='greater')\n",
      "        >>> result.pvalue\n",
      "        0.18406106910639114\n",
      "        \n",
      "        The null hypothesis cannot be rejected at the 5% level of significance\n",
      "        because the returned p-value is greater than the critical value of 5%.\n",
      "        \n",
      "        The estimated proportion is simply ``3/15``:\n",
      "        \n",
      "        >>> result.proportion_estimate\n",
      "        0.2\n",
      "        \n",
      "        We can use the `proportion_ci()` method of the result to compute the\n",
      "        confidence interval of the estimate:\n",
      "        \n",
      "        >>> result.proportion_ci(confidence_level=0.95)\n",
      "        ConfidenceInterval(low=0.05684686759024681, high=1.0)\n",
      "    \n",
      "    bootstrap(data, statistic, *, vectorized=True, paired=False, axis=0, confidence_level=0.95, n_resamples=9999, batch=None, method='BCa', random_state=None)\n",
      "        Compute a two-sided bootstrap confidence interval of a statistic.\n",
      "        \n",
      "        When `method` is ``'percentile'``, a bootstrap confidence interval is\n",
      "        computed according to the following procedure.\n",
      "        \n",
      "        1. Resample the data: for each sample in `data` and for each of\n",
      "           `n_resamples`, take a random sample of the original sample\n",
      "           (with replacement) of the same size as the original sample.\n",
      "        \n",
      "        2. Compute the bootstrap distribution of the statistic: for each set of\n",
      "           resamples, compute the test statistic.\n",
      "        \n",
      "        3. Determine the confidence interval: find the interval of the bootstrap\n",
      "           distribution that is\n",
      "        \n",
      "           - symmetric about the median and\n",
      "           - contains `confidence_level` of the resampled statistic values.\n",
      "        \n",
      "        While the ``'percentile'`` method is the most intuitive, it is rarely\n",
      "        used in practice. Two more common methods are available, ``'basic'``\n",
      "        ('reverse percentile') and ``'BCa'`` ('bias-corrected and accelerated');\n",
      "        they differ in how step 3 is performed.\n",
      "        \n",
      "        If the samples in `data` are  taken at random from their respective\n",
      "        distributions :math:`n` times, the confidence interval returned by\n",
      "        `bootstrap` will contain the true value of the statistic for those\n",
      "        distributions approximately `confidence_level`:math:`\\, \\times \\, n` times.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : sequence of array-like\n",
      "             Each element of data is a sample from an underlying distribution.\n",
      "        statistic : callable\n",
      "            Statistic for which the confidence interval is to be calculated.\n",
      "            `statistic` must be a callable that accepts ``len(data)`` samples\n",
      "            as separate arguments and returns the resulting statistic.\n",
      "            If `vectorized` is set ``True``,\n",
      "            `statistic` must also accept a keyword argument `axis` and be\n",
      "            vectorized to compute the statistic along the provided `axis`.\n",
      "        vectorized : bool, default: ``True``\n",
      "            If `vectorized` is set ``False``, `statistic` will not be passed\n",
      "            keyword argument `axis`, and is assumed to calculate the statistic\n",
      "            only for 1D samples.\n",
      "        paired : bool, default: ``False``\n",
      "            Whether the statistic treats corresponding elements of the samples\n",
      "            in `data` as paired.\n",
      "        axis : int, default: ``0``\n",
      "            The axis of the samples in `data` along which the `statistic` is\n",
      "            calculated.\n",
      "        confidence_level : float, default: ``0.95``\n",
      "            The confidence level of the confidence interval.\n",
      "        n_resamples : int, default: ``9999``\n",
      "            The number of resamples performed to form the bootstrap distribution\n",
      "            of the statistic.\n",
      "        batch : int, optional\n",
      "            The number of resamples to process in each vectorized call to\n",
      "            `statistic`. Memory usage is O(`batch`*``n``), where ``n`` is the\n",
      "            sample size. Default is ``None``, in which case ``batch = n_resamples``\n",
      "            (or ``batch = max(n_resamples, n)`` for ``method='BCa'``).\n",
      "        method : {'percentile', 'basic', 'bca'}, default: ``'BCa'``\n",
      "            Whether to return the 'percentile' bootstrap confidence interval\n",
      "            (``'percentile'``), the 'reverse' or the bias-corrected and accelerated\n",
      "            bootstrap confidence interval (``'BCa'``).\n",
      "            Note that only ``'percentile'`` and ``'basic'`` support multi-sample\n",
      "            statistics at this time.\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is ``None`` (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "            Pseudorandom number generator state used to generate resamples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : BootstrapResult\n",
      "            An object with attributes:\n",
      "        \n",
      "            confidence_interval : ConfidenceInterval\n",
      "                The bootstrap confidence interval as an instance of\n",
      "                `collections.namedtuple` with attributes `low` and `high`.\n",
      "            standard_error : float or ndarray\n",
      "                The bootstrap standard error, that is, the sample standard\n",
      "                deviation of the bootstrap distribution\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap,\n",
      "           Chapman & Hall/CRC, Boca Raton, FL, USA (1993)\n",
      "        .. [2] Nathaniel E. Helwig, \"Bootstrap Confidence Intervals\",\n",
      "           http://users.stat.umn.edu/~helwig/notes/bootci-Notes.pdf\n",
      "        .. [3] Bootstrapping (statistics), Wikipedia,\n",
      "           https://en.wikipedia.org/wiki/Bootstrapping_(statistics)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Suppose we have sampled data from an unknown distribution.\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> from scipy.stats import norm\n",
      "        >>> dist = norm(loc=2, scale=4)  # our \"unknown\" distribution\n",
      "        >>> data = dist.rvs(size=100, random_state=rng)\n",
      "        \n",
      "        We are interested int the standard deviation of the distribution.\n",
      "        \n",
      "        >>> std_true = dist.std()      # the true value of the statistic\n",
      "        >>> print(std_true)\n",
      "        4.0\n",
      "        >>> std_sample = np.std(data)  # the sample statistic\n",
      "        >>> print(std_sample)\n",
      "        3.9460644295563863\n",
      "        \n",
      "        We can calculate a 90% confidence interval of the statistic using\n",
      "        `bootstrap`.\n",
      "        \n",
      "        >>> from scipy.stats import bootstrap\n",
      "        >>> data = (data,)  # samples must be in a sequence\n",
      "        >>> res = bootstrap(data, np.std, confidence_level=0.9,\n",
      "        ...                 random_state=rng)\n",
      "        >>> print(res.confidence_interval)\n",
      "        ConfidenceInterval(low=3.57655333533867, high=4.382043696342881)\n",
      "        \n",
      "        If we sample from the distribution 1000 times and form a bootstrap\n",
      "        confidence interval for each sample, the confidence interval\n",
      "        contains the true value of the statistic approximately 900 times.\n",
      "        \n",
      "        >>> n_trials = 1000\n",
      "        >>> ci_contains_true_std = 0\n",
      "        >>> for i in range(n_trials):\n",
      "        ...    data = (dist.rvs(size=100, random_state=rng),)\n",
      "        ...    ci = bootstrap(data, np.std, confidence_level=0.9, n_resamples=1000,\n",
      "        ...                   random_state=rng).confidence_interval\n",
      "        ...    if ci[0] < std_true < ci[1]:\n",
      "        ...        ci_contains_true_std += 1\n",
      "        >>> print(ci_contains_true_std)\n",
      "        875\n",
      "        \n",
      "        Rather than writing a loop, we can also determine the confidence intervals\n",
      "        for all 1000 samples at once.\n",
      "        \n",
      "        >>> data = (dist.rvs(size=(n_trials, 100), random_state=rng),)\n",
      "        >>> res = bootstrap(data, np.std, axis=-1, confidence_level=0.9,\n",
      "        ...                 n_resamples=1000, random_state=rng)\n",
      "        >>> ci_l, ci_u = res.confidence_interval\n",
      "        \n",
      "        Here, `ci_l` and `ci_u` contain the confidence interval for each of the\n",
      "        ``n_trials = 1000`` samples.\n",
      "        \n",
      "        >>> print(ci_l[995:])\n",
      "        [3.77729695 3.75090233 3.45829131 3.34078217 3.48072829]\n",
      "        >>> print(ci_u[995:])\n",
      "        [4.88316666 4.86924034 4.32032996 4.2822427  4.59360598]\n",
      "        \n",
      "        And again, approximately 90% contain the true value, ``std_true = 4``.\n",
      "        \n",
      "        >>> print(np.sum((ci_l < std_true) & (std_true < ci_u)))\n",
      "        900\n",
      "        \n",
      "        `bootstrap` can also be used to estimate confidence intervals of\n",
      "        multi-sample statistics, including those calculated by hypothesis\n",
      "        tests. `scipy.stats.mood` perform's Mood's test for equal scale parameters,\n",
      "        and it returns two outputs: a statistic, and a p-value. To get a\n",
      "        confidence interval for the test statistic, we first wrap\n",
      "        `scipy.stats.mood` in a function that accepts two sample arguments,\n",
      "        accepts an `axis` keyword argument, and returns only the statistic.\n",
      "        \n",
      "        >>> from scipy.stats import mood\n",
      "        >>> def my_statistic(sample1, sample2, axis):\n",
      "        ...     statistic, _ = mood(sample1, sample2, axis=-1)\n",
      "        ...     return statistic\n",
      "        \n",
      "        Here, we use the 'percentile' method with the default 95% confidence level.\n",
      "        \n",
      "        >>> sample1 = norm.rvs(scale=1, size=100, random_state=rng)\n",
      "        >>> sample2 = norm.rvs(scale=2, size=100, random_state=rng)\n",
      "        >>> data = (sample1, sample2)\n",
      "        >>> res = bootstrap(data, my_statistic, method='basic', random_state=rng)\n",
      "        >>> print(mood(sample1, sample2)[0])  # element 0 is the statistic\n",
      "        -5.521109549096542\n",
      "        >>> print(res.confidence_interval)\n",
      "        ConfidenceInterval(low=-7.255994487314675, high=-4.016202624747605)\n",
      "        \n",
      "        The bootstrap estimate of the standard error is also available.\n",
      "        \n",
      "        >>> print(res.standard_error)\n",
      "        0.8344963846318795\n",
      "        \n",
      "        Paired-sample statistics work, too. For example, consider the Pearson\n",
      "        correlation coefficient.\n",
      "        \n",
      "        >>> from scipy.stats import pearsonr\n",
      "        >>> n = 100\n",
      "        >>> x = np.linspace(0, 10, n)\n",
      "        >>> y = x + rng.uniform(size=n)\n",
      "        >>> print(pearsonr(x, y)[0])  # element 0 is the statistic\n",
      "        0.9962357936065914\n",
      "        \n",
      "        We wrap `pearsonr` so that it returns only the statistic.\n",
      "        \n",
      "        >>> def my_statistic(x, y):\n",
      "        ...     return pearsonr(x, y)[0]\n",
      "        \n",
      "        We call `bootstrap` using ``paired=True``.\n",
      "        Also, since ``my_statistic`` isn't vectorized to calculate the statistic\n",
      "        along a given axis, we pass in ``vectorized=False``.\n",
      "        \n",
      "        >>> res = bootstrap((x, y), my_statistic, vectorized=False, paired=True,\n",
      "        ...                 random_state=rng)\n",
      "        >>> print(res.confidence_interval)\n",
      "        ConfidenceInterval(low=0.9950085825848624, high=0.9971212407917498)\n",
      "    \n",
      "    boschloo_exact(table, alternative='two-sided', n=32)\n",
      "        Perform Boschloo's exact test on a 2x2 contingency table.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        table : array_like of ints\n",
      "            A 2x2 contingency table.  Elements should be non-negative integers.\n",
      "        \n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
      "            Please see explanations in the Notes section below.\n",
      "        \n",
      "        n : int, optional\n",
      "            Number of sampling points used in the construction of the sampling\n",
      "            method. Note that this argument will automatically be converted to\n",
      "            the next higher power of 2 since `scipy.stats.qmc.Sobol` is used to\n",
      "            select sample points. Default is 32. Must be positive. In most cases,\n",
      "            32 points is enough to reach good precision. More points comes at\n",
      "            performance cost.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ber : BoschlooExactResult\n",
      "            A result object with the following attributes.\n",
      "        \n",
      "            statistic : float\n",
      "                The statistic used in Boschloo's test; that is, the p-value\n",
      "                from Fisher's exact test.\n",
      "        \n",
      "            pvalue : float\n",
      "                P-value, the probability of obtaining a distribution at least as\n",
      "                extreme as the one that was actually observed, assuming that the\n",
      "                null hypothesis is true.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chi2_contingency : Chi-square test of independence of variables in a\n",
      "            contingency table.\n",
      "        fisher_exact : Fisher exact test on a 2x2 contingency table.\n",
      "        barnard_exact : Barnard's exact test, which is a more powerful alternative\n",
      "            than Fisher's exact test for 2x2 contingency tables.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Boschloo's test is an exact test used in the analysis of contingency\n",
      "        tables. It examines the association of two categorical variables, and\n",
      "        is a uniformly more powerful alternative to Fisher's exact test\n",
      "        for 2x2 contingency tables.\n",
      "        \n",
      "        Let's define :math:`X_0` a 2x2 matrix representing the observed sample,\n",
      "        where each column stores the binomial experiment, as in the example\n",
      "        below. Let's also define :math:`p_1, p_2` the theoretical binomial\n",
      "        probabilities for  :math:`x_{11}` and :math:`x_{12}`. When using\n",
      "        Boschloo exact test, we can assert three different null hypotheses :\n",
      "        \n",
      "        - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 < p_2`,\n",
      "          with `alternative` = \"less\"\n",
      "        \n",
      "        - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 > p_2`,\n",
      "          with `alternative` = \"greater\"\n",
      "        \n",
      "        - :math:`H_0 : p_1=p_2` versus :math:`H_1 : p_1 \\neq p_2`,\n",
      "          with `alternative` = \"two-sided\" (default one)\n",
      "        \n",
      "        Boschloo's exact test uses the p-value of Fisher's exact test as a \n",
      "        statistic, and Boschloo's p-value is the probability under the null \n",
      "        hypothesis of observing such an extreme value of this statistic.\n",
      "        \n",
      "        Boschloo's and Barnard's are both more powerful than Fisher's exact\n",
      "        test.\n",
      "        \n",
      "        .. versionadded:: 1.7.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R.D. Boschloo. \"Raised conditional level of significance for the\n",
      "           2 x 2-table when testing the equality of two probabilities\",\n",
      "           Statistica Neerlandica, 24(1), 1970\n",
      "        \n",
      "        .. [2] \"Boschloo's test\", Wikipedia,\n",
      "           https://en.wikipedia.org/wiki/Boschloo%27s_test\n",
      "        \n",
      "        .. [3] Lise M. Saari et al. \"Employee attitudes and job satisfaction\",\n",
      "           Human Resource Management, 43(4), 395-407, 2004,\n",
      "           :doi:`10.1002/hrm.20032`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In the following example, we consider the article \"Employee\n",
      "        attitudes and job satisfaction\" [3]_\n",
      "        which reports the results of a survey from 63 scientists and 117 college\n",
      "        professors. Of the 63 scientists, 31 said they were very satisfied with\n",
      "        their jobs, whereas 74 of the college professors were very satisfied\n",
      "        with their work. Is this significant evidence that college\n",
      "        professors are happier with their work than scientists?\n",
      "        The following table summarizes the data mentioned above::\n",
      "        \n",
      "                             college professors   scientists\n",
      "            Very Satisfied   74                     31\n",
      "            Dissatisfied     43                     32\n",
      "        \n",
      "        When working with statistical hypothesis testing, we usually use a\n",
      "        threshold probability or significance level upon which we decide\n",
      "        to reject the null hypothesis :math:`H_0`. Suppose we choose the common\n",
      "        significance level of 5%.\n",
      "        \n",
      "        Our alternative hypothesis is that college professors are truly more\n",
      "        satisfied with their work than scientists. Therefore, we expect\n",
      "        :math:`p_1` the proportion of very satisfied college professors to be\n",
      "        greater than :math:`p_2`, the proportion of very satisfied scientists.\n",
      "        We thus call `boschloo_exact` with the ``alternative=\"greater\"`` option:\n",
      "        \n",
      "        >>> import scipy.stats as stats\n",
      "        >>> res = stats.boschloo_exact([[74, 31], [43, 32]], alternative=\"greater\")\n",
      "        >>> res.statistic\n",
      "        0.0483...\n",
      "        >>> res.pvalue\n",
      "        0.0355...\n",
      "        \n",
      "        Under the null hypothesis that scientists are happier in their work than\n",
      "        college professors, the probability of obtaining test\n",
      "        results at least as extreme as the observed data is approximately 3.55%.\n",
      "        Since this p-value is less than our chosen significance level, we have\n",
      "        evidence to reject :math:`H_0` in favor of the alternative hypothesis.\n",
      "    \n",
      "    boxcox(x, lmbda=None, alpha=None, optimizer=None)\n",
      "        Return a dataset transformed by a Box-Cox power transformation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Input array.  Must be positive 1-dimensional.  Must not be constant.\n",
      "        lmbda : {None, scalar}, optional\n",
      "            If `lmbda` is not None, do the transformation for that value.\n",
      "            If `lmbda` is None, find the lambda that maximizes the log-likelihood\n",
      "            function and return it as the second output argument.\n",
      "        alpha : {None, float}, optional\n",
      "            If ``alpha`` is not None, return the ``100 * (1-alpha)%`` confidence\n",
      "            interval for `lmbda` as the third output argument.\n",
      "            Must be between 0.0 and 1.0.\n",
      "        optimizer : callable, optional\n",
      "            If `lmbda` is None, `optimizer` is the scalar optimizer used to find\n",
      "            the value of `lmbda` that minimizes the negative log-likelihood\n",
      "            function. `optimizer` is a callable that accepts one argument:\n",
      "        \n",
      "            fun : callable\n",
      "                The objective function, which evaluates the negative\n",
      "                log-likelihood function at a provided value of `lmbda`\n",
      "        \n",
      "            and returns an object, such as an instance of\n",
      "            `scipy.optimize.OptimizeResult`, which holds the optimal value of\n",
      "            `lmbda` in an attribute `x`.\n",
      "        \n",
      "            See the example in `boxcox_normmax` or the documentation of\n",
      "            `scipy.optimize.minimize_scalar` for more information.\n",
      "        \n",
      "            If `lmbda` is not None, `optimizer` is ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        boxcox : ndarray\n",
      "            Box-Cox power transformed array.\n",
      "        maxlog : float, optional\n",
      "            If the `lmbda` parameter is None, the second returned argument is\n",
      "            the lambda that maximizes the log-likelihood function.\n",
      "        (min_ci, max_ci) : tuple of float, optional\n",
      "            If `lmbda` parameter is None and ``alpha`` is not None, this returned\n",
      "            tuple of floats represents the minimum and maximum confidence limits\n",
      "            given ``alpha``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, boxcox_normplot, boxcox_normmax, boxcox_llf\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Box-Cox transform is given by::\n",
      "        \n",
      "            y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
      "                log(x),                  for lmbda = 0\n",
      "        \n",
      "        `boxcox` requires the input data to be positive.  Sometimes a Box-Cox\n",
      "        transformation provides a shift parameter to achieve this; `boxcox` does\n",
      "        not.  Such a shift parameter is equivalent to adding a positive constant to\n",
      "        `x` before calling `boxcox`.\n",
      "        \n",
      "        The confidence limits returned when ``alpha`` is provided give the interval\n",
      "        where:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            llf(\\hat{\\lambda}) - llf(\\lambda) < \\frac{1}{2}\\chi^2(1 - \\alpha, 1),\n",
      "        \n",
      "        with ``llf`` the log-likelihood function and :math:`\\chi^2` the chi-squared\n",
      "        function.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal of the\n",
      "        Royal Statistical Society B, 26, 211-252 (1964).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        We generate some random variates from a non-normal distribution and make a\n",
      "        probability plot for it, to show it is non-normal in the tails:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax1 = fig.add_subplot(211)\n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n",
      "        >>> ax1.set_xlabel('')\n",
      "        >>> ax1.set_title('Probplot against normal distribution')\n",
      "        \n",
      "        We now use `boxcox` to transform the data so it's closest to normal:\n",
      "        \n",
      "        >>> ax2 = fig.add_subplot(212)\n",
      "        >>> xt, _ = stats.boxcox(x)\n",
      "        >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n",
      "        >>> ax2.set_title('Probplot after Box-Cox transformation')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    boxcox_llf(lmb, data)\n",
      "        The boxcox log-likelihood function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        lmb : scalar\n",
      "            Parameter for Box-Cox transformation.  See `boxcox` for details.\n",
      "        data : array_like\n",
      "            Data to calculate Box-Cox log-likelihood for.  If `data` is\n",
      "            multi-dimensional, the log-likelihood is calculated along the first\n",
      "            axis.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        llf : float or ndarray\n",
      "            Box-Cox log-likelihood of `data` given `lmb`.  A float for 1-D `data`,\n",
      "            an array otherwise.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        boxcox, probplot, boxcox_normplot, boxcox_normmax\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Box-Cox log-likelihood function is defined here as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            llf = (\\lambda - 1) \\sum_i(\\log(x_i)) -\n",
      "                  N/2 \\log(\\sum_i (y_i - \\bar{y})^2 / N),\n",
      "        \n",
      "        where ``y`` is the Box-Cox transformed input data ``x``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
      "        \n",
      "        Generate some random variates and calculate Box-Cox log-likelihood values\n",
      "        for them for a range of ``lmbda`` values:\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = stats.loggamma.rvs(5, loc=10, size=1000, random_state=rng)\n",
      "        >>> lmbdas = np.linspace(-2, 10)\n",
      "        >>> llf = np.zeros(lmbdas.shape, dtype=float)\n",
      "        >>> for ii, lmbda in enumerate(lmbdas):\n",
      "        ...     llf[ii] = stats.boxcox_llf(lmbda, x)\n",
      "        \n",
      "        Also find the optimal lmbda value with `boxcox`:\n",
      "        \n",
      "        >>> x_most_normal, lmbda_optimal = stats.boxcox(x)\n",
      "        \n",
      "        Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n",
      "        horizontal line to check that that's really the optimum:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(lmbdas, llf, 'b.-')\n",
      "        >>> ax.axhline(stats.boxcox_llf(lmbda_optimal, x), color='r')\n",
      "        >>> ax.set_xlabel('lmbda parameter')\n",
      "        >>> ax.set_ylabel('Box-Cox log-likelihood')\n",
      "        \n",
      "        Now add some probability plots to show that where the log-likelihood is\n",
      "        maximized the data transformed with `boxcox` looks closest to normal:\n",
      "        \n",
      "        >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
      "        >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
      "        ...     xt = stats.boxcox(x, lmbda=lmbda)\n",
      "        ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
      "        ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
      "        ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
      "        ...     ax_inset.set_xticklabels([])\n",
      "        ...     ax_inset.set_yticklabels([])\n",
      "        ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    boxcox_normmax(x, brack=None, method='pearsonr', optimizer=None)\n",
      "        Compute optimal Box-Cox transform parameter for input data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        brack : 2-tuple, optional, default (-2.0, 2.0)\n",
      "             The starting interval for a downhill bracket search for the default\n",
      "             `optimize.brent` solver. Note that this is in most cases not\n",
      "             critical; the final result is allowed to be outside this bracket.\n",
      "             If `optimizer` is passed, `brack` must be None.\n",
      "        method : str, optional\n",
      "            The method to determine the optimal transform parameter (`boxcox`\n",
      "            ``lmbda`` parameter). Options are:\n",
      "        \n",
      "            'pearsonr'  (default)\n",
      "                Maximizes the Pearson correlation coefficient between\n",
      "                ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be\n",
      "                normally-distributed.\n",
      "        \n",
      "            'mle'\n",
      "                Minimizes the log-likelihood `boxcox_llf`.  This is the method used\n",
      "                in `boxcox`.\n",
      "        \n",
      "            'all'\n",
      "                Use all optimization methods available, and return all results.\n",
      "                Useful to compare different methods.\n",
      "        optimizer : callable, optional\n",
      "            `optimizer` is a callable that accepts one argument:\n",
      "        \n",
      "            fun : callable\n",
      "                The objective function to be optimized. `fun` accepts one argument,\n",
      "                the Box-Cox transform parameter `lmbda`, and returns the negative\n",
      "                log-likelihood function at the provided value. The job of `optimizer`\n",
      "                is to find the value of `lmbda` that minimizes `fun`.\n",
      "        \n",
      "            and returns an object, such as an instance of\n",
      "            `scipy.optimize.OptimizeResult`, which holds the optimal value of\n",
      "            `lmbda` in an attribute `x`.\n",
      "        \n",
      "            See the example below or the documentation of\n",
      "            `scipy.optimize.minimize_scalar` for more information.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        maxlog : float or ndarray\n",
      "            The optimal transform parameter found.  An array instead of a scalar\n",
      "            for ``method='all'``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        boxcox, boxcox_llf, boxcox_normplot, scipy.optimize.minimize_scalar\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        We can generate some data and determine the optimal ``lmbda`` in various\n",
      "        ways:\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n",
      "        >>> y, lmax_mle = stats.boxcox(x)\n",
      "        >>> lmax_pearsonr = stats.boxcox_normmax(x)\n",
      "        \n",
      "        >>> lmax_mle\n",
      "        1.4613865614008015\n",
      "        >>> lmax_pearsonr\n",
      "        1.6685004886804342\n",
      "        >>> stats.boxcox_normmax(x, method='all')\n",
      "        array([1.66850049, 1.46138656])\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.boxcox_normplot(x, -10, 10, plot=ax)\n",
      "        >>> ax.axvline(lmax_mle, color='r')\n",
      "        >>> ax.axvline(lmax_pearsonr, color='g', ls='--')\n",
      "        \n",
      "        >>> plt.show()\n",
      "        \n",
      "        Alternatively, we can define our own `optimizer` function. Suppose we\n",
      "        are only interested in values of `lmbda` on the interval [6, 7], we\n",
      "        want to use `scipy.optimize.minimize_scalar` with ``method='bounded'``,\n",
      "        and we want to use tighter tolerances when optimizing the log-likelihood\n",
      "        function. To do this, we define a function that accepts positional argument\n",
      "        `fun` and uses `scipy.optimize.minimize_scalar` to minimize `fun` subject\n",
      "        to the provided bounds and tolerances:\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> options = {'xatol': 1e-12}  # absolute tolerance on `x`\n",
      "        >>> def optimizer(fun):\n",
      "        ...     return optimize.minimize_scalar(fun, bounds=(6, 7),\n",
      "        ...                                     method=\"bounded\", options=options)\n",
      "        >>> stats.boxcox_normmax(x, optimizer=optimizer)\n",
      "        6.000...\n",
      "    \n",
      "    boxcox_normplot(x, la, lb, plot=None, N=80)\n",
      "        Compute parameters for a Box-Cox normality plot, optionally show it.\n",
      "        \n",
      "        A Box-Cox normality plot shows graphically what the best transformation\n",
      "        parameter is to use in `boxcox` to obtain a distribution that is close\n",
      "        to normal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        la, lb : scalar\n",
      "            The lower and upper bounds for the ``lmbda`` values to pass to `boxcox`\n",
      "            for Box-Cox transformations.  These are also the limits of the\n",
      "            horizontal axis of the plot if that is generated.\n",
      "        plot : object, optional\n",
      "            If given, plots the quantiles and least squares fit.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        N : int, optional\n",
      "            Number of points on the horizontal axis (equally distributed from\n",
      "            `la` to `lb`).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lmbdas : ndarray\n",
      "            The ``lmbda`` values for which a Box-Cox transform was done.\n",
      "        ppcc : ndarray\n",
      "            Probability Plot Correlelation Coefficient, as obtained from `probplot`\n",
      "            when fitting the Box-Cox transformed input `x` against a normal\n",
      "            distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, boxcox, boxcox_normmax, boxcox_llf, ppcc_max\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Even if `plot` is given, the figure is not shown or saved by\n",
      "        `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n",
      "        should be used after calling `probplot`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Generate some non-normally distributed data, and create a Box-Cox plot:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.boxcox_normplot(x, -20, 20, plot=ax)\n",
      "        \n",
      "        Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n",
      "        the same plot:\n",
      "        \n",
      "        >>> _, maxlog = stats.boxcox(x)\n",
      "        >>> ax.axvline(maxlog, color='r')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    brunnermunzel(x, y, alternative='two-sided', distribution='t', nan_policy='propagate')\n",
      "        Compute the Brunner-Munzel test on samples x and y.\n",
      "        \n",
      "        The Brunner-Munzel test is a nonparametric test of the null hypothesis that\n",
      "        when values are taken one by one from each group, the probabilities of\n",
      "        getting large values in both groups are equal.\n",
      "        Unlike the Wilcoxon-Mann-Whitney's U test, this does not require the\n",
      "        assumption of equivariance of two groups. Note that this does not assume\n",
      "        the distributions are same. This test works on two independent samples,\n",
      "        which may have different sizes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Array of samples, should be one-dimensional.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        distribution : {'t', 'normal'}, optional\n",
      "            Defines how to get the p-value.\n",
      "            The following options are available (default is 't'):\n",
      "        \n",
      "              * 't': get the p-value by t-distribution\n",
      "              * 'normal': get the p-value by standard normal distribution.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The Brunner-Munzer W statistic.\n",
      "        pvalue : float\n",
      "            p-value assuming an t distribution. One-sided or\n",
      "            two-sided, depending on the choice of `alternative` and `distribution`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        mannwhitneyu : Mann-Whitney rank test on two samples.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Brunner and Munzel recommended to estimate the p-value by t-distribution\n",
      "        when the size of data is 50 or less. If the size is lower than 10, it would\n",
      "        be better to use permuted Brunner Munzel test (see [2]_).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brunner, E. and Munzel, U. \"The nonparametric Benhrens-Fisher\n",
      "               problem: Asymptotic theory and a small-sample approximation\".\n",
      "               Biometrical Journal. Vol. 42(2000): 17-25.\n",
      "        .. [2] Neubert, K. and Brunner, E. \"A studentized permutation test for the\n",
      "               non-parametric Behrens-Fisher problem\". Computational Statistics and\n",
      "               Data Analysis. Vol. 51(2007): 5192-5204.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x1 = [1,2,1,1,1,1,1,1,1,1,2,4,1,1]\n",
      "        >>> x2 = [3,3,4,3,1,2,3,1,1,5,4]\n",
      "        >>> w, p_value = stats.brunnermunzel(x1, x2)\n",
      "        >>> w\n",
      "        3.1374674823029505\n",
      "        >>> p_value\n",
      "        0.0057862086661515377\n",
      "    \n",
      "    chi2_contingency(observed, correction=True, lambda_=None)\n",
      "        Chi-square test of independence of variables in a contingency table.\n",
      "        \n",
      "        This function computes the chi-square statistic and p-value for the\n",
      "        hypothesis test of independence of the observed frequencies in the\n",
      "        contingency table [1]_ `observed`.  The expected frequencies are computed\n",
      "        based on the marginal sums under the assumption of independence; see\n",
      "        `scipy.stats.contingency.expected_freq`.  The number of degrees of\n",
      "        freedom is (expressed using numpy functions and attributes)::\n",
      "        \n",
      "            dof = observed.size - sum(observed.shape) + observed.ndim - 1\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        observed : array_like\n",
      "            The contingency table. The table contains the observed frequencies\n",
      "            (i.e. number of occurrences) in each category.  In the two-dimensional\n",
      "            case, the table is often described as an \"R x C table\".\n",
      "        correction : bool, optional\n",
      "            If True, *and* the degrees of freedom is 1, apply Yates' correction\n",
      "            for continuity.  The effect of the correction is to adjust each\n",
      "            observed value by 0.5 towards the corresponding expected value.\n",
      "        lambda_ : float or str, optional\n",
      "            By default, the statistic computed in this test is Pearson's\n",
      "            chi-squared statistic [2]_.  `lambda_` allows a statistic from the\n",
      "            Cressie-Read power divergence family [3]_ to be used instead.  See\n",
      "            `power_divergence` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        chi2 : float\n",
      "            The test statistic.\n",
      "        p : float\n",
      "            The p-value of the test\n",
      "        dof : int\n",
      "            Degrees of freedom\n",
      "        expected : ndarray, same shape as `observed`\n",
      "            The expected frequencies, based on the marginal sums of the table.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        contingency.expected_freq\n",
      "        fisher_exact\n",
      "        chisquare\n",
      "        power_divergence\n",
      "        barnard_exact\n",
      "        boschloo_exact\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        An often quoted guideline for the validity of this calculation is that\n",
      "        the test should be used only if the observed and expected frequencies\n",
      "        in each cell are at least 5.\n",
      "        \n",
      "        This is a test for the independence of different categories of a\n",
      "        population. The test is only meaningful when the dimension of\n",
      "        `observed` is two or more.  Applying the test to a one-dimensional\n",
      "        table will always result in `expected` equal to `observed` and a\n",
      "        chi-square statistic equal to 0.\n",
      "        \n",
      "        This function does not handle masked arrays, because the calculation\n",
      "        does not make sense with missing values.\n",
      "        \n",
      "        Like stats.chisquare, this function computes a chi-square statistic;\n",
      "        the convenience this function provides is to figure out the expected\n",
      "        frequencies and degrees of freedom from the given contingency table.\n",
      "        If these were already known, and if the Yates' correction was not\n",
      "        required, one could use stats.chisquare.  That is, if one calls::\n",
      "        \n",
      "            chi2, p, dof, ex = chi2_contingency(obs, correction=False)\n",
      "        \n",
      "        then the following is true::\n",
      "        \n",
      "            (chi2, p) == stats.chisquare(obs.ravel(), f_exp=ex.ravel(),\n",
      "                                         ddof=obs.size - 1 - dof)\n",
      "        \n",
      "        The `lambda_` argument was added in version 0.13.0 of scipy.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Contingency table\",\n",
      "               https://en.wikipedia.org/wiki/Contingency_table\n",
      "        .. [2] \"Pearson's chi-squared test\",\n",
      "               https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test\n",
      "        .. [3] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
      "               Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
      "               pp. 440-464.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        A two-way example (2 x 3):\n",
      "        \n",
      "        >>> from scipy.stats import chi2_contingency\n",
      "        >>> obs = np.array([[10, 10, 20], [20, 20, 20]])\n",
      "        >>> chi2_contingency(obs)\n",
      "        (2.7777777777777777,\n",
      "         0.24935220877729619,\n",
      "         2,\n",
      "         array([[ 12.,  12.,  16.],\n",
      "                [ 18.,  18.,  24.]]))\n",
      "        \n",
      "        Perform the test using the log-likelihood ratio (i.e. the \"G-test\")\n",
      "        instead of Pearson's chi-squared statistic.\n",
      "        \n",
      "        >>> g, p, dof, expctd = chi2_contingency(obs, lambda_=\"log-likelihood\")\n",
      "        >>> g, p\n",
      "        (2.7688587616781319, 0.25046668010954165)\n",
      "        \n",
      "        A four-way example (2 x 2 x 2 x 2):\n",
      "        \n",
      "        >>> obs = np.array(\n",
      "        ...     [[[[12, 17],\n",
      "        ...        [11, 16]],\n",
      "        ...       [[11, 12],\n",
      "        ...        [15, 16]]],\n",
      "        ...      [[[23, 15],\n",
      "        ...        [30, 22]],\n",
      "        ...       [[14, 17],\n",
      "        ...        [15, 16]]]])\n",
      "        >>> chi2_contingency(obs)\n",
      "        (8.7584514426741897,\n",
      "         0.64417725029295503,\n",
      "         11,\n",
      "         array([[[[ 14.15462386,  14.15462386],\n",
      "                  [ 16.49423111,  16.49423111]],\n",
      "                 [[ 11.2461395 ,  11.2461395 ],\n",
      "                  [ 13.10500554,  13.10500554]]],\n",
      "                [[[ 19.5591166 ,  19.5591166 ],\n",
      "                  [ 22.79202844,  22.79202844]],\n",
      "                 [[ 15.54012004,  15.54012004],\n",
      "                  [ 18.10873492,  18.10873492]]]]))\n",
      "    \n",
      "    chisquare(f_obs, f_exp=None, ddof=0, axis=0)\n",
      "        Calculate a one-way chi-square test.\n",
      "        \n",
      "        The chi-square test tests the null hypothesis that the categorical data\n",
      "        has the given frequencies.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f_obs : array_like\n",
      "            Observed frequencies in each category.\n",
      "        f_exp : array_like, optional\n",
      "            Expected frequencies in each category.  By default the categories are\n",
      "            assumed to be equally likely.\n",
      "        ddof : int, optional\n",
      "            \"Delta degrees of freedom\": adjustment to the degrees of freedom\n",
      "            for the p-value.  The p-value is computed using a chi-squared\n",
      "            distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n",
      "            is the number of observed frequencies.  The default value of `ddof`\n",
      "            is 0.\n",
      "        axis : int or None, optional\n",
      "            The axis of the broadcast result of `f_obs` and `f_exp` along which to\n",
      "            apply the test.  If axis is None, all values in `f_obs` are treated\n",
      "            as a single data set.  Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        chisq : float or ndarray\n",
      "            The chi-squared test statistic.  The value is a float if `axis` is\n",
      "            None or `f_obs` and `f_exp` are 1-D.\n",
      "        p : float or ndarray\n",
      "            The p-value of the test.  The value is a float if `ddof` and the\n",
      "            return value `chisq` are scalars.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.stats.power_divergence\n",
      "        scipy.stats.fisher_exact : Fisher exact test on a 2x2 contingency table.\n",
      "        scipy.stats.barnard_exact : An unconditional exact test. An alternative\n",
      "            to chi-squared test for small sample sizes.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This test is invalid when the observed or expected frequencies in each\n",
      "        category are too small.  A typical rule is that all of the observed\n",
      "        and expected frequencies should be at least 5. According to [3]_, the\n",
      "        total number of samples is recommended to be greater than 13,\n",
      "        otherwise exact tests (such as Barnard's Exact test) should be used\n",
      "        because they do not overreject.\n",
      "        \n",
      "        Also, the sum of the observed and expected frequencies must be the same\n",
      "        for the test to be valid; `chisquare` raises an error if the sums do not\n",
      "        agree within a relative tolerance of ``1e-8``.\n",
      "        \n",
      "        The default degrees of freedom, k-1, are for the case when no parameters\n",
      "        of the distribution are estimated. If p parameters are estimated by\n",
      "        efficient maximum likelihood then the correct degrees of freedom are\n",
      "        k-1-p. If the parameters are estimated in a different way, then the\n",
      "        dof can be between k-1-p and k-1. However, it is also possible that\n",
      "        the asymptotic distribution is not chi-square, in which case this test\n",
      "        is not appropriate.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n",
      "               Statistics\". Chapter 8.\n",
      "               https://web.archive.org/web/20171022032306/http://vassarstats.net:80/textbook/ch8pt1.html\n",
      "        .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n",
      "        .. [3] Pearson, Karl. \"On the criterion that a given system of deviations from the probable\n",
      "               in the case of a correlated system of variables is such that it can be reasonably\n",
      "               supposed to have arisen from random sampling\", Philosophical Magazine. Series 5. 50\n",
      "               (1900), pp. 157-175.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        When just `f_obs` is given, it is assumed that the expected frequencies\n",
      "        are uniform and given by the mean of the observed frequencies.\n",
      "        \n",
      "        >>> from scipy.stats import chisquare\n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12])\n",
      "        (2.0, 0.84914503608460956)\n",
      "        \n",
      "        With `f_exp` the expected frequencies can be given.\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12], f_exp=[16, 16, 16, 16, 16, 8])\n",
      "        (3.5, 0.62338762774958223)\n",
      "        \n",
      "        When `f_obs` is 2-D, by default the test is applied to each column.\n",
      "        \n",
      "        >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n",
      "        >>> obs.shape\n",
      "        (6, 2)\n",
      "        >>> chisquare(obs)\n",
      "        (array([ 2.        ,  6.66666667]), array([ 0.84914504,  0.24663415]))\n",
      "        \n",
      "        By setting ``axis=None``, the test is applied to all data in the array,\n",
      "        which is equivalent to applying the test to the flattened array.\n",
      "        \n",
      "        >>> chisquare(obs, axis=None)\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        >>> chisquare(obs.ravel())\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        \n",
      "        `ddof` is the change to make to the default degrees of freedom.\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12], ddof=1)\n",
      "        (2.0, 0.73575888234288467)\n",
      "        \n",
      "        The calculation of the p-values is done by broadcasting the\n",
      "        chi-squared statistic with `ddof`.\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n",
      "        (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n",
      "        \n",
      "        `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n",
      "        shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n",
      "        `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n",
      "        statistics, we use ``axis=1``:\n",
      "        \n",
      "        >>> chisquare([16, 18, 16, 14, 12, 12],\n",
      "        ...           f_exp=[[16, 16, 16, 16, 16, 8], [8, 20, 20, 16, 12, 12]],\n",
      "        ...           axis=1)\n",
      "        (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n",
      "    \n",
      "    circmean(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate')\n",
      "        Compute the circular mean for samples in a range.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : array_like\n",
      "            Input array.\n",
      "        high : float or int, optional\n",
      "            High boundary for circular mean range.  Default is ``2*pi``.\n",
      "        low : float or int, optional\n",
      "            Low boundary for circular mean range.  Default is 0.\n",
      "        axis : int, optional\n",
      "            Axis along which means are computed.  The default is to compute\n",
      "            the mean of the flattened array.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        circmean : float\n",
      "            Circular mean.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import circmean\n",
      "        >>> circmean([0.1, 2*np.pi+0.2, 6*np.pi+0.3])\n",
      "        0.2\n",
      "        \n",
      "        >>> from scipy.stats import circmean\n",
      "        >>> circmean([0.2, 1.4, 2.6], high = 1, low = 0)\n",
      "        0.4\n",
      "    \n",
      "    circstd(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate')\n",
      "        Compute the circular standard deviation for samples assumed to be in the\n",
      "        range [low to high].\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : array_like\n",
      "            Input array.\n",
      "        high : float or int, optional\n",
      "            High boundary for circular standard deviation range.\n",
      "            Default is ``2*pi``.\n",
      "        low : float or int, optional\n",
      "            Low boundary for circular standard deviation range.  Default is 0.\n",
      "        axis : int, optional\n",
      "            Axis along which standard deviations are computed.  The default is\n",
      "            to compute the standard deviation of the flattened array.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        circstd : float\n",
      "            Circular standard deviation.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This uses a definition of circular standard deviation that in the limit of\n",
      "        small angles returns a number close to the 'linear' standard deviation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import circstd\n",
      "        >>> circstd([0, 0.1*np.pi/2, 0.001*np.pi, 0.03*np.pi/2])\n",
      "        0.063564063306\n",
      "    \n",
      "    circvar(samples, high=6.283185307179586, low=0, axis=None, nan_policy='propagate')\n",
      "        Compute the circular variance for samples assumed to be in a range.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        samples : array_like\n",
      "            Input array.\n",
      "        high : float or int, optional\n",
      "            High boundary for circular variance range.  Default is ``2*pi``.\n",
      "        low : float or int, optional\n",
      "            Low boundary for circular variance range.  Default is 0.\n",
      "        axis : int, optional\n",
      "            Axis along which variances are computed.  The default is to compute\n",
      "            the variance of the flattened array.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        circvar : float\n",
      "            Circular variance.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This uses a definition of circular variance that in the limit of small\n",
      "        angles returns a number close to the 'linear' variance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import circvar\n",
      "        >>> circvar([0, 2*np.pi/3, 5*np.pi/3])\n",
      "        2.19722457734\n",
      "    \n",
      "    combine_pvalues(pvalues, method='fisher', weights=None)\n",
      "        Combine p-values from independent tests bearing upon the same hypothesis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pvalues : array_like, 1-D\n",
      "            Array of p-values assumed to come from independent tests.\n",
      "        method : {'fisher', 'pearson', 'tippett', 'stouffer',\n",
      "                  'mudholkar_george'}, optional\n",
      "        \n",
      "            Name of method to use to combine p-values.\n",
      "            The following methods are available (default is 'fisher'):\n",
      "        \n",
      "              * 'fisher': Fisher's method (Fisher's combined probability test), the\n",
      "                sum of the logarithm of the p-values\n",
      "              * 'pearson': Pearson's method (similar to Fisher's but uses sum of the\n",
      "                complement of the p-values inside the logarithms)\n",
      "              * 'tippett': Tippett's method (minimum of p-values)\n",
      "              * 'stouffer': Stouffer's Z-score method\n",
      "              * 'mudholkar_george': the difference of Fisher's and Pearson's methods\n",
      "                divided by 2\n",
      "        weights : array_like, 1-D, optional\n",
      "            Optional array of weights used only for Stouffer's Z-score method.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic: float\n",
      "            The statistic calculated by the specified method.\n",
      "        pval: float\n",
      "            The combined p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Fisher's method (also known as Fisher's combined probability test) [1]_ uses\n",
      "        a chi-squared statistic to compute a combined p-value. The closely related\n",
      "        Stouffer's Z-score method [2]_ uses Z-scores rather than p-values. The\n",
      "        advantage of Stouffer's method is that it is straightforward to introduce\n",
      "        weights, which can make Stouffer's method more powerful than Fisher's\n",
      "        method when the p-values are from studies of different size [6]_ [7]_.\n",
      "        The Pearson's method uses :math:`log(1-p_i)` inside the sum whereas Fisher's\n",
      "        method uses :math:`log(p_i)` [4]_. For Fisher's and Pearson's method, the\n",
      "        sum of the logarithms is multiplied by -2 in the implementation. This\n",
      "        quantity has a chi-square distribution that determines the p-value. The\n",
      "        `mudholkar_george` method is the difference of the Fisher's and Pearson's\n",
      "        test statistics, each of which include the -2 factor [4]_. However, the\n",
      "        `mudholkar_george` method does not include these -2 factors. The test\n",
      "        statistic of `mudholkar_george` is the sum of logisitic random variables and\n",
      "        equation 3.6 in [3]_ is used to approximate the p-value based on Student's\n",
      "        t-distribution.\n",
      "        \n",
      "        Fisher's method may be extended to combine p-values from dependent tests\n",
      "        [5]_. Extensions such as Brown's method and Kost's method are not currently\n",
      "        implemented.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Fisher%27s_method\n",
      "        .. [2] https://en.wikipedia.org/wiki/Fisher%27s_method#Relation_to_Stouffer.27s_Z-score_method\n",
      "        .. [3] George, E. O., and G. S. Mudholkar. \"On the convolution of logistic\n",
      "               random variables.\" Metrika 30.1 (1983): 1-13.\n",
      "        .. [4] Heard, N. and Rubin-Delanchey, P. \"Choosing between methods of\n",
      "               combining p-values.\"  Biometrika 105.1 (2018): 239-246.\n",
      "        .. [5] Whitlock, M. C. \"Combining probability from independent tests: the\n",
      "               weighted Z-method is superior to Fisher's approach.\" Journal of\n",
      "               Evolutionary Biology 18, no. 5 (2005): 1368-1373.\n",
      "        .. [6] Zaykin, Dmitri V. \"Optimally weighted Z-test is a powerful method\n",
      "               for combining probabilities in meta-analysis.\" Journal of\n",
      "               Evolutionary Biology 24, no. 8 (2011): 1836-1841.\n",
      "        .. [7] https://en.wikipedia.org/wiki/Extensions_of_Fisher%27s_method\n",
      "    \n",
      "    cramervonmises(rvs, cdf, args=())\n",
      "        Perform the one-sample Cramér-von Mises test for goodness of fit.\n",
      "        \n",
      "        This performs a test of the goodness of fit of a cumulative distribution\n",
      "        function (cdf) :math:`F` compared to the empirical distribution function\n",
      "        :math:`F_n` of observed random variates :math:`X_1, ..., X_n` that are\n",
      "        assumed to be independent and identically distributed ([1]_).\n",
      "        The null hypothesis is that the :math:`X_i` have cumulative distribution\n",
      "        :math:`F`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rvs : array_like\n",
      "            A 1-D array of observed values of the random variables :math:`X_i`.\n",
      "        cdf : str or callable\n",
      "            The cumulative distribution function :math:`F` to test the\n",
      "            observations against. If a string, it should be the name of a\n",
      "            distribution in `scipy.stats`. If a callable, that callable is used\n",
      "            to calculate the cdf: ``cdf(x, *args) -> float``.\n",
      "        args : tuple, optional\n",
      "            Distribution parameters. These are assumed to be known; see Notes.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : object with attributes\n",
      "            statistic : float\n",
      "                Cramér-von Mises statistic.\n",
      "            pvalue : float\n",
      "                The p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstest, cramervonmises_2samp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        The p-value relies on the approximation given by equation 1.8 in [2]_.\n",
      "        It is important to keep in mind that the p-value is only accurate if\n",
      "        one tests a simple hypothesis, i.e. the parameters of the reference\n",
      "        distribution are known. If the parameters are estimated from the data\n",
      "        (composite hypothesis), the computed p-value is not reliable.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cramér-von Mises criterion, Wikipedia,\n",
      "               https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93von_Mises_criterion\n",
      "        .. [2] Csorgo, S. and Faraway, J. (1996). The Exact and Asymptotic\n",
      "               Distribution of Cramér-von Mises Statistics. Journal of the\n",
      "               Royal Statistical Society, pp. 221-234.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Suppose we wish to test whether data generated by ``scipy.stats.norm.rvs``\n",
      "        were, in fact, drawn from the standard normal distribution. We choose a\n",
      "        significance level of alpha=0.05.\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = stats.norm.rvs(size=500, random_state=rng)\n",
      "        >>> res = stats.cramervonmises(x, 'norm')\n",
      "        >>> res.statistic, res.pvalue\n",
      "        (0.49121480855028343, 0.04189256516661377)\n",
      "        \n",
      "        The p-value 0.79 exceeds our chosen significance level, so we do not\n",
      "        reject the null hypothesis that the observed sample is drawn from the\n",
      "        standard normal distribution.\n",
      "        \n",
      "        Now suppose we wish to check whether the same samples shifted by 2.1 is\n",
      "        consistent with being drawn from a normal distribution with a mean of 2.\n",
      "        \n",
      "        >>> y = x + 2.1\n",
      "        >>> res = stats.cramervonmises(y, 'norm', args=(2,))\n",
      "        >>> res.statistic, res.pvalue\n",
      "        (0.07400330012187435, 0.7274595666160468)\n",
      "        \n",
      "        Here we have used the `args` keyword to specify the mean (``loc``)\n",
      "        of the normal distribution to test the data against. This is equivalent\n",
      "        to the following, in which we create a frozen normal distribution with\n",
      "        mean 2.1, then pass its ``cdf`` method as an argument.\n",
      "        \n",
      "        >>> frozen_dist = stats.norm(loc=2)\n",
      "        >>> res = stats.cramervonmises(y, frozen_dist.cdf)\n",
      "        >>> res.statistic, res.pvalue\n",
      "        (0.07400330012187435, 0.7274595666160468)\n",
      "        \n",
      "        In either case, we would reject the null hypothesis that the observed\n",
      "        sample is drawn from a normal distribution with a mean of 2 (and default\n",
      "        variance of 1) because the p-value 0.04 is less than our chosen\n",
      "        significance level.\n",
      "    \n",
      "    cramervonmises_2samp(x, y, method='auto')\n",
      "        Perform the two-sample Cramér-von Mises test for goodness of fit.\n",
      "        \n",
      "        This is the two-sample version of the Cramér-von Mises test ([1]_):\n",
      "        for two independent samples :math:`X_1, ..., X_n` and\n",
      "        :math:`Y_1, ..., Y_m`, the null hypothesis is that the samples\n",
      "        come from the same (unspecified) continuous distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            A 1-D array of observed values of the random variables :math:`X_i`.\n",
      "        y : array_like\n",
      "            A 1-D array of observed values of the random variables :math:`Y_i`.\n",
      "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
      "            The method used to compute the p-value, see Notes for details.\n",
      "            The default is 'auto'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : object with attributes\n",
      "            statistic : float\n",
      "                Cramér-von Mises statistic.\n",
      "            pvalue : float\n",
      "                The p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cramervonmises, anderson_ksamp, epps_singleton_2samp, ks_2samp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 1.7.0\n",
      "        \n",
      "        The statistic is computed according to equation 9 in [2]_. The\n",
      "        calculation of the p-value depends on the keyword `method`:\n",
      "        \n",
      "        - ``asymptotic``: The p-value is approximated by using the limiting\n",
      "          distribution of the test statistic.\n",
      "        - ``exact``: The exact p-value is computed by enumerating all\n",
      "          possible combinations of the test statistic, see [2]_.\n",
      "        \n",
      "        The exact calculation will be very slow even for moderate sample\n",
      "        sizes as the number of combinations increases rapidly with the\n",
      "        size of the samples. If ``method=='auto'``, the exact approach\n",
      "        is used if both samples contain less than 10 observations,\n",
      "        otherwise the asymptotic distribution is used.\n",
      "        \n",
      "        If the underlying distribution is not continuous, the p-value is likely to\n",
      "        be conservative (Section 6.2 in [3]_). When ranking the data to compute\n",
      "        the test statistic, midranks are used if there are ties.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Cramer-von_Mises_criterion\n",
      "        .. [2] Anderson, T.W. (1962). On the distribution of the two-sample\n",
      "               Cramer-von-Mises criterion. The Annals of Mathematical\n",
      "               Statistics, pp. 1148-1159.\n",
      "        .. [3] Conover, W.J., Practical Nonparametric Statistics, 1971.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Suppose we wish to test whether two samples generated by\n",
      "        ``scipy.stats.norm.rvs`` have the same distribution. We choose a\n",
      "        significance level of alpha=0.05.\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = stats.norm.rvs(size=100, random_state=rng)\n",
      "        >>> y = stats.norm.rvs(size=70, random_state=rng)\n",
      "        >>> res = stats.cramervonmises_2samp(x, y)\n",
      "        >>> res.statistic, res.pvalue\n",
      "        (0.29376470588235293, 0.1412873014573014)\n",
      "        \n",
      "        The p-value exceeds our chosen significance level, so we do not\n",
      "        reject the null hypothesis that the observed samples are drawn from the\n",
      "        same distribution.\n",
      "        \n",
      "        For small sample sizes, one can compute the exact p-values:\n",
      "        \n",
      "        >>> x = stats.norm.rvs(size=7, random_state=rng)\n",
      "        >>> y = stats.t.rvs(df=2, size=6, random_state=rng)\n",
      "        >>> res = stats.cramervonmises_2samp(x, y, method='exact')\n",
      "        >>> res.statistic, res.pvalue\n",
      "        (0.197802197802198, 0.31643356643356646)\n",
      "        \n",
      "        The p-value based on the asymptotic distribution is a good approximation\n",
      "        even though the sample size is small.\n",
      "        \n",
      "        >>> res = stats.cramervonmises_2samp(x, y, method='asymptotic')\n",
      "        >>> res.statistic, res.pvalue\n",
      "        (0.197802197802198, 0.2966041181527128)\n",
      "        \n",
      "        Independent of the method, one would not reject the null hypothesis at the\n",
      "        chosen significance level in this example.\n",
      "    \n",
      "    cumfreq(a, numbins=10, defaultreallimits=None, weights=None)\n",
      "        Return a cumulative frequency histogram, using the histogram function.\n",
      "        \n",
      "        A cumulative histogram is a mapping that counts the cumulative number of\n",
      "        observations in all of the bins up to the specified bin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        numbins : int, optional\n",
      "            The number of bins to use for the histogram. Default is 10.\n",
      "        defaultreallimits : tuple (lower, upper), optional\n",
      "            The lower and upper values for the range of the histogram.\n",
      "            If no value is given, a range slightly larger than the range of the\n",
      "            values in `a` is used. Specifically ``(a.min() - s, a.max() + s)``,\n",
      "            where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n",
      "        weights : array_like, optional\n",
      "            The weights for each value in `a`. Default is None, which gives each\n",
      "            value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        cumcount : ndarray\n",
      "            Binned values of cumulative frequency.\n",
      "        lowerlimit : float\n",
      "            Lower real limit\n",
      "        binsize : float\n",
      "            Width of each bin.\n",
      "        extrapoints : int\n",
      "            Extra points.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from numpy.random import default_rng\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = default_rng()\n",
      "        >>> x = [1, 4, 2, 1, 3, 1]\n",
      "        >>> res = stats.cumfreq(x, numbins=4, defaultreallimits=(1.5, 5))\n",
      "        >>> res.cumcount\n",
      "        array([ 1.,  2.,  3.,  3.])\n",
      "        >>> res.extrapoints\n",
      "        3\n",
      "        \n",
      "        Create a normal distribution with 1000 random values\n",
      "        \n",
      "        >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n",
      "        \n",
      "        Calculate cumulative frequencies\n",
      "        \n",
      "        >>> res = stats.cumfreq(samples, numbins=25)\n",
      "        \n",
      "        Calculate space of values for x\n",
      "        \n",
      "        >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.cumcount.size,\n",
      "        ...                                  res.cumcount.size)\n",
      "        \n",
      "        Plot histogram and cumulative histogram\n",
      "        \n",
      "        >>> fig = plt.figure(figsize=(10, 4))\n",
      "        >>> ax1 = fig.add_subplot(1, 2, 1)\n",
      "        >>> ax2 = fig.add_subplot(1, 2, 2)\n",
      "        >>> ax1.hist(samples, bins=25)\n",
      "        >>> ax1.set_title('Histogram')\n",
      "        >>> ax2.bar(x, res.cumcount, width=res.binsize)\n",
      "        >>> ax2.set_title('Cumulative histogram')\n",
      "        >>> ax2.set_xlim([x.min(), x.max()])\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    describe(a, axis=0, ddof=1, bias=True, nan_policy='propagate')\n",
      "        Compute several descriptive statistics of the passed array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input data.\n",
      "        axis : int or None, optional\n",
      "            Axis along which statistics are calculated. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom (only for variance).  Default is 1.\n",
      "        bias : bool, optional\n",
      "            If False, then the skewness and kurtosis calculations are corrected\n",
      "            for statistical bias.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nobs : int or ndarray of ints\n",
      "            Number of observations (length of data along `axis`).\n",
      "            When 'omit' is chosen as nan_policy, the length along each axis\n",
      "            slice is counted separately.\n",
      "        minmax: tuple of ndarrays or floats\n",
      "            Minimum and maximum value of `a` along the given axis.\n",
      "        mean : ndarray or float\n",
      "            Arithmetic mean of `a` along the given axis.\n",
      "        variance : ndarray or float\n",
      "            Unbiased variance of `a` along the given axis; denominator is number\n",
      "            of observations minus one.\n",
      "        skewness : ndarray or float\n",
      "            Skewness of `a` along the given axis, based on moment calculations\n",
      "            with denominator equal to the number of observations, i.e. no degrees\n",
      "            of freedom correction.\n",
      "        kurtosis : ndarray or float\n",
      "            Kurtosis (Fisher) of `a` along the given axis.  The kurtosis is\n",
      "            normalized so that it is zero for the normal distribution.  No\n",
      "            degrees of freedom are used.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        skew, kurtosis\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(10)\n",
      "        >>> stats.describe(a)\n",
      "        DescribeResult(nobs=10, minmax=(0, 9), mean=4.5,\n",
      "                       variance=9.166666666666666, skewness=0.0,\n",
      "                       kurtosis=-1.2242424242424244)\n",
      "        >>> b = [[1, 2], [3, 4]]\n",
      "        >>> stats.describe(b)\n",
      "        DescribeResult(nobs=2, minmax=(array([1, 2]), array([3, 4])),\n",
      "                       mean=array([2., 3.]), variance=array([2., 2.]),\n",
      "                       skewness=array([0., 0.]), kurtosis=array([-2., -2.]))\n",
      "    \n",
      "    differential_entropy(values: 'np.typing.ArrayLike', *, window_length: 'Optional[int]' = None, base: 'Optional[float]' = None, axis: 'int' = 0, method: 'str' = 'auto') -> 'Union[np.number, np.ndarray]'\n",
      "        Given a sample of a distribution, estimate the differential entropy.\n",
      "        \n",
      "        Several estimation methods are available using the `method` parameter. By\n",
      "        default, a method is selected based the size of the sample.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        values : sequence\n",
      "            Sample from a continuous distribution.\n",
      "        window_length : int, optional\n",
      "            Window length for computing Vasicek estimate. Must be an integer\n",
      "            between 1 and half of the sample size. If ``None`` (the default), it\n",
      "            uses the heuristic value\n",
      "        \n",
      "            .. math::\n",
      "                \\left \\lfloor \\sqrt{n} + 0.5 \\right \\rfloor\n",
      "        \n",
      "            where :math:`n` is the sample size. This heuristic was originally\n",
      "            proposed in [2]_ and has become common in the literature.\n",
      "        base : float, optional\n",
      "            The logarithmic base to use, defaults to ``e`` (natural logarithm).\n",
      "        axis : int, optional\n",
      "            The axis along which the differential entropy is calculated.\n",
      "            Default is 0.\n",
      "        method : {'vasicek', 'van es', 'ebrahimi', 'correa', 'auto'}, optional\n",
      "            The method used to estimate the differential entropy from the sample.\n",
      "            Default is ``'auto'``.  See Notes for more information.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        entropy : float\n",
      "            The calculated differential entropy.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function will converge to the true differential entropy in the limit\n",
      "        \n",
      "        .. math::\n",
      "            n \\to \\infty, \\quad m \\to \\infty, \\quad \\frac{m}{n} \\to 0\n",
      "        \n",
      "        The optimal choice of ``window_length`` for a given sample size depends on\n",
      "        the (unknown) distribution. Typically, the smoother the density of the\n",
      "        distribution, the larger the optimal value of ``window_length`` [1]_.\n",
      "        \n",
      "        The following options are available for the `method` parameter.\n",
      "        \n",
      "        * ``'vasicek'`` uses the estimator presented in [1]_. This is\n",
      "          one of the first and most influential estimators of differential entropy.\n",
      "        * ``'van es'`` uses the bias-corrected estimator presented in [3]_, which\n",
      "          is not only consistent but, under some conditions, asymptotically normal.\n",
      "        * ``'ebrahimi'`` uses an estimator presented in [4]_, which was shown\n",
      "          in simulation to have smaller bias and mean squared error than\n",
      "          the Vasicek estimator.\n",
      "        * ``'correa'`` uses the estimator presented in [5]_ based on local linear\n",
      "          regression. In a simulation study, it had consistently smaller mean\n",
      "          square error than the Vasiceck estimator, but it is more expensive to\n",
      "          compute.\n",
      "        * ``'auto'`` selects the method automatically (default). Currently,\n",
      "          this selects ``'van es'`` for very small samples (<10), ``'ebrahimi'``\n",
      "          for moderate sample sizes (11-1000), and ``'vasicek'`` for larger\n",
      "          samples, but this behavior is subject to change in future versions.\n",
      "        \n",
      "        All estimators are implemented as described in [6]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Vasicek, O. (1976). A test for normality based on sample entropy.\n",
      "               Journal of the Royal Statistical Society:\n",
      "               Series B (Methodological), 38(1), 54-59.\n",
      "        .. [2] Crzcgorzewski, P., & Wirczorkowski, R. (1999). Entropy-based\n",
      "               goodness-of-fit test for exponentiality. Communications in\n",
      "               Statistics-Theory and Methods, 28(5), 1183-1202.\n",
      "        .. [3] Van Es, B. (1992). Estimating functionals related to a density by a\n",
      "               class of statistics based on spacings. Scandinavian Journal of\n",
      "               Statistics, 61-72.\n",
      "        .. [4] Ebrahimi, N., Pflughoeft, K., & Soofi, E. S. (1994). Two measures\n",
      "               of sample entropy. Statistics & Probability Letters, 20(3), 225-234.\n",
      "        .. [5] Correa, J. C. (1995). A new estimator of entropy. Communications\n",
      "               in Statistics-Theory and Methods, 24(10), 2439-2449.\n",
      "        .. [6] Noughabi, H. A. (2015). Entropy Estimation Using Numerical Methods.\n",
      "               Annals of Data Science, 2(2), 231-241.\n",
      "               https://link.springer.com/article/10.1007/s40745-015-0045-9\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import differential_entropy, norm\n",
      "        \n",
      "        Entropy of a standard normal distribution:\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> values = rng.standard_normal(100)\n",
      "        >>> differential_entropy(values)\n",
      "        1.3407817436640392\n",
      "        \n",
      "        Compare with the true entropy:\n",
      "        \n",
      "        >>> float(norm.entropy())\n",
      "        1.4189385332046727\n",
      "        \n",
      "        For several sample sizes between 5 and 1000, compare the accuracy of\n",
      "        the ``'vasicek'``, ``'van es'``, and ``'ebrahimi'`` methods. Specifically,\n",
      "        compare the root mean squared error (over 1000 trials) between the estimate\n",
      "        and the true differential entropy of the distribution.\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>>\n",
      "        >>>\n",
      "        >>> def rmse(res, expected):\n",
      "        ...     '''Root mean squared error'''\n",
      "        ...     return np.sqrt(np.mean((res - expected)**2))\n",
      "        >>>\n",
      "        >>>\n",
      "        >>> a, b = np.log10(5), np.log10(1000)\n",
      "        >>> ns = np.round(np.logspace(a, b, 10)).astype(int)\n",
      "        >>> reps = 1000  # number of repetitions for each sample size\n",
      "        >>> expected = stats.expon.entropy()\n",
      "        >>>\n",
      "        >>> method_errors = {'vasicek': [], 'van es': [], 'ebrahimi': []}\n",
      "        >>> for method in method_errors:\n",
      "        ...     for n in ns:\n",
      "        ...        rvs = stats.expon.rvs(size=(reps, n), random_state=rng)\n",
      "        ...        res = stats.differential_entropy(rvs, method=method, axis=-1)\n",
      "        ...        error = rmse(res, expected)\n",
      "        ...        method_errors[method].append(error)\n",
      "        >>>\n",
      "        >>> for method, errors in method_errors.items():\n",
      "        ...     plt.loglog(ns, errors, label=method)\n",
      "        >>>\n",
      "        >>> plt.legend()\n",
      "        >>> plt.xlabel('sample size')\n",
      "        >>> plt.ylabel('RMSE (1000 trials)')\n",
      "        >>> plt.title('Entropy Estimator Error (Exponential Distribution)')\n",
      "    \n",
      "    energy_distance(u_values, v_values, u_weights=None, v_weights=None)\n",
      "        Compute the energy distance between two 1D distributions.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u_values, v_values : array_like\n",
      "            Values observed in the (empirical) distribution.\n",
      "        u_weights, v_weights : array_like, optional\n",
      "            Weight for each value. If unspecified, each value is assigned the same\n",
      "            weight.\n",
      "            `u_weights` (resp. `v_weights`) must have the same length as\n",
      "            `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
      "            must still be positive and finite so that the weights can be normalized\n",
      "            to sum to 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance : float\n",
      "            The computed distance between the distributions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The energy distance between two distributions :math:`u` and :math:`v`, whose\n",
      "        respective CDFs are :math:`U` and :math:`V`, equals to:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D(u, v) = \\left( 2\\mathbb E|X - Y| - \\mathbb E|X - X'| -\n",
      "            \\mathbb E|Y - Y'| \\right)^{1/2}\n",
      "        \n",
      "        where :math:`X` and :math:`X'` (resp. :math:`Y` and :math:`Y'`) are\n",
      "        independent random variables whose probability distribution is :math:`u`\n",
      "        (resp. :math:`v`).\n",
      "        \n",
      "        As shown in [2]_, for one-dimensional real-valued variables, the energy\n",
      "        distance is linked to the non-distribution-free version of the Cramér-von\n",
      "        Mises distance:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D(u, v) = \\sqrt{2} l_2(u, v) = \\left( 2 \\int_{-\\infty}^{+\\infty} (U-V)^2\n",
      "            \\right)^{1/2}\n",
      "        \n",
      "        Note that the common Cramér-von Mises criterion uses the distribution-free\n",
      "        version of the distance. See [2]_ (section 2), for more details about both\n",
      "        versions of the distance.\n",
      "        \n",
      "        The input distributions can be empirical, therefore coming from samples\n",
      "        whose values are effectively inputs of the function, or they can be seen as\n",
      "        generalized functions, in which case they are weighted sums of Dirac delta\n",
      "        functions located at the specified values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Energy distance\", https://en.wikipedia.org/wiki/Energy_distance\n",
      "        .. [2] Szekely \"E-statistics: The energy of statistical samples.\" Bowling\n",
      "               Green State University, Department of Mathematics and Statistics,\n",
      "               Technical Report 02-16 (2002).\n",
      "        .. [3] Rizzo, Szekely \"Energy distance.\" Wiley Interdisciplinary Reviews:\n",
      "               Computational Statistics, 8(1):27-38 (2015).\n",
      "        .. [4] Bellemare, Danihelka, Dabney, Mohamed, Lakshminarayanan, Hoyer,\n",
      "               Munos \"The Cramer Distance as a Solution to Biased Wasserstein\n",
      "               Gradients\" (2017). :arXiv:`1705.10743`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import energy_distance\n",
      "        >>> energy_distance([0], [2])\n",
      "        2.0000000000000004\n",
      "        >>> energy_distance([0, 8], [0, 8], [3, 1], [2, 2])\n",
      "        1.0000000000000002\n",
      "        >>> energy_distance([0.7, 7.4, 2.4, 6.8], [1.4, 8. ],\n",
      "        ...                 [2.1, 4.2, 7.4, 8. ], [7.6, 8.8])\n",
      "        0.88003340976158217\n",
      "    \n",
      "    entropy(pk, qk=None, base=None, axis=0)\n",
      "        Calculate the entropy of a distribution for given probability values.\n",
      "        \n",
      "        If only probabilities `pk` are given, the entropy is calculated as\n",
      "        ``S = -sum(pk * log(pk), axis=axis)``.\n",
      "        \n",
      "        If `qk` is not None, then compute the Kullback-Leibler divergence\n",
      "        ``S = sum(pk * log(pk / qk), axis=axis)``.\n",
      "        \n",
      "        This routine will normalize `pk` and `qk` if they don't sum to 1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pk : sequence\n",
      "            Defines the (discrete) distribution. ``pk[i]`` is the (possibly\n",
      "            unnormalized) probability of event ``i``.\n",
      "        qk : sequence, optional\n",
      "            Sequence against which the relative entropy is computed. Should be in\n",
      "            the same format as `pk`.\n",
      "        base : float, optional\n",
      "            The logarithmic base to use, defaults to ``e`` (natural logarithm).\n",
      "        axis: int, optional\n",
      "            The axis along which the entropy is calculated. Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        S : float\n",
      "            The calculated entropy.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from scipy.stats import entropy\n",
      "        \n",
      "        Bernoulli trial with different p.\n",
      "        The outcome of a fair coin is the most uncertain:\n",
      "        \n",
      "        >>> entropy([1/2, 1/2], base=2)\n",
      "        1.0\n",
      "        \n",
      "        The outcome of a biased coin is less uncertain:\n",
      "        \n",
      "        >>> entropy([9/10, 1/10], base=2)\n",
      "        0.46899559358928117\n",
      "        \n",
      "        Relative entropy:\n",
      "        \n",
      "        >>> entropy([1/2, 1/2], qk=[9/10, 1/10])\n",
      "        0.5108256237659907\n",
      "    \n",
      "    epps_singleton_2samp(x, y, t=(0.4, 0.8))\n",
      "        Compute the Epps-Singleton (ES) test statistic.\n",
      "        \n",
      "        Test the null hypothesis that two samples have the same underlying\n",
      "        probability distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like\n",
      "            The two samples of observations to be tested. Input must not have more\n",
      "            than one dimension. Samples can have different lengths.\n",
      "        t : array-like, optional\n",
      "            The points (t1, ..., tn) where the empirical characteristic function is\n",
      "            to be evaluated. It should be positive distinct numbers. The default\n",
      "            value (0.4, 0.8) is proposed in [1]_. Input must not have more than\n",
      "            one dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The associated p-value based on the asymptotic chi2-distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp, anderson_ksamp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Testing whether two samples are generated by the same underlying\n",
      "        distribution is a classical question in statistics. A widely used test is\n",
      "        the Kolmogorov-Smirnov (KS) test which relies on the empirical\n",
      "        distribution function. Epps and Singleton introduce a test based on the\n",
      "        empirical characteristic function in [1]_.\n",
      "        \n",
      "        One advantage of the ES test compared to the KS test is that is does\n",
      "        not assume a continuous distribution. In [1]_, the authors conclude\n",
      "        that the test also has a higher power than the KS test in many\n",
      "        examples. They recommend the use of the ES test for discrete samples as\n",
      "        well as continuous samples with at least 25 observations each, whereas\n",
      "        `anderson_ksamp` is recommended for smaller sample sizes in the\n",
      "        continuous case.\n",
      "        \n",
      "        The p-value is computed from the asymptotic distribution of the test\n",
      "        statistic which follows a `chi2` distribution. If the sample size of both\n",
      "        `x` and `y` is below 25, the small sample correction proposed in [1]_ is\n",
      "        applied to the test statistic.\n",
      "        \n",
      "        The default values of `t` are determined in [1]_ by considering\n",
      "        various distributions and finding good values that lead to a high power\n",
      "        of the test in general. Table III in [1]_ gives the optimal values for\n",
      "        the distributions tested in that study. The values of `t` are scaled by\n",
      "        the semi-interquartile range in the implementation, see [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] T. W. Epps and K. J. Singleton, \"An omnibus test for the two-sample\n",
      "           problem using the empirical characteristic function\", Journal of\n",
      "           Statistical Computation and Simulation 26, p. 177--203, 1986.\n",
      "        \n",
      "        .. [2] S. J. Goerg and J. Kaiser, \"Nonparametric testing of distributions\n",
      "           - the Epps-Singleton two-sample test using the empirical characteristic\n",
      "           function\", The Stata Journal 9(3), p. 454--465, 2009.\n",
      "    \n",
      "    f_oneway(*args, axis=0)\n",
      "        Perform one-way ANOVA.\n",
      "        \n",
      "        The one-way ANOVA tests the null hypothesis that two or more groups have\n",
      "        the same population mean.  The test is applied to samples from two or\n",
      "        more groups, possibly with differing sizes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            The sample measurements for each group.  There must be at least\n",
      "            two arguments.  If the arrays are multidimensional, then all the\n",
      "            dimensions of the array must be the same except for `axis`.\n",
      "        axis : int, optional\n",
      "            Axis of the input arrays along which the test is applied.\n",
      "            Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The computed F statistic of the test.\n",
      "        pvalue : float\n",
      "            The associated p-value from the F distribution.\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        F_onewayConstantInputWarning\n",
      "            Raised if each of the input arrays is constant array.\n",
      "            In this case the F statistic is either infinite or isn't defined,\n",
      "            so ``np.inf`` or ``np.nan`` is returned.\n",
      "        \n",
      "        F_onewayBadInputSizesWarning\n",
      "            Raised if the length of any input array is 0, or if all the input\n",
      "            arrays have length 1.  ``np.nan`` is returned for the F statistic\n",
      "            and the p-value in these cases.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The ANOVA test has important assumptions that must be satisfied in order\n",
      "        for the associated p-value to be valid.\n",
      "        \n",
      "        1. The samples are independent.\n",
      "        2. Each sample is from a normally distributed population.\n",
      "        3. The population standard deviations of the groups are all equal.  This\n",
      "           property is known as homoscedasticity.\n",
      "        \n",
      "        If these assumptions are not true for a given set of data, it may still\n",
      "        be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) or\n",
      "        the Alexander-Govern test (`scipy.stats.alexandergovern`) although with\n",
      "        some loss of power.\n",
      "        \n",
      "        The length of each group must be at least one, and there must be at\n",
      "        least one group with length greater than one.  If these conditions\n",
      "        are not satisfied, a warning is generated and (``np.nan``, ``np.nan``)\n",
      "        is returned.\n",
      "        \n",
      "        If each group contains constant values, and there exist at least two\n",
      "        groups with different values, the function generates a warning and\n",
      "        returns (``np.inf``, 0).\n",
      "        \n",
      "        If all values in all groups are the same, function generates a warning\n",
      "        and returns (``np.nan``, ``np.nan``).\n",
      "        \n",
      "        The algorithm is from Heiman [2]_, pp.394-7.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Lowry, \"Concepts and Applications of Inferential Statistics\",\n",
      "               Chapter 14, 2014, http://vassarstats.net/textbook/\n",
      "        \n",
      "        .. [2] G.W. Heiman, \"Understanding research methods and statistics: An\n",
      "               integrated introduction for psychology\", Houghton, Mifflin and\n",
      "               Company, 2001.\n",
      "        \n",
      "        .. [3] G.H. McDonald, \"Handbook of Biological Statistics\", One-way ANOVA.\n",
      "               http://www.biostathandbook.com/onewayanova.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import f_oneway\n",
      "        \n",
      "        Here are some data [3]_ on a shell measurement (the length of the anterior\n",
      "        adductor muscle scar, standardized by dividing by length) in the mussel\n",
      "        Mytilus trossulus from five locations: Tillamook, Oregon; Newport, Oregon;\n",
      "        Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n",
      "        much larger data set used in McDonald et al. (1991).\n",
      "        \n",
      "        >>> tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735,\n",
      "        ...              0.0659, 0.0923, 0.0836]\n",
      "        >>> newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835,\n",
      "        ...            0.0725]\n",
      "        >>> petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n",
      "        >>> magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764,\n",
      "        ...            0.0689]\n",
      "        >>> tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]\n",
      "        >>> f_oneway(tillamook, newport, petersburg, magadan, tvarminne)\n",
      "        F_onewayResult(statistic=7.121019471642447, pvalue=0.0002812242314534544)\n",
      "        \n",
      "        `f_oneway` accepts multidimensional input arrays.  When the inputs\n",
      "        are multidimensional and `axis` is not given, the test is performed\n",
      "        along the first axis of the input arrays.  For the following data, the\n",
      "        test is performed three times, once for each column.\n",
      "        \n",
      "        >>> a = np.array([[9.87, 9.03, 6.81],\n",
      "        ...               [7.18, 8.35, 7.00],\n",
      "        ...               [8.39, 7.58, 7.68],\n",
      "        ...               [7.45, 6.33, 9.35],\n",
      "        ...               [6.41, 7.10, 9.33],\n",
      "        ...               [8.00, 8.24, 8.44]])\n",
      "        >>> b = np.array([[6.35, 7.30, 7.16],\n",
      "        ...               [6.65, 6.68, 7.63],\n",
      "        ...               [5.72, 7.73, 6.72],\n",
      "        ...               [7.01, 9.19, 7.41],\n",
      "        ...               [7.75, 7.87, 8.30],\n",
      "        ...               [6.90, 7.97, 6.97]])\n",
      "        >>> c = np.array([[3.31, 8.77, 1.01],\n",
      "        ...               [8.25, 3.24, 3.62],\n",
      "        ...               [6.32, 8.81, 5.19],\n",
      "        ...               [7.48, 8.83, 8.91],\n",
      "        ...               [8.59, 6.01, 6.07],\n",
      "        ...               [3.07, 9.72, 7.48]])\n",
      "        >>> F, p = f_oneway(a, b, c)\n",
      "        >>> F\n",
      "        array([1.75676344, 0.03701228, 3.76439349])\n",
      "        >>> p\n",
      "        array([0.20630784, 0.96375203, 0.04733157])\n",
      "    \n",
      "    find_repeats(arr)\n",
      "        Find repeats and repeat counts.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arr : array_like\n",
      "            Input array. This is cast to float64.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        values : ndarray\n",
      "            The unique values from the (flattened) input that are repeated.\n",
      "        \n",
      "        counts : ndarray\n",
      "            Number of times the corresponding 'value' is repeated.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In numpy >= 1.9 `numpy.unique` provides similar functionality. The main\n",
      "        difference is that `find_repeats` only returns repeated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.find_repeats([2, 1, 2, 3, 2, 2, 5])\n",
      "        RepeatedResults(values=array([2.]), counts=array([4]))\n",
      "        \n",
      "        >>> stats.find_repeats([[10, 20, 1, 2], [5, 5, 4, 4]])\n",
      "        RepeatedResults(values=array([4.,  5.]), counts=array([2, 2]))\n",
      "    \n",
      "    fisher_exact(table, alternative='two-sided')\n",
      "        Perform a Fisher exact test on a 2x2 contingency table.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        table : array_like of ints\n",
      "            A 2x2 contingency table.  Elements must be non-negative integers.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "            * 'two-sided'\n",
      "            * 'less': one-sided\n",
      "            * 'greater': one-sided\n",
      "        \n",
      "            See the Notes for more details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        oddsratio : float\n",
      "            This is prior odds ratio and not a posterior estimate.\n",
      "        p_value : float\n",
      "            P-value, the probability of obtaining a distribution at least as\n",
      "            extreme as the one that was actually observed, assuming that the\n",
      "            null hypothesis is true.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chi2_contingency : Chi-square test of independence of variables in a\n",
      "            contingency table.  This can be used as an alternative to\n",
      "            `fisher_exact` when the numbers in the table are large.\n",
      "        barnard_exact : Barnard's exact test, which is a more powerful alternative\n",
      "            than Fisher's exact test for 2x2 contingency tables.\n",
      "        boschloo_exact : Boschloo's exact test, which is a more powerful alternative\n",
      "            than Fisher's exact test for 2x2 contingency tables.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        *Null hypothesis and p-values*\n",
      "        \n",
      "        The null hypothesis is that the input table is from the hypergeometric\n",
      "        distribution with parameters (as used in `hypergeom`)\n",
      "        ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n",
      "        input table is ``[[a, b], [c, d]]``.  This distribution has support\n",
      "        ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n",
      "        in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n",
      "        can be interpreted as the upper-left element of a 2x2 table, so the\n",
      "        tables in the distribution have form::\n",
      "        \n",
      "            [  x           n - x     ]\n",
      "            [N - x    M - (n + N) + x]\n",
      "        \n",
      "        For example, if::\n",
      "        \n",
      "            table = [6  2]\n",
      "                    [1  4]\n",
      "        \n",
      "        then the support is ``2 <= x <= 7``, and the tables in the distribution\n",
      "        are::\n",
      "        \n",
      "            [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n",
      "            [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n",
      "        \n",
      "        The probability of each table is given by the hypergeometric distribution\n",
      "        ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n",
      "        three significant digits)::\n",
      "        \n",
      "            x       2      3      4      5       6        7\n",
      "            p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n",
      "        \n",
      "        These can be computed with::\n",
      "        \n",
      "            >>> from scipy.stats import hypergeom\n",
      "            >>> table = np.array([[6, 2], [1, 4]])\n",
      "            >>> M = table.sum()\n",
      "            >>> n = table[0].sum()\n",
      "            >>> N = table[:, 0].sum()\n",
      "            >>> start, end = hypergeom.support(M, n, N)\n",
      "            >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n",
      "            array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n",
      "                   0.004662  ])\n",
      "        \n",
      "        The two-sided p-value is the probability that, under the null hypothesis,\n",
      "        a random table would have a probability equal to or less than the\n",
      "        probability of the input table.  For our example, the probability of\n",
      "        the input table (where ``x = 6``) is 0.0816.  The x values where the\n",
      "        probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n",
      "        is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n",
      "        \n",
      "            >>> from scipy.stats import fisher_exact\n",
      "            >>> oddsr, p = fisher_exact(table, alternative='two-sided')\n",
      "            >>> p\n",
      "            0.10256410256410257\n",
      "        \n",
      "        The one-sided p-value for ``alternative='greater'`` is the probability\n",
      "        that a random table has ``x >= a``, which in our example is ``x >= 6``,\n",
      "        or ``0.0816 + 0.00466 ~= 0.08626``::\n",
      "        \n",
      "            >>> oddsr, p = fisher_exact(table, alternative='greater')\n",
      "            >>> p\n",
      "            0.08624708624708627\n",
      "        \n",
      "        This is equivalent to computing the survival function of the\n",
      "        distribution at ``x = 5`` (one less than ``x`` from the input table,\n",
      "        because we want to include the probability of ``x = 6`` in the sum)::\n",
      "        \n",
      "            >>> hypergeom.sf(5, M, n, N)\n",
      "            0.08624708624708627\n",
      "        \n",
      "        For ``alternative='less'``, the one-sided p-value is the probability\n",
      "        that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n",
      "        or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n",
      "        \n",
      "            >>> oddsr, p = fisher_exact(table, alternative='less')\n",
      "            >>> p\n",
      "            0.9953379953379957\n",
      "        \n",
      "        This is equivalent to computing the cumulative distribution function\n",
      "        of the distribution at ``x = 6``:\n",
      "        \n",
      "            >>> hypergeom.cdf(6, M, n, N)\n",
      "            0.9953379953379957\n",
      "        \n",
      "        *Odds ratio*\n",
      "        \n",
      "        The calculated odds ratio is different from the one R uses. This SciPy\n",
      "        implementation returns the (more common) \"unconditional Maximum\n",
      "        Likelihood Estimate\", while R uses the \"conditional Maximum Likelihood\n",
      "        Estimate\".\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Say we spend a few days counting whales and sharks in the Atlantic and\n",
      "        Indian oceans. In the Atlantic ocean we find 8 whales and 1 shark, in the\n",
      "        Indian ocean 2 whales and 5 sharks. Then our contingency table is::\n",
      "        \n",
      "                    Atlantic  Indian\n",
      "            whales     8        2\n",
      "            sharks     1        5\n",
      "        \n",
      "        We use this table to find the p-value:\n",
      "        \n",
      "        >>> from scipy.stats import fisher_exact\n",
      "        >>> oddsratio, pvalue = fisher_exact([[8, 2], [1, 5]])\n",
      "        >>> pvalue\n",
      "        0.0349...\n",
      "        \n",
      "        The probability that we would observe this or an even more imbalanced ratio\n",
      "        by chance is about 3.5%.  A commonly used significance level is 5%--if we\n",
      "        adopt that, we can therefore conclude that our observed imbalance is\n",
      "        statistically significant; whales prefer the Atlantic while sharks prefer\n",
      "        the Indian ocean.\n",
      "    \n",
      "    fligner(*args, center='median', proportiontocut=0.05)\n",
      "        Perform Fligner-Killeen test for equality of variance.\n",
      "        \n",
      "        Fligner's test tests the null hypothesis that all input samples\n",
      "        are from populations with equal variances.  Fligner-Killeen's test is\n",
      "        distribution free when populations are identical [2]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            Arrays of sample data.  Need not be the same length.\n",
      "        center : {'mean', 'median', 'trimmed'}, optional\n",
      "            Keyword argument controlling which function of the data is used in\n",
      "            computing the test statistic.  The default is 'median'.\n",
      "        proportiontocut : float, optional\n",
      "            When `center` is 'trimmed', this gives the proportion of data points\n",
      "            to cut from each end. (See `scipy.stats.trim_mean`.)\n",
      "            Default is 0.05.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bartlett : A parametric test for equality of k variances in normal samples\n",
      "        levene : A robust parametric test for equality of k variances\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As with Levene's test there are three variants of Fligner's test that\n",
      "        differ by the measure of central tendency used in the test.  See `levene`\n",
      "        for more information.\n",
      "        \n",
      "        Conover et al. (1981) examine many of the existing parametric and\n",
      "        nonparametric tests by extensive simulations and they conclude that the\n",
      "        tests proposed by Fligner and Killeen (1976) and Levene (1960) appear to be\n",
      "        superior in terms of robustness of departures from normality and power [3]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
      "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
      "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
      "               University.\n",
      "               https://cecas.clemson.edu/~cspark/cv/paper/qif/draftqif2.pdf\n",
      "        \n",
      "        .. [2] Fligner, M.A. and Killeen, T.J. (1976). Distribution-free two-sample\n",
      "               tests for scale. 'Journal of the American Statistical Association.'\n",
      "               71(353), 210-213.\n",
      "        \n",
      "        .. [3] Park, C. and Lindsay, B. G. (1999). Robust Scale Estimation and\n",
      "               Hypothesis Testing based on Quadratic Inference Function. Technical\n",
      "               Report #99-03, Center for Likelihood Studies, Pennsylvania State\n",
      "               University.\n",
      "        \n",
      "        .. [4] Conover, W. J., Johnson, M. E. and Johnson M. M. (1981). A\n",
      "               comparative study of tests for homogeneity of variances, with\n",
      "               applications to the outer continental shelf biding data.\n",
      "               Technometrics, 23(4), 351-361.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Test whether or not the lists `a`, `b` and `c` come from populations\n",
      "        with equal variances.\n",
      "        \n",
      "        >>> from scipy.stats import fligner\n",
      "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
      "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
      "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
      "        >>> stat, p = fligner(a, b, c)\n",
      "        >>> p\n",
      "        0.00450826080004775\n",
      "        \n",
      "        The small p-value suggests that the populations do not have equal\n",
      "        variances.\n",
      "        \n",
      "        This is not surprising, given that the sample variance of `b` is much\n",
      "        larger than that of `a` and `c`:\n",
      "        \n",
      "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
      "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
      "    \n",
      "    friedmanchisquare(*args)\n",
      "        Compute the Friedman test for repeated measurements.\n",
      "        \n",
      "        The Friedman test tests the null hypothesis that repeated measurements of\n",
      "        the same individuals have the same distribution.  It is often used\n",
      "        to test for consistency among measurements obtained in different ways.\n",
      "        For example, if two measurement techniques are used on the same set of\n",
      "        individuals, the Friedman test can be used to determine if the two\n",
      "        measurement techniques are consistent.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        measurements1, measurements2, measurements3... : array_like\n",
      "            Arrays of measurements.  All of the arrays must have the same number\n",
      "            of elements.  At least 3 sets of measurements must be given.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic, correcting for ties.\n",
      "        pvalue : float\n",
      "            The associated p-value assuming that the test statistic has a chi\n",
      "            squared distribution.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Due to the assumption that the test statistic has a chi squared\n",
      "        distribution, the p-value is only reliable for n > 10 and more than\n",
      "        6 repeated measurements.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Friedman_test\n",
      "    \n",
      "    gmean(a, axis=0, dtype=None, weights=None)\n",
      "        Compute the geometric mean along the specified axis.\n",
      "        \n",
      "        Return the geometric average of the array elements.\n",
      "        That is:  n-th root of (x1 * x2 * ... * xn)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the geometric mean is computed. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        dtype : dtype, optional\n",
      "            Type of the returned array and of the accumulator in which the\n",
      "            elements are summed. If dtype is not specified, it defaults to the\n",
      "            dtype of a, unless a has an integer dtype with a precision less than\n",
      "            that of the default platform integer. In that case, the default\n",
      "            platform integer is used.\n",
      "        weights : array_like, optional\n",
      "            The weights array can either be 1-D (in which case its length must be\n",
      "            the size of `a` along the given `axis`) or of the same shape as `a`.\n",
      "            Default is None, which gives each value a weight of 1.0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        gmean : ndarray\n",
      "            See `dtype` parameter above.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.mean : Arithmetic average\n",
      "        numpy.average : Weighted average\n",
      "        hmean : Harmonic mean\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The geometric average is computed over a single dimension of the input\n",
      "        array, axis=0 by default, or all values in the array if axis=None.\n",
      "        float64 intermediate and return values are used for integer inputs.\n",
      "        \n",
      "        Use masked arrays to ignore any non-finite values in the input or that\n",
      "        arise in the calculations such as Not a Number and infinity because masked\n",
      "        arrays automatically mask any non-finite values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Weighted Geometric Mean\", *Wikipedia*, https://en.wikipedia.org/wiki/Weighted_geometric_mean.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gmean\n",
      "        >>> gmean([1, 4])\n",
      "        2.0\n",
      "        >>> gmean([1, 2, 3, 4, 5, 6, 7])\n",
      "        3.3800151591412964\n",
      "    \n",
      "    gstd(a, axis=0, ddof=1)\n",
      "        Calculate the geometric standard deviation of an array.\n",
      "        \n",
      "        The geometric standard deviation describes the spread of a set of numbers\n",
      "        where the geometric mean is preferred. It is a multiplicative factor, and\n",
      "        so a dimensionless quantity.\n",
      "        \n",
      "        It is defined as the exponent of the standard deviation of ``log(a)``.\n",
      "        Mathematically the population geometric standard deviation can be\n",
      "        evaluated as::\n",
      "        \n",
      "            gstd = exp(std(log(a)))\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            An array like object containing the sample data.\n",
      "        axis : int, tuple or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Degree of freedom correction in the calculation of the\n",
      "            geometric standard deviation. Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray or float\n",
      "            An array of the geometric standard deviation. If `axis` is None or `a`\n",
      "            is a 1d array a float is returned.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As the calculation requires the use of logarithms the geometric standard\n",
      "        deviation only supports strictly positive values. Any non-positive or\n",
      "        infinite values will raise a `ValueError`.\n",
      "        The geometric standard deviation is sometimes confused with the exponent of\n",
      "        the standard deviation, ``exp(std(a))``. Instead the geometric standard\n",
      "        deviation is ``exp(std(log(a)))``.\n",
      "        The default value for `ddof` is different to the default value (0) used\n",
      "        by other ddof containing functions, such as ``np.std`` and ``np.nanstd``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find the geometric standard deviation of a log-normally distributed sample.\n",
      "        Note that the standard deviation of the distribution is one, on a\n",
      "        log scale this evaluates to approximately ``exp(1)``.\n",
      "        \n",
      "        >>> from scipy.stats import gstd\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> sample = rng.lognormal(mean=0, sigma=1, size=1000)\n",
      "        >>> gstd(sample)\n",
      "        2.810010162475324\n",
      "        \n",
      "        Compute the geometric standard deviation of a multidimensional array and\n",
      "        of a given axis.\n",
      "        \n",
      "        >>> a = np.arange(1, 25).reshape(2, 3, 4)\n",
      "        >>> gstd(a, axis=None)\n",
      "        2.2944076136018947\n",
      "        >>> gstd(a, axis=2)\n",
      "        array([[1.82424757, 1.22436866, 1.13183117],\n",
      "               [1.09348306, 1.07244798, 1.05914985]])\n",
      "        >>> gstd(a, axis=(1,2))\n",
      "        array([2.12939215, 1.22120169])\n",
      "        \n",
      "        The geometric standard deviation further handles masked arrays.\n",
      "        \n",
      "        >>> a = np.arange(1, 25).reshape(2, 3, 4)\n",
      "        >>> ma = np.ma.masked_where(a > 16, a)\n",
      "        >>> ma\n",
      "        masked_array(\n",
      "          data=[[[1, 2, 3, 4],\n",
      "                 [5, 6, 7, 8],\n",
      "                 [9, 10, 11, 12]],\n",
      "                [[13, 14, 15, 16],\n",
      "                 [--, --, --, --],\n",
      "                 [--, --, --, --]]],\n",
      "          mask=[[[False, False, False, False],\n",
      "                 [False, False, False, False],\n",
      "                 [False, False, False, False]],\n",
      "                [[False, False, False, False],\n",
      "                 [ True,  True,  True,  True],\n",
      "                 [ True,  True,  True,  True]]],\n",
      "          fill_value=999999)\n",
      "        >>> gstd(ma, axis=2)\n",
      "        masked_array(\n",
      "          data=[[1.8242475707663655, 1.2243686572447428, 1.1318311657788478],\n",
      "                [1.0934830582350938, --, --]],\n",
      "          mask=[[False, False, False],\n",
      "                [False,  True,  True]],\n",
      "          fill_value=999999)\n",
      "    \n",
      "    hmean(a, axis=0, dtype=None)\n",
      "        Calculate the harmonic mean along the specified axis.\n",
      "        \n",
      "        That is:  n / (1/x1 + 1/x2 + ... + 1/xn)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array, masked array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the harmonic mean is computed. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        dtype : dtype, optional\n",
      "            Type of the returned array and of the accumulator in which the\n",
      "            elements are summed. If `dtype` is not specified, it defaults to the\n",
      "            dtype of `a`, unless `a` has an integer `dtype` with a precision less\n",
      "            than that of the default platform integer. In that case, the default\n",
      "            platform integer is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        hmean : ndarray\n",
      "            See `dtype` parameter above.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.mean : Arithmetic average\n",
      "        numpy.average : Weighted average\n",
      "        gmean : Geometric mean\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The harmonic mean is computed over a single dimension of the input\n",
      "        array, axis=0 by default, or all values in the array if axis=None.\n",
      "        float64 intermediate and return values are used for integer inputs.\n",
      "        \n",
      "        Use masked arrays to ignore any non-finite values in the input or that\n",
      "        arise in the calculations such as Not a Number and infinity.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import hmean\n",
      "        >>> hmean([1, 4])\n",
      "        1.6000000000000001\n",
      "        >>> hmean([1, 2, 3, 4, 5, 6, 7])\n",
      "        2.6997245179063363\n",
      "    \n",
      "    iqr(x, axis=None, rng=(25, 75), scale=1.0, nan_policy='propagate', interpolation='linear', keepdims=False)\n",
      "        Compute the interquartile range of the data along the specified axis.\n",
      "        \n",
      "        The interquartile range (IQR) is the difference between the 75th and\n",
      "        25th percentile of the data. It is a measure of the dispersion\n",
      "        similar to standard deviation or variance, but is much more robust\n",
      "        against outliers [2]_.\n",
      "        \n",
      "        The ``rng`` parameter allows this function to compute other\n",
      "        percentile ranges than the actual IQR. For example, setting\n",
      "        ``rng=(0, 100)`` is equivalent to `numpy.ptp`.\n",
      "        \n",
      "        The IQR of an empty array is `np.nan`.\n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or sequence of int, optional\n",
      "            Axis along which the range is computed. The default is to\n",
      "            compute the IQR for the entire array.\n",
      "        rng : Two-element sequence containing floats in range of [0,100] optional\n",
      "            Percentiles over which to compute the range. Each must be\n",
      "            between 0 and 100, inclusive. The default is the true IQR:\n",
      "            `(25, 75)`. The order of the elements is not important.\n",
      "        scale : scalar or str, optional\n",
      "            The numerical value of scale will be divided out of the final\n",
      "            result. The following string values are recognized:\n",
      "        \n",
      "              * 'raw' : No scaling, just return the raw IQR.\n",
      "                **Deprecated!**  Use `scale=1` instead.\n",
      "              * 'normal' : Scale by\n",
      "                :math:`2 \\sqrt{2} erf^{-1}(\\frac{1}{2}) \\approx 1.349`.\n",
      "        \n",
      "            The default is 1.0. The use of scale='raw' is deprecated.\n",
      "            Array-like scale is also allowed, as long\n",
      "            as it broadcasts correctly to the output such that\n",
      "            ``out / scale`` is a valid operation. The output dimensions\n",
      "            depend on the input array, `x`, the `axis` argument, and the\n",
      "            `keepdims` flag.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        interpolation : {'linear', 'lower', 'higher', 'midpoint',\n",
      "                         'nearest'}, optional\n",
      "        \n",
      "            Specifies the interpolation method to use when the percentile\n",
      "            boundaries lie between two data points `i` and `j`.\n",
      "            The following options are available (default is 'linear'):\n",
      "        \n",
      "              * 'linear': `i + (j - i) * fraction`, where `fraction` is the\n",
      "                fractional part of the index surrounded by `i` and `j`.\n",
      "              * 'lower': `i`.\n",
      "              * 'higher': `j`.\n",
      "              * 'nearest': `i` or `j` whichever is nearest.\n",
      "              * 'midpoint': `(i + j) / 2`.\n",
      "        \n",
      "        keepdims : bool, optional\n",
      "            If this is set to `True`, the reduced axes are left in the\n",
      "            result as dimensions with size one. With this option, the result\n",
      "            will broadcast correctly against the original array `x`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        iqr : scalar or ndarray\n",
      "            If ``axis=None``, a scalar is returned. If the input contains\n",
      "            integers or floats of smaller precision than ``np.float64``, then the\n",
      "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
      "            the same as that of the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.std, numpy.var\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is heavily dependent on the version of `numpy` that is\n",
      "        installed. Versions greater than 1.11.0b3 are highly recommended, as they\n",
      "        include a number of enhancements and fixes to `numpy.percentile` and\n",
      "        `numpy.nanpercentile` that affect the operation of this function. The\n",
      "        following modifications apply:\n",
      "        \n",
      "        Below 1.10.0 : `nan_policy` is poorly defined.\n",
      "            The default behavior of `numpy.percentile` is used for 'propagate'. This\n",
      "            is a hybrid of 'omit' and 'propagate' that mostly yields a skewed\n",
      "            version of 'omit' since NaNs are sorted to the end of the data. A\n",
      "            warning is raised if there are NaNs in the data.\n",
      "        Below 1.9.0: `numpy.nanpercentile` does not exist.\n",
      "            This means that `numpy.percentile` is used regardless of `nan_policy`\n",
      "            and a warning is issued. See previous item for a description of the\n",
      "            behavior.\n",
      "        Below 1.9.0: `keepdims` and `interpolation` are not supported.\n",
      "            The keywords get ignored with a warning if supplied with non-default\n",
      "            values. However, multiple axes are still supported.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Interquartile range\" https://en.wikipedia.org/wiki/Interquartile_range\n",
      "        .. [2] \"Robust measures of scale\" https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
      "        .. [3] \"Quantile\" https://en.wikipedia.org/wiki/Quantile\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import iqr\n",
      "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
      "        >>> x\n",
      "        array([[10,  7,  4],\n",
      "               [ 3,  2,  1]])\n",
      "        >>> iqr(x)\n",
      "        4.0\n",
      "        >>> iqr(x, axis=0)\n",
      "        array([ 3.5,  2.5,  1.5])\n",
      "        >>> iqr(x, axis=1)\n",
      "        array([ 3.,  1.])\n",
      "        >>> iqr(x, axis=1, keepdims=True)\n",
      "        array([[ 3.],\n",
      "               [ 1.]])\n",
      "    \n",
      "    itemfreq(*args, **kwds)\n",
      "        `itemfreq` is deprecated!\n",
      "        `itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "        \n",
      "        Return a 2-D array of item frequencies.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (N,) array_like\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        itemfreq : (K, 2) ndarray\n",
      "            A 2-D frequency table.  Column 1 contains sorted, unique values from\n",
      "            `a`, column 2 contains their respective counts.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.array([1, 1, 5, 0, 1, 2, 2, 0, 1, 4])\n",
      "        >>> stats.itemfreq(a)\n",
      "        array([[ 0.,  2.],\n",
      "               [ 1.,  4.],\n",
      "               [ 2.,  2.],\n",
      "               [ 4.,  1.],\n",
      "               [ 5.,  1.]])\n",
      "        >>> np.bincount(a)\n",
      "        array([2, 4, 2, 0, 1, 1])\n",
      "        \n",
      "        >>> stats.itemfreq(a/10.)\n",
      "        array([[ 0. ,  2. ],\n",
      "               [ 0.1,  4. ],\n",
      "               [ 0.2,  2. ],\n",
      "               [ 0.4,  1. ],\n",
      "               [ 0.5,  1. ]])\n",
      "    \n",
      "    jarque_bera(x)\n",
      "        Perform the Jarque-Bera goodness of fit test on sample data.\n",
      "        \n",
      "        The Jarque-Bera test tests whether the sample data has the skewness and\n",
      "        kurtosis matching a normal distribution.\n",
      "        \n",
      "        Note that this test only works for a large enough number of data samples\n",
      "        (>2000) as the test statistic asymptotically has a Chi-squared distribution\n",
      "        with 2 degrees of freedom.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Observations of a random variable.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        jb_value : float\n",
      "            The test statistic.\n",
      "        p : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Jarque, C. and Bera, A. (1980) \"Efficient tests for normality,\n",
      "               homoscedasticity and serial independence of regression residuals\",\n",
      "               6 Econometric Letters 255-259.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = rng.normal(0, 1, 100000)\n",
      "        >>> jarque_bera_test = stats.jarque_bera(x)\n",
      "        >>> jarque_bera_test\n",
      "        Jarque_beraResult(statistic=3.3415184718131554, pvalue=0.18810419594996775)\n",
      "        >>> jarque_bera_test.statistic\n",
      "        3.3415184718131554\n",
      "        >>> jarque_bera_test.pvalue\n",
      "        0.18810419594996775\n",
      "    \n",
      "    kendalltau(x, y, initial_lexsort=None, nan_policy='propagate', method='auto', variant='b')\n",
      "        Calculate Kendall's tau, a correlation measure for ordinal data.\n",
      "        \n",
      "        Kendall's tau is a measure of the correspondence between two rankings.\n",
      "        Values close to 1 indicate strong agreement, and values close to -1\n",
      "        indicate strong disagreement. This implements two variants of Kendall's\n",
      "        tau: tau-b (the default) and tau-c (also known as Stuart's tau-c). These\n",
      "        differ only in how they are normalized to lie within the range -1 to 1;\n",
      "        the hypothesis tests (their p-values) are identical. Kendall's original\n",
      "        tau-a is not implemented separately because both tau-b and tau-c reduce\n",
      "        to tau-a in the absence of ties.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of rankings, of the same shape. If arrays are not 1-D, they\n",
      "            will be flattened to 1-D.\n",
      "        initial_lexsort : bool, optional\n",
      "            Unused (deprecated).\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
      "            Defines which method is used to calculate the p-value [5]_.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto': selects the appropriate method based on a trade-off\n",
      "                between speed and accuracy\n",
      "              * 'asymptotic': uses a normal approximation valid for large samples\n",
      "              * 'exact': computes the exact p-value, but can only be used if no ties\n",
      "                are present. As the sample size increases, the 'exact' computation\n",
      "                time may grow and the result may lose some precision.\n",
      "        \n",
      "        variant: {'b', 'c'}, optional\n",
      "            Defines which variant of Kendall's tau is returned. Default is 'b'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float\n",
      "           The tau statistic.\n",
      "        pvalue : float\n",
      "           The two-sided p-value for a hypothesis test whose null hypothesis is\n",
      "           an absence of association, tau = 0.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
      "        theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n",
      "        weightedtau : Computes a weighted version of Kendall's tau.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The definition of Kendall's tau that is used is [2]_::\n",
      "        \n",
      "          tau_b = (P - Q) / sqrt((P + Q + T) * (P + Q + U))\n",
      "        \n",
      "          tau_c = 2 (P - Q) / (n**2 * (m - 1) / m)\n",
      "        \n",
      "        where P is the number of concordant pairs, Q the number of discordant\n",
      "        pairs, T the number of ties only in `x`, and U the number of ties only in\n",
      "        `y`.  If a tie occurs for the same pair in both `x` and `y`, it is not\n",
      "        added to either T or U. n is the total number of samples, and m is the\n",
      "        number of unique values in either `x` or `y`, whichever is smaller.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Maurice G. Kendall, \"A New Measure of Rank Correlation\", Biometrika\n",
      "               Vol. 30, No. 1/2, pp. 81-93, 1938.\n",
      "        .. [2] Maurice G. Kendall, \"The treatment of ties in ranking problems\",\n",
      "               Biometrika Vol. 33, No. 3, pp. 239-251. 1945.\n",
      "        .. [3] Gottfried E. Noether, \"Elements of Nonparametric Statistics\", John\n",
      "               Wiley & Sons, 1967.\n",
      "        .. [4] Peter M. Fenwick, \"A new data structure for cumulative frequency\n",
      "               tables\", Software: Practice and Experience, Vol. 24, No. 3,\n",
      "               pp. 327-336, 1994.\n",
      "        .. [5] Maurice G. Kendall, \"Rank Correlation Methods\" (4th Edition),\n",
      "               Charles Griffin & Co., 1970.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x1 = [12, 2, 1, 12, 2]\n",
      "        >>> x2 = [1, 4, 7, 1, 0]\n",
      "        >>> tau, p_value = stats.kendalltau(x1, x2)\n",
      "        >>> tau\n",
      "        -0.47140452079103173\n",
      "        >>> p_value\n",
      "        0.2827454599327748\n",
      "    \n",
      "    kruskal(*args, nan_policy='propagate')\n",
      "        Compute the Kruskal-Wallis H-test for independent samples.\n",
      "        \n",
      "        The Kruskal-Wallis H-test tests the null hypothesis that the population\n",
      "        median of all of the groups are equal.  It is a non-parametric version of\n",
      "        ANOVA.  The test works on 2 or more independent samples, which may have\n",
      "        different sizes.  Note that rejecting the null hypothesis does not\n",
      "        indicate which of the groups differs.  Post hoc comparisons between\n",
      "        groups are required to determine which groups are different.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "           Two or more arrays with the sample measurements can be given as\n",
      "           arguments. Samples must be one-dimensional.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "           The Kruskal-Wallis H statistic, corrected for ties.\n",
      "        pvalue : float\n",
      "           The p-value for the test using the assumption that H has a chi\n",
      "           square distribution. The p-value returned is the survival function of\n",
      "           the chi square distribution evaluated at H.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        f_oneway : 1-way ANOVA.\n",
      "        mannwhitneyu : Mann-Whitney rank test on two samples.\n",
      "        friedmanchisquare : Friedman test for repeated measurements.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Due to the assumption that H has a chi square distribution, the number\n",
      "        of samples in each group must not be too small.  A typical rule is\n",
      "        that each sample must have at least 5 measurements.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n",
      "           One-Criterion Variance Analysis\", Journal of the American Statistical\n",
      "           Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n",
      "        .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = [1, 3, 5, 7, 9]\n",
      "        >>> y = [2, 4, 6, 8, 10]\n",
      "        >>> stats.kruskal(x, y)\n",
      "        KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n",
      "        \n",
      "        >>> x = [1, 1, 1]\n",
      "        >>> y = [2, 2, 2]\n",
      "        >>> z = [2, 2]\n",
      "        >>> stats.kruskal(x, y, z)\n",
      "        KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n",
      "    \n",
      "    ks_1samp(x, cdf, args=(), alternative='two-sided', mode='auto')\n",
      "        Performs the one-sample Kolmogorov-Smirnov test for goodness of fit.\n",
      "        \n",
      "        This test compares the underlying distribution F(x) of a sample\n",
      "        against a given continuous distribution G(x). See Notes for a description\n",
      "        of the available null and alternative hypotheses.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            a 1-D array of observations of iid random variables.\n",
      "        cdf : callable\n",
      "            callable used to calculate the cdf.\n",
      "        args : tuple, sequence, optional\n",
      "            Distribution parameters, used with `cdf`.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
      "            Please see explanations in the Notes below.\n",
      "        mode : {'auto', 'exact', 'approx', 'asymp'}, optional\n",
      "            Defines the distribution used for calculating the p-value.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto' : selects one of the other options.\n",
      "              * 'exact' : uses the exact distribution of test statistic.\n",
      "              * 'approx' : approximates the two-sided probability with twice\n",
      "                the one-sided probability\n",
      "              * 'asymp': uses asymptotic distribution of test statistic\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            KS test statistic, either D, D+ or D- (depending on the value\n",
      "            of 'alternative')\n",
      "        pvalue :  float\n",
      "            One-tailed or two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp, kstest\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        There are three options for the null and corresponding alternative\n",
      "        hypothesis that can be selected using the `alternative` parameter.\n",
      "        \n",
      "        - `two-sided`: The null hypothesis is that the two distributions are\n",
      "          identical, F(x)=G(x) for all x; the alternative is that they are not\n",
      "          identical.\n",
      "        \n",
      "        - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n",
      "          alternative is that F(x) < G(x) for at least one x.\n",
      "        \n",
      "        - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n",
      "          alternative is that F(x) > G(x) for at least one x.\n",
      "        \n",
      "        Note that the alternative hypotheses describe the *CDFs* of the\n",
      "        underlying distributions, not the observed values. For example,\n",
      "        suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n",
      "        x1 tend to be less than those in x2.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        >>> x = np.linspace(-15, 15, 9)\n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf)\n",
      "        (0.44435602715924361, 0.038850142705171065)\n",
      "        \n",
      "        >>> stats.ks_1samp(stats.norm.rvs(size=100, random_state=rng),\n",
      "        ...                stats.norm.cdf)\n",
      "        KstestResult(statistic=0.165471391799..., pvalue=0.007331283245...)\n",
      "        \n",
      "        *Test against one-sided alternative hypothesis*\n",
      "        \n",
      "        Shift distribution to larger values, so that `` CDF(x) < norm.cdf(x)``:\n",
      "        \n",
      "        >>> x = stats.norm.rvs(loc=0.2, size=100, random_state=rng)\n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf, alternative='less')\n",
      "        KstestResult(statistic=0.100203351482..., pvalue=0.125544644447...)\n",
      "        \n",
      "        Reject null hypothesis in favor of alternative hypothesis: less\n",
      "        \n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf, alternative='greater')\n",
      "        KstestResult(statistic=0.018749806388..., pvalue=0.920581859791...)\n",
      "        \n",
      "        Reject null hypothesis in favor of alternative hypothesis: greater\n",
      "        \n",
      "        >>> stats.ks_1samp(x, stats.norm.cdf)\n",
      "        KstestResult(statistic=0.100203351482..., pvalue=0.250616879765...)\n",
      "        \n",
      "        Don't reject null hypothesis in favor of alternative hypothesis: two-sided\n",
      "        \n",
      "        *Testing t distributed random variables against normal distribution*\n",
      "        \n",
      "        With 100 degrees of freedom the t distribution looks close to the normal\n",
      "        distribution, and the K-S test does not reject the hypothesis that the\n",
      "        sample came from the normal distribution:\n",
      "        \n",
      "        >>> stats.ks_1samp(stats.t.rvs(100,size=100, random_state=rng),\n",
      "        ...                stats.norm.cdf)\n",
      "        KstestResult(statistic=0.064273776544..., pvalue=0.778737758305...)\n",
      "        \n",
      "        With 3 degrees of freedom the t distribution looks sufficiently different\n",
      "        from the normal distribution, that we can reject the hypothesis that the\n",
      "        sample came from the normal distribution at the 10% level:\n",
      "        \n",
      "        >>> stats.ks_1samp(stats.t.rvs(3,size=100, random_state=rng),\n",
      "        ...                stats.norm.cdf)\n",
      "        KstestResult(statistic=0.128678487493..., pvalue=0.066569081515...)\n",
      "    \n",
      "    ks_2samp(data1, data2, alternative='two-sided', mode='auto')\n",
      "        Performs the two-sample Kolmogorov-Smirnov test for goodness of fit.\n",
      "        \n",
      "        This test compares the underlying continuous distributions F(x) and G(x)\n",
      "        of two independent samples.  See Notes for a description\n",
      "        of the available null and alternative hypotheses.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data1, data2 : array_like, 1-Dimensional\n",
      "            Two arrays of sample observations assumed to be drawn from a continuous\n",
      "            distribution, sample sizes can be different.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
      "            Please see explanations in the Notes below.\n",
      "        mode : {'auto', 'exact', 'asymp'}, optional\n",
      "            Defines the method used for calculating the p-value.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto' : use 'exact' for small size arrays, 'asymp' for large\n",
      "              * 'exact' : use exact distribution of test statistic\n",
      "              * 'asymp' : use asymptotic distribution of test statistic\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            KS statistic.\n",
      "        pvalue : float\n",
      "            One-tailed or two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstest, ks_1samp, epps_singleton_2samp, anderson_ksamp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        There are three options for the null and corresponding alternative\n",
      "        hypothesis that can be selected using the `alternative` parameter.\n",
      "        \n",
      "        - `two-sided`: The null hypothesis is that the two distributions are\n",
      "          identical, F(x)=G(x) for all x; the alternative is that they are not\n",
      "          identical.\n",
      "        \n",
      "        - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n",
      "          alternative is that F(x) < G(x) for at least one x.\n",
      "        \n",
      "        - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n",
      "          alternative is that F(x) > G(x) for at least one x.\n",
      "        \n",
      "        Note that the alternative hypotheses describe the *CDFs* of the\n",
      "        underlying distributions, not the observed values. For example,\n",
      "        suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n",
      "        x1 tend to be less than those in x2.\n",
      "        \n",
      "        \n",
      "        If the KS statistic is small or the p-value is high, then we cannot\n",
      "        reject the null hypothesis in favor of the alternative.\n",
      "        \n",
      "        If the mode is 'auto', the computation is exact if the sample sizes are\n",
      "        less than 10000.  For larger sizes, the computation uses the\n",
      "        Kolmogorov-Smirnov distributions to compute an approximate value.\n",
      "        \n",
      "        The 'two-sided' 'exact' computation computes the complementary probability\n",
      "        and then subtracts from 1.  As such, the minimum probability it can return\n",
      "        is about 1e-16.  While the algorithm itself is exact, numerical\n",
      "        errors may accumulate for large sample sizes.   It is most suited to\n",
      "        situations in which one of the sample sizes is only a few thousand.\n",
      "        \n",
      "        We generally follow Hodges' treatment of Drion/Gnedenko/Korolyuk [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Hodges, J.L. Jr.,  \"The Significance Probability of the Smirnov\n",
      "               Two-Sample Test,\" Arkiv fiur Matematik, 3, No. 43 (1958), 469-86.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        >>> n1 = 200  # size of first sample\n",
      "        >>> n2 = 300  # size of second sample\n",
      "        \n",
      "        For a different distribution, we can reject the null hypothesis since the\n",
      "        pvalue is below 1%:\n",
      "        \n",
      "        >>> rvs1 = stats.norm.rvs(size=n1, loc=0., scale=1, random_state=rng)\n",
      "        >>> rvs2 = stats.norm.rvs(size=n2, loc=0.5, scale=1.5, random_state=rng)\n",
      "        >>> stats.ks_2samp(rvs1, rvs2)\n",
      "         KstestResult(statistic=0.24833333333333332, pvalue=5.846586728086578e-07)\n",
      "        \n",
      "        For a slightly different distribution, we cannot reject the null hypothesis\n",
      "        at a 10% or lower alpha since the p-value at 0.144 is higher than 10%\n",
      "        \n",
      "        >>> rvs3 = stats.norm.rvs(size=n2, loc=0.01, scale=1.0, random_state=rng)\n",
      "        >>> stats.ks_2samp(rvs1, rvs3)\n",
      "        KstestResult(statistic=0.07833333333333334, pvalue=0.4379658456442945)\n",
      "        \n",
      "        For an identical distribution, we cannot reject the null hypothesis since\n",
      "        the p-value is high, 41%:\n",
      "        \n",
      "        >>> rvs4 = stats.norm.rvs(size=n2, loc=0.0, scale=1.0, random_state=rng)\n",
      "        >>> stats.ks_2samp(rvs1, rvs4)\n",
      "        KstestResult(statistic=0.12166666666666667, pvalue=0.05401863039081145)\n",
      "    \n",
      "    kstat(data, n=2)\n",
      "        Return the nth k-statistic (1<=n<=4 so far).\n",
      "        \n",
      "        The nth k-statistic k_n is the unique symmetric unbiased estimator of the\n",
      "        nth cumulant kappa_n.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input array. Note that n-D input gets flattened.\n",
      "        n : int, {1, 2, 3, 4}, optional\n",
      "            Default is equal to 2.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kstat : float\n",
      "            The nth k-statistic.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstatvar: Returns an unbiased estimator of the variance of the k-statistic.\n",
      "        moment: Returns the n-th central moment about the mean for a sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For a sample size n, the first few k-statistics are given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            k_{1} = \\mu\n",
      "            k_{2} = \\frac{n}{n-1} m_{2}\n",
      "            k_{3} = \\frac{ n^{2} } {(n-1) (n-2)} m_{3}\n",
      "            k_{4} = \\frac{ n^{2} [(n + 1)m_{4} - 3(n - 1) m^2_{2}]} {(n-1) (n-2) (n-3)}\n",
      "        \n",
      "        where :math:`\\mu` is the sample mean, :math:`m_2` is the sample\n",
      "        variance, and :math:`m_i` is the i-th sample central moment.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        http://mathworld.wolfram.com/k-Statistic.html\n",
      "        \n",
      "        http://mathworld.wolfram.com/Cumulant.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> from numpy.random import default_rng\n",
      "        >>> rng = default_rng()\n",
      "        \n",
      "        As sample size increases, n-th moment and n-th k-statistic converge to the\n",
      "        same number (although they aren't identical). In the case of the normal\n",
      "        distribution, they converge to zero.\n",
      "        \n",
      "        >>> for n in [2, 3, 4, 5, 6, 7]:\n",
      "        ...     x = rng.normal(size=10**n)\n",
      "        ...     m, k = stats.moment(x, 3), stats.kstat(x, 3)\n",
      "        ...     print(\"%.3g %.3g %.3g\" % (m, k, m-k))\n",
      "        -0.631 -0.651 0.0194  # random\n",
      "        0.0282 0.0283 -8.49e-05\n",
      "        -0.0454 -0.0454 1.36e-05\n",
      "        7.53e-05 7.53e-05 -2.26e-09\n",
      "        0.00166 0.00166 -4.99e-09\n",
      "        -2.88e-06 -2.88e-06 8.63e-13\n",
      "    \n",
      "    kstatvar(data, n=2)\n",
      "        Return an unbiased estimator of the variance of the k-statistic.\n",
      "        \n",
      "        See `kstat` for more details of the k-statistic.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input array. Note that n-D input gets flattened.\n",
      "        n : int, {1, 2}, optional\n",
      "            Default is equal to 2.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kstatvar : float\n",
      "            The nth k-statistic variance.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstat: Returns the n-th k-statistic.\n",
      "        moment: Returns the n-th central moment about the mean for a sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The variances of the first few k-statistics are given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            var(k_{1}) = \\frac{\\kappa^2}{n}\n",
      "            var(k_{2}) = \\frac{\\kappa^4}{n} + \\frac{2\\kappa^2_{2}}{n - 1}\n",
      "            var(k_{3}) = \\frac{\\kappa^6}{n} + \\frac{9 \\kappa_2 \\kappa_4}{n - 1} +\n",
      "                         \\frac{9 \\kappa^2_{3}}{n - 1} +\n",
      "                         \\frac{6 n \\kappa^3_{2}}{(n-1) (n-2)}\n",
      "            var(k_{4}) = \\frac{\\kappa^8}{n} + \\frac{16 \\kappa_2 \\kappa_6}{n - 1} +\n",
      "                         \\frac{48 \\kappa_{3} \\kappa_5}{n - 1} +\n",
      "                         \\frac{34 \\kappa^2_{4}}{n-1} + \\frac{72 n \\kappa^2_{2} \\kappa_4}{(n - 1) (n - 2)} +\n",
      "                         \\frac{144 n \\kappa_{2} \\kappa^2_{3}}{(n - 1) (n - 2)} +\n",
      "                         \\frac{24 (n + 1) n \\kappa^4_{2}}{(n - 1) (n - 2) (n - 3)}\n",
      "    \n",
      "    kstest(rvs, cdf, args=(), N=20, alternative='two-sided', mode='auto')\n",
      "        Performs the (one-sample or two-sample) Kolmogorov-Smirnov test for\n",
      "        goodness of fit.\n",
      "        \n",
      "        The one-sample test compares the underlying distribution F(x) of a sample\n",
      "        against a given distribution G(x). The two-sample test compares the\n",
      "        underlying distributions of two independent samples. Both tests are valid\n",
      "        only for continuous distributions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rvs : str, array_like, or callable\n",
      "            If an array, it should be a 1-D array of observations of random\n",
      "            variables.\n",
      "            If a callable, it should be a function to generate random variables;\n",
      "            it is required to have a keyword argument `size`.\n",
      "            If a string, it should be the name of a distribution in `scipy.stats`,\n",
      "            which will be used to generate random variables.\n",
      "        cdf : str, array_like or callable\n",
      "            If array_like, it should be a 1-D array of observations of random\n",
      "            variables, and the two-sample test is performed\n",
      "            (and rvs must be array_like).\n",
      "            If a callable, that callable is used to calculate the cdf.\n",
      "            If a string, it should be the name of a distribution in `scipy.stats`,\n",
      "            which will be used as the cdf function.\n",
      "        args : tuple, sequence, optional\n",
      "            Distribution parameters, used if `rvs` or `cdf` are strings or\n",
      "            callables.\n",
      "        N : int, optional\n",
      "            Sample size if `rvs` is string or callable.  Default is 20.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the null and alternative hypotheses. Default is 'two-sided'.\n",
      "            Please see explanations in the Notes below.\n",
      "        mode : {'auto', 'exact', 'approx', 'asymp'}, optional\n",
      "            Defines the distribution used for calculating the p-value.\n",
      "            The following options are available (default is 'auto'):\n",
      "        \n",
      "              * 'auto' : selects one of the other options.\n",
      "              * 'exact' : uses the exact distribution of test statistic.\n",
      "              * 'approx' : approximates the two-sided probability with twice the\n",
      "                one-sided probability\n",
      "              * 'asymp': uses asymptotic distribution of test statistic\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            KS test statistic, either D, D+ or D-.\n",
      "        pvalue :  float\n",
      "            One-tailed or two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ks_2samp\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        There are three options for the null and corresponding alternative\n",
      "        hypothesis that can be selected using the `alternative` parameter.\n",
      "        \n",
      "        - `two-sided`: The null hypothesis is that the two distributions are\n",
      "          identical, F(x)=G(x) for all x; the alternative is that they are not\n",
      "          identical.\n",
      "        \n",
      "        - `less`: The null hypothesis is that F(x) >= G(x) for all x; the\n",
      "          alternative is that F(x) < G(x) for at least one x.\n",
      "        \n",
      "        - `greater`: The null hypothesis is that F(x) <= G(x) for all x; the\n",
      "          alternative is that F(x) > G(x) for at least one x.\n",
      "        \n",
      "        Note that the alternative hypotheses describe the *CDFs* of the\n",
      "        underlying distributions, not the observed values. For example,\n",
      "        suppose x1 ~ F and x2 ~ G. If F(x) > G(x) for all x, the values in\n",
      "        x1 tend to be less than those in x2.\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        >>> x = np.linspace(-15, 15, 9)\n",
      "        >>> stats.kstest(x, 'norm')\n",
      "        KstestResult(statistic=0.444356027159..., pvalue=0.038850140086...)\n",
      "        \n",
      "        >>> stats.kstest(stats.norm.rvs(size=100, random_state=rng), stats.norm.cdf)\n",
      "        KstestResult(statistic=0.165471391799..., pvalue=0.007331283245...)\n",
      "        \n",
      "        The above lines are equivalent to:\n",
      "        \n",
      "        >>> stats.kstest(stats.norm.rvs, 'norm', N=100)\n",
      "        KstestResult(statistic=0.113810164200..., pvalue=0.138690052319...)  # may vary\n",
      "        \n",
      "        *Test against one-sided alternative hypothesis*\n",
      "        \n",
      "        Shift distribution to larger values, so that ``CDF(x) < norm.cdf(x)``:\n",
      "        \n",
      "        >>> x = stats.norm.rvs(loc=0.2, size=100, random_state=rng)\n",
      "        >>> stats.kstest(x, 'norm', alternative='less')\n",
      "        KstestResult(statistic=0.1002033514..., pvalue=0.1255446444...)\n",
      "        \n",
      "        Reject null hypothesis in favor of alternative hypothesis: less\n",
      "        \n",
      "        >>> stats.kstest(x, 'norm', alternative='greater')\n",
      "        KstestResult(statistic=0.018749806388..., pvalue=0.920581859791...)\n",
      "        \n",
      "        Don't reject null hypothesis in favor of alternative hypothesis: greater\n",
      "        \n",
      "        >>> stats.kstest(x, 'norm')\n",
      "        KstestResult(statistic=0.100203351482..., pvalue=0.250616879765...)\n",
      "        \n",
      "        *Testing t distributed random variables against normal distribution*\n",
      "        \n",
      "        With 100 degrees of freedom the t distribution looks close to the normal\n",
      "        distribution, and the K-S test does not reject the hypothesis that the\n",
      "        sample came from the normal distribution:\n",
      "        \n",
      "        >>> stats.kstest(stats.t.rvs(100, size=100, random_state=rng), 'norm')\n",
      "        KstestResult(statistic=0.064273776544..., pvalue=0.778737758305...)\n",
      "        \n",
      "        With 3 degrees of freedom the t distribution looks sufficiently different\n",
      "        from the normal distribution, that we can reject the hypothesis that the\n",
      "        sample came from the normal distribution at the 10% level:\n",
      "        \n",
      "        >>> stats.kstest(stats.t.rvs(3, size=100, random_state=rng), 'norm')\n",
      "        KstestResult(statistic=0.128678487493..., pvalue=0.066569081515...)\n",
      "    \n",
      "    kurtosis(a, axis=0, fisher=True, bias=True, nan_policy='propagate')\n",
      "        Compute the kurtosis (Fisher or Pearson) of a dataset.\n",
      "        \n",
      "        Kurtosis is the fourth central moment divided by the square of the\n",
      "        variance. If Fisher's definition is used, then 3.0 is subtracted from\n",
      "        the result to give 0.0 for a normal distribution.\n",
      "        \n",
      "        If bias is False then the kurtosis is calculated using k statistics to\n",
      "        eliminate bias coming from biased moment estimators\n",
      "        \n",
      "        Use `kurtosistest` to see if result is close enough to normal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array\n",
      "            Data for which the kurtosis is calculated.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the kurtosis is calculated. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        fisher : bool, optional\n",
      "            If True, Fisher's definition is used (normal ==> 0.0). If False,\n",
      "            Pearson's definition is used (normal ==> 3.0).\n",
      "        bias : bool, optional\n",
      "            If False, then the calculations are corrected for statistical bias.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kurtosis : array\n",
      "            The kurtosis of values along an axis. If all values are equal,\n",
      "            return -3 for Fisher's definition and 0 for Pearson's definition.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In Fisher's definiton, the kurtosis of the normal distribution is zero.\n",
      "        In the following example, the kurtosis is close to zero, because it was\n",
      "        calculated from the dataset, not from the continuous distribution.\n",
      "        \n",
      "        >>> from scipy.stats import norm, kurtosis\n",
      "        >>> data = norm.rvs(size=1000, random_state=3)\n",
      "        >>> kurtosis(data)\n",
      "        -0.06928694200380558\n",
      "        \n",
      "        The distribution with a higher kurtosis has a heavier tail.\n",
      "        The zero valued kurtosis of the normal distribution in Fisher's definition\n",
      "        can serve as a reference point.\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> import scipy.stats as stats\n",
      "        >>> from scipy.stats import kurtosis\n",
      "        \n",
      "        >>> x = np.linspace(-5, 5, 100)\n",
      "        >>> ax = plt.subplot()\n",
      "        >>> distnames = ['laplace', 'norm', 'uniform']\n",
      "        \n",
      "        >>> for distname in distnames:\n",
      "        ...     if distname == 'uniform':\n",
      "        ...         dist = getattr(stats, distname)(loc=-2, scale=4)\n",
      "        ...     else:\n",
      "        ...         dist = getattr(stats, distname)\n",
      "        ...     data = dist.rvs(size=1000)\n",
      "        ...     kur = kurtosis(data, fisher=True)\n",
      "        ...     y = dist.pdf(x)\n",
      "        ...     ax.plot(x, y, label=\"{}, {}\".format(distname, round(kur, 3)))\n",
      "        ...     ax.legend()\n",
      "        \n",
      "        The Laplace distribution has a heavier tail than the normal distribution.\n",
      "        The uniform distribution (which has negative kurtosis) has the thinnest\n",
      "        tail.\n",
      "    \n",
      "    kurtosistest(a, axis=0, nan_policy='propagate', alternative='two-sided')\n",
      "        Test whether a dataset has normal kurtosis.\n",
      "        \n",
      "        This function tests the null hypothesis that the kurtosis\n",
      "        of the population from which the sample was drawn is that\n",
      "        of the normal distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array\n",
      "            Array of the sample data.\n",
      "        axis : int or None, optional\n",
      "           Axis along which to compute test. Default is 0. If None,\n",
      "           compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "            * 'two-sided': the kurtosis of the distribution underlying the sample\n",
      "              is different from that of the normal distribution\n",
      "            * 'less': the kurtosis of the distribution underlying the sample\n",
      "              is less than that of the normal distribution\n",
      "            * 'greater': the kurtosis of the distribution underlying the sample\n",
      "              is greater than that of the normal distribution\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The computed z-score for this test.\n",
      "        pvalue : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Valid only for n>20. This function uses the method described in [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] see e.g. F. J. Anscombe, W. J. Glynn, \"Distribution of the kurtosis\n",
      "           statistic b2 for normal samples\", Biometrika, vol. 70, pp. 227-234, 1983.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import kurtosistest\n",
      "        >>> kurtosistest(list(range(20)))\n",
      "        KurtosistestResult(statistic=-1.7058104152122062, pvalue=0.08804338332528348)\n",
      "        >>> kurtosistest(list(range(20)), alternative='less')\n",
      "        KurtosistestResult(statistic=-1.7058104152122062, pvalue=0.04402169166264174)\n",
      "        >>> kurtosistest(list(range(20)), alternative='greater')\n",
      "        KurtosistestResult(statistic=-1.7058104152122062, pvalue=0.9559783083373583)\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> s = rng.normal(0, 1, 1000)\n",
      "        >>> kurtosistest(s)\n",
      "        KurtosistestResult(statistic=-1.475047944490622, pvalue=0.14019965402996987)\n",
      "    \n",
      "    levene(*args, center='median', proportiontocut=0.05)\n",
      "        Perform Levene test for equal variances.\n",
      "        \n",
      "        The Levene test tests the null hypothesis that all input samples\n",
      "        are from populations with equal variances.  Levene's test is an\n",
      "        alternative to Bartlett's test `bartlett` in the case where\n",
      "        there are significant deviations from normality.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            The sample data, possibly with different lengths. Only one-dimensional\n",
      "            samples are accepted.\n",
      "        center : {'mean', 'median', 'trimmed'}, optional\n",
      "            Which function of the data to use in the test.  The default\n",
      "            is 'median'.\n",
      "        proportiontocut : float, optional\n",
      "            When `center` is 'trimmed', this gives the proportion of data points\n",
      "            to cut from each end. (See `scipy.stats.trim_mean`.)\n",
      "            Default is 0.05.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        pvalue : float\n",
      "            The p-value for the test.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Three variations of Levene's test are possible.  The possibilities\n",
      "        and their recommended usages are:\n",
      "        \n",
      "          * 'median' : Recommended for skewed (non-normal) distributions>\n",
      "          * 'mean' : Recommended for symmetric, moderate-tailed distributions.\n",
      "          * 'trimmed' : Recommended for heavy-tailed distributions.\n",
      "        \n",
      "        The test version using the mean was proposed in the original article\n",
      "        of Levene ([2]_) while the median and trimmed mean have been studied by\n",
      "        Brown and Forsythe ([3]_), sometimes also referred to as Brown-Forsythe\n",
      "        test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm\n",
      "        .. [2] Levene, H. (1960). In Contributions to Probability and Statistics:\n",
      "               Essays in Honor of Harold Hotelling, I. Olkin et al. eds.,\n",
      "               Stanford University Press, pp. 278-292.\n",
      "        .. [3] Brown, M. B. and Forsythe, A. B. (1974), Journal of the American\n",
      "               Statistical Association, 69, 364-367\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Test whether or not the lists `a`, `b` and `c` come from populations\n",
      "        with equal variances.\n",
      "        \n",
      "        >>> from scipy.stats import levene\n",
      "        >>> a = [8.88, 9.12, 9.04, 8.98, 9.00, 9.08, 9.01, 8.85, 9.06, 8.99]\n",
      "        >>> b = [8.88, 8.95, 9.29, 9.44, 9.15, 9.58, 8.36, 9.18, 8.67, 9.05]\n",
      "        >>> c = [8.95, 9.12, 8.95, 8.85, 9.03, 8.84, 9.07, 8.98, 8.86, 8.98]\n",
      "        >>> stat, p = levene(a, b, c)\n",
      "        >>> p\n",
      "        0.002431505967249681\n",
      "        \n",
      "        The small p-value suggests that the populations do not have equal\n",
      "        variances.\n",
      "        \n",
      "        This is not surprising, given that the sample variance of `b` is much\n",
      "        larger than that of `a` and `c`:\n",
      "        \n",
      "        >>> [np.var(x, ddof=1) for x in [a, b, c]]\n",
      "        [0.007054444444444413, 0.13073888888888888, 0.008890000000000002]\n",
      "    \n",
      "    linregress(x, y=None, alternative='two-sided')\n",
      "        Calculate a linear least-squares regression for two sets of measurements.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Two sets of measurements.  Both arrays should have the same length.  If\n",
      "            only `x` is given (and ``y=None``), then it must be a two-dimensional\n",
      "            array where one dimension has length 2.  The two sets of measurements\n",
      "            are then found by splitting the array along the length-2 dimension. In\n",
      "            the case where ``y=None`` and `x` is a 2x2 array, ``linregress(x)`` is\n",
      "            equivalent to ``linregress(x[0], x[1])``.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
      "            The following options are available:\n",
      "        \n",
      "            * 'two-sided': the slope of the regression line is nonzero\n",
      "            * 'less': the slope of the regression line is less than zero\n",
      "            * 'greater':  the slope of the regression line is greater than zero\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result : ``LinregressResult`` instance\n",
      "            The return value is an object with the following attributes:\n",
      "        \n",
      "            slope : float\n",
      "                Slope of the regression line.\n",
      "            intercept : float\n",
      "                Intercept of the regression line.\n",
      "            rvalue : float\n",
      "                Correlation coefficient.\n",
      "            pvalue : float\n",
      "                The p-value for a hypothesis test whose null hypothesis is\n",
      "                that the slope is zero, using Wald Test with t-distribution of\n",
      "                the test statistic. See `alternative` above for alternative\n",
      "                hypotheses.\n",
      "            stderr : float\n",
      "                Standard error of the estimated slope (gradient), under the\n",
      "                assumption of residual normality.\n",
      "            intercept_stderr : float\n",
      "                Standard error of the estimated intercept, under the assumption\n",
      "                of residual normality.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.optimize.curve_fit :\n",
      "            Use non-linear least squares to fit a function to data.\n",
      "        scipy.optimize.leastsq :\n",
      "            Minimize the sum of squares of a set of equations.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Missing values are considered pair-wise: if a value is missing in `x`,\n",
      "        the corresponding value in `y` is masked.\n",
      "        \n",
      "        For compatibility with older versions of SciPy, the return value acts\n",
      "        like a ``namedtuple`` of length 5, with fields ``slope``, ``intercept``,\n",
      "        ``rvalue``, ``pvalue`` and ``stderr``, so one can continue to write::\n",
      "        \n",
      "            slope, intercept, r, p, se = linregress(x, y)\n",
      "        \n",
      "        With that style, however, the standard error of the intercept is not\n",
      "        available.  To have access to all the computed values, including the\n",
      "        standard error of the intercept, use the return value as an object\n",
      "        with attributes, e.g.::\n",
      "        \n",
      "            result = linregress(x, y)\n",
      "            print(result.intercept, result.intercept_stderr)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        Generate some data:\n",
      "        \n",
      "        >>> x = rng.random(10)\n",
      "        >>> y = 1.6*x + rng.random(10)\n",
      "        \n",
      "        Perform the linear regression:\n",
      "        \n",
      "        >>> res = stats.linregress(x, y)\n",
      "        \n",
      "        Coefficient of determination (R-squared):\n",
      "        \n",
      "        >>> print(f\"R-squared: {res.rvalue**2:.6f}\")\n",
      "        R-squared: 0.717533\n",
      "        \n",
      "        Plot the data along with the fitted line:\n",
      "        \n",
      "        >>> plt.plot(x, y, 'o', label='original data')\n",
      "        >>> plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Calculate 95% confidence interval on slope and intercept:\n",
      "        \n",
      "        >>> # Two-sided inverse Students t-distribution\n",
      "        >>> # p - probability, df - degrees of freedom\n",
      "        >>> from scipy.stats import t\n",
      "        >>> tinv = lambda p, df: abs(t.ppf(p/2, df))\n",
      "        \n",
      "        >>> ts = tinv(0.05, len(x)-2)\n",
      "        >>> print(f\"slope (95%): {res.slope:.6f} +/- {ts*res.stderr:.6f}\")\n",
      "        slope (95%): 1.453392 +/- 0.743465\n",
      "        >>> print(f\"intercept (95%): {res.intercept:.6f}\"\n",
      "        ...       f\" +/- {ts*res.intercept_stderr:.6f}\")\n",
      "        intercept (95%): 0.616950 +/- 0.544475\n",
      "    \n",
      "    mannwhitneyu(x, y, use_continuity=True, alternative='two-sided', axis=0, method='auto')\n",
      "        Perform the Mann-Whitney U rank test on two independent samples.\n",
      "        \n",
      "        The Mann-Whitney U test is a nonparametric test of the null hypothesis\n",
      "        that the distribution underlying sample `x` is the same as the\n",
      "        distribution underlying sample `y`. It is often used as a test of\n",
      "        of difference in location between distributions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like\n",
      "            N-d arrays of samples. The arrays must be broadcastable except along\n",
      "            the dimension given by `axis`.\n",
      "        use_continuity : bool, optional\n",
      "                Whether a continuity correction (1/2) should be applied.\n",
      "                Default is True when `method` is ``'asymptotic'``; has no effect\n",
      "                otherwise.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
      "            Let *F(u)* and *G(u)* be the cumulative distribution functions of the\n",
      "            distributions underlying `x` and `y`, respectively. Then the following\n",
      "            alternative hypotheses are available:\n",
      "        \n",
      "            * 'two-sided': the distributions are not equal, i.e. *F(u) ≠ G(u)* for\n",
      "              at least one *u*.\n",
      "            * 'less': the distribution underlying `x` is stochastically less\n",
      "              than the distribution underlying `y`, i.e. *F(u) > G(u)* for all *u*.\n",
      "            * 'greater': the distribution underlying `x` is stochastically greater\n",
      "              than the distribution underlying `y`, i.e. *F(u) < G(u)* for all *u*.\n",
      "        \n",
      "            Under a more restrictive set of assumptions, the alternative hypotheses\n",
      "            can be expressed in terms of the locations of the distributions;\n",
      "            see [5] section 5.1.\n",
      "        axis : int, optional\n",
      "            Axis along which to perform the test. Default is 0.\n",
      "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
      "            Selects the method used to calculate the *p*-value.\n",
      "            Default is 'auto'. The following options are available.\n",
      "        \n",
      "            * ``'asymptotic'``: compares the standardized test statistic\n",
      "              against the normal distribution, correcting for ties.\n",
      "            * ``'exact'``: computes the exact *p*-value by comparing the observed\n",
      "              :math:`U` statistic against the exact distribution of the :math:`U`\n",
      "              statistic under the null hypothesis. No correction is made for ties.\n",
      "            * ``'auto'``: chooses ``'exact'`` when the size of one of the samples\n",
      "              is less than 8 and there are no ties; chooses ``'asymptotic'``\n",
      "              otherwise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : MannwhitneyuResult\n",
      "            An object containing attributes:\n",
      "        \n",
      "            statistic : float\n",
      "                The Mann-Whitney U statistic corresponding with sample `x`. See\n",
      "                Notes for the test statistic corresponding with sample `y`.\n",
      "            pvalue : float\n",
      "                The associated *p*-value for the chosen `alternative`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If ``U1`` is the statistic corresponding with sample `x`, then the\n",
      "        statistic corresponding with sample `y` is\n",
      "        `U2 = `x.shape[axis] * y.shape[axis] - U1``.\n",
      "        \n",
      "        `mannwhitneyu` is for independent samples. For related / paired samples,\n",
      "        consider `scipy.stats.wilcoxon`.\n",
      "        \n",
      "        `method` ``'exact'`` is recommended when there are no ties and when either\n",
      "        sample size is less than 8 [1]_. The implementation follows the recurrence\n",
      "        relation originally proposed in [1]_ as it is described in [3]_.\n",
      "        Note that the exact method is *not* corrected for ties, but\n",
      "        `mannwhitneyu` will not raise errors or warnings if there are ties in the\n",
      "        data.\n",
      "        \n",
      "        The Mann-Whitney U test is a non-parametric version of the t-test for\n",
      "        independent samples. When the the means of samples from the populations\n",
      "        are normally distributed, consider `scipy.stats.ttest_ind`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.stats.wilcoxon, scipy.stats.ranksums, scipy.stats.ttest_ind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] H.B. Mann and D.R. Whitney, \"On a test of whether one of two random\n",
      "               variables is stochastically larger than the other\", The Annals of\n",
      "               Mathematical Statistics, Vol. 18, pp. 50-60, 1947.\n",
      "        .. [2] Mann-Whitney U Test, Wikipedia,\n",
      "               http://en.wikipedia.org/wiki/Mann-Whitney_U_test\n",
      "        .. [3] A. Di Bucchianico, \"Combinatorics, computer algebra, and the\n",
      "               Wilcoxon-Mann-Whitney test\", Journal of Statistical Planning and\n",
      "               Inference, Vol. 79, pp. 349-364, 1999.\n",
      "        .. [4] Rosie Shier, \"Statistics: 2.3 The Mann-Whitney U Test\", Mathematics\n",
      "               Learning Support Centre, 2004.\n",
      "        .. [5] Michael P. Fay and Michael A. Proschan. \"Wilcoxon-Mann-Whitney\n",
      "               or t-test? On assumptions for hypothesis tests and multiple \\\n",
      "               interpretations of decision rules.\" Statistics surveys, Vol. 4, pp.\n",
      "               1-39, 2010. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2857732/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We follow the example from [4]_: nine randomly sampled young adults were\n",
      "        diagnosed with type II diabetes at the ages below.\n",
      "        \n",
      "        >>> males = [19, 22, 16, 29, 24]\n",
      "        >>> females = [20, 11, 17, 12]\n",
      "        \n",
      "        We use the Mann-Whitney U test to assess whether there is a statistically\n",
      "        significant difference in the diagnosis age of males and females.\n",
      "        The null hypothesis is that the distribution of male diagnosis ages is\n",
      "        the same as the distribution of female diagnosis ages. We decide\n",
      "        that a confidence level of 95% is required to reject the null hypothesis\n",
      "        in favor of the alternative that that the distributions are different.\n",
      "        Since the number of samples is very small and there are no ties in the\n",
      "        data, we can compare the observed test statistic against the *exact*\n",
      "        distribution of the test statistic under the null hypothesis.\n",
      "        \n",
      "        >>> from scipy.stats import mannwhitneyu\n",
      "        >>> U1, p = mannwhitneyu(males, females, method=\"exact\")\n",
      "        >>> print(U1)\n",
      "        17.0\n",
      "        \n",
      "        `mannwhitneyu` always reports the statistic associated with the first\n",
      "        sample, which, in this case, is males. This agrees with :math:`U_M = 17`\n",
      "        reported in [4]_. The statistic associated with the second statistic\n",
      "        can be calculated:\n",
      "        \n",
      "        >>> nx, ny = len(males), len(females)\n",
      "        >>> U2 = nx*ny - U1\n",
      "        >>> print(U2)\n",
      "        3.0\n",
      "        \n",
      "        This agrees with :math:`U_F = 3` reported in [4]_. The two-sided\n",
      "        *p*-value can be calculated from either statistic, and the value produced\n",
      "        by `mannwhitneyu` agrees with :math:`p = 0.11` reported in [4]_.\n",
      "        \n",
      "        >>> print(p)\n",
      "        0.1111111111111111\n",
      "        \n",
      "        The exact distribution of the test statistic is asymptotically normal, so\n",
      "        the example continues by comparing the exact *p*-value against the\n",
      "        *p*-value produced using the normal approximation.\n",
      "        \n",
      "        >>> _, pnorm = mannwhitneyu(males, females, method=\"asymptotic\")\n",
      "        >>> print(pnorm)\n",
      "        0.11134688653314041\n",
      "        \n",
      "        Here `mannwhitneyu`'s reported *p*-value appears to conflict with the\n",
      "        value :math:`p = 0.09` given in [4]_. The reason is that [4]_\n",
      "        does not apply the continuity correction performed by `mannwhitneyu`;\n",
      "        `mannwhitneyu` reduces the distance between the test statistic and the\n",
      "        mean :math:`\\mu = n_x n_y / 2` by 0.5 to correct for the fact that the\n",
      "        discrete statistic is being compared against a continuous distribution.\n",
      "        Here, the :math:`U` statistic used is less than the mean, so we reduce\n",
      "        the distance by adding 0.5 in the numerator.\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from scipy.stats import norm\n",
      "        >>> U = min(U1, U2)\n",
      "        >>> N = nx + ny\n",
      "        >>> z = (U - nx*ny/2 + 0.5) / np.sqrt(nx*ny * (N + 1)/ 12)\n",
      "        >>> p = 2 * norm.cdf(z)  # use CDF to get p-value from smaller statistic\n",
      "        >>> print(p)\n",
      "        0.11134688653314041\n",
      "        \n",
      "        If desired, we can disable the continuity correction to get a result\n",
      "        that agrees with that reported in [4]_.\n",
      "        \n",
      "        >>> _, pnorm = mannwhitneyu(males, females, use_continuity=False,\n",
      "        ...                         method=\"asymptotic\")\n",
      "        >>> print(pnorm)\n",
      "        0.0864107329737\n",
      "        \n",
      "        Regardless of whether we perform an exact or asymptotic test, the\n",
      "        probability of the test statistic being as extreme or more extreme by\n",
      "        chance exceeds 5%, so we do not consider the results statistically\n",
      "        significant.\n",
      "        \n",
      "        Suppose that, before seeing the data, we had hypothesized that females\n",
      "        would tend to be diagnosed at a younger age than males.\n",
      "        In that case, it would be natural to provide the female ages as the\n",
      "        first input, and we would have performed a one-sided test using\n",
      "        ``alternative = 'less'``: females are diagnosed at an age that is\n",
      "        stochastically less than that of males.\n",
      "        \n",
      "        >>> res = mannwhitneyu(females, males, alternative=\"less\", method=\"exact\")\n",
      "        >>> print(res)\n",
      "        MannwhitneyuResult(statistic=3.0, pvalue=0.05555555555555555)\n",
      "        \n",
      "        Again, the probability of getting a sufficiently low value of the\n",
      "        test statistic by chance under the null hypothesis is greater than 5%,\n",
      "        so we do not reject the null hypothesis in favor of our alternative.\n",
      "        \n",
      "        If it is reasonable to assume that the means of samples from the\n",
      "        populations are normally distributed, we could have used a t-test to\n",
      "        perform the analysis.\n",
      "        \n",
      "        >>> from scipy.stats import ttest_ind\n",
      "        >>> res = ttest_ind(females, males, alternative=\"less\")\n",
      "        >>> print(res)\n",
      "        Ttest_indResult(statistic=-2.239334696520584, pvalue=0.030068441095757924)\n",
      "        \n",
      "        Under this assumption, the *p*-value would be low enough to reject the\n",
      "        null hypothesis in favor of the alternative.\n",
      "    \n",
      "    median_abs_deviation(x, axis=0, center=<function median at 0x00000181AAAA4E50>, scale=1.0, nan_policy='propagate')\n",
      "        Compute the median absolute deviation of the data along the given axis.\n",
      "        \n",
      "        The median absolute deviation (MAD, [1]_) computes the median over the\n",
      "        absolute deviations from the median. It is a measure of dispersion\n",
      "        similar to the standard deviation but more robust to outliers [2]_.\n",
      "        \n",
      "        The MAD of an empty array is ``np.nan``.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the range is computed. Default is 0. If None, compute\n",
      "            the MAD over the entire array.\n",
      "        center : callable, optional\n",
      "            A function that will return the central value. The default is to use\n",
      "            np.median. Any user defined function used will need to have the\n",
      "            function signature ``func(arr, axis)``.\n",
      "        scale : scalar or str, optional\n",
      "            The numerical value of scale will be divided out of the final\n",
      "            result. The default is 1.0. The string \"normal\" is also accepted,\n",
      "            and results in `scale` being the inverse of the standard normal\n",
      "            quantile function at 0.75, which is approximately 0.67449.\n",
      "            Array-like scale is also allowed, as long as it broadcasts correctly\n",
      "            to the output such that ``out / scale`` is a valid operation. The\n",
      "            output dimensions depend on the input array, `x`, and the `axis`\n",
      "            argument.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mad : scalar or ndarray\n",
      "            If ``axis=None``, a scalar is returned. If the input contains\n",
      "            integers or floats of smaller precision than ``np.float64``, then the\n",
      "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
      "            the same as that of the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\n",
      "        scipy.stats.tstd, scipy.stats.tvar\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `center` argument only affects the calculation of the central value\n",
      "        around which the MAD is calculated. That is, passing in ``center=np.mean``\n",
      "        will calculate the MAD around the mean - it will not calculate the *mean*\n",
      "        absolute deviation.\n",
      "        \n",
      "        The input array may contain `inf`, but if `center` returns `inf`, the\n",
      "        corresponding MAD for that data will be `nan`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Median absolute deviation\",\n",
      "               https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
      "        .. [2] \"Robust measures of scale\",\n",
      "               https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        When comparing the behavior of `median_abs_deviation` with ``np.std``,\n",
      "        the latter is affected when we change a single value of an array to have an\n",
      "        outlier value while the MAD hardly changes:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n",
      "        >>> x.std()\n",
      "        0.9973906394005013\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        0.82832610097857\n",
      "        >>> x[0] = 345.6\n",
      "        >>> x.std()\n",
      "        34.42304872314415\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        0.8323442311590675\n",
      "        \n",
      "        Axis handling example:\n",
      "        \n",
      "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
      "        >>> x\n",
      "        array([[10,  7,  4],\n",
      "               [ 3,  2,  1]])\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        array([3.5, 2.5, 1.5])\n",
      "        >>> stats.median_abs_deviation(x, axis=None)\n",
      "        2.0\n",
      "        \n",
      "        Scale normal example:\n",
      "        \n",
      "        >>> x = stats.norm.rvs(size=1000000, scale=2, random_state=123456)\n",
      "        >>> stats.median_abs_deviation(x)\n",
      "        1.3487398527041636\n",
      "        >>> stats.median_abs_deviation(x, scale='normal')\n",
      "        1.9996446978061115\n",
      "    \n",
      "    median_absolute_deviation(*args, **kwds)\n",
      "        `median_absolute_deviation` is deprecated, use `median_abs_deviation` instead!\n",
      "        \n",
      "        To preserve the existing default behavior, use\n",
      "        `scipy.stats.median_abs_deviation(..., scale=1/1.4826)`.\n",
      "        The value 1.4826 is not numerically precise for scaling\n",
      "        with a normal distribution. For a numerically precise value, use\n",
      "        `scipy.stats.median_abs_deviation(..., scale='normal')`.\n",
      "        \n",
      "        \n",
      "        Compute the median absolute deviation of the data along the given axis.\n",
      "        \n",
      "        The median absolute deviation (MAD, [1]_) computes the median over the\n",
      "        absolute deviations from the median. It is a measure of dispersion\n",
      "        similar to the standard deviation but more robust to outliers [2]_.\n",
      "        \n",
      "        The MAD of an empty array is ``np.nan``.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array or object that can be converted to an array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the range is computed. Default is 0. If None, compute\n",
      "            the MAD over the entire array.\n",
      "        center : callable, optional\n",
      "            A function that will return the central value. The default is to use\n",
      "            np.median. Any user defined function used will need to have the function\n",
      "            signature ``func(arr, axis)``.\n",
      "        scale : int, optional\n",
      "            The scaling factor applied to the MAD. The default scale (1.4826)\n",
      "            ensures consistency with the standard deviation for normally distributed\n",
      "            data.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mad : scalar or ndarray\n",
      "            If ``axis=None``, a scalar is returned. If the input contains\n",
      "            integers or floats of smaller precision than ``np.float64``, then the\n",
      "            output data-type is ``np.float64``. Otherwise, the output data-type is\n",
      "            the same as that of the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.std, numpy.var, numpy.median, scipy.stats.iqr, scipy.stats.tmean,\n",
      "        scipy.stats.tstd, scipy.stats.tvar\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The `center` argument only affects the calculation of the central value\n",
      "        around which the MAD is calculated. That is, passing in ``center=np.mean``\n",
      "        will calculate the MAD around the mean - it will not calculate the *mean*\n",
      "        absolute deviation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Median absolute deviation\",\n",
      "               https://en.wikipedia.org/wiki/Median_absolute_deviation\n",
      "        .. [2] \"Robust measures of scale\",\n",
      "               https://en.wikipedia.org/wiki/Robust_measures_of_scale\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        When comparing the behavior of `median_absolute_deviation` with ``np.std``,\n",
      "        the latter is affected when we change a single value of an array to have an\n",
      "        outlier value while the MAD hardly changes:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> x = stats.norm.rvs(size=100, scale=1, random_state=123456)\n",
      "        >>> x.std()\n",
      "        0.9973906394005013\n",
      "        >>> stats.median_absolute_deviation(x)\n",
      "        1.2280762773108278\n",
      "        >>> x[0] = 345.6\n",
      "        >>> x.std()\n",
      "        34.42304872314415\n",
      "        >>> stats.median_absolute_deviation(x)\n",
      "        1.2340335571164334\n",
      "        \n",
      "        Axis handling example:\n",
      "        \n",
      "        >>> x = np.array([[10, 7, 4], [3, 2, 1]])\n",
      "        >>> x\n",
      "        array([[10,  7,  4],\n",
      "               [ 3,  2,  1]])\n",
      "        >>> stats.median_absolute_deviation(x)\n",
      "        array([5.1891, 3.7065, 2.2239])\n",
      "        >>> stats.median_absolute_deviation(x, axis=None)\n",
      "        2.9652\n",
      "    \n",
      "    median_test(*args, ties='below', correction=True, lambda_=1, nan_policy='propagate')\n",
      "        Perform a Mood's median test.\n",
      "        \n",
      "        Test that two or more samples come from populations with the same median.\n",
      "        \n",
      "        Let ``n = len(args)`` be the number of samples.  The \"grand median\" of\n",
      "        all the data is computed, and a contingency table is formed by\n",
      "        classifying the values in each sample as being above or below the grand\n",
      "        median.  The contingency table, along with `correction` and `lambda_`,\n",
      "        are passed to `scipy.stats.chi2_contingency` to compute the test statistic\n",
      "        and p-value.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sample1, sample2, ... : array_like\n",
      "            The set of samples.  There must be at least two samples.\n",
      "            Each sample must be a one-dimensional sequence containing at least\n",
      "            one value.  The samples are not required to have the same length.\n",
      "        ties : str, optional\n",
      "            Determines how values equal to the grand median are classified in\n",
      "            the contingency table.  The string must be one of::\n",
      "        \n",
      "                \"below\":\n",
      "                    Values equal to the grand median are counted as \"below\".\n",
      "                \"above\":\n",
      "                    Values equal to the grand median are counted as \"above\".\n",
      "                \"ignore\":\n",
      "                    Values equal to the grand median are not counted.\n",
      "        \n",
      "            The default is \"below\".\n",
      "        correction : bool, optional\n",
      "            If True, *and* there are just two samples, apply Yates' correction\n",
      "            for continuity when computing the test statistic associated with\n",
      "            the contingency table.  Default is True.\n",
      "        lambda_ : float or str, optional\n",
      "            By default, the statistic computed in this test is Pearson's\n",
      "            chi-squared statistic.  `lambda_` allows a statistic from the\n",
      "            Cressie-Read power divergence family to be used instead.  See\n",
      "            `power_divergence` for details.\n",
      "            Default is 1 (Pearson's chi-squared statistic).\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stat : float\n",
      "            The test statistic.  The statistic that is returned is determined by\n",
      "            `lambda_`.  The default is Pearson's chi-squared statistic.\n",
      "        p : float\n",
      "            The p-value of the test.\n",
      "        m : float\n",
      "            The grand median.\n",
      "        table : ndarray\n",
      "            The contingency table.  The shape of the table is (2, n), where\n",
      "            n is the number of samples.  The first row holds the counts of the\n",
      "            values above the grand median, and the second row holds the counts\n",
      "            of the values below the grand median.  The table allows further\n",
      "            analysis with, for example, `scipy.stats.chi2_contingency`, or with\n",
      "            `scipy.stats.fisher_exact` if there are two samples, without having\n",
      "            to recompute the table.  If ``nan_policy`` is \"propagate\" and there\n",
      "            are nans in the input, the return value for ``table`` is ``None``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kruskal : Compute the Kruskal-Wallis H-test for independent samples.\n",
      "        mannwhitneyu : Computes the Mann-Whitney rank test on samples x and y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Mood, A. M., Introduction to the Theory of Statistics. McGraw-Hill\n",
      "            (1950), pp. 394-399.\n",
      "        .. [2] Zar, J. H., Biostatistical Analysis, 5th ed. Prentice Hall (2010).\n",
      "            See Sections 8.12 and 10.15.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        A biologist runs an experiment in which there are three groups of plants.\n",
      "        Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants.\n",
      "        Each plant produces a number of seeds.  The seed counts for each group\n",
      "        are::\n",
      "        \n",
      "            Group 1: 10 14 14 18 20 22 24 25 31 31 32 39 43 43 48 49\n",
      "            Group 2: 28 30 31 33 34 35 36 40 44 55 57 61 91 92 99\n",
      "            Group 3:  0  3  9 22 23 25 25 33 34 34 40 45 46 48 62 67 84\n",
      "        \n",
      "        The following code applies Mood's median test to these samples.\n",
      "        \n",
      "        >>> g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]\n",
      "        >>> g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]\n",
      "        >>> g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]\n",
      "        >>> from scipy.stats import median_test\n",
      "        >>> stat, p, med, tbl = median_test(g1, g2, g3)\n",
      "        \n",
      "        The median is\n",
      "        \n",
      "        >>> med\n",
      "        34.0\n",
      "        \n",
      "        and the contingency table is\n",
      "        \n",
      "        >>> tbl\n",
      "        array([[ 5, 10,  7],\n",
      "               [11,  5, 10]])\n",
      "        \n",
      "        `p` is too large to conclude that the medians are not the same:\n",
      "        \n",
      "        >>> p\n",
      "        0.12609082774093244\n",
      "        \n",
      "        The \"G-test\" can be performed by passing ``lambda_=\"log-likelihood\"`` to\n",
      "        `median_test`.\n",
      "        \n",
      "        >>> g, p, med, tbl = median_test(g1, g2, g3, lambda_=\"log-likelihood\")\n",
      "        >>> p\n",
      "        0.12224779737117837\n",
      "        \n",
      "        The median occurs several times in the data, so we'll get a different\n",
      "        result if, for example, ``ties=\"above\"`` is used:\n",
      "        \n",
      "        >>> stat, p, med, tbl = median_test(g1, g2, g3, ties=\"above\")\n",
      "        >>> p\n",
      "        0.063873276069553273\n",
      "        \n",
      "        >>> tbl\n",
      "        array([[ 5, 11,  9],\n",
      "               [11,  4,  8]])\n",
      "        \n",
      "        This example demonstrates that if the data set is not large and there\n",
      "        are values equal to the median, the p-value can be sensitive to the\n",
      "        choice of `ties`.\n",
      "    \n",
      "    mode(a, axis=0, nan_policy='propagate')\n",
      "        Return an array of the modal (most common) value in the passed array.\n",
      "        \n",
      "        If there is more than one such value, only the smallest is returned.\n",
      "        The bin-count for the modal bins is also returned.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            n-dimensional array of which to find mode(s).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mode : ndarray\n",
      "            Array of modal values.\n",
      "        count : ndarray\n",
      "            Array of counts for each mode.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = np.array([[6, 8, 3, 0],\n",
      "        ...               [3, 2, 1, 7],\n",
      "        ...               [8, 1, 8, 4],\n",
      "        ...               [5, 3, 0, 5],\n",
      "        ...               [4, 7, 5, 9]])\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.mode(a)\n",
      "        ModeResult(mode=array([[3, 1, 0, 0]]), count=array([[1, 1, 1, 1]]))\n",
      "        \n",
      "        To get mode of whole array, specify ``axis=None``:\n",
      "        \n",
      "        >>> stats.mode(a, axis=None)\n",
      "        ModeResult(mode=array([3]), count=array([3]))\n",
      "    \n",
      "    moment(a, moment=1, axis=0, nan_policy='propagate')\n",
      "        Calculate the nth moment about the mean for a sample.\n",
      "        \n",
      "        A moment is a specific quantitative measure of the shape of a set of\n",
      "        points. It is often used to calculate coefficients of skewness and kurtosis\n",
      "        due to its close relationship with them.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "           Input array.\n",
      "        moment : int or array_like of ints, optional\n",
      "           Order of central moment that is returned. Default is 1.\n",
      "        axis : int or None, optional\n",
      "           Axis along which the central moment is computed. Default is 0.\n",
      "           If None, compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        n-th central moment : ndarray or float\n",
      "           The appropriate moment along the given axis or over all values if axis\n",
      "           is None. The denominator for the moment calculation is the number of\n",
      "           observations, no degrees of freedom correction is done.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kurtosis, skew, describe\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The k-th central moment of a data sample is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            m_k = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^k\n",
      "        \n",
      "        Where n is the number of samples and x-bar is the mean. This function uses\n",
      "        exponentiation by squares [1]_ for efficiency.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://eli.thegreenplace.net/2009/03/21/efficient-integer-exponentiation-algorithms\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import moment\n",
      "        >>> moment([1, 2, 3, 4, 5], moment=1)\n",
      "        0.0\n",
      "        >>> moment([1, 2, 3, 4, 5], moment=2)\n",
      "        2.0\n",
      "    \n",
      "    mood(x, y, axis=0, alternative='two-sided')\n",
      "        Perform Mood's test for equal scale parameters.\n",
      "        \n",
      "        Mood's two-sample test for scale parameters is a non-parametric\n",
      "        test for the null hypothesis that two samples are drawn from the\n",
      "        same distribution with the same scale parameter.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of sample data.\n",
      "        axis : int, optional\n",
      "            The axis along which the samples are tested.  `x` and `y` can be of\n",
      "            different length along `axis`.\n",
      "            If `axis` is None, `x` and `y` are flattened and the test is done on\n",
      "            all values in the flattened arrays.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
      "            The following options are available:\n",
      "        \n",
      "            * 'two-sided': the scales of the distributions underlying `x` and `y`\n",
      "              are different.\n",
      "            * 'less': the scale of the distribution underlying `x` is less than\n",
      "              the scale of the distribution underlying `y`.\n",
      "            * 'greater': the scale of the distribution underlying `x` is greater\n",
      "              than the scale of the distribution underlying `y`.\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : scalar or ndarray\n",
      "            The z-score for the hypothesis test.  For 1-D inputs a scalar is\n",
      "            returned.\n",
      "        p-value : scalar ndarray\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fligner : A non-parametric test for the equality of k variances\n",
      "        ansari : A non-parametric test for the equality of 2 variances\n",
      "        bartlett : A parametric test for equality of k variances in normal samples\n",
      "        levene : A parametric test for equality of k variances\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The data are assumed to be drawn from probability distributions ``f(x)``\n",
      "        and ``f(x/s) / s`` respectively, for some probability density function f.\n",
      "        The null hypothesis is that ``s == 1``.\n",
      "        \n",
      "        For multi-dimensional arrays, if the inputs are of shapes\n",
      "        ``(n0, n1, n2, n3)``  and ``(n0, m1, n2, n3)``, then if ``axis=1``, the\n",
      "        resulting z and p values will have shape ``(n0, n2, n3)``.  Note that\n",
      "        ``n1`` and ``m1`` don't have to be equal, but the other dimensions do.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x2 = rng.standard_normal((2, 45, 6, 7))\n",
      "        >>> x1 = rng.standard_normal((2, 30, 6, 7))\n",
      "        >>> z, p = stats.mood(x1, x2, axis=1)\n",
      "        >>> p.shape\n",
      "        (2, 6, 7)\n",
      "        \n",
      "        Find the number of points where the difference in scale is not significant:\n",
      "        \n",
      "        >>> (p > 0.1).sum()\n",
      "        78\n",
      "        \n",
      "        Perform the test with different scales:\n",
      "        \n",
      "        >>> x1 = rng.standard_normal((2, 30))\n",
      "        >>> x2 = rng.standard_normal((2, 35)) * 10.0\n",
      "        >>> stats.mood(x1, x2, axis=1)\n",
      "        (array([-5.76174136, -6.12650783]), array([8.32505043e-09, 8.98287869e-10]))\n",
      "    \n",
      "    multiscale_graphcorr(x, y, compute_distance=<function _euclidean_dist at 0x00000181ABB72040>, reps=1000, workers=1, is_twosamp=False, random_state=None)\n",
      "        Computes the Multiscale Graph Correlation (MGC) test statistic.\n",
      "        \n",
      "        Specifically, for each point, MGC finds the :math:`k`-nearest neighbors for\n",
      "        one property (e.g. cloud density), and the :math:`l`-nearest neighbors for\n",
      "        the other property (e.g. grass wetness) [1]_. This pair :math:`(k, l)` is\n",
      "        called the \"scale\". A priori, however, it is not know which scales will be\n",
      "        most informative. So, MGC computes all distance pairs, and then efficiently\n",
      "        computes the distance correlations for all scales. The local correlations\n",
      "        illustrate which scales are relatively informative about the relationship.\n",
      "        The key, therefore, to successfully discover and decipher relationships\n",
      "        between disparate data modalities is to adaptively determine which scales\n",
      "        are the most informative, and the geometric implication for the most\n",
      "        informative scales. Doing so not only provides an estimate of whether the\n",
      "        modalities are related, but also provides insight into how the\n",
      "        determination was made. This is especially important in high-dimensional\n",
      "        data, where simple visualizations do not reveal relationships to the\n",
      "        unaided human eye. Characterizations of this implementation in particular\n",
      "        have been derived from and benchmarked within in [2]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : ndarray\n",
      "            If ``x`` and ``y`` have shapes ``(n, p)`` and ``(n, q)`` where `n` is\n",
      "            the number of samples and `p` and `q` are the number of dimensions,\n",
      "            then the MGC independence test will be run.  Alternatively, ``x`` and\n",
      "            ``y`` can have shapes ``(n, n)`` if they are distance or similarity\n",
      "            matrices, and ``compute_distance`` must be sent to ``None``. If ``x``\n",
      "            and ``y`` have shapes ``(n, p)`` and ``(m, p)``, an unpaired\n",
      "            two-sample MGC test will be run.\n",
      "        compute_distance : callable, optional\n",
      "            A function that computes the distance or similarity among the samples\n",
      "            within each data matrix. Set to ``None`` if ``x`` and ``y`` are\n",
      "            already distance matrices. The default uses the euclidean norm metric.\n",
      "            If you are calling a custom function, either create the distance\n",
      "            matrix before-hand or create a function of the form\n",
      "            ``compute_distance(x)`` where `x` is the data matrix for which\n",
      "            pairwise distances are calculated.\n",
      "        reps : int, optional\n",
      "            The number of replications used to estimate the null when using the\n",
      "            permutation test. The default is ``1000``.\n",
      "        workers : int or map-like callable, optional\n",
      "            If ``workers`` is an int the population is subdivided into ``workers``\n",
      "            sections and evaluated in parallel (uses ``multiprocessing.Pool\n",
      "            <multiprocessing>``). Supply ``-1`` to use all cores available to the\n",
      "            Process. Alternatively supply a map-like callable, such as\n",
      "            ``multiprocessing.Pool.map`` for evaluating the p-value in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable. The default is ``1``.\n",
      "        is_twosamp : bool, optional\n",
      "            If `True`, a two sample test will be run. If ``x`` and ``y`` have\n",
      "            shapes ``(n, p)`` and ``(m, p)``, this optional will be overriden and\n",
      "            set to ``True``. Set to ``True`` if ``x`` and ``y`` both have shapes\n",
      "            ``(n, p)`` and a two sample test is desired. The default is ``False``.\n",
      "            Note that this will not run if inputs are distance matrices.\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        stat : float\n",
      "            The sample MGC test statistic within `[-1, 1]`.\n",
      "        pvalue : float\n",
      "            The p-value obtained via permutation.\n",
      "        mgc_dict : dict\n",
      "            Contains additional useful additional returns containing the following\n",
      "            keys:\n",
      "        \n",
      "                - mgc_map : ndarray\n",
      "                    A 2D representation of the latent geometry of the relationship.\n",
      "                    of the relationship.\n",
      "                - opt_scale : (int, int)\n",
      "                    The estimated optimal scale as a `(x, y)` pair.\n",
      "                - null_dist : list\n",
      "                    The null distribution derived from the permuted matrices\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pearsonr : Pearson correlation coefficient and p-value for testing\n",
      "                   non-correlation.\n",
      "        kendalltau : Calculates Kendall's tau.\n",
      "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A description of the process of MGC and applications on neuroscience data\n",
      "        can be found in [1]_. It is performed using the following steps:\n",
      "        \n",
      "        #. Two distance matrices :math:`D^X` and :math:`D^Y` are computed and\n",
      "           modified to be mean zero columnwise. This results in two\n",
      "           :math:`n \\times n` distance matrices :math:`A` and :math:`B` (the\n",
      "           centering and unbiased modification) [3]_.\n",
      "        \n",
      "        #. For all values :math:`k` and :math:`l` from :math:`1, ..., n`,\n",
      "        \n",
      "           * The :math:`k`-nearest neighbor and :math:`l`-nearest neighbor graphs\n",
      "             are calculated for each property. Here, :math:`G_k (i, j)` indicates\n",
      "             the :math:`k`-smallest values of the :math:`i`-th row of :math:`A`\n",
      "             and :math:`H_l (i, j)` indicates the :math:`l` smallested values of\n",
      "             the :math:`i`-th row of :math:`B`\n",
      "        \n",
      "           * Let :math:`\\circ` denotes the entry-wise matrix product, then local\n",
      "             correlations are summed and normalized using the following statistic:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            c^{kl} = \\frac{\\sum_{ij} A G_k B H_l}\n",
      "                          {\\sqrt{\\sum_{ij} A^2 G_k \\times \\sum_{ij} B^2 H_l}}\n",
      "        \n",
      "        #. The MGC test statistic is the smoothed optimal local correlation of\n",
      "           :math:`\\{ c^{kl} \\}`. Denote the smoothing operation as :math:`R(\\cdot)`\n",
      "           (which essentially set all isolated large correlations) as 0 and\n",
      "           connected large correlations the same as before, see [3]_.) MGC is,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MGC_n (x, y) = \\max_{(k, l)} R \\left(c^{kl} \\left( x_n, y_n \\right)\n",
      "                                                        \\right)\n",
      "        \n",
      "        The test statistic returns a value between :math:`(-1, 1)` since it is\n",
      "        normalized.\n",
      "        \n",
      "        The p-value returned is calculated using a permutation test. This process\n",
      "        is completed by first randomly permuting :math:`y` to estimate the null\n",
      "        distribution and then calculating the probability of observing a test\n",
      "        statistic, under the null, at least as extreme as the observed test\n",
      "        statistic.\n",
      "        \n",
      "        MGC requires at least 5 samples to run with reliable results. It can also\n",
      "        handle high-dimensional data sets.\n",
      "        In addition, by manipulating the input data matrices, the two-sample\n",
      "        testing problem can be reduced to the independence testing problem [4]_.\n",
      "        Given sample data :math:`U` and :math:`V` of sizes :math:`p \\times n`\n",
      "        :math:`p \\times m`, data matrix :math:`X` and :math:`Y` can be created as\n",
      "        follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            X = [U | V] \\in \\mathcal{R}^{p \\times (n + m)}\n",
      "            Y = [0_{1 \\times n} | 1_{1 \\times m}] \\in \\mathcal{R}^{(n + m)}\n",
      "        \n",
      "        Then, the MGC statistic can be calculated as normal. This methodology can\n",
      "        be extended to similar tests such as distance correlation [4]_.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Vogelstein, J. T., Bridgeford, E. W., Wang, Q., Priebe, C. E.,\n",
      "               Maggioni, M., & Shen, C. (2019). Discovering and deciphering\n",
      "               relationships across disparate data modalities. ELife.\n",
      "        .. [2] Panda, S., Palaniappan, S., Xiong, J., Swaminathan, A.,\n",
      "               Ramachandran, S., Bridgeford, E. W., ... Vogelstein, J. T. (2019).\n",
      "               mgcpy: A Comprehensive High Dimensional Independence Testing Python\n",
      "               Package. :arXiv:`1907.02088`\n",
      "        .. [3] Shen, C., Priebe, C.E., & Vogelstein, J. T. (2019). From distance\n",
      "               correlation to multiscale graph correlation. Journal of the American\n",
      "               Statistical Association.\n",
      "        .. [4] Shen, C. & Vogelstein, J. T. (2018). The Exact Equivalence of\n",
      "               Distance and Kernel Methods for Hypothesis Testing.\n",
      "               :arXiv:`1806.05514`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import multiscale_graphcorr\n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = x\n",
      "        >>> stat, pvalue, _ = multiscale_graphcorr(x, y, workers=-1)\n",
      "        >>> '%.1f, %.3f' % (stat, pvalue)\n",
      "        '1.0, 0.001'\n",
      "        \n",
      "        Alternatively,\n",
      "        \n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = x\n",
      "        >>> mgc = multiscale_graphcorr(x, y)\n",
      "        >>> '%.1f, %.3f' % (mgc.stat, mgc.pvalue)\n",
      "        '1.0, 0.001'\n",
      "        \n",
      "        To run an unpaired two-sample test,\n",
      "        \n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = np.arange(79)\n",
      "        >>> mgc = multiscale_graphcorr(x, y)\n",
      "        >>> '%.3f, %.2f' % (mgc.stat, mgc.pvalue)  # doctest: +SKIP\n",
      "        '0.033, 0.02'\n",
      "        \n",
      "        or, if shape of the inputs are the same,\n",
      "        \n",
      "        >>> x = np.arange(100)\n",
      "        >>> y = x\n",
      "        >>> mgc = multiscale_graphcorr(x, y, is_twosamp=True)\n",
      "        >>> '%.3f, %.1f' % (mgc.stat, mgc.pvalue)  # doctest: +SKIP\n",
      "        '-0.008, 1.0'\n",
      "    \n",
      "    mvsdist(data)\n",
      "        'Frozen' distributions for mean, variance, and standard deviation of data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array_like\n",
      "            Input array. Converted to 1-D using ravel.\n",
      "            Requires 2 or more data-points.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mdist : \"frozen\" distribution object\n",
      "            Distribution object representing the mean of the data.\n",
      "        vdist : \"frozen\" distribution object\n",
      "            Distribution object representing the variance of the data.\n",
      "        sdist : \"frozen\" distribution object\n",
      "            Distribution object representing the standard deviation of the data.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bayes_mvs\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The return values from ``bayes_mvs(data)`` is equivalent to\n",
      "        ``tuple((x.mean(), x.interval(0.90)) for x in mvsdist(data))``.\n",
      "        \n",
      "        In other words, calling ``<dist>.mean()`` and ``<dist>.interval(0.90)``\n",
      "        on the three distribution objects returned from this function will give\n",
      "        the same results that are returned from `bayes_mvs`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        T.E. Oliphant, \"A Bayesian perspective on estimating mean, variance, and\n",
      "        standard-deviation from data\", https://scholarsarchive.byu.edu/facpub/278,\n",
      "        2006.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> data = [6, 9, 12, 7, 8, 8, 13]\n",
      "        >>> mean, var, std = stats.mvsdist(data)\n",
      "        \n",
      "        We now have frozen distribution objects \"mean\", \"var\" and \"std\" that we can\n",
      "        examine:\n",
      "        \n",
      "        >>> mean.mean()\n",
      "        9.0\n",
      "        >>> mean.interval(0.95)\n",
      "        (6.6120585482655692, 11.387941451734431)\n",
      "        >>> mean.std()\n",
      "        1.1952286093343936\n",
      "    \n",
      "    normaltest(a, axis=0, nan_policy='propagate')\n",
      "        Test whether a sample differs from a normal distribution.\n",
      "        \n",
      "        This function tests the null hypothesis that a sample comes\n",
      "        from a normal distribution.  It is based on D'Agostino and\n",
      "        Pearson's [1]_, [2]_ test that combines skew and kurtosis to\n",
      "        produce an omnibus test of normality.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            The array containing the sample to be tested.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. Default is 0. If None,\n",
      "            compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            ``s^2 + k^2``, where ``s`` is the z-score returned by `skewtest` and\n",
      "            ``k`` is the z-score returned by `kurtosistest`.\n",
      "        pvalue : float or array\n",
      "           A 2-sided chi squared probability for the hypothesis test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] D'Agostino, R. B. (1971), \"An omnibus test of normality for\n",
      "               moderate and large sample size\", Biometrika, 58, 341-348\n",
      "        \n",
      "        .. [2] D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from\n",
      "               normality\", Biometrika, 60, 613-622\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> pts = 1000\n",
      "        >>> a = rng.normal(0, 1, size=pts)\n",
      "        >>> b = rng.normal(2, 1, size=pts)\n",
      "        >>> x = np.concatenate((a, b))\n",
      "        >>> k2, p = stats.normaltest(x)\n",
      "        >>> alpha = 1e-3\n",
      "        >>> print(\"p = {:g}\".format(p))\n",
      "        p = 8.4713e-19\n",
      "        >>> if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
      "        ...     print(\"The null hypothesis can be rejected\")\n",
      "        ... else:\n",
      "        ...     print(\"The null hypothesis cannot be rejected\")\n",
      "        The null hypothesis can be rejected\n",
      "    \n",
      "    obrientransform(*args)\n",
      "        Compute the O'Brien transform on input data (any number of arrays).\n",
      "        \n",
      "        Used to test for homogeneity of variance prior to running one-way stats.\n",
      "        Each array in ``*args`` is one level of a factor.\n",
      "        If `f_oneway` is run on the transformed data and found significant,\n",
      "        the variances are unequal.  From Maxwell and Delaney [1]_, p.112.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        args : tuple of array_like\n",
      "            Any number of arrays.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        obrientransform : ndarray\n",
      "            Transformed data for use in an ANOVA.  The first dimension\n",
      "            of the result corresponds to the sequence of transformed\n",
      "            arrays.  If the arrays given are all 1-D of the same length,\n",
      "            the return value is a 2-D array; otherwise it is a 1-D array\n",
      "            of type object, with each element being an ndarray.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] S. E. Maxwell and H. D. Delaney, \"Designing Experiments and\n",
      "               Analyzing Data: A Model Comparison Perspective\", Wadsworth, 1990.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We'll test the following data sets for differences in their variance.\n",
      "        \n",
      "        >>> x = [10, 11, 13, 9, 7, 12, 12, 9, 10]\n",
      "        >>> y = [13, 21, 5, 10, 8, 14, 10, 12, 7, 15]\n",
      "        \n",
      "        Apply the O'Brien transform to the data.\n",
      "        \n",
      "        >>> from scipy.stats import obrientransform\n",
      "        >>> tx, ty = obrientransform(x, y)\n",
      "        \n",
      "        Use `scipy.stats.f_oneway` to apply a one-way ANOVA test to the\n",
      "        transformed data.\n",
      "        \n",
      "        >>> from scipy.stats import f_oneway\n",
      "        >>> F, p = f_oneway(tx, ty)\n",
      "        >>> p\n",
      "        0.1314139477040335\n",
      "        \n",
      "        If we require that ``p < 0.05`` for significance, we cannot conclude\n",
      "        that the variances are different.\n",
      "    \n",
      "    page_trend_test(data, ranked=False, predicted_ranks=None, method='auto')\n",
      "        Perform Page's Test, a measure of trend in observations between treatments.\n",
      "        \n",
      "        Page's Test (also known as Page's :math:`L` test) is useful when:\n",
      "        \n",
      "        * there are :math:`n \\geq 3` treatments,\n",
      "        * :math:`m \\geq 2` subjects are observed for each treatment, and\n",
      "        * the observations are hypothesized to have a particular order.\n",
      "        \n",
      "        Specifically, the test considers the null hypothesis that\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            m_1 = m_2 = m_3 \\cdots = m_n,\n",
      "        \n",
      "        where :math:`m_j` is the mean of the observed quantity under treatment\n",
      "        :math:`j`, against the alternative hypothesis that\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            m_1 \\leq m_2 \\leq m_3 \\leq \\cdots \\leq m_n,\n",
      "        \n",
      "        where at least one inequality is strict.\n",
      "        \n",
      "        As noted by [4]_, Page's :math:`L` test has greater statistical power than\n",
      "        the Friedman test against the alternative that there is a difference in\n",
      "        trend, as Friedman's test only considers a difference in the means of the\n",
      "        observations without considering their order. Whereas Spearman :math:`\\rho`\n",
      "        considers the correlation between the ranked observations of two variables\n",
      "        (e.g. the airspeed velocity of a swallow vs. the weight of the coconut it\n",
      "        carries), Page's :math:`L` is concerned with a trend in an observation\n",
      "        (e.g. the airspeed velocity of a swallow) across several distinct\n",
      "        treatments (e.g. carrying each of five coconuts of different weight) even\n",
      "        as the observation is repeated with multiple subjects (e.g. one European\n",
      "        swallow and one African swallow).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data : array-like\n",
      "            A :math:`m \\times n` array; the element in row :math:`i` and\n",
      "            column :math:`j` is the observation corresponding with subject\n",
      "            :math:`i` and treatment :math:`j`. By default, the columns are\n",
      "            assumed to be arranged in order of increasing predicted mean.\n",
      "        \n",
      "        ranked : boolean, optional\n",
      "            By default, `data` is assumed to be observations rather than ranks;\n",
      "            it will be ranked with `scipy.stats.rankdata` along ``axis=1``. If\n",
      "            `data` is provided in the form of ranks, pass argument ``True``.\n",
      "        \n",
      "        predicted_ranks : array-like, optional\n",
      "            The predicted ranks of the column means. If not specified,\n",
      "            the columns are assumed to be arranged in order of increasing\n",
      "            predicted mean, so the default `predicted_ranks` are\n",
      "            :math:`[1, 2, \\dots, n-1, n]`.\n",
      "        \n",
      "        method : {'auto', 'asymptotic', 'exact'}, optional\n",
      "            Selects the method used to calculate the *p*-value. The following\n",
      "            options are available.\n",
      "        \n",
      "            * 'auto': selects between 'exact' and 'asymptotic' to\n",
      "              achieve reasonably accurate results in reasonable time (default)\n",
      "            * 'asymptotic': compares the standardized test statistic against\n",
      "              the normal distribution\n",
      "            * 'exact': computes the exact *p*-value by comparing the observed\n",
      "              :math:`L` statistic against those realized by all possible\n",
      "              permutations of ranks (under the null hypothesis that each\n",
      "              permutation is equally likely)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : PageTrendTestResult\n",
      "            An object containing attributes:\n",
      "        \n",
      "            statistic : float\n",
      "                Page's :math:`L` test statistic.\n",
      "            pvalue : float\n",
      "                The associated *p*-value\n",
      "            method : {'asymptotic', 'exact'}\n",
      "                The method used to compute the *p*-value\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rankdata, friedmanchisquare, spearmanr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As noted in [1]_, \"the :math:`n` 'treatments' could just as well represent\n",
      "        :math:`n` objects or events or performances or persons or trials ranked.\"\n",
      "        Similarly, the :math:`m` 'subjects' could equally stand for :math:`m`\n",
      "        \"groupings by ability or some other control variable, or judges doing\n",
      "        the ranking, or random replications of some other sort.\"\n",
      "        \n",
      "        The procedure for calculating the :math:`L` statistic, adapted from\n",
      "        [1]_, is:\n",
      "        \n",
      "        1. \"Predetermine with careful logic the appropriate hypotheses\n",
      "           concerning the predicted ording of the experimental results.\n",
      "           If no reasonable basis for ordering any treatments is known, the\n",
      "           :math:`L` test is not appropriate.\"\n",
      "        2. \"As in other experiments, determine at what level of confidence\n",
      "           you will reject the null hypothesis that there is no agreement of\n",
      "           experimental results with the monotonic hypothesis.\"\n",
      "        3. \"Cast the experimental material into a two-way table of :math:`n`\n",
      "           columns (treatments, objects ranked, conditions) and :math:`m`\n",
      "           rows (subjects, replication groups, levels of control variables).\"\n",
      "        4. \"When experimental observations are recorded, rank them across each\n",
      "           row\", e.g. ``ranks = scipy.stats.rankdata(data, axis=1)``.\n",
      "        5. \"Add the ranks in each column\", e.g.\n",
      "           ``colsums = np.sum(ranks, axis=0)``.\n",
      "        6. \"Multiply each sum of ranks by the predicted rank for that same\n",
      "           column\", e.g. ``products = predicted_ranks * colsums``.\n",
      "        7. \"Sum all such products\", e.g. ``L = products.sum()``.\n",
      "        \n",
      "        [1]_ continues by suggesting use of the standardized statistic\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\chi_L^2 = \\frac{\\left[12L-3mn(n+1)^2\\right]^2}{mn^2(n^2-1)(n+1)}\n",
      "        \n",
      "        \"which is distributed approximately as chi-square with 1 degree of\n",
      "        freedom. The ordinary use of :math:`\\chi^2` tables would be\n",
      "        equivalent to a two-sided test of agreement. If a one-sided test\n",
      "        is desired, *as will almost always be the case*, the probability\n",
      "        discovered in the chi-square table should be *halved*.\"\n",
      "        \n",
      "        However, this standardized statistic does not distinguish between the\n",
      "        observed values being well correlated with the predicted ranks and being\n",
      "        _anti_-correlated with the predicted ranks. Instead, we follow [2]_\n",
      "        and calculate the standardized statistic\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\Lambda = \\frac{L - E_0}{\\sqrt{V_0}},\n",
      "        \n",
      "        where :math:`E_0 = \\frac{1}{4} mn(n+1)^2` and\n",
      "        :math:`V_0 = \\frac{1}{144} mn^2(n+1)(n^2-1)`, \"which is asymptotically\n",
      "        normal under the null hypothesis\".\n",
      "        \n",
      "        The *p*-value for ``method='exact'`` is generated by comparing the observed\n",
      "        value of :math:`L` against the :math:`L` values generated for all\n",
      "        :math:`(n!)^m` possible permutations of ranks. The calculation is performed\n",
      "        using the recursive method of [5].\n",
      "        \n",
      "        The *p*-values are not adjusted for the possibility of ties. When\n",
      "        ties are present, the reported  ``'exact'`` *p*-values may be somewhat\n",
      "        larger (i.e. more conservative) than the true *p*-value [2]_. The\n",
      "        ``'asymptotic'``` *p*-values, however, tend to be smaller (i.e. less\n",
      "        conservative) than the ``'exact'`` *p*-values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Ellis Batten Page, \"Ordered hypotheses for multiple treatments:\n",
      "           a significant test for linear ranks\", *Journal of the American\n",
      "           Statistical Association* 58(301), p. 216--230, 1963.\n",
      "        \n",
      "        .. [2] Markus Neuhauser, *Nonparametric Statistical Test: A computational\n",
      "           approach*, CRC Press, p. 150--152, 2012.\n",
      "        \n",
      "        .. [3] Statext LLC, \"Page's L Trend Test - Easy Statistics\", *Statext -\n",
      "           Statistics Study*, https://www.statext.com/practice/PageTrendTest03.php,\n",
      "           Accessed July 12, 2020.\n",
      "        \n",
      "        .. [4] \"Page's Trend Test\", *Wikipedia*, WikimediaFoundation,\n",
      "           https://en.wikipedia.org/wiki/Page%27s_trend_test,\n",
      "           Accessed July 12, 2020.\n",
      "        \n",
      "        .. [5] Robert E. Odeh, \"The exact distribution of Page's L-statistic in\n",
      "           the two-way layout\", *Communications in Statistics - Simulation and\n",
      "           Computation*,  6(1), p. 49--61, 1977.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We use the example from [3]_: 10 students are asked to rate three\n",
      "        teaching methods - tutorial, lecture, and seminar - on a scale of 1-5,\n",
      "        with 1 being the lowest and 5 being the highest. We have decided that\n",
      "        a confidence level of 99% is required to reject the null hypothesis in\n",
      "        favor of our alternative: that the seminar will have the highest ratings\n",
      "        and the tutorial will have the lowest. Initially, the data have been\n",
      "        tabulated with each row representing an individual student's ratings of\n",
      "        the three methods in the following order: tutorial, lecture, seminar.\n",
      "        \n",
      "        >>> table = [[3, 4, 3],\n",
      "        ...          [2, 2, 4],\n",
      "        ...          [3, 3, 5],\n",
      "        ...          [1, 3, 2],\n",
      "        ...          [2, 3, 2],\n",
      "        ...          [2, 4, 5],\n",
      "        ...          [1, 2, 4],\n",
      "        ...          [3, 4, 4],\n",
      "        ...          [2, 4, 5],\n",
      "        ...          [1, 3, 4]]\n",
      "        \n",
      "        Because the tutorial is hypothesized to have the lowest ratings, the\n",
      "        column corresponding with tutorial rankings should be first; the seminar\n",
      "        is hypothesized to have the highest ratings, so its column should be last.\n",
      "        Since the columns are already arranged in this order of increasing\n",
      "        predicted mean, we can pass the table directly into `page_trend_test`.\n",
      "        \n",
      "        >>> from scipy.stats import page_trend_test\n",
      "        >>> res = page_trend_test(table)\n",
      "        >>> res\n",
      "        PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n",
      "                            method='exact')\n",
      "        \n",
      "        This *p*-value indicates that there is a 0.1819% chance that\n",
      "        the :math:`L` statistic would reach such an extreme value under the null\n",
      "        hypothesis. Because 0.1819% is less than 1%, we have evidence to reject\n",
      "        the null hypothesis in favor of our alternative at a 99% confidence level.\n",
      "        \n",
      "        The value of the :math:`L` statistic is 133.5. To check this manually,\n",
      "        we rank the data such that high scores correspond with high ranks, settling\n",
      "        ties with an average rank:\n",
      "        \n",
      "        >>> from scipy.stats import rankdata\n",
      "        >>> ranks = rankdata(table, axis=1)\n",
      "        >>> ranks\n",
      "        array([[1.5, 3. , 1.5],\n",
      "               [1.5, 1.5, 3. ],\n",
      "               [1.5, 1.5, 3. ],\n",
      "               [1. , 3. , 2. ],\n",
      "               [1.5, 3. , 1.5],\n",
      "               [1. , 2. , 3. ],\n",
      "               [1. , 2. , 3. ],\n",
      "               [1. , 2.5, 2.5],\n",
      "               [1. , 2. , 3. ],\n",
      "               [1. , 2. , 3. ]])\n",
      "        \n",
      "        We add the ranks within each column, multiply the sums by the\n",
      "        predicted ranks, and sum the products.\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> m, n = ranks.shape\n",
      "        >>> predicted_ranks = np.arange(1, n+1)\n",
      "        >>> L = (predicted_ranks * np.sum(ranks, axis=0)).sum()\n",
      "        >>> res.statistic == L\n",
      "        True\n",
      "        \n",
      "        As presented in [3]_, the asymptotic approximation of the *p*-value is the\n",
      "        survival function of the normal distribution evaluated at the standardized\n",
      "        test statistic:\n",
      "        \n",
      "        >>> from scipy.stats import norm\n",
      "        >>> E0 = (m*n*(n+1)**2)/4\n",
      "        >>> V0 = (m*n**2*(n+1)*(n**2-1))/144\n",
      "        >>> Lambda = (L-E0)/np.sqrt(V0)\n",
      "        >>> p = norm.sf(Lambda)\n",
      "        >>> p\n",
      "        0.0012693433690751756\n",
      "        \n",
      "        This does not precisely match the *p*-value reported by `page_trend_test`\n",
      "        above. The asymptotic distribution is not very accurate, nor conservative,\n",
      "        for :math:`m \\leq 12` and :math:`n \\leq 8`, so `page_trend_test` chose to\n",
      "        use ``method='exact'`` based on the dimensions of the table and the\n",
      "        recommendations in Page's original paper [1]_. To override\n",
      "        `page_trend_test`'s choice, provide the `method` argument.\n",
      "        \n",
      "        >>> res = page_trend_test(table, method=\"asymptotic\")\n",
      "        >>> res\n",
      "        PageTrendTestResult(statistic=133.5, pvalue=0.0012693433690751756,\n",
      "                            method='asymptotic')\n",
      "        \n",
      "        If the data are already ranked, we can pass in the ``ranks`` instead of\n",
      "        the ``table`` to save computation time.\n",
      "        \n",
      "        >>> res = page_trend_test(ranks,             # ranks of data\n",
      "        ...                       ranked=True,       # data is already ranked\n",
      "        ...                       )\n",
      "        >>> res\n",
      "        PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n",
      "                            method='exact')\n",
      "        \n",
      "        Suppose the raw data had been tabulated in an order different from the\n",
      "        order of predicted means, say lecture, seminar, tutorial.\n",
      "        \n",
      "        >>> table = np.asarray(table)[:, [1, 2, 0]]\n",
      "        \n",
      "        Since the arrangement of this table is not consistent with the assumed\n",
      "        ordering, we can either rearrange the table or provide the\n",
      "        `predicted_ranks`. Remembering that the lecture is predicted\n",
      "        to have the middle rank, the seminar the highest, and tutorial the lowest,\n",
      "        we pass:\n",
      "        \n",
      "        >>> res = page_trend_test(table,             # data as originally tabulated\n",
      "        ...                       predicted_ranks=[2, 3, 1],  # our predicted order\n",
      "        ...                       )\n",
      "        >>> res\n",
      "        PageTrendTestResult(statistic=133.5, pvalue=0.0018191161948127822,\n",
      "                            method='exact')\n",
      "    \n",
      "    pearsonr(x, y)\n",
      "        Pearson correlation coefficient and p-value for testing non-correlation.\n",
      "        \n",
      "        The Pearson correlation coefficient [1]_ measures the linear relationship\n",
      "        between two datasets.  The calculation of the p-value relies on the\n",
      "        assumption that each dataset is normally distributed.  (See Kowalski [3]_\n",
      "        for a discussion of the effects of non-normality of the input on the\n",
      "        distribution of the correlation coefficient.)  Like other correlation\n",
      "        coefficients, this one varies between -1 and +1 with 0 implying no\n",
      "        correlation. Correlations of -1 or +1 imply an exact linear relationship.\n",
      "        Positive correlations imply that as x increases, so does y. Negative\n",
      "        correlations imply that as x increases, y decreases.\n",
      "        \n",
      "        The p-value roughly indicates the probability of an uncorrelated system\n",
      "        producing datasets that have a Pearson correlation at least as extreme\n",
      "        as the one computed from these datasets.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : (N,) array_like\n",
      "            Input array.\n",
      "        y : (N,) array_like\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        r : float\n",
      "            Pearson's correlation coefficient.\n",
      "        p-value : float\n",
      "            Two-tailed p-value.\n",
      "        \n",
      "        Warns\n",
      "        -----\n",
      "        PearsonRConstantInputWarning\n",
      "            Raised if an input is a constant array.  The correlation coefficient\n",
      "            is not defined in this case, so ``np.nan`` is returned.\n",
      "        \n",
      "        PearsonRNearConstantInputWarning\n",
      "            Raised if an input is \"nearly\" constant.  The array ``x`` is considered\n",
      "            nearly constant if ``norm(x - mean(x)) < 1e-13 * abs(mean(x))``.\n",
      "            Numerical errors in the calculation ``x - mean(x)`` in this case might\n",
      "            result in an inaccurate calculation of r.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        spearmanr : Spearman rank-order correlation coefficient.\n",
      "        kendalltau : Kendall's tau, a correlation measure for ordinal data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The correlation coefficient is calculated as follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            r = \\frac{\\sum (x - m_x) (y - m_y)}\n",
      "                     {\\sqrt{\\sum (x - m_x)^2 \\sum (y - m_y)^2}}\n",
      "        \n",
      "        where :math:`m_x` is the mean of the vector :math:`x` and :math:`m_y` is\n",
      "        the mean of the vector :math:`y`.\n",
      "        \n",
      "        Under the assumption that :math:`x` and :math:`m_y` are drawn from\n",
      "        independent normal distributions (so the population correlation coefficient\n",
      "        is 0), the probability density function of the sample correlation\n",
      "        coefficient :math:`r` is ([1]_, [2]_):\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(r) = \\frac{{(1-r^2)}^{n/2-2}}{\\mathrm{B}(\\frac{1}{2},\\frac{n}{2}-1)}\n",
      "        \n",
      "        where n is the number of samples, and B is the beta function.  This\n",
      "        is sometimes referred to as the exact distribution of r.  This is\n",
      "        the distribution that is used in `pearsonr` to compute the p-value.\n",
      "        The distribution is a beta distribution on the interval [-1, 1],\n",
      "        with equal shape parameters a = b = n/2 - 1.  In terms of SciPy's\n",
      "        implementation of the beta distribution, the distribution of r is::\n",
      "        \n",
      "            dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
      "        \n",
      "        The p-value returned by `pearsonr` is a two-sided p-value.  For a\n",
      "        given sample with correlation coefficient r, the p-value is\n",
      "        the probability that abs(r') of a random sample x' and y' drawn from\n",
      "        the population with zero correlation would be greater than or equal\n",
      "        to abs(r).  In terms of the object ``dist`` shown above, the p-value\n",
      "        for a given r and length n can be computed as::\n",
      "        \n",
      "            p = 2*dist.cdf(-abs(r))\n",
      "        \n",
      "        When n is 2, the above continuous distribution is not well-defined.\n",
      "        One can interpret the limit of the beta distribution as the shape\n",
      "        parameters a and b approach a = b = 0 as a discrete distribution with\n",
      "        equal probability masses at r = 1 and r = -1.  More directly, one\n",
      "        can observe that, given the data x = [x1, x2] and y = [y1, y2], and\n",
      "        assuming x1 != x2 and y1 != y2, the only possible values for r are 1\n",
      "        and -1.  Because abs(r') for any sample x' and y' with length 2 will\n",
      "        be 1, the two-sided p-value for a sample of length 2 is always 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Pearson correlation coefficient\", Wikipedia,\n",
      "               https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
      "        .. [2] Student, \"Probable error of a correlation coefficient\",\n",
      "               Biometrika, Volume 6, Issue 2-3, 1 September 1908, pp. 302-310.\n",
      "        .. [3] C. J. Kowalski, \"On the Effects of Non-Normality on the Distribution\n",
      "               of the Sample Product-Moment Correlation Coefficient\"\n",
      "               Journal of the Royal Statistical Society. Series C (Applied\n",
      "               Statistics), Vol. 21, No. 1 (1972), pp. 1-12.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n",
      "        >>> b = np.arange(7)\n",
      "        >>> stats.pearsonr(a, b)\n",
      "        (0.8660254037844386, 0.011724811003954649)\n",
      "        \n",
      "        >>> stats.pearsonr([1, 2, 3, 4, 5], [10, 9, 2.5, 6, 4])\n",
      "        (-0.7426106572325057, 0.1505558088534455)\n",
      "    \n",
      "    percentileofscore(a, score, kind='rank')\n",
      "        Compute the percentile rank of a score relative to a list of scores.\n",
      "        \n",
      "        A `percentileofscore` of, for example, 80% means that 80% of the\n",
      "        scores in `a` are below the given score. In the case of gaps or\n",
      "        ties, the exact definition depends on the optional keyword, `kind`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of scores to which `score` is compared.\n",
      "        score : int or float\n",
      "            Score that is compared to the elements in `a`.\n",
      "        kind : {'rank', 'weak', 'strict', 'mean'}, optional\n",
      "            Specifies the interpretation of the resulting score.\n",
      "            The following options are available (default is 'rank'):\n",
      "        \n",
      "              * 'rank': Average percentage ranking of score.  In case of multiple\n",
      "                matches, average the percentage rankings of all matching scores.\n",
      "              * 'weak': This kind corresponds to the definition of a cumulative\n",
      "                distribution function.  A percentileofscore of 80% means that 80%\n",
      "                of values are less than or equal to the provided score.\n",
      "              * 'strict': Similar to \"weak\", except that only values that are\n",
      "                strictly less than the given score are counted.\n",
      "              * 'mean': The average of the \"weak\" and \"strict\" scores, often used\n",
      "                in testing.  See https://en.wikipedia.org/wiki/Percentile_rank\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        pcos : float\n",
      "            Percentile-position of score (0-100) relative to `a`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.percentile\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Three-quarters of the given values lie below a given score:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> stats.percentileofscore([1, 2, 3, 4], 3)\n",
      "        75.0\n",
      "        \n",
      "        With multiple matches, note how the scores of the two matches, 0.6\n",
      "        and 0.8 respectively, are averaged:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3)\n",
      "        70.0\n",
      "        \n",
      "        Only 2/5 values are strictly less than 3:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='strict')\n",
      "        40.0\n",
      "        \n",
      "        But 4/5 values are less than or equal to 3:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='weak')\n",
      "        80.0\n",
      "        \n",
      "        The average between the weak and the strict scores is:\n",
      "        \n",
      "        >>> stats.percentileofscore([1, 2, 3, 3, 4], 3, kind='mean')\n",
      "        60.0\n",
      "    \n",
      "    pointbiserialr(x, y)\n",
      "        Calculate a point biserial correlation coefficient and its p-value.\n",
      "        \n",
      "        The point biserial correlation is used to measure the relationship\n",
      "        between a binary variable, x, and a continuous variable, y. Like other\n",
      "        correlation coefficients, this one varies between -1 and +1 with 0\n",
      "        implying no correlation. Correlations of -1 or +1 imply a determinative\n",
      "        relationship.\n",
      "        \n",
      "        This function uses a shortcut formula but produces the same result as\n",
      "        `pearsonr`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like of bools\n",
      "            Input array.\n",
      "        y : array_like\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float\n",
      "            R value.\n",
      "        pvalue : float\n",
      "            Two-sided p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `pointbiserialr` uses a t-test with ``n-1`` degrees of freedom.\n",
      "        It is equivalent to `pearsonr`.\n",
      "        \n",
      "        The value of the point-biserial correlation can be calculated from:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            r_{pb} = \\frac{\\overline{Y_{1}} -\n",
      "                     \\overline{Y_{0}}}{s_{y}}\\sqrt{\\frac{N_{1} N_{2}}{N (N - 1))}}\n",
      "        \n",
      "        Where :math:`Y_{0}` and :math:`Y_{1}` are means of the metric\n",
      "        observations coded 0 and 1 respectively; :math:`N_{0}` and :math:`N_{1}`\n",
      "        are number of observations coded 0 and 1 respectively; :math:`N` is the\n",
      "        total number of observations and :math:`s_{y}` is the standard\n",
      "        deviation of all the metric observations.\n",
      "        \n",
      "        A value of :math:`r_{pb}` that is significantly different from zero is\n",
      "        completely equivalent to a significant difference in means between the two\n",
      "        groups. Thus, an independent groups t Test with :math:`N-2` degrees of\n",
      "        freedom may be used to test whether :math:`r_{pb}` is nonzero. The\n",
      "        relation between the t-statistic for comparing two independent groups and\n",
      "        :math:`r_{pb}` is given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            t = \\sqrt{N - 2}\\frac{r_{pb}}{\\sqrt{1 - r^{2}_{pb}}}\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Lev, \"The Point Biserial Coefficient of Correlation\", Ann. Math.\n",
      "               Statist., Vol. 20, no.1, pp. 125-126, 1949.\n",
      "        \n",
      "        .. [2] R.F. Tate, \"Correlation Between a Discrete and a Continuous\n",
      "               Variable. Point-Biserial Correlation.\", Ann. Math. Statist., Vol. 25,\n",
      "               np. 3, pp. 603-607, 1954.\n",
      "        \n",
      "        .. [3] D. Kornbrot \"Point Biserial Correlation\", In Wiley StatsRef:\n",
      "               Statistics Reference Online (eds N. Balakrishnan, et al.), 2014.\n",
      "               :doi:`10.1002/9781118445112.stat06227`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.array([0, 0, 0, 1, 1, 1, 1])\n",
      "        >>> b = np.arange(7)\n",
      "        >>> stats.pointbiserialr(a, b)\n",
      "        (0.8660254037844386, 0.011724811003954652)\n",
      "        >>> stats.pearsonr(a, b)\n",
      "        (0.86602540378443871, 0.011724811003954626)\n",
      "        >>> np.corrcoef(a, b)\n",
      "        array([[ 1.       ,  0.8660254],\n",
      "               [ 0.8660254,  1.       ]])\n",
      "    \n",
      "    power_divergence(f_obs, f_exp=None, ddof=0, axis=0, lambda_=None)\n",
      "        Cressie-Read power divergence statistic and goodness of fit test.\n",
      "        \n",
      "        This function tests the null hypothesis that the categorical data\n",
      "        has the given frequencies, using the Cressie-Read power divergence\n",
      "        statistic.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f_obs : array_like\n",
      "            Observed frequencies in each category.\n",
      "        f_exp : array_like, optional\n",
      "            Expected frequencies in each category.  By default the categories are\n",
      "            assumed to be equally likely.\n",
      "        ddof : int, optional\n",
      "            \"Delta degrees of freedom\": adjustment to the degrees of freedom\n",
      "            for the p-value.  The p-value is computed using a chi-squared\n",
      "            distribution with ``k - 1 - ddof`` degrees of freedom, where `k`\n",
      "            is the number of observed frequencies.  The default value of `ddof`\n",
      "            is 0.\n",
      "        axis : int or None, optional\n",
      "            The axis of the broadcast result of `f_obs` and `f_exp` along which to\n",
      "            apply the test.  If axis is None, all values in `f_obs` are treated\n",
      "            as a single data set.  Default is 0.\n",
      "        lambda_ : float or str, optional\n",
      "            The power in the Cressie-Read power divergence statistic.  The default\n",
      "            is 1.  For convenience, `lambda_` may be assigned one of the following\n",
      "            strings, in which case the corresponding numerical value is used::\n",
      "        \n",
      "                String              Value   Description\n",
      "                \"pearson\"             1     Pearson's chi-squared statistic.\n",
      "                                            In this case, the function is\n",
      "                                            equivalent to `stats.chisquare`.\n",
      "                \"log-likelihood\"      0     Log-likelihood ratio. Also known as\n",
      "                                            the G-test [3]_.\n",
      "                \"freeman-tukey\"      -1/2   Freeman-Tukey statistic.\n",
      "                \"mod-log-likelihood\" -1     Modified log-likelihood ratio.\n",
      "                \"neyman\"             -2     Neyman's statistic.\n",
      "                \"cressie-read\"        2/3   The power recommended in [5]_.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or ndarray\n",
      "            The Cressie-Read power divergence test statistic.  The value is\n",
      "            a float if `axis` is None or if` `f_obs` and `f_exp` are 1-D.\n",
      "        pvalue : float or ndarray\n",
      "            The p-value of the test.  The value is a float if `ddof` and the\n",
      "            return value `stat` are scalars.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chisquare\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This test is invalid when the observed or expected frequencies in each\n",
      "        category are too small.  A typical rule is that all of the observed\n",
      "        and expected frequencies should be at least 5.\n",
      "        \n",
      "        Also, the sum of the observed and expected frequencies must be the same\n",
      "        for the test to be valid; `power_divergence` raises an error if the sums\n",
      "        do not agree within a relative tolerance of ``1e-8``.\n",
      "        \n",
      "        When `lambda_` is less than zero, the formula for the statistic involves\n",
      "        dividing by `f_obs`, so a warning or error may be generated if any value\n",
      "        in `f_obs` is 0.\n",
      "        \n",
      "        Similarly, a warning or error may be generated if any value in `f_exp` is\n",
      "        zero when `lambda_` >= 0.\n",
      "        \n",
      "        The default degrees of freedom, k-1, are for the case when no parameters\n",
      "        of the distribution are estimated. If p parameters are estimated by\n",
      "        efficient maximum likelihood then the correct degrees of freedom are\n",
      "        k-1-p. If the parameters are estimated in a different way, then the\n",
      "        dof can be between k-1-p and k-1. However, it is also possible that\n",
      "        the asymptotic distribution is not a chisquare, in which case this\n",
      "        test is not appropriate.\n",
      "        \n",
      "        This function handles masked arrays.  If an element of `f_obs` or `f_exp`\n",
      "        is masked, then data at that position is ignored, and does not count\n",
      "        towards the size of the data set.\n",
      "        \n",
      "        .. versionadded:: 0.13.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n",
      "               Statistics\". Chapter 8.\n",
      "               https://web.archive.org/web/20171015035606/http://faculty.vassar.edu/lowry/ch8pt1.html\n",
      "        .. [2] \"Chi-squared test\", https://en.wikipedia.org/wiki/Chi-squared_test\n",
      "        .. [3] \"G-test\", https://en.wikipedia.org/wiki/G-test\n",
      "        .. [4] Sokal, R. R. and Rohlf, F. J. \"Biometry: the principles and\n",
      "               practice of statistics in biological research\", New York: Freeman\n",
      "               (1981)\n",
      "        .. [5] Cressie, N. and Read, T. R. C., \"Multinomial Goodness-of-Fit\n",
      "               Tests\", J. Royal Stat. Soc. Series B, Vol. 46, No. 3 (1984),\n",
      "               pp. 440-464.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        (See `chisquare` for more examples.)\n",
      "        \n",
      "        When just `f_obs` is given, it is assumed that the expected frequencies\n",
      "        are uniform and given by the mean of the observed frequencies.  Here we\n",
      "        perform a G-test (i.e. use the log-likelihood ratio statistic):\n",
      "        \n",
      "        >>> from scipy.stats import power_divergence\n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12], lambda_='log-likelihood')\n",
      "        (2.006573162632538, 0.84823476779463769)\n",
      "        \n",
      "        The expected frequencies can be given with the `f_exp` argument:\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12],\n",
      "        ...                  f_exp=[16, 16, 16, 16, 16, 8],\n",
      "        ...                  lambda_='log-likelihood')\n",
      "        (3.3281031458963746, 0.6495419288047497)\n",
      "        \n",
      "        When `f_obs` is 2-D, by default the test is applied to each column.\n",
      "        \n",
      "        >>> obs = np.array([[16, 18, 16, 14, 12, 12], [32, 24, 16, 28, 20, 24]]).T\n",
      "        >>> obs.shape\n",
      "        (6, 2)\n",
      "        >>> power_divergence(obs, lambda_=\"log-likelihood\")\n",
      "        (array([ 2.00657316,  6.77634498]), array([ 0.84823477,  0.23781225]))\n",
      "        \n",
      "        By setting ``axis=None``, the test is applied to all data in the array,\n",
      "        which is equivalent to applying the test to the flattened array.\n",
      "        \n",
      "        >>> power_divergence(obs, axis=None)\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        >>> power_divergence(obs.ravel())\n",
      "        (23.31034482758621, 0.015975692534127565)\n",
      "        \n",
      "        `ddof` is the change to make to the default degrees of freedom.\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=1)\n",
      "        (2.0, 0.73575888234288467)\n",
      "        \n",
      "        The calculation of the p-values is done by broadcasting the\n",
      "        test statistic with `ddof`.\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12], ddof=[0,1,2])\n",
      "        (2.0, array([ 0.84914504,  0.73575888,  0.5724067 ]))\n",
      "        \n",
      "        `f_obs` and `f_exp` are also broadcast.  In the following, `f_obs` has\n",
      "        shape (6,) and `f_exp` has shape (2, 6), so the result of broadcasting\n",
      "        `f_obs` and `f_exp` has shape (2, 6).  To compute the desired chi-squared\n",
      "        statistics, we must use ``axis=1``:\n",
      "        \n",
      "        >>> power_divergence([16, 18, 16, 14, 12, 12],\n",
      "        ...                  f_exp=[[16, 16, 16, 16, 16, 8],\n",
      "        ...                         [8, 20, 20, 16, 12, 12]],\n",
      "        ...                  axis=1)\n",
      "        (array([ 3.5 ,  9.25]), array([ 0.62338763,  0.09949846]))\n",
      "    \n",
      "    ppcc_max(x, brack=(0.0, 1.0), dist='tukeylambda')\n",
      "        Calculate the shape parameter that maximizes the PPCC.\n",
      "        \n",
      "        The probability plot correlation coefficient (PPCC) plot can be used\n",
      "        to determine the optimal shape parameter for a one-parameter family\n",
      "        of distributions. ``ppcc_max`` returns the shape parameter that would\n",
      "        maximize the probability plot correlation coefficient for the given\n",
      "        data to a one-parameter family of distributions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c) where (a<b<c). If bracket consists of two numbers (a, c)\n",
      "            then they are assumed to be a starting interval for a downhill bracket\n",
      "            search (see `scipy.optimize.brent`).\n",
      "        dist : str or stats.distributions instance, optional\n",
      "            Distribution or distribution function name.  Objects that look enough\n",
      "            like a stats.distributions instance (i.e. they have a ``ppf`` method)\n",
      "            are also accepted.  The default is ``'tukeylambda'``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        shape_value : float\n",
      "            The shape parameter at which the probability plot correlation\n",
      "            coefficient reaches its max value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ppcc_plot, probplot, boxcox\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The brack keyword serves as a starting point which is useful in corner\n",
      "        cases. One can use a plot to obtain a rough visual estimate of the location\n",
      "        for the maximum to start the search near it.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J.J. Filliben, \"The Probability Plot Correlation Coefficient Test\n",
      "               for Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n",
      "        .. [2] Engineering Statistics Handbook, NIST/SEMATEC,\n",
      "               https://www.itl.nist.gov/div898/handbook/eda/section3/ppccplot.htm\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First we generate some random data from a Weibull distribution\n",
      "        with shape parameter 2.5:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> c = 2.5\n",
      "        >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n",
      "        \n",
      "        Generate the PPCC plot for this data with the Weibull distribution.\n",
      "        \n",
      "        >>> fig, ax = plt.subplots(figsize=(8, 6))\n",
      "        >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax)\n",
      "        \n",
      "        We calculate the value where the shape should reach its maximum and a\n",
      "        red line is drawn there. The line should coincide with the highest\n",
      "        point in the PPCC graph.\n",
      "        \n",
      "        >>> cmax = stats.ppcc_max(x, brack=(c/2, 2*c), dist='weibull_min')\n",
      "        >>> ax.axvline(cmax, color='r')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    ppcc_plot(x, a, b, dist='tukeylambda', plot=None, N=80)\n",
      "        Calculate and optionally plot probability plot correlation coefficient.\n",
      "        \n",
      "        The probability plot correlation coefficient (PPCC) plot can be used to\n",
      "        determine the optimal shape parameter for a one-parameter family of\n",
      "        distributions.  It cannot be used for distributions without shape\n",
      "        parameters\n",
      "        (like the normal distribution) or with multiple shape parameters.\n",
      "        \n",
      "        By default a Tukey-Lambda distribution (`stats.tukeylambda`) is used. A\n",
      "        Tukey-Lambda PPCC plot interpolates from long-tailed to short-tailed\n",
      "        distributions via an approximately normal one, and is therefore\n",
      "        particularly useful in practice.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        a, b : scalar\n",
      "            Lower and upper bounds of the shape parameter to use.\n",
      "        dist : str or stats.distributions instance, optional\n",
      "            Distribution or distribution function name.  Objects that look enough\n",
      "            like a stats.distributions instance (i.e. they have a ``ppf`` method)\n",
      "            are also accepted.  The default is ``'tukeylambda'``.\n",
      "        plot : object, optional\n",
      "            If given, plots PPCC against the shape parameter.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        N : int, optional\n",
      "            Number of points on the horizontal axis (equally distributed from\n",
      "            `a` to `b`).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        svals : ndarray\n",
      "            The shape values for which `ppcc` was calculated.\n",
      "        ppcc : ndarray\n",
      "            The calculated probability plot correlation coefficient values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ppcc_max, probplot, boxcox_normplot, tukeylambda\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        J.J. Filliben, \"The Probability Plot Correlation Coefficient Test for\n",
      "        Normality\", Technometrics, Vol. 17, pp. 111-117, 1975.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First we generate some random data from a Weibull distribution\n",
      "        with shape parameter 2.5, and plot the histogram of the data:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> c = 2.5\n",
      "        >>> x = stats.weibull_min.rvs(c, scale=4, size=2000, random_state=rng)\n",
      "        \n",
      "        Take a look at the histogram of the data.\n",
      "        \n",
      "        >>> fig1, ax = plt.subplots(figsize=(9, 4))\n",
      "        >>> ax.hist(x, bins=50)\n",
      "        >>> ax.set_title('Histogram of x')\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Now we explore this data with a PPCC plot as well as the related\n",
      "        probability plot and Box-Cox normplot.  A red line is drawn where we\n",
      "        expect the PPCC value to be maximal (at the shape parameter ``c``\n",
      "        used above):\n",
      "        \n",
      "        >>> fig2 = plt.figure(figsize=(12, 4))\n",
      "        >>> ax1 = fig2.add_subplot(1, 3, 1)\n",
      "        >>> ax2 = fig2.add_subplot(1, 3, 2)\n",
      "        >>> ax3 = fig2.add_subplot(1, 3, 3)\n",
      "        >>> res = stats.probplot(x, plot=ax1)\n",
      "        >>> res = stats.boxcox_normplot(x, -4, 4, plot=ax2)\n",
      "        >>> res = stats.ppcc_plot(x, c/2, 2*c, dist='weibull_min', plot=ax3)\n",
      "        >>> ax3.axvline(c, color='r')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    probplot(x, sparams=(), dist='norm', fit=True, plot=None, rvalue=False)\n",
      "        Calculate quantiles for a probability plot, and optionally show the plot.\n",
      "        \n",
      "        Generates a probability plot of sample data against the quantiles of a\n",
      "        specified theoretical distribution (the normal distribution by default).\n",
      "        `probplot` optionally calculates a best-fit line for the data and plots the\n",
      "        results using Matplotlib or a given plot function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Sample/response data from which `probplot` creates the plot.\n",
      "        sparams : tuple, optional\n",
      "            Distribution-specific shape parameters (shape parameters plus location\n",
      "            and scale).\n",
      "        dist : str or stats.distributions instance, optional\n",
      "            Distribution or distribution function name. The default is 'norm' for a\n",
      "            normal probability plot.  Objects that look enough like a\n",
      "            stats.distributions instance (i.e. they have a ``ppf`` method) are also\n",
      "            accepted.\n",
      "        fit : bool, optional\n",
      "            Fit a least-squares regression (best-fit) line to the sample data if\n",
      "            True (default).\n",
      "        plot : object, optional\n",
      "            If given, plots the quantiles.\n",
      "            If given and `fit` is True, also plots the least squares fit.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        (osm, osr) : tuple of ndarrays\n",
      "            Tuple of theoretical quantiles (osm, or order statistic medians) and\n",
      "            ordered responses (osr).  `osr` is simply sorted input `x`.\n",
      "            For details on how `osm` is calculated see the Notes section.\n",
      "        (slope, intercept, r) : tuple of floats, optional\n",
      "            Tuple  containing the result of the least-squares fit, if that is\n",
      "            performed by `probplot`. `r` is the square root of the coefficient of\n",
      "            determination.  If ``fit=False`` and ``plot=None``, this tuple is not\n",
      "            returned.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Even if `plot` is given, the figure is not shown or saved by `probplot`;\n",
      "        ``plt.show()`` or ``plt.savefig('figname.png')`` should be used after\n",
      "        calling `probplot`.\n",
      "        \n",
      "        `probplot` generates a probability plot, which should not be confused with\n",
      "        a Q-Q or a P-P plot.  Statsmodels has more extensive functionality of this\n",
      "        type, see ``statsmodels.api.ProbPlot``.\n",
      "        \n",
      "        The formula used for the theoretical quantiles (horizontal axis of the\n",
      "        probability plot) is Filliben's estimate::\n",
      "        \n",
      "            quantiles = dist.ppf(val), for\n",
      "        \n",
      "                    0.5**(1/n),                  for i = n\n",
      "              val = (i - 0.3175) / (n + 0.365),  for i = 2, ..., n-1\n",
      "                    1 - 0.5**(1/n),              for i = 1\n",
      "        \n",
      "        where ``i`` indicates the i-th ordered value and ``n`` is the total number\n",
      "        of values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> nsample = 100\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        A t distribution with small degrees of freedom:\n",
      "        \n",
      "        >>> ax1 = plt.subplot(221)\n",
      "        >>> x = stats.t.rvs(3, size=nsample, random_state=rng)\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        A t distribution with larger degrees of freedom:\n",
      "        \n",
      "        >>> ax2 = plt.subplot(222)\n",
      "        >>> x = stats.t.rvs(25, size=nsample, random_state=rng)\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        A mixture of two normal distributions with broadcasting:\n",
      "        \n",
      "        >>> ax3 = plt.subplot(223)\n",
      "        >>> x = stats.norm.rvs(loc=[0,5], scale=[1,1.5],\n",
      "        ...                    size=(nsample//2,2), random_state=rng).ravel()\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        A standard normal distribution:\n",
      "        \n",
      "        >>> ax4 = plt.subplot(224)\n",
      "        >>> x = stats.norm.rvs(loc=0, scale=1, size=nsample, random_state=rng)\n",
      "        >>> res = stats.probplot(x, plot=plt)\n",
      "        \n",
      "        Produce a new figure with a loggamma distribution, using the ``dist`` and\n",
      "        ``sparams`` keywords:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> x = stats.loggamma.rvs(c=2.5, size=500, random_state=rng)\n",
      "        >>> res = stats.probplot(x, dist=stats.loggamma, sparams=(2.5,), plot=ax)\n",
      "        >>> ax.set_title(\"Probplot for loggamma dist with shape parameter 2.5\")\n",
      "        \n",
      "        Show the results with Matplotlib:\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    rankdata(a, method='average', *, axis=None)\n",
      "        Assign ranks to data, dealing with ties appropriately.\n",
      "        \n",
      "        By default (``axis=None``), the data array is first flattened, and a flat\n",
      "        array of ranks is returned. Separately reshape the rank array to the\n",
      "        shape of the data array if desired (see Examples).\n",
      "        \n",
      "        Ranks begin at 1.  The `method` argument controls how ranks are assigned\n",
      "        to equal values.  See [1]_ for further discussion of ranking methods.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            The array of values to be ranked.\n",
      "        method : {'average', 'min', 'max', 'dense', 'ordinal'}, optional\n",
      "            The method used to assign ranks to tied elements.\n",
      "            The following methods are available (default is 'average'):\n",
      "        \n",
      "              * 'average': The average of the ranks that would have been assigned to\n",
      "                all the tied values is assigned to each value.\n",
      "              * 'min': The minimum of the ranks that would have been assigned to all\n",
      "                the tied values is assigned to each value.  (This is also\n",
      "                referred to as \"competition\" ranking.)\n",
      "              * 'max': The maximum of the ranks that would have been assigned to all\n",
      "                the tied values is assigned to each value.\n",
      "              * 'dense': Like 'min', but the rank of the next highest element is\n",
      "                assigned the rank immediately after those assigned to the tied\n",
      "                elements.\n",
      "              * 'ordinal': All values are given a distinct rank, corresponding to\n",
      "                the order that the values occur in `a`.\n",
      "        axis : {None, int}, optional\n",
      "            Axis along which to perform the ranking. If ``None``, the data array\n",
      "            is first flattened.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ranks : ndarray\n",
      "             An array of size equal to the size of `a`, containing rank\n",
      "             scores.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Ranking\", https://en.wikipedia.org/wiki/Ranking\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import rankdata\n",
      "        >>> rankdata([0, 2, 3, 2])\n",
      "        array([ 1. ,  2.5,  4. ,  2.5])\n",
      "        >>> rankdata([0, 2, 3, 2], method='min')\n",
      "        array([ 1,  2,  4,  2])\n",
      "        >>> rankdata([0, 2, 3, 2], method='max')\n",
      "        array([ 1,  3,  4,  3])\n",
      "        >>> rankdata([0, 2, 3, 2], method='dense')\n",
      "        array([ 1,  2,  3,  2])\n",
      "        >>> rankdata([0, 2, 3, 2], method='ordinal')\n",
      "        array([ 1,  2,  4,  3])\n",
      "        >>> rankdata([[0, 2], [3, 2]]).reshape(2,2)\n",
      "        array([[1. , 2.5],\n",
      "              [4. , 2.5]])\n",
      "        >>> rankdata([[0, 2, 2], [3, 2, 5]], axis=1)\n",
      "        array([[1. , 2.5, 2.5],\n",
      "               [2. , 1. , 3. ]])\n",
      "    \n",
      "    ranksums(x, y, alternative='two-sided')\n",
      "        Compute the Wilcoxon rank-sum statistic for two samples.\n",
      "        \n",
      "        The Wilcoxon rank-sum test tests the null hypothesis that two sets\n",
      "        of measurements are drawn from the same distribution.  The alternative\n",
      "        hypothesis is that values in one sample are more likely to be\n",
      "        larger than the values in the other sample.\n",
      "        \n",
      "        This test should be used to compare two samples from continuous\n",
      "        distributions.  It does not handle ties between measurements\n",
      "        in x and y.  For tie-handling and an optional continuity correction\n",
      "        see `scipy.stats.mannwhitneyu`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x,y : array_like\n",
      "            The data from the two samples.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
      "            The following options are available:\n",
      "        \n",
      "            * 'two-sided': one of the distributions (underlying `x` or `y`) is\n",
      "              stochastically greater than the other.\n",
      "            * 'less': the distribution underlying `x` is stochastically less\n",
      "              than the distribution underlying `y`.\n",
      "            * 'greater': the distribution underlying `x` is stochastically greater\n",
      "              than the distribution underlying `y`.\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic under the large-sample approximation that the\n",
      "            rank sum statistic is normally distributed.\n",
      "        pvalue : float\n",
      "            The p-value of the test.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Wilcoxon_rank-sum_test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We can test the hypothesis that two independent unequal-sized samples are\n",
      "        drawn from the same distribution with computing the Wilcoxon rank-sum\n",
      "        statistic.\n",
      "        \n",
      "        >>> from scipy.stats import ranksums\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> sample1 = rng.uniform(-1, 1, 200)\n",
      "        >>> sample2 = rng.uniform(-0.5, 1.5, 300) # a shifted distribution\n",
      "        >>> ranksums(sample1, sample2)\n",
      "        RanksumsResult(statistic=-7.887059, pvalue=3.09390448e-15)  # may vary\n",
      "        >>> ranksums(sample1, sample2, alternative='less')\n",
      "        RanksumsResult(statistic=-7.750585297581713, pvalue=4.573497606342543e-15) # may vary\n",
      "        >>> ranksums(sample1, sample2, alternative='greater')\n",
      "        RanksumsResult(statistic=-7.750585297581713, pvalue=0.9999999999999954) # may vary\n",
      "        \n",
      "        The p-value of less than ``0.05`` indicates that this test rejects the\n",
      "        hypothesis at the 5% significance level.\n",
      "    \n",
      "    relfreq(a, numbins=10, defaultreallimits=None, weights=None)\n",
      "        Return a relative frequency histogram, using the histogram function.\n",
      "        \n",
      "        A relative frequency  histogram is a mapping of the number of\n",
      "        observations in each of the bins relative to the total of observations.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        numbins : int, optional\n",
      "            The number of bins to use for the histogram. Default is 10.\n",
      "        defaultreallimits : tuple (lower, upper), optional\n",
      "            The lower and upper values for the range of the histogram.\n",
      "            If no value is given, a range slightly larger than the range of the\n",
      "            values in a is used. Specifically ``(a.min() - s, a.max() + s)``,\n",
      "            where ``s = (1/2)(a.max() - a.min()) / (numbins - 1)``.\n",
      "        weights : array_like, optional\n",
      "            The weights for each value in `a`. Default is None, which gives each\n",
      "            value a weight of 1.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        frequency : ndarray\n",
      "            Binned values of relative frequency.\n",
      "        lowerlimit : float\n",
      "            Lower real limit.\n",
      "        binsize : float\n",
      "            Width of each bin.\n",
      "        extrapoints : int\n",
      "            Extra points.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from numpy.random import default_rng\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = default_rng()\n",
      "        >>> a = np.array([2, 4, 1, 2, 3, 2])\n",
      "        >>> res = stats.relfreq(a, numbins=4)\n",
      "        >>> res.frequency\n",
      "        array([ 0.16666667, 0.5       , 0.16666667,  0.16666667])\n",
      "        >>> np.sum(res.frequency)  # relative frequencies should add up to 1\n",
      "        1.0\n",
      "        \n",
      "        Create a normal distribution with 1000 random values\n",
      "        \n",
      "        >>> samples = stats.norm.rvs(size=1000, random_state=rng)\n",
      "        \n",
      "        Calculate relative frequencies\n",
      "        \n",
      "        >>> res = stats.relfreq(samples, numbins=25)\n",
      "        \n",
      "        Calculate space of values for x\n",
      "        \n",
      "        >>> x = res.lowerlimit + np.linspace(0, res.binsize*res.frequency.size,\n",
      "        ...                                  res.frequency.size)\n",
      "        \n",
      "        Plot relative frequency histogram\n",
      "        \n",
      "        >>> fig = plt.figure(figsize=(5, 4))\n",
      "        >>> ax = fig.add_subplot(1, 1, 1)\n",
      "        >>> ax.bar(x, res.frequency, width=res.binsize)\n",
      "        >>> ax.set_title('Relative frequency histogram')\n",
      "        >>> ax.set_xlim([x.min(), x.max()])\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    rvs_ratio_uniforms(pdf, umax, vmin, vmax, size=1, c=0, random_state=None)\n",
      "        Generate random samples from a probability density function using the\n",
      "        ratio-of-uniforms method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pdf : callable\n",
      "            A function with signature `pdf(x)` that is proportional to the\n",
      "            probability density function of the distribution.\n",
      "        umax : float\n",
      "            The upper bound of the bounding rectangle in the u-direction.\n",
      "        vmin : float\n",
      "            The lower bound of the bounding rectangle in the v-direction.\n",
      "        vmax : float\n",
      "            The upper bound of the bounding rectangle in the v-direction.\n",
      "        size : int or tuple of ints, optional\n",
      "            Defining number of random variates (default is 1).\n",
      "        c : float, optional.\n",
      "            Shift parameter of ratio-of-uniforms method, see Notes. Default is 0.\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rvs : ndarray\n",
      "            The random variates distributed according to the probability\n",
      "            distribution defined by the pdf.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Given a univariate probability density function `pdf` and a constant `c`,\n",
      "        define the set ``A = {(u, v) : 0 < u <= sqrt(pdf(v/u + c))}``.\n",
      "        If `(U, V)` is a random vector uniformly distributed over `A`,\n",
      "        then `V/U + c` follows a distribution according to `pdf`.\n",
      "        \n",
      "        The above result (see [1]_, [2]_) can be used to sample random variables\n",
      "        using only the pdf, i.e. no inversion of the cdf is required. Typical\n",
      "        choices of `c` are zero or the mode of `pdf`. The set `A` is a subset of\n",
      "        the rectangle ``R = [0, umax] x [vmin, vmax]`` where\n",
      "        \n",
      "        - ``umax = sup sqrt(pdf(x))``\n",
      "        - ``vmin = inf (x - c) sqrt(pdf(x))``\n",
      "        - ``vmax = sup (x - c) sqrt(pdf(x))``\n",
      "        \n",
      "        In particular, these values are finite if `pdf` is bounded and\n",
      "        ``x**2 * pdf(x)`` is bounded (i.e. subquadratic tails).\n",
      "        One can generate `(U, V)` uniformly on `R` and return\n",
      "        `V/U + c` if `(U, V)` are also in `A` which can be directly\n",
      "        verified.\n",
      "        \n",
      "        The algorithm is not changed if one replaces `pdf` by k * `pdf` for any\n",
      "        constant k > 0. Thus, it is often convenient to work with a function\n",
      "        that is proportional to the probability density function by dropping\n",
      "        unneccessary normalization factors.\n",
      "        \n",
      "        Intuitively, the method works well if `A` fills up most of the\n",
      "        enclosing rectangle such that the probability is high that `(U, V)`\n",
      "        lies in `A` whenever it lies in `R` as the number of required\n",
      "        iterations becomes too large otherwise. To be more precise, note that\n",
      "        the expected number of iterations to draw `(U, V)` uniformly\n",
      "        distributed on `R` such that `(U, V)` is also in `A` is given by\n",
      "        the ratio ``area(R) / area(A) = 2 * umax * (vmax - vmin) / area(pdf)``,\n",
      "        where `area(pdf)` is the integral of `pdf` (which is equal to one if the\n",
      "        probability density function is used but can take on other values if a\n",
      "        function proportional to the density is used). The equality holds since\n",
      "        the area of `A` is equal to 0.5 * area(pdf) (Theorem 7.1 in [1]_).\n",
      "        If the sampling fails to generate a single random variate after 50000\n",
      "        iterations (i.e. not a single draw is in `A`), an exception is raised.\n",
      "        \n",
      "        If the bounding rectangle is not correctly specified (i.e. if it does not\n",
      "        contain `A`), the algorithm samples from a distribution different from\n",
      "        the one given by `pdf`. It is therefore recommended to perform a\n",
      "        test such as `~scipy.stats.kstest` as a check.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] L. Devroye, \"Non-Uniform Random Variate Generation\",\n",
      "           Springer-Verlag, 1986.\n",
      "        \n",
      "        .. [2] W. Hoermann and J. Leydold, \"Generating generalized inverse Gaussian\n",
      "           random variates\", Statistics and Computing, 24(4), p. 547--557, 2014.\n",
      "        \n",
      "        .. [3] A.J. Kinderman and J.F. Monahan, \"Computer Generation of Random\n",
      "           Variables Using the Ratio of Uniform Deviates\",\n",
      "           ACM Transactions on Mathematical Software, 3(3), p. 257--260, 1977.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        Simulate normally distributed random variables. It is easy to compute the\n",
      "        bounding rectangle explicitly in that case. For simplicity, we drop the\n",
      "        normalization factor of the density.\n",
      "        \n",
      "        >>> f = lambda x: np.exp(-x**2 / 2)\n",
      "        >>> v_bound = np.sqrt(f(np.sqrt(2))) * np.sqrt(2)\n",
      "        >>> umax, vmin, vmax = np.sqrt(f(0)), -v_bound, v_bound\n",
      "        >>> rvs = stats.rvs_ratio_uniforms(f, umax, vmin, vmax, size=2500,\n",
      "        ...                                random_state=rng)\n",
      "        \n",
      "        The K-S test confirms that the random variates are indeed normally\n",
      "        distributed (normality is not rejected at 5% significance level):\n",
      "        \n",
      "        >>> stats.kstest(rvs, 'norm')[1]\n",
      "        0.250634764150542\n",
      "        \n",
      "        The exponential distribution provides another example where the bounding\n",
      "        rectangle can be determined explicitly.\n",
      "        \n",
      "        >>> rvs = stats.rvs_ratio_uniforms(lambda x: np.exp(-x), umax=1,\n",
      "        ...                                vmin=0, vmax=2*np.exp(-1), size=1000,\n",
      "        ...                                random_state=rng)\n",
      "        >>> stats.kstest(rvs, 'expon')[1]\n",
      "        0.21121052054580314\n",
      "    \n",
      "    scoreatpercentile(a, per, limit=(), interpolation_method='fraction', axis=None)\n",
      "        Calculate the score at a given percentile of the input sequence.\n",
      "        \n",
      "        For example, the score at `per=50` is the median. If the desired quantile\n",
      "        lies between two data points, we interpolate between them, according to\n",
      "        the value of `interpolation`. If the parameter `limit` is provided, it\n",
      "        should be a tuple (lower, upper) of two values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            A 1-D array of values from which to extract score.\n",
      "        per : array_like\n",
      "            Percentile(s) at which to extract score.  Values should be in range\n",
      "            [0,100].\n",
      "        limit : tuple, optional\n",
      "            Tuple of two scalars, the lower and upper limits within which to\n",
      "            compute the percentile. Values of `a` outside\n",
      "            this (closed) interval will be ignored.\n",
      "        interpolation_method : {'fraction', 'lower', 'higher'}, optional\n",
      "            Specifies the interpolation method to use,\n",
      "            when the desired quantile lies between two data points `i` and `j`\n",
      "            The following options are available (default is 'fraction'):\n",
      "        \n",
      "              * 'fraction': ``i + (j - i) * fraction`` where ``fraction`` is the\n",
      "                fractional part of the index surrounded by ``i`` and ``j``\n",
      "              * 'lower': ``i``\n",
      "              * 'higher': ``j``\n",
      "        \n",
      "        axis : int, optional\n",
      "            Axis along which the percentiles are computed. Default is None. If\n",
      "            None, compute over the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray\n",
      "            Score at percentile(s).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        percentileofscore, numpy.percentile\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function will become obsolete in the future.\n",
      "        For NumPy 1.9 and higher, `numpy.percentile` provides all the functionality\n",
      "        that `scoreatpercentile` provides.  And it's significantly faster.\n",
      "        Therefore it's recommended to use `numpy.percentile` for users that have\n",
      "        numpy >= 1.9.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(100)\n",
      "        >>> stats.scoreatpercentile(a, 50)\n",
      "        49.5\n",
      "    \n",
      "    sem(a, axis=0, ddof=1, nan_policy='propagate')\n",
      "        Compute standard error of the mean.\n",
      "        \n",
      "        Calculate the standard error of the mean (or standard error of\n",
      "        measurement) of the values in the input array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            An array containing the values for which the standard error is\n",
      "            returned.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees-of-freedom. How many degrees of freedom to adjust\n",
      "            for bias in limited samples relative to the population estimate\n",
      "            of variance. Defaults to 1.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s : ndarray or float\n",
      "            The standard error of the mean in the sample(s), along the input axis.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The default value for `ddof` is different to the default (0) used by other\n",
      "        ddof containing routines, such as np.std and np.nanstd.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find standard error along the first axis:\n",
      "        \n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(20).reshape(5,4)\n",
      "        >>> stats.sem(a)\n",
      "        array([ 2.8284,  2.8284,  2.8284,  2.8284])\n",
      "        \n",
      "        Find standard error across the whole array, using n degrees of freedom:\n",
      "        \n",
      "        >>> stats.sem(a, axis=None, ddof=0)\n",
      "        1.2893796958227628\n",
      "    \n",
      "    shapiro(x)\n",
      "        Perform the Shapiro-Wilk test for normality.\n",
      "        \n",
      "        The Shapiro-Wilk test tests the null hypothesis that the\n",
      "        data was drawn from a normal distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Array of sample data.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The test statistic.\n",
      "        p-value : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        anderson : The Anderson-Darling test for normality\n",
      "        kstest : The Kolmogorov-Smirnov test for goodness of fit.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm used is described in [4]_ but censoring parameters as\n",
      "        described are not implemented. For N > 5000 the W test statistic is accurate\n",
      "        but the p-value may not be.\n",
      "        \n",
      "        The chance of rejecting the null hypothesis when it is true is close to 5%\n",
      "        regardless of sample size.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n",
      "        .. [2] Shapiro, S. S. & Wilk, M.B (1965). An analysis of variance test for\n",
      "               normality (complete samples), Biometrika, Vol. 52, pp. 591-611.\n",
      "        .. [3] Razali, N. M. & Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk,\n",
      "               Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of\n",
      "               Statistical Modeling and Analytics, Vol. 2, pp. 21-33.\n",
      "        .. [4] ALGORITHM AS R94 APPL. STATIST. (1995) VOL. 44, NO. 4.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = stats.norm.rvs(loc=5, scale=3, size=100, random_state=rng)\n",
      "        >>> shapiro_test = stats.shapiro(x)\n",
      "        >>> shapiro_test\n",
      "        ShapiroResult(statistic=0.9813305735588074, pvalue=0.16855233907699585)\n",
      "        >>> shapiro_test.statistic\n",
      "        0.9813305735588074\n",
      "        >>> shapiro_test.pvalue\n",
      "        0.16855233907699585\n",
      "    \n",
      "    siegelslopes(y, x=None, method='hierarchical')\n",
      "        Computes the Siegel estimator for a set of points (x, y).\n",
      "        \n",
      "        `siegelslopes` implements a method for robust linear regression\n",
      "        using repeated medians (see [1]_) to fit a line to the points (x, y).\n",
      "        The method is robust to outliers with an asymptotic breakdown point\n",
      "        of 50%.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Dependent variable.\n",
      "        x : array_like or None, optional\n",
      "            Independent variable. If None, use ``arange(len(y))`` instead.\n",
      "        method : {'hierarchical', 'separate'}\n",
      "            If 'hierarchical', estimate the intercept using the estimated\n",
      "            slope ``medslope`` (default option).\n",
      "            If 'separate', estimate the intercept independent of the estimated\n",
      "            slope. See Notes for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        medslope : float\n",
      "            Estimate of the slope of the regression line.\n",
      "        medintercept : float\n",
      "            Estimate of the intercept of the regression line.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        theilslopes : a similar technique without repeated medians\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        With ``n = len(y)``, compute ``m_j`` as the median of\n",
      "        the slopes from the point ``(x[j], y[j])`` to all other `n-1` points.\n",
      "        ``medslope`` is then the median of all slopes ``m_j``.\n",
      "        Two ways are given to estimate the intercept in [1]_ which can be chosen\n",
      "        via the parameter ``method``.\n",
      "        The hierarchical approach uses the estimated slope ``medslope``\n",
      "        and computes ``medintercept`` as the median of ``y - medslope*x``.\n",
      "        The other approach estimates the intercept separately as follows: for\n",
      "        each point ``(x[j], y[j])``, compute the intercepts of all the `n-1`\n",
      "        lines through the remaining points and take the median ``i_j``.\n",
      "        ``medintercept`` is the median of the ``i_j``.\n",
      "        \n",
      "        The implementation computes `n` times the median of a vector of size `n`\n",
      "        which can be slow for large vectors. There are more efficient algorithms\n",
      "        (see [2]_) which are not implemented here.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] A. Siegel, \"Robust Regression Using Repeated Medians\",\n",
      "               Biometrika, Vol. 69, pp. 242-244, 1982.\n",
      "        \n",
      "        .. [2] A. Stein and M. Werman, \"Finding the repeated median regression\n",
      "               line\", Proceedings of the Third Annual ACM-SIAM Symposium on\n",
      "               Discrete Algorithms, pp. 409-413, 1992.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> x = np.linspace(-5, 5, num=150)\n",
      "        >>> y = x + np.random.normal(size=x.size)\n",
      "        >>> y[11:15] += 10  # add outliers\n",
      "        >>> y[-5:] -= 7\n",
      "        \n",
      "        Compute the slope and intercept.  For comparison, also compute the\n",
      "        least-squares fit with `linregress`:\n",
      "        \n",
      "        >>> res = stats.siegelslopes(y, x)\n",
      "        >>> lsq_res = stats.linregress(x, y)\n",
      "        \n",
      "        Plot the results. The Siegel regression line is shown in red. The green\n",
      "        line shows the least-squares fit for comparison.\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, y, 'b.')\n",
      "        >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n",
      "        >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    sigmaclip(a, low=4.0, high=4.0)\n",
      "        Perform iterative sigma-clipping of array elements.\n",
      "        \n",
      "        Starting from the full sample, all elements outside the critical range are\n",
      "        removed, i.e. all elements of the input array `c` that satisfy either of\n",
      "        the following conditions::\n",
      "        \n",
      "            c < mean(c) - std(c)*low\n",
      "            c > mean(c) + std(c)*high\n",
      "        \n",
      "        The iteration continues with the updated sample until no\n",
      "        elements are outside the (updated) range.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Data array, will be raveled if not 1-D.\n",
      "        low : float, optional\n",
      "            Lower bound factor of sigma clipping. Default is 4.\n",
      "        high : float, optional\n",
      "            Upper bound factor of sigma clipping. Default is 4.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        clipped : ndarray\n",
      "            Input array with clipped elements removed.\n",
      "        lower : float\n",
      "            Lower threshold value use for clipping.\n",
      "        upper : float\n",
      "            Upper threshold value use for clipping.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import sigmaclip\n",
      "        >>> a = np.concatenate((np.linspace(9.5, 10.5, 31),\n",
      "        ...                     np.linspace(0, 20, 5)))\n",
      "        >>> fact = 1.5\n",
      "        >>> c, low, upp = sigmaclip(a, fact, fact)\n",
      "        >>> c\n",
      "        array([  9.96666667,  10.        ,  10.03333333,  10.        ])\n",
      "        >>> c.var(), c.std()\n",
      "        (0.00055555555555555165, 0.023570226039551501)\n",
      "        >>> low, c.mean() - fact*c.std(), c.min()\n",
      "        (9.9646446609406727, 9.9646446609406727, 9.9666666666666668)\n",
      "        >>> upp, c.mean() + fact*c.std(), c.max()\n",
      "        (10.035355339059327, 10.035355339059327, 10.033333333333333)\n",
      "        \n",
      "        >>> a = np.concatenate((np.linspace(9.5, 10.5, 11),\n",
      "        ...                     np.linspace(-100, -50, 3)))\n",
      "        >>> c, low, upp = sigmaclip(a, 1.8, 1.8)\n",
      "        >>> (c == np.linspace(9.5, 10.5, 11)).all()\n",
      "        True\n",
      "    \n",
      "    skew(a, axis=0, bias=True, nan_policy='propagate')\n",
      "        Compute the sample skewness of a data set.\n",
      "        \n",
      "        For normally distributed data, the skewness should be about zero. For\n",
      "        unimodal continuous distributions, a skewness value greater than zero means\n",
      "        that there is more weight in the right tail of the distribution. The\n",
      "        function `skewtest` can be used to determine if the skewness value\n",
      "        is close enough to zero, statistically speaking.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : ndarray\n",
      "            Input array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which skewness is calculated. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        bias : bool, optional\n",
      "            If False, then the calculations are corrected for statistical bias.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        skewness : ndarray\n",
      "            The skewness of values along an axis, returning 0 where all values are\n",
      "            equal.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The sample skewness is computed as the Fisher-Pearson coefficient\n",
      "        of skewness, i.e.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            g_1=\\frac{m_3}{m_2^{3/2}}\n",
      "        \n",
      "        where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            m_i=\\frac{1}{N}\\sum_{n=1}^N(x[n]-\\bar{x})^i\n",
      "        \n",
      "        is the biased sample :math:`i\\texttt{th}` central moment, and\n",
      "        :math:`\\bar{x}` is\n",
      "        the sample mean.  If ``bias`` is False, the calculations are\n",
      "        corrected for bias and the value computed is the adjusted\n",
      "        Fisher-Pearson standardized moment coefficient, i.e.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            G_1=\\frac{k_3}{k_2^{3/2}}=\n",
      "                \\frac{\\sqrt{N(N-1)}}{N-2}\\frac{m_3}{m_2^{3/2}}.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "           Section 2.2.24.1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import skew\n",
      "        >>> skew([1, 2, 3, 4, 5])\n",
      "        0.0\n",
      "        >>> skew([2, 8, 0, 4, 1, 9, 9, 0])\n",
      "        0.2650554122698573\n",
      "    \n",
      "    skewtest(a, axis=0, nan_policy='propagate', alternative='two-sided')\n",
      "        Test whether the skew is different from the normal distribution.\n",
      "        \n",
      "        This function tests the null hypothesis that the skewness of\n",
      "        the population that the sample was drawn from is the same\n",
      "        as that of a corresponding normal distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array\n",
      "            The data to be tested.\n",
      "        axis : int or None, optional\n",
      "           Axis along which statistics are calculated. Default is 0.\n",
      "           If None, compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
      "            The following options are available:\n",
      "        \n",
      "            * 'two-sided': the skewness of the distribution underlying the sample\n",
      "              is different from that of the normal distribution (i.e. 0)\n",
      "            * 'less': the skewness of the distribution underlying the sample\n",
      "              is less than that of the normal distribution\n",
      "            * 'greater': the skewness of the distribution underlying the sample\n",
      "              is greater than that of the normal distribution\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            The computed z-score for this test.\n",
      "        pvalue : float\n",
      "            The p-value for the hypothesis test.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The sample size must be at least 8.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. B. D'Agostino, A. J. Belanger and R. B. D'Agostino Jr.,\n",
      "                \"A suggestion for using powerful and informative tests of\n",
      "                normality\", American Statistician 44, pp. 316-321, 1990.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import skewtest\n",
      "        >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "        SkewtestResult(statistic=1.0108048609177787, pvalue=0.3121098361421897)\n",
      "        >>> skewtest([2, 8, 0, 4, 1, 9, 9, 0])\n",
      "        SkewtestResult(statistic=0.44626385374196975, pvalue=0.6554066631275459)\n",
      "        >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8000])\n",
      "        SkewtestResult(statistic=3.571773510360407, pvalue=0.0003545719905823133)\n",
      "        >>> skewtest([100, 100, 100, 100, 100, 100, 100, 101])\n",
      "        SkewtestResult(statistic=3.5717766638478072, pvalue=0.000354567720281634)\n",
      "        >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8], alternative='less')\n",
      "        SkewtestResult(statistic=1.0108048609177787, pvalue=0.8439450819289052)\n",
      "        >>> skewtest([1, 2, 3, 4, 5, 6, 7, 8], alternative='greater')\n",
      "        SkewtestResult(statistic=1.0108048609177787, pvalue=0.15605491807109484)\n",
      "    \n",
      "    somersd(x, y=None)\n",
      "        Calculates Somers' D, an asymmetric measure of ordinal association.\n",
      "        \n",
      "        Like Kendall's :math:`\\tau`, Somers' :math:`D` is a measure of the\n",
      "        correspondence between two rankings. Both statistics consider the\n",
      "        difference between the number of concordant and discordant pairs in two\n",
      "        rankings :math:`X` and :math:`Y`, and both are normalized such that values\n",
      "        close  to 1 indicate strong agreement and values close to -1 indicate\n",
      "        strong disagreement. They differ in how they are normalized. To show the\n",
      "        relationship, Somers' :math:`D` can be defined in terms of Kendall's\n",
      "        :math:`\\tau_a`:\n",
      "        \n",
      "        .. math::\n",
      "            D(Y|X) = \\frac{\\tau_a(X, Y)}{\\tau_a(X, X)}\n",
      "        \n",
      "        Suppose the first ranking :math:`X` has :math:`r` distinct ranks and the\n",
      "        second ranking :math:`Y` has :math:`s` distinct ranks. These two lists of\n",
      "        :math:`n` rankings can also be viewed as an :math:`r \\times s` contingency\n",
      "        table in which element :math:`i, j` is the number of rank pairs with rank\n",
      "        :math:`i` in ranking :math:`X` and rank :math:`j` in ranking :math:`Y`.\n",
      "        Accordingly, `somersd` also allows the input data to be supplied as a\n",
      "        single, 2D contingency table instead of as two separate, 1D rankings.\n",
      "        \n",
      "        Note that the definition of Somers' :math:`D` is asymmetric: in general,\n",
      "        :math:`D(Y|X) \\neq D(X|Y)`. ``somersd(x, y)`` calculates Somers'\n",
      "        :math:`D(Y|X)`: the \"row\" variable :math:`X` is treated as an independent\n",
      "        variable, and the \"column\" variable :math:`Y` is dependent. For Somers'\n",
      "        :math:`D(X|Y)`, swap the input lists or transpose the input table.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x: array_like\n",
      "            1D array of rankings, treated as the (row) independent variable.\n",
      "            Alternatively, a 2D contingency table.\n",
      "        y: array_like\n",
      "            If `x` is a 1D array of rankings, `y` is a 1D array of rankings of the\n",
      "            same length, treated as the (column) dependent variable.\n",
      "            If `x` is 2D, `y` is ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : SomersDResult\n",
      "            A `SomersDResult` object with the following fields:\n",
      "        \n",
      "                correlation : float\n",
      "                   The Somers' :math:`D` statistic.\n",
      "                pvalue : float\n",
      "                   The two-sided p-value for a hypothesis test whose null\n",
      "                   hypothesis is an absence of association, :math:`D=0`.\n",
      "                   See notes for more information.\n",
      "                table : 2D array\n",
      "                   The contingency table formed from rankings `x` and `y` (or the\n",
      "                   provided contingency table, if `x` is a 2D array)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kendalltau : Calculates Kendall's tau, another correlation measure.\n",
      "        weightedtau : Computes a weighted version of Kendall's tau.\n",
      "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
      "        pearsonr : Calculates a Pearson correlation coefficient.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function follows the contingency table approach of [2]_ and\n",
      "        [3]_. *p*-values are computed based on an asymptotic approximation of\n",
      "        the test statistic distribution under the null hypothesis :math:`D=0`.\n",
      "        \n",
      "        Theoretically, hypothesis tests based on Kendall's :math:`tau` and Somers'\n",
      "        :math:`D` should be identical.\n",
      "        However, the *p*-values returned by `kendalltau` are based\n",
      "        on the null hypothesis of *independence* between :math:`X` and :math:`Y`\n",
      "        (i.e. the population from which pairs in :math:`X` and :math:`Y` are\n",
      "        sampled contains equal numbers of all possible pairs), which is more\n",
      "        specific than the null hypothesis :math:`D=0` used here. If the null\n",
      "        hypothesis of independence is desired, it is acceptable to use the\n",
      "        *p*-value returned by `kendalltau` with the statistic returned by\n",
      "        `somersd` and vice versa. For more information, see [2]_.\n",
      "        \n",
      "        Contingency tables are formatted according to the convention used by\n",
      "        SAS and R: the first ranking supplied (``x``) is the \"row\" variable, and\n",
      "        the second ranking supplied (``y``) is the \"column\" variable. This is\n",
      "        opposite the convention of Somers' original paper [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Robert H. Somers, \"A New Asymmetric Measure of Association for\n",
      "               Ordinal Variables\", *American Sociological Review*, Vol. 27, No. 6,\n",
      "               pp. 799--811, 1962.\n",
      "        \n",
      "        .. [2] Morton B. Brown and Jacqueline K. Benedetti, \"Sampling Behavior of\n",
      "               Tests for Correlation in Two-Way Contingency Tables\", *Journal of\n",
      "               the American Statistical Association* Vol. 72, No. 358, pp.\n",
      "               309--315, 1977.\n",
      "        \n",
      "        .. [3] SAS Institute, Inc., \"The FREQ Procedure (Book Excerpt)\",\n",
      "               *SAS/STAT 9.2 User's Guide, Second Edition*, SAS Publishing, 2009.\n",
      "        \n",
      "        .. [4] Laerd Statistics, \"Somers' d using SPSS Statistics\", *SPSS\n",
      "               Statistics Tutorials and Statistical Guides*,\n",
      "               https://statistics.laerd.com/spss-tutorials/somers-d-using-spss-statistics.php,\n",
      "               Accessed July 31, 2020.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We calculate Somers' D for the example given in [4]_, in which a hotel\n",
      "        chain owner seeks to determine the association between hotel room\n",
      "        cleanliness and customer satisfaction. The independent variable, hotel\n",
      "        room cleanliness, is ranked on an ordinal scale: \"below average (1)\",\n",
      "        \"average (2)\", or \"above average (3)\". The dependent variable, customer\n",
      "        satisfaction, is ranked on a second scale: \"very dissatisfied (1)\",\n",
      "        \"moderately dissatisfied (2)\", \"neither dissatisfied nor satisfied (3)\",\n",
      "        \"moderately satisfied (4)\", or \"very satisfied (5)\". 189 customers\n",
      "        respond to the survey, and the results are cast into a contingency table\n",
      "        with the hotel room cleanliness as the \"row\" variable and customer\n",
      "        satisfaction as the \"column\" variable.\n",
      "        \n",
      "        +-----+-----+-----+-----+-----+-----+\n",
      "        |     | (1) | (2) | (3) | (4) | (5) |\n",
      "        +=====+=====+=====+=====+=====+=====+\n",
      "        | (1) | 27  | 25  | 14  | 7   | 0   |\n",
      "        +-----+-----+-----+-----+-----+-----+\n",
      "        | (2) | 7   | 14  | 18  | 35  | 12  |\n",
      "        +-----+-----+-----+-----+-----+-----+\n",
      "        | (3) | 1   | 3   | 2   | 7   | 17  |\n",
      "        +-----+-----+-----+-----+-----+-----+\n",
      "        \n",
      "        For example, 27 customers assigned their room a cleanliness ranking of\n",
      "        \"below average (1)\" and a corresponding satisfaction of \"very\n",
      "        dissatisfied (1)\". We perform the analysis as follows.\n",
      "        \n",
      "        >>> from scipy.stats import somersd\n",
      "        >>> table = [[27, 25, 14, 7, 0], [7, 14, 18, 35, 12], [1, 3, 2, 7, 17]]\n",
      "        >>> res = somersd(table)\n",
      "        >>> res.statistic\n",
      "        0.6032766111513396\n",
      "        >>> res.pvalue\n",
      "        1.0007091191074533e-27\n",
      "        \n",
      "        The value of the Somers' D statistic is approximately 0.6, indicating\n",
      "        a positive correlation between room cleanliness and customer satisfaction\n",
      "        in the sample.\n",
      "        The *p*-value is very small, indicating a very small probability of\n",
      "        observing such an extreme value of the statistic under the null\n",
      "        hypothesis that the statistic of the entire population (from which\n",
      "        our sample of 189 customers is drawn) is zero. This supports the\n",
      "        alternative hypothesis that the true value of Somers' D for the population\n",
      "        is nonzero.\n",
      "    \n",
      "    spearmanr(a, b=None, axis=0, nan_policy='propagate', alternative='two-sided')\n",
      "        Calculate a Spearman correlation coefficient with associated p-value.\n",
      "        \n",
      "        The Spearman rank-order correlation coefficient is a nonparametric measure\n",
      "        of the monotonicity of the relationship between two datasets. Unlike the\n",
      "        Pearson correlation, the Spearman correlation does not assume that both\n",
      "        datasets are normally distributed. Like other correlation coefficients,\n",
      "        this one varies between -1 and +1 with 0 implying no correlation.\n",
      "        Correlations of -1 or +1 imply an exact monotonic relationship. Positive\n",
      "        correlations imply that as x increases, so does y. Negative correlations\n",
      "        imply that as x increases, y decreases.\n",
      "        \n",
      "        The p-value roughly indicates the probability of an uncorrelated system\n",
      "        producing datasets that have a Spearman correlation at least as extreme\n",
      "        as the one computed from these datasets. The p-values are not entirely\n",
      "        reliable but are probably reasonable for datasets larger than 500 or so.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : 1D or 2D array_like, b is optional\n",
      "            One or two 1-D or 2-D arrays containing multiple variables and\n",
      "            observations. When these are 1-D, each represents a vector of\n",
      "            observations of a single variable. For the behavior in the 2-D case,\n",
      "            see under ``axis``, below.\n",
      "            Both arrays need to have the same length in the ``axis`` dimension.\n",
      "        axis : int or None, optional\n",
      "            If axis=0 (default), then each column represents a variable, with\n",
      "            observations in the rows. If axis=1, the relationship is transposed:\n",
      "            each row represents a variable, while the columns contain observations.\n",
      "            If axis=None, then both arrays will be raveled.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis. Default is 'two-sided'.\n",
      "            The following options are available:\n",
      "        \n",
      "            * 'two-sided': the correlation is nonzero\n",
      "            * 'less': the correlation is negative (less than zero)\n",
      "            * 'greater':  the correlation is positive (greater than zero)\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float or ndarray (2-D square)\n",
      "            Spearman correlation matrix or correlation coefficient (if only 2\n",
      "            variables are given as parameters. Correlation matrix is square with\n",
      "            length equal to total number of variables (columns or rows) in ``a``\n",
      "            and ``b`` combined.\n",
      "        pvalue : float\n",
      "            The p-value for a hypothesis test whose null hypotheisis\n",
      "            is that two sets of data are uncorrelated. See `alternative` above\n",
      "            for alternative hypotheses. `pvalue` has the same\n",
      "            shape as `correlation`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "           Section  14.7\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.spearmanr([1,2,3,4,5], [5,6,7,8,7])\n",
      "        SpearmanrResult(correlation=0.82078..., pvalue=0.08858...)\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x2n = rng.standard_normal((100, 2))\n",
      "        >>> y2n = rng.standard_normal((100, 2))\n",
      "        >>> stats.spearmanr(x2n)\n",
      "        SpearmanrResult(correlation=-0.07960396039603959, pvalue=0.4311168705769747)\n",
      "        >>> stats.spearmanr(x2n[:,0], x2n[:,1])\n",
      "        SpearmanrResult(correlation=-0.07960396039603959, pvalue=0.4311168705769747)\n",
      "        >>> rho, pval = stats.spearmanr(x2n, y2n)\n",
      "        >>> rho\n",
      "        array([[ 1.        , -0.07960396, -0.08314431,  0.09662166],\n",
      "               [-0.07960396,  1.        , -0.14448245,  0.16738074],\n",
      "               [-0.08314431, -0.14448245,  1.        ,  0.03234323],\n",
      "               [ 0.09662166,  0.16738074,  0.03234323,  1.        ]])\n",
      "        >>> pval\n",
      "        array([[0.        , 0.43111687, 0.41084066, 0.33891628],\n",
      "               [0.43111687, 0.        , 0.15151618, 0.09600687],\n",
      "               [0.41084066, 0.15151618, 0.        , 0.74938561],\n",
      "               [0.33891628, 0.09600687, 0.74938561, 0.        ]])\n",
      "        >>> rho, pval = stats.spearmanr(x2n.T, y2n.T, axis=1)\n",
      "        >>> rho\n",
      "        array([[ 1.        , -0.07960396, -0.08314431,  0.09662166],\n",
      "               [-0.07960396,  1.        , -0.14448245,  0.16738074],\n",
      "               [-0.08314431, -0.14448245,  1.        ,  0.03234323],\n",
      "               [ 0.09662166,  0.16738074,  0.03234323,  1.        ]])\n",
      "        >>> stats.spearmanr(x2n, y2n, axis=None)\n",
      "        SpearmanrResult(correlation=0.044981624540613524, pvalue=0.5270803651336189)\n",
      "        >>> stats.spearmanr(x2n.ravel(), y2n.ravel())\n",
      "        SpearmanrResult(correlation=0.044981624540613524, pvalue=0.5270803651336189)\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> xint = rng.integers(10, size=(100, 2))\n",
      "        >>> stats.spearmanr(xint)\n",
      "        SpearmanrResult(correlation=0.09800224850707953, pvalue=0.3320271757932076)\n",
      "    \n",
      "    theilslopes(y, x=None, alpha=0.95)\n",
      "        Computes the Theil-Sen estimator for a set of points (x, y).\n",
      "        \n",
      "        `theilslopes` implements a method for robust linear regression.  It\n",
      "        computes the slope as the median of all slopes between paired values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Dependent variable.\n",
      "        x : array_like or None, optional\n",
      "            Independent variable. If None, use ``arange(len(y))`` instead.\n",
      "        alpha : float, optional\n",
      "            Confidence degree between 0 and 1. Default is 95% confidence.\n",
      "            Note that `alpha` is symmetric around 0.5, i.e. both 0.1 and 0.9 are\n",
      "            interpreted as \"find the 90% confidence interval\".\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        medslope : float\n",
      "            Theil slope.\n",
      "        medintercept : float\n",
      "            Intercept of the Theil line, as ``median(y) - medslope*median(x)``.\n",
      "        lo_slope : float\n",
      "            Lower bound of the confidence interval on `medslope`.\n",
      "        up_slope : float\n",
      "            Upper bound of the confidence interval on `medslope`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        siegelslopes : a similar technique using repeated medians\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The implementation of `theilslopes` follows [1]_. The intercept is\n",
      "        not defined in [1]_, and here it is defined as ``median(y) -\n",
      "        medslope*median(x)``, which is given in [3]_. Other definitions of\n",
      "        the intercept exist in the literature. A confidence interval for\n",
      "        the intercept is not given as this question is not addressed in\n",
      "        [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] P.K. Sen, \"Estimates of the regression coefficient based on\n",
      "               Kendall's tau\", J. Am. Stat. Assoc., Vol. 63, pp. 1379-1389, 1968.\n",
      "        .. [2] H. Theil, \"A rank-invariant method of linear and polynomial\n",
      "               regression analysis I, II and III\",  Nederl. Akad. Wetensch., Proc.\n",
      "               53:, pp. 386-392, pp. 521-525, pp. 1397-1412, 1950.\n",
      "        .. [3] W.L. Conover, \"Practical nonparametric statistics\", 2nd ed.,\n",
      "               John Wiley and Sons, New York, pp. 493.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> x = np.linspace(-5, 5, num=150)\n",
      "        >>> y = x + np.random.normal(size=x.size)\n",
      "        >>> y[11:15] += 10  # add outliers\n",
      "        >>> y[-5:] -= 7\n",
      "        \n",
      "        Compute the slope, intercept and 90% confidence interval.  For comparison,\n",
      "        also compute the least-squares fit with `linregress`:\n",
      "        \n",
      "        >>> res = stats.theilslopes(y, x, 0.90)\n",
      "        >>> lsq_res = stats.linregress(x, y)\n",
      "        \n",
      "        Plot the results. The Theil-Sen regression line is shown in red, with the\n",
      "        dashed red lines illustrating the confidence interval of the slope (note\n",
      "        that the dashed red lines are not the confidence interval of the regression\n",
      "        as the confidence interval of the intercept is not included). The green\n",
      "        line shows the least-squares fit for comparison.\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, y, 'b.')\n",
      "        >>> ax.plot(x, res[1] + res[0] * x, 'r-')\n",
      "        >>> ax.plot(x, res[1] + res[2] * x, 'r--')\n",
      "        >>> ax.plot(x, res[1] + res[3] * x, 'r--')\n",
      "        >>> ax.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    tiecorrect(rankvals)\n",
      "        Tie correction factor for Mann-Whitney U and Kruskal-Wallis H tests.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rankvals : array_like\n",
      "            A 1-D sequence of ranks.  Typically this will be the array\n",
      "            returned by `~scipy.stats.rankdata`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        factor : float\n",
      "            Correction factor for U or H.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rankdata : Assign ranks to the data\n",
      "        mannwhitneyu : Mann-Whitney rank test\n",
      "        kruskal : Kruskal-Wallis H test\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Siegel, S. (1956) Nonparametric Statistics for the Behavioral\n",
      "               Sciences.  New York: McGraw-Hill.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import tiecorrect, rankdata\n",
      "        >>> tiecorrect([1, 2.5, 2.5, 4])\n",
      "        0.9\n",
      "        >>> ranks = rankdata([1, 3, 2, 4, 5, 7, 2, 8, 4])\n",
      "        >>> ranks\n",
      "        array([ 1. ,  4. ,  2.5,  5.5,  7. ,  8. ,  2.5,  9. ,  5.5])\n",
      "        >>> tiecorrect(ranks)\n",
      "        0.9833333333333333\n",
      "    \n",
      "    tmax(a, upperlimit=None, axis=0, inclusive=True, nan_policy='propagate')\n",
      "        Compute the trimmed maximum.\n",
      "        \n",
      "        This function computes the maximum value of an array along a given axis,\n",
      "        while ignoring values larger than a specified upper limit.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        upperlimit : None or float, optional\n",
      "            Values in the input array greater than the given limit will be ignored.\n",
      "            When upperlimit is None, then all values are used. The default value\n",
      "            is None.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        inclusive : {True, False}, optional\n",
      "            This flag determines whether values exactly equal to the upper limit\n",
      "            are included.  The default value is True.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tmax : float, int or ndarray\n",
      "            Trimmed maximum.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tmax(x)\n",
      "        19\n",
      "        \n",
      "        >>> stats.tmax(x, 13)\n",
      "        13\n",
      "        \n",
      "        >>> stats.tmax(x, 13, inclusive=False)\n",
      "        12\n",
      "    \n",
      "    tmean(a, limits=None, inclusive=(True, True), axis=None)\n",
      "        Compute the trimmed mean.\n",
      "        \n",
      "        This function finds the arithmetic mean of given values, ignoring values\n",
      "        outside the given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored.  When limits is None (default), then all\n",
      "            values are used.  Either of the limit values in the tuple can also be\n",
      "            None representing a half-open interval.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. Default is None.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tmean : float\n",
      "            Trimmed mean.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        trim_mean : Returns mean after trimming a proportion from both tails.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tmean(x)\n",
      "        9.5\n",
      "        >>> stats.tmean(x, (3,17))\n",
      "        10.0\n",
      "    \n",
      "    tmin(a, lowerlimit=None, axis=0, inclusive=True, nan_policy='propagate')\n",
      "        Compute the trimmed minimum.\n",
      "        \n",
      "        This function finds the miminum value of an array `a` along the\n",
      "        specified axis, but only considering values greater than a specified\n",
      "        lower limit.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        lowerlimit : None or float, optional\n",
      "            Values in the input array less than the given limit will be ignored.\n",
      "            When lowerlimit is None, then all values are used. The default value\n",
      "            is None.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        inclusive : {True, False}, optional\n",
      "            This flag determines whether values exactly equal to the lower limit\n",
      "            are included.  The default value is True.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tmin : float, int or ndarray\n",
      "            Trimmed minimum.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tmin(x)\n",
      "        0\n",
      "        \n",
      "        >>> stats.tmin(x, 13)\n",
      "        13\n",
      "        \n",
      "        >>> stats.tmin(x, 13, inclusive=False)\n",
      "        14\n",
      "    \n",
      "    trim1(a, proportiontocut, tail='right', axis=0)\n",
      "        Slice off a proportion from ONE end of the passed array distribution.\n",
      "        \n",
      "        If `proportiontocut` = 0.1, slices off 'leftmost' or 'rightmost'\n",
      "        10% of scores. The lowest or highest values are trimmed (depending on\n",
      "        the tail).\n",
      "        Slice off less if proportion results in a non-integer slice index\n",
      "        (i.e. conservatively slices off `proportiontocut` ).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        proportiontocut : float\n",
      "            Fraction to cut off of 'left' or 'right' of distribution.\n",
      "        tail : {'left', 'right'}, optional\n",
      "            Defaults to 'right'.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to trim data. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        trim1 : ndarray\n",
      "            Trimmed version of array `a`. The order of the trimmed content is\n",
      "            undefined.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(20)\n",
      "        >>> b = stats.trim1(a, 0.5, 'left')\n",
      "        >>> b\n",
      "        array([10, 11, 12, 13, 14, 16, 15, 17, 18, 19])\n",
      "    \n",
      "    trim_mean(a, proportiontocut, axis=0)\n",
      "        Return mean of array after trimming distribution from both tails.\n",
      "        \n",
      "        If `proportiontocut` = 0.1, slices off 'leftmost' and 'rightmost' 10% of\n",
      "        scores. The input is sorted before slicing. Slices off less if proportion\n",
      "        results in a non-integer slice index (i.e., conservatively slices off\n",
      "        `proportiontocut` ).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        proportiontocut : float\n",
      "            Fraction to cut off of both tails of the distribution.\n",
      "        axis : int or None, optional\n",
      "            Axis along which the trimmed means are computed. Default is 0.\n",
      "            If None, compute over the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        trim_mean : ndarray\n",
      "            Mean of trimmed array.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        trimboth\n",
      "        tmean : Compute the trimmed mean ignoring values outside given `limits`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.trim_mean(x, 0.1)\n",
      "        9.5\n",
      "        >>> x2 = x.reshape(5, 4)\n",
      "        >>> x2\n",
      "        array([[ 0,  1,  2,  3],\n",
      "               [ 4,  5,  6,  7],\n",
      "               [ 8,  9, 10, 11],\n",
      "               [12, 13, 14, 15],\n",
      "               [16, 17, 18, 19]])\n",
      "        >>> stats.trim_mean(x2, 0.25)\n",
      "        array([  8.,   9.,  10.,  11.])\n",
      "        >>> stats.trim_mean(x2, 0.25, axis=1)\n",
      "        array([  1.5,   5.5,   9.5,  13.5,  17.5])\n",
      "    \n",
      "    trimboth(a, proportiontocut, axis=0)\n",
      "        Slice off a proportion of items from both ends of an array.\n",
      "        \n",
      "        Slice off the passed proportion of items from both ends of the passed\n",
      "        array (i.e., with `proportiontocut` = 0.1, slices leftmost 10% **and**\n",
      "        rightmost 10% of scores). The trimmed values are the lowest and\n",
      "        highest ones.\n",
      "        Slice off less if proportion results in a non-integer slice index (i.e.\n",
      "        conservatively slices off `proportiontocut`).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Data to trim.\n",
      "        proportiontocut : float\n",
      "            Proportion (in range 0-1) of total data set to trim of each end.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to trim data. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Trimmed version of array `a`. The order of the trimmed content\n",
      "            is undefined.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        trim_mean\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> a = np.arange(20)\n",
      "        >>> b = stats.trimboth(a, 0.1)\n",
      "        >>> b.shape\n",
      "        (16,)\n",
      "    \n",
      "    tsem(a, limits=None, inclusive=(True, True), axis=0, ddof=1)\n",
      "        Compute the trimmed standard error of the mean.\n",
      "        \n",
      "        This function finds the standard error of the mean for given\n",
      "        values, ignoring values outside the given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored. When limits is None, then all values are\n",
      "            used. Either of the limit values in the tuple can also be None\n",
      "            representing a half-open interval.  The default value is None.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom.  Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tsem : float\n",
      "            Trimmed standard error of the mean.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `tsem` uses unbiased sample standard deviation, i.e. it uses a\n",
      "        correction factor ``n / (n - 1)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tsem(x)\n",
      "        1.3228756555322954\n",
      "        >>> stats.tsem(x, (3,17))\n",
      "        1.1547005383792515\n",
      "    \n",
      "    tstd(a, limits=None, inclusive=(True, True), axis=0, ddof=1)\n",
      "        Compute the trimmed sample standard deviation.\n",
      "        \n",
      "        This function finds the sample standard deviation of given values,\n",
      "        ignoring values outside the given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored. When limits is None, then all values are\n",
      "            used. Either of the limit values in the tuple can also be None\n",
      "            representing a half-open interval.  The default value is None.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom.  Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tstd : float\n",
      "            Trimmed sample standard deviation.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `tstd` computes the unbiased sample standard deviation, i.e. it uses a\n",
      "        correction factor ``n / (n - 1)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tstd(x)\n",
      "        5.9160797830996161\n",
      "        >>> stats.tstd(x, (3,17))\n",
      "        4.4721359549995796\n",
      "    \n",
      "    ttest_1samp(a, popmean, axis=0, nan_policy='propagate', alternative='two-sided')\n",
      "        Calculate the T-test for the mean of ONE group of scores.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that the expected value\n",
      "        (mean) of a sample of independent observations `a` is equal to the given\n",
      "        population mean, `popmean`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Sample observation.\n",
      "        popmean : float or array_like\n",
      "            Expected value in null hypothesis. If array_like, then it must have the\n",
      "            same shape as `a` excluding the axis dimension.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test; default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        \n",
      "            .. versionadded:: 1.6.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            t-statistic.\n",
      "        pvalue : float or array\n",
      "            Two-sided p-value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> rvs = stats.norm.rvs(loc=5, scale=10, size=(50, 2), random_state=rng)\n",
      "        \n",
      "        Test if mean of random sample is equal to true mean, and different mean.\n",
      "        We reject the null hypothesis in the second case and don't reject it in\n",
      "        the first case.\n",
      "        \n",
      "        >>> stats.ttest_1samp(rvs, 5.0)\n",
      "        Ttest_1sampResult(statistic=array([-2.09794637, -1.75977004]), pvalue=array([0.04108952, 0.08468867]))\n",
      "        >>> stats.ttest_1samp(rvs, 0.0)\n",
      "        Ttest_1sampResult(statistic=array([1.64495065, 1.62095307]), pvalue=array([0.10638103, 0.11144602]))\n",
      "        \n",
      "        Examples using axis and non-scalar dimension for population mean.\n",
      "        \n",
      "        >>> result = stats.ttest_1samp(rvs, [5.0, 0.0])\n",
      "        >>> result.statistic\n",
      "        array([-2.09794637,  1.62095307])\n",
      "        >>> result.pvalue\n",
      "        array([0.04108952, 0.11144602])\n",
      "        \n",
      "        >>> result = stats.ttest_1samp(rvs.T, [5.0, 0.0], axis=1)\n",
      "        >>> result.statistic\n",
      "        array([-2.09794637,  1.62095307])\n",
      "        >>> result.pvalue\n",
      "        array([0.04108952, 0.11144602])\n",
      "        \n",
      "        >>> result = stats.ttest_1samp(rvs, [[5.0], [0.0]])\n",
      "        >>> result.statistic\n",
      "        array([[-2.09794637, -1.75977004],\n",
      "               [ 1.64495065,  1.62095307]])\n",
      "        >>> result.pvalue\n",
      "        array([[0.04108952, 0.08468867],\n",
      "               [0.10638103, 0.11144602]])\n",
      "    \n",
      "    ttest_ind(a, b, axis=0, equal_var=True, nan_policy='propagate', permutations=None, random_state=None, alternative='two-sided', trim=0)\n",
      "        Calculate the T-test for the means of *two independent* samples of scores.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that 2 independent samples\n",
      "        have identical average (expected) values. This test assumes that the\n",
      "        populations have identical variances by default.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array_like\n",
      "            The arrays must have the same shape, except in the dimension\n",
      "            corresponding to `axis` (the first, by default).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. If None, compute over the whole\n",
      "            arrays, `a`, and `b`.\n",
      "        equal_var : bool, optional\n",
      "            If True (default), perform a standard independent 2 sample test\n",
      "            that assumes equal population variances [1]_.\n",
      "            If False, perform Welch's t-test, which does not assume equal\n",
      "            population variance [2]_.\n",
      "        \n",
      "            .. versionadded:: 0.11.0\n",
      "        \n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "            The 'omit' option is not currently available for permutation tests or\n",
      "            one-sided asympyotic tests.\n",
      "        \n",
      "        permutations : non-negative int, np.inf, or None (default), optional\n",
      "            If 0 or None (default), use the t-distribution to calculate p-values.\n",
      "            Otherwise, `permutations` is  the number of random permutations that\n",
      "            will be used to estimate p-values using a permutation test. If\n",
      "            `permutations` equals or exceeds the number of distinct partitions of\n",
      "            the pooled data, an exact test is performed instead (i.e. each\n",
      "            distinct partition is used exactly once). See Notes for details.\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "            Pseudorandom number generator state used to generate permutations\n",
      "            (used only when `permutations` is not None).\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        \n",
      "            .. versionadded:: 1.6.0\n",
      "        \n",
      "        trim : float, optional\n",
      "            If nonzero, performs a trimmed (Yuen's) t-test.\n",
      "            Defines the fraction of elements to be trimmed from each end of the\n",
      "            input samples. If 0 (default), no elements will be trimmed from either\n",
      "            side. The number of trimmed elements from each tail is the floor of the\n",
      "            trim times the number of elements. Valid range is [0, .5).\n",
      "        \n",
      "            .. versionadded:: 1.7\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            The calculated t-statistic.\n",
      "        pvalue : float or array\n",
      "            The two-tailed p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Suppose we observe two independent samples, e.g. flower petal lengths, and\n",
      "        we are considering whether the two samples were drawn from the same\n",
      "        population (e.g. the same species of flower or two species with similar\n",
      "        petal characteristics) or two different populations.\n",
      "        \n",
      "        The t-test quantifies the difference between the arithmetic means\n",
      "        of the two samples. The p-value quantifies the probability of observing\n",
      "        as or more extreme values assuming the null hypothesis, that the\n",
      "        samples are drawn from populations with the same population means, is true.\n",
      "        A p-value larger than a chosen threshold (e.g. 5% or 1%) indicates that\n",
      "        our observation is not so unlikely to have occurred by chance. Therefore,\n",
      "        we do not reject the null hypothesis of equal population means.\n",
      "        If the p-value is smaller than our threshold, then we have evidence\n",
      "        against the null hypothesis of equal population means.\n",
      "        \n",
      "        By default, the p-value is determined by comparing the t-statistic of the\n",
      "        observed data against a theoretical t-distribution.\n",
      "        When ``1 < permutations < binom(n, k)``, where\n",
      "        \n",
      "        * ``k`` is the number of observations in `a`,\n",
      "        * ``n`` is the total number of observations in `a` and `b`, and\n",
      "        * ``binom(n, k)`` is the binomial coefficient (``n`` choose ``k``),\n",
      "        \n",
      "        the data are pooled (concatenated), randomly assigned to either group `a`\n",
      "        or `b`, and the t-statistic is calculated. This process is performed\n",
      "        repeatedly (`permutation` times), generating a distribution of the\n",
      "        t-statistic under the null hypothesis, and the t-statistic of the observed\n",
      "        data is compared to this distribution to determine the p-value. When\n",
      "        ``permutations >= binom(n, k)``, an exact test is performed: the data are\n",
      "        partitioned between the groups in each distinct way exactly once.\n",
      "        \n",
      "        The permutation test can be computationally expensive and not necessarily\n",
      "        more accurate than the analytical test, but it does not make strong\n",
      "        assumptions about the shape of the underlying distribution.\n",
      "        \n",
      "        Use of trimming is commonly referred to as the trimmed t-test. At times\n",
      "        called Yuen's t-test, this is an extension of Welch's t-test, with the\n",
      "        difference being the use of winsorized means in calculation of the variance\n",
      "        and the trimmed sample size in calculation of the statistic. Trimming is\n",
      "        reccomended if the underlying distribution is long-tailed or contaminated\n",
      "        with outliers [4]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
      "        \n",
      "        .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
      "        \n",
      "        .. [3] http://en.wikipedia.org/wiki/Resampling_%28statistics%29\n",
      "        \n",
      "        .. [4] Yuen, Karen K. \"The Two-Sample Trimmed t for Unequal Population\n",
      "               Variances.\" Biometrika, vol. 61, no. 1, 1974, pp. 165-170. JSTOR,\n",
      "               www.jstor.org/stable/2334299. Accessed 30 Mar. 2021.\n",
      "        \n",
      "        .. [5] Yuen, Karen K., and W. J. Dixon. \"The Approximate Behaviour and\n",
      "               Performance of the Two-Sample Trimmed t.\" Biometrika, vol. 60,\n",
      "               no. 2, 1973, pp. 369-374. JSTOR, www.jstor.org/stable/2334550.\n",
      "               Accessed 30 Mar. 2021.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        Test with sample with identical means:\n",
      "        \n",
      "        >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "        >>> rvs2 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "        >>> stats.ttest_ind(rvs1, rvs2)\n",
      "        Ttest_indResult(statistic=-0.4390847099199348, pvalue=0.6606952038870015)\n",
      "        >>> stats.ttest_ind(rvs1, rvs2, equal_var=False)\n",
      "        Ttest_indResult(statistic=-0.4390847099199348, pvalue=0.6606952553131064)\n",
      "        \n",
      "        `ttest_ind` underestimates p for unequal variances:\n",
      "        \n",
      "        >>> rvs3 = stats.norm.rvs(loc=5, scale=20, size=500, random_state=rng)\n",
      "        >>> stats.ttest_ind(rvs1, rvs3)\n",
      "        Ttest_indResult(statistic=-1.6370984482905417, pvalue=0.1019251574705033)\n",
      "        >>> stats.ttest_ind(rvs1, rvs3, equal_var=False)\n",
      "        Ttest_indResult(statistic=-1.637098448290542, pvalue=0.10202110497954867)\n",
      "        \n",
      "        When ``n1 != n2``, the equal variance t-statistic is no longer equal to the\n",
      "        unequal variance t-statistic:\n",
      "        \n",
      "        >>> rvs4 = stats.norm.rvs(loc=5, scale=20, size=100, random_state=rng)\n",
      "        >>> stats.ttest_ind(rvs1, rvs4)\n",
      "        Ttest_indResult(statistic=-1.9481646859513422, pvalue=0.05186270935842703)\n",
      "        >>> stats.ttest_ind(rvs1, rvs4, equal_var=False)\n",
      "        Ttest_indResult(statistic=-1.3146566100751664, pvalue=0.1913495266513811)\n",
      "        \n",
      "        T-test with different means, variance, and n:\n",
      "        \n",
      "        >>> rvs5 = stats.norm.rvs(loc=8, scale=20, size=100, random_state=rng)\n",
      "        >>> stats.ttest_ind(rvs1, rvs5)\n",
      "        Ttest_indResult(statistic=-2.8415950600298774, pvalue=0.0046418707568707885)\n",
      "        >>> stats.ttest_ind(rvs1, rvs5, equal_var=False)\n",
      "        Ttest_indResult(statistic=-1.8686598649188084, pvalue=0.06434714193919686)\n",
      "        \n",
      "        When performing a permutation test, more permutations typically yields\n",
      "        more accurate results. Use a ``np.random.Generator`` to ensure\n",
      "        reproducibility:\n",
      "        \n",
      "        >>> stats.ttest_ind(rvs1, rvs5, permutations=10000,\n",
      "        ...                 random_state=rng)\n",
      "        Ttest_indResult(statistic=-2.8415950600298774, pvalue=0.0052)\n",
      "        \n",
      "        Take these two samples, one of which has an extreme tail.\n",
      "        \n",
      "        >>> a = (56, 128.6, 12, 123.8, 64.34, 78, 763.3)\n",
      "        >>> b = (1.1, 2.9, 4.2)\n",
      "        \n",
      "        Use the `trim` keyword to perform a trimmed (Yuen) t-test. For example,\n",
      "        using 20% trimming, ``trim=.2``, the test will reduce the impact of one\n",
      "        (``np.floor(trim*len(a))``) element from each tail of sample `a`. It will\n",
      "        have no effect on sample `b` because ``np.floor(trim*len(b))`` is 0.\n",
      "        \n",
      "        >>> stats.ttest_ind(a, b, trim=.2)\n",
      "        Ttest_indResult(statistic=3.4463884028073513,\n",
      "                        pvalue=0.01369338726499547)\n",
      "    \n",
      "    ttest_ind_from_stats(mean1, std1, nobs1, mean2, std2, nobs2, equal_var=True, alternative='two-sided')\n",
      "        T-test for means of two independent samples from descriptive statistics.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that two independent\n",
      "        samples have identical average (expected) values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        mean1 : array_like\n",
      "            The mean(s) of sample 1.\n",
      "        std1 : array_like\n",
      "            The standard deviation(s) of sample 1.\n",
      "        nobs1 : array_like\n",
      "            The number(s) of observations of sample 1.\n",
      "        mean2 : array_like\n",
      "            The mean(s) of sample 2.\n",
      "        std2 : array_like\n",
      "            The standard deviations(s) of sample 2.\n",
      "        nobs2 : array_like\n",
      "            The number(s) of observations of sample 2.\n",
      "        equal_var : bool, optional\n",
      "            If True (default), perform a standard independent 2 sample test\n",
      "            that assumes equal population variances [1]_.\n",
      "            If False, perform Welch's t-test, which does not assume equal\n",
      "            population variance [2]_.\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        \n",
      "            .. versionadded:: 1.6.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            The calculated t-statistics.\n",
      "        pvalue : float or array\n",
      "            The two-tailed p-value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.stats.ttest_ind\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.16.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/T-test#Independent_two-sample_t-test\n",
      "        \n",
      "        .. [2] https://en.wikipedia.org/wiki/Welch%27s_t-test\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Suppose we have the summary data for two samples, as follows::\n",
      "        \n",
      "                             Sample   Sample\n",
      "                       Size   Mean   Variance\n",
      "            Sample 1    13    15.0     87.5\n",
      "            Sample 2    11    12.0     39.0\n",
      "        \n",
      "        Apply the t-test to this data (with the assumption that the population\n",
      "        variances are equal):\n",
      "        \n",
      "        >>> from scipy.stats import ttest_ind_from_stats\n",
      "        >>> ttest_ind_from_stats(mean1=15.0, std1=np.sqrt(87.5), nobs1=13,\n",
      "        ...                      mean2=12.0, std2=np.sqrt(39.0), nobs2=11)\n",
      "        Ttest_indResult(statistic=0.9051358093310269, pvalue=0.3751996797581487)\n",
      "        \n",
      "        For comparison, here is the data from which those summary statistics\n",
      "        were taken.  With this data, we can compute the same result using\n",
      "        `scipy.stats.ttest_ind`:\n",
      "        \n",
      "        >>> a = np.array([1, 3, 4, 6, 11, 13, 15, 19, 22, 24, 25, 26, 26])\n",
      "        >>> b = np.array([2, 4, 6, 9, 11, 13, 14, 15, 18, 19, 21])\n",
      "        >>> from scipy.stats import ttest_ind\n",
      "        >>> ttest_ind(a, b)\n",
      "        Ttest_indResult(statistic=0.905135809331027, pvalue=0.3751996797581486)\n",
      "        \n",
      "        Suppose we instead have binary data and would like to apply a t-test to\n",
      "        compare the proportion of 1s in two independent groups::\n",
      "        \n",
      "                              Number of    Sample     Sample\n",
      "                        Size    ones        Mean     Variance\n",
      "            Sample 1    150      30         0.2        0.16\n",
      "            Sample 2    200      45         0.225      0.174375\n",
      "        \n",
      "        The sample mean :math:`\\hat{p}` is the proportion of ones in the sample\n",
      "        and the variance for a binary observation is estimated by\n",
      "        :math:`\\hat{p}(1-\\hat{p})`.\n",
      "        \n",
      "        >>> ttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.16), nobs1=150,\n",
      "        ...                      mean2=0.225, std2=np.sqrt(0.17437), nobs2=200)\n",
      "        Ttest_indResult(statistic=-0.564327545549774, pvalue=0.5728947691244874)\n",
      "        \n",
      "        For comparison, we could compute the t statistic and p-value using\n",
      "        arrays of 0s and 1s and `scipy.stat.ttest_ind`, as above.\n",
      "        \n",
      "        >>> group1 = np.array([1]*30 + [0]*(150-30))\n",
      "        >>> group2 = np.array([1]*45 + [0]*(200-45))\n",
      "        >>> ttest_ind(group1, group2)\n",
      "        Ttest_indResult(statistic=-0.5627179589855622, pvalue=0.573989277115258)\n",
      "    \n",
      "    ttest_rel(a, b, axis=0, nan_policy='propagate', alternative='two-sided')\n",
      "        Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
      "        \n",
      "        This is a two-sided test for the null hypothesis that 2 related or\n",
      "        repeated samples have identical average (expected) values.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array_like\n",
      "            The arrays must have the same shape.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to compute test. If None, compute over the whole\n",
      "            arrays, `a`, and `b`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "              * 'propagate': returns nan\n",
      "              * 'raise': throws an error\n",
      "              * 'omit': performs the calculations ignoring nan values\n",
      "        alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "            Defines the alternative hypothesis.\n",
      "            The following options are available (default is 'two-sided'):\n",
      "        \n",
      "              * 'two-sided'\n",
      "              * 'less': one-sided\n",
      "              * 'greater': one-sided\n",
      "        \n",
      "              .. versionadded:: 1.6.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float or array\n",
      "            t-statistic.\n",
      "        pvalue : float or array\n",
      "            Two-sided p-value.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Examples for use are scores of the same set of student in\n",
      "        different exams, or repeated sampling from the same units. The\n",
      "        test measures whether the average score differs significantly\n",
      "        across samples (e.g. exams). If we observe a large p-value, for\n",
      "        example greater than 0.05 or 0.1 then we cannot reject the null\n",
      "        hypothesis of identical average scores. If the p-value is smaller\n",
      "        than the threshold, e.g. 1%, 5% or 10%, then we reject the null\n",
      "        hypothesis of equal averages. Small p-values are associated with\n",
      "        large t-statistics.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        https://en.wikipedia.org/wiki/T-test#Dependent_t-test_for_paired_samples\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> rng = np.random.default_rng()\n",
      "        \n",
      "        >>> rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "        >>> rvs2 = (stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
      "        ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
      "        >>> stats.ttest_rel(rvs1, rvs2)\n",
      "        Ttest_relResult(statistic=-0.4549717054410304, pvalue=0.6493274702088672)\n",
      "        >>> rvs3 = (stats.norm.rvs(loc=8, scale=10, size=500, random_state=rng)\n",
      "        ...         + stats.norm.rvs(scale=0.2, size=500, random_state=rng))\n",
      "        >>> stats.ttest_rel(rvs1, rvs3)\n",
      "        Ttest_relResult(statistic=-5.879467544540889, pvalue=7.540777129099917e-09)\n",
      "    \n",
      "    tvar(a, limits=None, inclusive=(True, True), axis=0, ddof=1)\n",
      "        Compute the trimmed variance.\n",
      "        \n",
      "        This function computes the sample variance of an array of values,\n",
      "        while ignoring values which are outside of given `limits`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Array of values.\n",
      "        limits : None or (lower limit, upper limit), optional\n",
      "            Values in the input array less than the lower limit or greater than the\n",
      "            upper limit will be ignored. When limits is None, then all values are\n",
      "            used. Either of the limit values in the tuple can also be None\n",
      "            representing a half-open interval.  The default value is None.\n",
      "        inclusive : (bool, bool), optional\n",
      "            A tuple consisting of the (lower flag, upper flag).  These flags\n",
      "            determine whether values exactly equal to the lower or upper limits\n",
      "            are included.  The default value is (True, True).\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over the\n",
      "            whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom.  Default is 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        tvar : float\n",
      "            Trimmed variance.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `tvar` computes the unbiased sample variance, i.e. it uses a correction\n",
      "        factor ``n / (n - 1)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = np.arange(20)\n",
      "        >>> stats.tvar(x)\n",
      "        35.0\n",
      "        >>> stats.tvar(x, (3,17))\n",
      "        20.0\n",
      "    \n",
      "    variation(a, axis=0, nan_policy='propagate', ddof=0)\n",
      "        Compute the coefficient of variation.\n",
      "        \n",
      "        The coefficient of variation is the standard deviation divided by the\n",
      "        mean.  This function is equivalent to::\n",
      "        \n",
      "            np.std(x, axis=axis, ddof=ddof) / np.mean(x)\n",
      "        \n",
      "        The default for ``ddof`` is 0, but many definitions of the coefficient\n",
      "        of variation use the square root of the unbiased sample variance\n",
      "        for the sample standard deviation, which corresponds to ``ddof=1``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to calculate the coefficient of variation. Default\n",
      "            is 0. If None, compute over the whole array `a`.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan.\n",
      "            The following options are available (default is 'propagate'):\n",
      "        \n",
      "            * 'propagate': returns nan\n",
      "            * 'raise': throws an error\n",
      "            * 'omit': performs the calculations ignoring nan values\n",
      "        \n",
      "        ddof : int, optional\n",
      "            Delta degrees of freedom.  Default is 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        variation : ndarray\n",
      "            The calculated variation along the requested axis.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard\n",
      "           Probability and Statistics Tables and Formulae. Chapman & Hall: New\n",
      "           York. 2000.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import variation\n",
      "        >>> variation([1, 2, 3, 4, 5])\n",
      "        0.47140452079103173\n",
      "    \n",
      "    wasserstein_distance(u_values, v_values, u_weights=None, v_weights=None)\n",
      "        Compute the first Wasserstein distance between two 1D distributions.\n",
      "        \n",
      "        This distance is also known as the earth mover's distance, since it can be\n",
      "        seen as the minimum amount of \"work\" required to transform :math:`u` into\n",
      "        :math:`v`, where \"work\" is measured as the amount of distribution weight\n",
      "        that must be moved, multiplied by the distance it has to be moved.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        u_values, v_values : array_like\n",
      "            Values observed in the (empirical) distribution.\n",
      "        u_weights, v_weights : array_like, optional\n",
      "            Weight for each value. If unspecified, each value is assigned the same\n",
      "            weight.\n",
      "            `u_weights` (resp. `v_weights`) must have the same length as\n",
      "            `u_values` (resp. `v_values`). If the weight sum differs from 1, it\n",
      "            must still be positive and finite so that the weights can be normalized\n",
      "            to sum to 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distance : float\n",
      "            The computed distance between the distributions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The first Wasserstein distance between the distributions :math:`u` and\n",
      "        :math:`v` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            l_1 (u, v) = \\inf_{\\pi \\in \\Gamma (u, v)} \\int_{\\mathbb{R} \\times\n",
      "            \\mathbb{R}} |x-y| \\mathrm{d} \\pi (x, y)\n",
      "        \n",
      "        where :math:`\\Gamma (u, v)` is the set of (probability) distributions on\n",
      "        :math:`\\mathbb{R} \\times \\mathbb{R}` whose marginals are :math:`u` and\n",
      "        :math:`v` on the first and second factors respectively.\n",
      "        \n",
      "        If :math:`U` and :math:`V` are the respective CDFs of :math:`u` and\n",
      "        :math:`v`, this distance also equals to:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            l_1(u, v) = \\int_{-\\infty}^{+\\infty} |U-V|\n",
      "        \n",
      "        See [2]_ for a proof of the equivalence of both definitions.\n",
      "        \n",
      "        The input distributions can be empirical, therefore coming from samples\n",
      "        whose values are effectively inputs of the function, or they can be seen as\n",
      "        generalized functions, in which case they are weighted sums of Dirac delta\n",
      "        functions located at the specified values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Wasserstein metric\", https://en.wikipedia.org/wiki/Wasserstein_metric\n",
      "        .. [2] Ramdas, Garcia, Cuturi \"On Wasserstein Two Sample Testing and Related\n",
      "               Families of Nonparametric Tests\" (2015). :arXiv:`1509.02237`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import wasserstein_distance\n",
      "        >>> wasserstein_distance([0, 1, 3], [5, 6, 8])\n",
      "        5.0\n",
      "        >>> wasserstein_distance([0, 1], [0, 1], [3, 1], [2, 2])\n",
      "        0.25\n",
      "        >>> wasserstein_distance([3.4, 3.9, 7.5, 7.8], [4.5, 1.4],\n",
      "        ...                      [1.4, 0.9, 3.1, 7.2], [3.2, 3.5])\n",
      "        4.0781331438047861\n",
      "    \n",
      "    weightedtau(x, y, rank=True, weigher=None, additive=True)\n",
      "        Compute a weighted version of Kendall's :math:`\\tau`.\n",
      "        \n",
      "        The weighted :math:`\\tau` is a weighted version of Kendall's\n",
      "        :math:`\\tau` in which exchanges of high weight are more influential than\n",
      "        exchanges of low weight. The default parameters compute the additive\n",
      "        hyperbolic version of the index, :math:`\\tau_\\mathrm h`, which has\n",
      "        been shown to provide the best balance between important and\n",
      "        unimportant elements [1]_.\n",
      "        \n",
      "        The weighting is defined by means of a rank array, which assigns a\n",
      "        nonnegative rank to each element (higher importance ranks being\n",
      "        associated with smaller values, e.g., 0 is the highest possible rank),\n",
      "        and a weigher function, which assigns a weight based on the rank to\n",
      "        each element. The weight of an exchange is then the sum or the product\n",
      "        of the weights of the ranks of the exchanged elements. The default\n",
      "        parameters compute :math:`\\tau_\\mathrm h`: an exchange between\n",
      "        elements with rank :math:`r` and :math:`s` (starting from zero) has\n",
      "        weight :math:`1/(r+1) + 1/(s+1)`.\n",
      "        \n",
      "        Specifying a rank array is meaningful only if you have in mind an\n",
      "        external criterion of importance. If, as it usually happens, you do\n",
      "        not have in mind a specific rank, the weighted :math:`\\tau` is\n",
      "        defined by averaging the values obtained using the decreasing\n",
      "        lexicographical rank by (`x`, `y`) and by (`y`, `x`). This is the\n",
      "        behavior with default parameters. Note that the convention used\n",
      "        here for ranking (lower values imply higher importance) is opposite\n",
      "        to that used by other SciPy statistical functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Arrays of scores, of the same shape. If arrays are not 1-D, they will\n",
      "            be flattened to 1-D.\n",
      "        rank : array_like of ints or bool, optional\n",
      "            A nonnegative rank assigned to each element. If it is None, the\n",
      "            decreasing lexicographical rank by (`x`, `y`) will be used: elements of\n",
      "            higher rank will be those with larger `x`-values, using `y`-values to\n",
      "            break ties (in particular, swapping `x` and `y` will give a different\n",
      "            result). If it is False, the element indices will be used\n",
      "            directly as ranks. The default is True, in which case this\n",
      "            function returns the average of the values obtained using the\n",
      "            decreasing lexicographical rank by (`x`, `y`) and by (`y`, `x`).\n",
      "        weigher : callable, optional\n",
      "            The weigher function. Must map nonnegative integers (zero\n",
      "            representing the most important element) to a nonnegative weight.\n",
      "            The default, None, provides hyperbolic weighing, that is,\n",
      "            rank :math:`r` is mapped to weight :math:`1/(r+1)`.\n",
      "        additive : bool, optional\n",
      "            If True, the weight of an exchange is computed by adding the\n",
      "            weights of the ranks of the exchanged elements; otherwise, the weights\n",
      "            are multiplied. The default is True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        correlation : float\n",
      "           The weighted :math:`\\tau` correlation index.\n",
      "        pvalue : float\n",
      "           Presently ``np.nan``, as the null statistics is unknown (even in the\n",
      "           additive hyperbolic case).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kendalltau : Calculates Kendall's tau.\n",
      "        spearmanr : Calculates a Spearman rank-order correlation coefficient.\n",
      "        theilslopes : Computes the Theil-Sen estimator for a set of points (x, y).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function uses an :math:`O(n \\log n)`, mergesort-based algorithm\n",
      "        [1]_ that is a weighted extension of Knight's algorithm for Kendall's\n",
      "        :math:`\\tau` [2]_. It can compute Shieh's weighted :math:`\\tau` [3]_\n",
      "        between rankings without ties (i.e., permutations) by setting\n",
      "        `additive` and `rank` to False, as the definition given in [1]_ is a\n",
      "        generalization of Shieh's.\n",
      "        \n",
      "        NaNs are considered the smallest possible score.\n",
      "        \n",
      "        .. versionadded:: 0.19.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Sebastiano Vigna, \"A weighted correlation index for rankings with\n",
      "               ties\", Proceedings of the 24th international conference on World\n",
      "               Wide Web, pp. 1166-1176, ACM, 2015.\n",
      "        .. [2] W.R. Knight, \"A Computer Method for Calculating Kendall's Tau with\n",
      "               Ungrouped Data\", Journal of the American Statistical Association,\n",
      "               Vol. 61, No. 314, Part 1, pp. 436-439, 1966.\n",
      "        .. [3] Grace S. Shieh. \"A weighted Kendall's tau statistic\", Statistics &\n",
      "               Probability Letters, Vol. 39, No. 1, pp. 17-24, 1998.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, 0]\n",
      "        >>> tau, p_value = stats.weightedtau(x, y)\n",
      "        >>> tau\n",
      "        -0.56694968153682723\n",
      "        >>> p_value\n",
      "        nan\n",
      "        >>> tau, p_value = stats.weightedtau(x, y, additive=False)\n",
      "        >>> tau\n",
      "        -0.62205716951801038\n",
      "        \n",
      "        NaNs are considered the smallest possible score:\n",
      "        \n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, np.nan]\n",
      "        >>> tau, _ = stats.weightedtau(x, y)\n",
      "        >>> tau\n",
      "        -0.56694968153682723\n",
      "        \n",
      "        This is exactly Kendall's tau:\n",
      "        \n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, 0]\n",
      "        >>> tau, _ = stats.weightedtau(x, y, weigher=lambda x: 1)\n",
      "        >>> tau\n",
      "        -0.47140452079103173\n",
      "        \n",
      "        >>> x = [12, 2, 1, 12, 2]\n",
      "        >>> y = [1, 4, 7, 1, 0]\n",
      "        >>> stats.weightedtau(x, y, rank=None)\n",
      "        WeightedTauResult(correlation=-0.4157652301037516, pvalue=nan)\n",
      "        >>> stats.weightedtau(y, x, rank=None)\n",
      "        WeightedTauResult(correlation=-0.7181341329699028, pvalue=nan)\n",
      "    \n",
      "    wilcoxon(x, y=None, zero_method='wilcox', correction=False, alternative='two-sided', mode='auto')\n",
      "        Calculate the Wilcoxon signed-rank test.\n",
      "        \n",
      "        The Wilcoxon signed-rank test tests the null hypothesis that two\n",
      "        related paired samples come from the same distribution. In particular,\n",
      "        it tests whether the distribution of the differences x - y is symmetric\n",
      "        about zero. It is a non-parametric version of the paired T-test.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Either the first set of measurements (in which case ``y`` is the second\n",
      "            set of measurements), or the differences between two sets of\n",
      "            measurements (in which case ``y`` is not to be specified.)  Must be\n",
      "            one-dimensional.\n",
      "        y : array_like, optional\n",
      "            Either the second set of measurements (if ``x`` is the first set of\n",
      "            measurements), or not specified (if ``x`` is the differences between\n",
      "            two sets of measurements.)  Must be one-dimensional.\n",
      "        zero_method : {\"pratt\", \"wilcox\", \"zsplit\"}, optional\n",
      "            The following options are available (default is \"wilcox\"):\n",
      "        \n",
      "              * \"pratt\": Includes zero-differences in the ranking process,\n",
      "                but drops the ranks of the zeros, see [4]_, (more conservative).\n",
      "              * \"wilcox\": Discards all zero-differences, the default.\n",
      "              * \"zsplit\": Includes zero-differences in the ranking process and\n",
      "                split the zero rank between positive and negative ones.\n",
      "        correction : bool, optional\n",
      "            If True, apply continuity correction by adjusting the Wilcoxon rank\n",
      "            statistic by 0.5 towards the mean value when computing the\n",
      "            z-statistic if a normal approximation is used.  Default is False.\n",
      "        alternative : {\"two-sided\", \"greater\", \"less\"}, optional\n",
      "            The alternative hypothesis to be tested, see Notes. Default is\n",
      "            \"two-sided\".\n",
      "        mode : {\"auto\", \"exact\", \"approx\"}\n",
      "            Method to calculate the p-value, see Notes. Default is \"auto\".\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        statistic : float\n",
      "            If ``alternative`` is \"two-sided\", the sum of the ranks of the\n",
      "            differences above or below zero, whichever is smaller.\n",
      "            Otherwise the sum of the ranks of the differences above zero.\n",
      "        pvalue : float\n",
      "            The p-value for the test depending on ``alternative`` and ``mode``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kruskal, mannwhitneyu\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The test has been introduced in [4]_. Given n independent samples\n",
      "        (xi, yi) from a bivariate distribution (i.e. paired samples),\n",
      "        it computes the differences di = xi - yi. One assumption of the test\n",
      "        is that the differences are symmetric, see [2]_.\n",
      "        The two-sided test has the null hypothesis that the median of the\n",
      "        differences is zero against the alternative that it is different from\n",
      "        zero. The one-sided test has the null hypothesis that the median is\n",
      "        positive against the alternative that it is negative\n",
      "        (``alternative == 'less'``), or vice versa (``alternative == 'greater.'``).\n",
      "        \n",
      "        To derive the p-value, the exact distribution (``mode == 'exact'``)\n",
      "        can be used for sample sizes of up to 25. The default ``mode == 'auto'``\n",
      "        uses the exact distribution if there are at most 25 observations and no\n",
      "        ties, otherwise a normal approximation is used (``mode == 'approx'``).\n",
      "        \n",
      "        The treatment of ties can be controlled by the parameter `zero_method`.\n",
      "        If ``zero_method == 'pratt'``, the normal approximation is adjusted as in\n",
      "        [5]_. A typical rule is to require that n > 20 ([2]_, p. 383).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\n",
      "        .. [2] Conover, W.J., Practical Nonparametric Statistics, 1971.\n",
      "        .. [3] Pratt, J.W., Remarks on Zeros and Ties in the Wilcoxon Signed\n",
      "           Rank Procedures, Journal of the American Statistical Association,\n",
      "           Vol. 54, 1959, pp. 655-667. :doi:`10.1080/01621459.1959.10501526`\n",
      "        .. [4] Wilcoxon, F., Individual Comparisons by Ranking Methods,\n",
      "           Biometrics Bulletin, Vol. 1, 1945, pp. 80-83. :doi:`10.2307/3001968`\n",
      "        .. [5] Cureton, E.E., The Normal Approximation to the Signed-Rank\n",
      "           Sampling Distribution When Zero Differences are Present,\n",
      "           Journal of the American Statistical Association, Vol. 62, 1967,\n",
      "           pp. 1068-1069. :doi:`10.1080/01621459.1967.10500917`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In [4]_, the differences in height between cross- and self-fertilized\n",
      "        corn plants is given as follows:\n",
      "        \n",
      "        >>> d = [6, 8, 14, 16, 23, 24, 28, 29, 41, -48, 49, 56, 60, -67, 75]\n",
      "        \n",
      "        Cross-fertilized plants appear to be be higher. To test the null\n",
      "        hypothesis that there is no height difference, we can apply the\n",
      "        two-sided test:\n",
      "        \n",
      "        >>> from scipy.stats import wilcoxon\n",
      "        >>> w, p = wilcoxon(d)\n",
      "        >>> w, p\n",
      "        (24.0, 0.041259765625)\n",
      "        \n",
      "        Hence, we would reject the null hypothesis at a confidence level of 5%,\n",
      "        concluding that there is a difference in height between the groups.\n",
      "        To confirm that the median of the differences can be assumed to be\n",
      "        positive, we use:\n",
      "        \n",
      "        >>> w, p = wilcoxon(d, alternative='greater')\n",
      "        >>> w, p\n",
      "        (96.0, 0.0206298828125)\n",
      "        \n",
      "        This shows that the null hypothesis that the median is negative can be\n",
      "        rejected at a confidence level of 5% in favor of the alternative that\n",
      "        the median is greater than zero. The p-values above are exact. Using the\n",
      "        normal approximation gives very similar values:\n",
      "        \n",
      "        >>> w, p = wilcoxon(d, mode='approx')\n",
      "        >>> w, p\n",
      "        (24.0, 0.04088813291185591)\n",
      "        \n",
      "        Note that the statistic changed to 96 in the one-sided case (the sum\n",
      "        of ranks of positive differences) whereas it is 24 in the two-sided\n",
      "        case (the minimum of sum of ranks above and below zero).\n",
      "    \n",
      "    yeojohnson(x, lmbda=None)\n",
      "        Return a dataset transformed by a Yeo-Johnson power transformation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Input array.  Should be 1-dimensional.\n",
      "        lmbda : float, optional\n",
      "            If ``lmbda`` is ``None``, find the lambda that maximizes the\n",
      "            log-likelihood function and return it as the second output argument.\n",
      "            Otherwise the transformation is done for the given value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        yeojohnson: ndarray\n",
      "            Yeo-Johnson power transformed array.\n",
      "        maxlog : float, optional\n",
      "            If the `lmbda` parameter is None, the second returned argument is\n",
      "            the lambda that maximizes the log-likelihood function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, yeojohnson_normplot, yeojohnson_normmax, yeojohnson_llf, boxcox\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Yeo-Johnson transform is given by::\n",
      "        \n",
      "            y = ((x + 1)**lmbda - 1) / lmbda,                for x >= 0, lmbda != 0\n",
      "                log(x + 1),                                  for x >= 0, lmbda = 0\n",
      "                -((-x + 1)**(2 - lmbda) - 1) / (2 - lmbda),  for x < 0, lmbda != 2\n",
      "                -log(-x + 1),                                for x < 0, lmbda = 2\n",
      "        \n",
      "        Unlike `boxcox`, `yeojohnson` does not require the input data to be\n",
      "        positive.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        I. Yeo and R.A. Johnson, \"A New Family of Power Transformations to\n",
      "        Improve Normality or Symmetry\", Biometrika 87.4 (2000):\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        We generate some random variates from a non-normal distribution and make a\n",
      "        probability plot for it, to show it is non-normal in the tails:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax1 = fig.add_subplot(211)\n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> prob = stats.probplot(x, dist=stats.norm, plot=ax1)\n",
      "        >>> ax1.set_xlabel('')\n",
      "        >>> ax1.set_title('Probplot against normal distribution')\n",
      "        \n",
      "        We now use `yeojohnson` to transform the data so it's closest to normal:\n",
      "        \n",
      "        >>> ax2 = fig.add_subplot(212)\n",
      "        >>> xt, lmbda = stats.yeojohnson(x)\n",
      "        >>> prob = stats.probplot(xt, dist=stats.norm, plot=ax2)\n",
      "        >>> ax2.set_title('Probplot after Yeo-Johnson transformation')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    yeojohnson_llf(lmb, data)\n",
      "        The yeojohnson log-likelihood function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        lmb : scalar\n",
      "            Parameter for Yeo-Johnson transformation. See `yeojohnson` for\n",
      "            details.\n",
      "        data : array_like\n",
      "            Data to calculate Yeo-Johnson log-likelihood for. If `data` is\n",
      "            multi-dimensional, the log-likelihood is calculated along the first\n",
      "            axis.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        llf : float\n",
      "            Yeo-Johnson log-likelihood of `data` given `lmb`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        yeojohnson, probplot, yeojohnson_normplot, yeojohnson_normmax\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Yeo-Johnson log-likelihood function is defined here as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            llf = -N/2 \\log(\\hat{\\sigma}^2) + (\\lambda - 1)\n",
      "                  \\sum_i \\text{ sign }(x_i)\\log(|x_i| + 1)\n",
      "        \n",
      "        where :math:`\\hat{\\sigma}^2` is estimated variance of the the Yeo-Johnson\n",
      "        transformed input data ``x``.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
      "        \n",
      "        Generate some random variates and calculate Yeo-Johnson log-likelihood\n",
      "        values for them for a range of ``lmbda`` values:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, loc=10, size=1000)\n",
      "        >>> lmbdas = np.linspace(-2, 10)\n",
      "        >>> llf = np.zeros(lmbdas.shape, dtype=float)\n",
      "        >>> for ii, lmbda in enumerate(lmbdas):\n",
      "        ...     llf[ii] = stats.yeojohnson_llf(lmbda, x)\n",
      "        \n",
      "        Also find the optimal lmbda value with `yeojohnson`:\n",
      "        \n",
      "        >>> x_most_normal, lmbda_optimal = stats.yeojohnson(x)\n",
      "        \n",
      "        Plot the log-likelihood as function of lmbda.  Add the optimal lmbda as a\n",
      "        horizontal line to check that that's really the optimum:\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(lmbdas, llf, 'b.-')\n",
      "        >>> ax.axhline(stats.yeojohnson_llf(lmbda_optimal, x), color='r')\n",
      "        >>> ax.set_xlabel('lmbda parameter')\n",
      "        >>> ax.set_ylabel('Yeo-Johnson log-likelihood')\n",
      "        \n",
      "        Now add some probability plots to show that where the log-likelihood is\n",
      "        maximized the data transformed with `yeojohnson` looks closest to normal:\n",
      "        \n",
      "        >>> locs = [3, 10, 4]  # 'lower left', 'center', 'lower right'\n",
      "        >>> for lmbda, loc in zip([-1, lmbda_optimal, 9], locs):\n",
      "        ...     xt = stats.yeojohnson(x, lmbda=lmbda)\n",
      "        ...     (osm, osr), (slope, intercept, r_sq) = stats.probplot(xt)\n",
      "        ...     ax_inset = inset_axes(ax, width=\"20%\", height=\"20%\", loc=loc)\n",
      "        ...     ax_inset.plot(osm, osr, 'c.', osm, slope*osm + intercept, 'k-')\n",
      "        ...     ax_inset.set_xticklabels([])\n",
      "        ...     ax_inset.set_yticklabels([])\n",
      "        ...     ax_inset.set_title(r'$\\lambda=%1.2f$' % lmbda)\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    yeojohnson_normmax(x, brack=(-2, 2))\n",
      "        Compute optimal Yeo-Johnson transform parameter.\n",
      "        \n",
      "        Compute optimal Yeo-Johnson transform parameter for input data, using\n",
      "        maximum likelihood estimation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        brack : 2-tuple, optional\n",
      "            The starting interval for a downhill bracket search with\n",
      "            `optimize.brent`. Note that this is in most cases not critical; the\n",
      "            final result is allowed to be outside this bracket.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        maxlog : float\n",
      "            The optimal transform parameter found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        yeojohnson, yeojohnson_llf, yeojohnson_normplot\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Generate some data and determine optimal ``lmbda``\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = stats.loggamma.rvs(5, size=30, random_state=rng) + 5\n",
      "        >>> lmax = stats.yeojohnson_normmax(x)\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.yeojohnson_normplot(x, -10, 10, plot=ax)\n",
      "        >>> ax.axvline(lmax, color='r')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    yeojohnson_normplot(x, la, lb, plot=None, N=80)\n",
      "        Compute parameters for a Yeo-Johnson normality plot, optionally show it.\n",
      "        \n",
      "        A Yeo-Johnson normality plot shows graphically what the best\n",
      "        transformation parameter is to use in `yeojohnson` to obtain a\n",
      "        distribution that is close to normal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        la, lb : scalar\n",
      "            The lower and upper bounds for the ``lmbda`` values to pass to\n",
      "            `yeojohnson` for Yeo-Johnson transformations. These are also the\n",
      "            limits of the horizontal axis of the plot if that is generated.\n",
      "        plot : object, optional\n",
      "            If given, plots the quantiles and least squares fit.\n",
      "            `plot` is an object that has to have methods \"plot\" and \"text\".\n",
      "            The `matplotlib.pyplot` module or a Matplotlib Axes object can be used,\n",
      "            or a custom object with the same methods.\n",
      "            Default is None, which means that no plot is created.\n",
      "        N : int, optional\n",
      "            Number of points on the horizontal axis (equally distributed from\n",
      "            `la` to `lb`).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lmbdas : ndarray\n",
      "            The ``lmbda`` values for which a Yeo-Johnson transform was done.\n",
      "        ppcc : ndarray\n",
      "            Probability Plot Correlelation Coefficient, as obtained from `probplot`\n",
      "            when fitting the Box-Cox transformed input `x` against a normal\n",
      "            distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        probplot, yeojohnson, yeojohnson_normmax, yeojohnson_llf, ppcc_max\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Even if `plot` is given, the figure is not shown or saved by\n",
      "        `boxcox_normplot`; ``plt.show()`` or ``plt.savefig('figname.png')``\n",
      "        should be used after calling `probplot`.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Generate some non-normally distributed data, and create a Yeo-Johnson plot:\n",
      "        \n",
      "        >>> x = stats.loggamma.rvs(5, size=500) + 5\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> prob = stats.yeojohnson_normplot(x, -20, 20, plot=ax)\n",
      "        \n",
      "        Determine and plot the optimal ``lmbda`` to transform ``x`` and plot it in\n",
      "        the same plot:\n",
      "        \n",
      "        >>> _, maxlog = stats.yeojohnson(x)\n",
      "        >>> ax.axvline(maxlog, color='r')\n",
      "        \n",
      "        >>> plt.show()\n",
      "    \n",
      "    zmap(scores, compare, axis=0, ddof=0, nan_policy='propagate')\n",
      "        Calculate the relative z-scores.\n",
      "        \n",
      "        Return an array of z-scores, i.e., scores that are standardized to\n",
      "        zero mean and unit variance, where mean and variance are calculated\n",
      "        from the comparison array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scores : array_like\n",
      "            The input for which z-scores are calculated.\n",
      "        compare : array_like\n",
      "            The input from which the mean and standard deviation of the\n",
      "            normalization are taken; assumed to have the same dimension as\n",
      "            `scores`.\n",
      "        axis : int or None, optional\n",
      "            Axis over which mean and variance of `compare` are calculated.\n",
      "            Default is 0. If None, compute over the whole array `scores`.\n",
      "        ddof : int, optional\n",
      "            Degrees of freedom correction in the calculation of the\n",
      "            standard deviation. Default is 0.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle the occurrence of nans in `compare`.\n",
      "            'propagate' returns nan, 'raise' raises an exception, 'omit'\n",
      "            performs the calculations ignoring nan values. Default is\n",
      "            'propagate'. Note that when the value is 'omit', nans in `scores`\n",
      "            also propagate to the output, but they do not affect the z-scores\n",
      "            computed for the non-nan values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        zscore : array_like\n",
      "            Z-scores, in the same shape as `scores`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function preserves ndarray subclasses, and works also with\n",
      "        matrices and masked arrays (it uses `asanyarray` instead of\n",
      "        `asarray` for parameters).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import zmap\n",
      "        >>> a = [0.5, 2.0, 2.5, 3]\n",
      "        >>> b = [0, 1, 2, 3, 4]\n",
      "        >>> zmap(a, b)\n",
      "        array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n",
      "    \n",
      "    zscore(a, axis=0, ddof=0, nan_policy='propagate')\n",
      "        Compute the z score.\n",
      "        \n",
      "        Compute the z score of each value in the sample, relative to the\n",
      "        sample mean and standard deviation.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            An array like object containing the sample data.\n",
      "        axis : int or None, optional\n",
      "            Axis along which to operate. Default is 0. If None, compute over\n",
      "            the whole array `a`.\n",
      "        ddof : int, optional\n",
      "            Degrees of freedom correction in the calculation of the\n",
      "            standard deviation. Default is 0.\n",
      "        nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "            Defines how to handle when input contains nan. 'propagate' returns nan,\n",
      "            'raise' throws an error, 'omit' performs the calculations ignoring nan\n",
      "            values. Default is 'propagate'.  Note that when the value is 'omit',\n",
      "            nans in the input also propagate to the output, but they do not affect\n",
      "            the z-scores computed for the non-nan values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        zscore : array_like\n",
      "            The z-scores, standardized by mean and standard deviation of\n",
      "            input array `a`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function preserves ndarray subclasses, and works also with\n",
      "        matrices and masked arrays (it uses `asanyarray` instead of\n",
      "        `asarray` for parameters).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,\n",
      "        ...                0.1954,  0.6307,  0.6599,  0.1065,  0.0508])\n",
      "        >>> from scipy import stats\n",
      "        >>> stats.zscore(a)\n",
      "        array([ 1.1273, -1.247 , -0.0552,  1.0923,  1.1664, -0.8559,  0.5786,\n",
      "                0.6748, -1.1488, -1.3324])\n",
      "        \n",
      "        Computing along a specified axis, using n-1 degrees of freedom\n",
      "        (``ddof=1``) to calculate the standard deviation:\n",
      "        \n",
      "        >>> b = np.array([[ 0.3148,  0.0478,  0.6243,  0.4608],\n",
      "        ...               [ 0.7149,  0.0775,  0.6072,  0.9656],\n",
      "        ...               [ 0.6341,  0.1403,  0.9759,  0.4064],\n",
      "        ...               [ 0.5918,  0.6948,  0.904 ,  0.3721],\n",
      "        ...               [ 0.0921,  0.2481,  0.1188,  0.1366]])\n",
      "        >>> stats.zscore(b, axis=1, ddof=1)\n",
      "        array([[-0.19264823, -1.28415119,  1.07259584,  0.40420358],\n",
      "               [ 0.33048416, -1.37380874,  0.04251374,  1.00081084],\n",
      "               [ 0.26796377, -1.12598418,  1.23283094, -0.37481053],\n",
      "               [-0.22095197,  0.24468594,  1.19042819, -1.21416216],\n",
      "               [-0.82780366,  1.4457416 , -0.43867764, -0.1792603 ]])\n",
      "        \n",
      "        An example with `nan_policy='omit'`:\n",
      "        \n",
      "        >>> x = np.array([[25.11, 30.10, np.nan, 32.02, 43.15],\n",
      "        ...               [14.95, 16.06, 121.25, 94.35, 29.81]])\n",
      "        >>> stats.zscore(x, axis=1, nan_policy='omit')\n",
      "        array([[-1.13490897, -0.37830299,         nan, -0.08718406,  1.60039602],\n",
      "               [-0.91611681, -0.89090508,  1.4983032 ,  0.88731639, -0.5785977 ]])\n",
      "\n",
      "DATA\n",
      "    __all__ = ['F_onewayBadInputSizesWarning', 'F_onewayConstantInputWarni...\n",
      "    alpha = <scipy.stats._continuous_distns.alpha_gen object>\n",
      "        An alpha continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `alpha` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `alpha` ([1]_, [2]_) is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a) = \\frac{1}{x^2 \\Phi(a) \\sqrt{2\\pi}} *\n",
      "                      \\exp(-\\frac{1}{2} (a-1/x)^2)\n",
      "        \n",
      "        where :math:`\\Phi` is the normal CDF, :math:`x > 0`, and :math:`a > 0`.\n",
      "        \n",
      "        `alpha` takes ``a`` as a shape parameter.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``alpha.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``alpha.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Johnson, Kotz, and Balakrishnan, \"Continuous Univariate\n",
      "               Distributions, Volume 1\", Second Edition, John Wiley and Sons,\n",
      "               p. 173 (1994).\n",
      "        .. [2] Anthony A. Salvia, \"Reliability applications of the Alpha\n",
      "               Distribution\", IEEE Transactions on Reliability, Vol. R-34,\n",
      "               No. 3, pp. 251-252 (1985).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import alpha\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 3.57\n",
      "        >>> mean, var, skew, kurt = alpha.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(alpha.ppf(0.01, a),\n",
      "        ...                 alpha.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, alpha.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='alpha pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = alpha(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = alpha.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], alpha.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = alpha.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    anglit = <scipy.stats._continuous_distns.anglit_gen object>\n",
      "        An anglit continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `anglit` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `anglit` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\sin(2x + \\pi/2) = \\cos(2x)\n",
      "        \n",
      "        for :math:`-\\pi/4 \\le x \\le \\pi/4`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``anglit.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``anglit.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import anglit\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = anglit.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(anglit.ppf(0.01),\n",
      "        ...                 anglit.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, anglit.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='anglit pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = anglit()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = anglit.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], anglit.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = anglit.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    arcsine = <scipy.stats._continuous_distns.arcsine_gen object>\n",
      "        An arcsine continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `arcsine` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `arcsine` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\pi \\sqrt{x (1-x)}}\n",
      "        \n",
      "        for :math:`0 < x < 1`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``arcsine.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``arcsine.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import arcsine\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = arcsine.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(arcsine.ppf(0.01),\n",
      "        ...                 arcsine.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, arcsine.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='arcsine pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = arcsine()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = arcsine.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], arcsine.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = arcsine.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    argus = <scipy.stats._continuous_distns.argus_gen object>\n",
      "        Argus distribution\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `argus` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(chi, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, chi, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, chi, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, chi, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, chi, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, chi, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, chi, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, chi, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, chi, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, chi, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(chi, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(chi, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(chi,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(chi, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(chi, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(chi, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(chi, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, chi, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `argus` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\chi) = \\frac{\\chi^3}{\\sqrt{2\\pi} \\Psi(\\chi)} x \\sqrt{1-x^2}\n",
      "                         \\exp(-\\chi^2 (1 - x^2)/2)\n",
      "        \n",
      "        for :math:`0 < x < 1` and :math:`\\chi > 0`, where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\Psi(\\chi) = \\Phi(\\chi) - \\chi \\phi(\\chi) - 1/2\n",
      "        \n",
      "        with :math:`\\Phi` and :math:`\\phi` being the CDF and PDF of a standard\n",
      "        normal distribution, respectively.\n",
      "        \n",
      "        `argus` takes :math:`\\chi` as shape a parameter.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``argus.pdf(x, chi, loc, scale)`` is identically\n",
      "        equivalent to ``argus.pdf(y, chi) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        .. versionadded:: 0.19.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"ARGUS distribution\",\n",
      "               https://en.wikipedia.org/wiki/ARGUS_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import argus\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> chi = 1\n",
      "        >>> mean, var, skew, kurt = argus.stats(chi, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(argus.ppf(0.01, chi),\n",
      "        ...                 argus.ppf(0.99, chi), 100)\n",
      "        >>> ax.plot(x, argus.pdf(x, chi),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='argus pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = argus(chi)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = argus.ppf([0.001, 0.5, 0.999], chi)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], argus.cdf(vals, chi))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = argus.rvs(chi, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    bernoulli = <scipy.stats._discrete_distns.bernoulli_gen object>\n",
      "        A Bernoulli discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `bernoulli` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(p, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, p, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, p, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, p, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, p, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, p, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, p, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, p, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, p, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(p, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(p, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(p, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(p, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(p, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(p, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, p, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `bernoulli` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           f(k) = \\begin{cases}1-p  &\\text{if } k = 0\\\\\n",
      "                               p    &\\text{if } k = 1\\end{cases}\n",
      "        \n",
      "        for :math:`k` in :math:`\\{0, 1\\}`, :math:`0 \\leq p \\leq 1`\n",
      "        \n",
      "        `bernoulli` takes :math:`p` as shape parameter,\n",
      "        where :math:`p` is the probability of a single success\n",
      "        and :math:`1-p` is the probability of a single failure.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``bernoulli.pmf(k, p, loc)`` is identically\n",
      "        equivalent to ``bernoulli.pmf(k - loc, p)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import bernoulli\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> p = 0.3\n",
      "        >>> mean, var, skew, kurt = bernoulli.stats(p, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(bernoulli.ppf(0.01, p),\n",
      "        ...               bernoulli.ppf(0.99, p))\n",
      "        >>> ax.plot(x, bernoulli.pmf(x, p), 'bo', ms=8, label='bernoulli pmf')\n",
      "        >>> ax.vlines(x, 0, bernoulli.pmf(x, p), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = bernoulli(p)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = bernoulli.cdf(x, p)\n",
      "        >>> np.allclose(x, bernoulli.ppf(prob, p))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = bernoulli.rvs(p, size=1000)\n",
      "    \n",
      "    beta = <scipy.stats._continuous_distns.beta_gen object>\n",
      "        A beta continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `beta` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `beta` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b) = \\frac{\\Gamma(a+b) x^{a-1} (1-x)^{b-1}}\n",
      "                              {\\Gamma(a) \\Gamma(b)}\n",
      "        \n",
      "        for :math:`0 <= x <= 1`, :math:`a > 0`, :math:`b > 0`, where\n",
      "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
      "        \n",
      "        `beta` takes :math:`a` and :math:`b` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``beta.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``beta.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import beta\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 2.31, 0.627\n",
      "        >>> mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(beta.ppf(0.01, a, b),\n",
      "        ...                 beta.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, beta.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='beta pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = beta(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = beta.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], beta.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = beta.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    betabinom = <scipy.stats._discrete_distns.betabinom_gen object>\n",
      "        A beta-binomial discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `betabinom` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(n, a, b, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, n, a, b, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, n, a, b, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, n, a, b, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, n, a, b, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, n, a, b, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, n, a, b, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, n, a, b, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, n, a, b, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(n, a, b, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(n, a, b, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(n, a, b), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(n, a, b, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(n, a, b, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(n, a, b, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(n, a, b, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, n, a, b, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The beta-binomial distribution is a binomial distribution with a\n",
      "        probability of success `p` that follows a beta distribution.\n",
      "        \n",
      "        The probability mass function for `betabinom` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           f(k) = \\binom{n}{k} \\frac{B(k + a, n - k + b)}{B(a, b)}\n",
      "        \n",
      "        for :math:`k \\in \\{0, 1, \\dots, n\\}`, :math:`n \\geq 0`, :math:`a > 0`,\n",
      "        :math:`b > 0`, where :math:`B(a, b)` is the beta function.\n",
      "        \n",
      "        `betabinom` takes :math:`n`, :math:`a`, and :math:`b` as shape parameters.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Beta-binomial_distribution\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``betabinom.pmf(k, n, a, b, loc)`` is identically\n",
      "        equivalent to ``betabinom.pmf(k - loc, n, a, b)``.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        beta, binom\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import betabinom\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> n, a, b = 5, 2.3, 0.63\n",
      "        >>> mean, var, skew, kurt = betabinom.stats(n, a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(betabinom.ppf(0.01, n, a, b),\n",
      "        ...               betabinom.ppf(0.99, n, a, b))\n",
      "        >>> ax.plot(x, betabinom.pmf(x, n, a, b), 'bo', ms=8, label='betabinom pmf')\n",
      "        >>> ax.vlines(x, 0, betabinom.pmf(x, n, a, b), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = betabinom(n, a, b)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = betabinom.cdf(x, n, a, b)\n",
      "        >>> np.allclose(x, betabinom.ppf(prob, n, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = betabinom.rvs(n, a, b, size=1000)\n",
      "    \n",
      "    betaprime = <scipy.stats._continuous_distns.betaprime_gen object>\n",
      "        A beta prime continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `betaprime` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `betaprime` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b) = \\frac{x^{a-1} (1+x)^{-a-b}}{\\beta(a, b)}\n",
      "        \n",
      "        for :math:`x >= 0`, :math:`a > 0`, :math:`b > 0`, where\n",
      "        :math:`\\beta(a, b)` is the beta function (see `scipy.special.beta`).\n",
      "        \n",
      "        `betaprime` takes ``a`` and ``b`` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``betaprime.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``betaprime.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import betaprime\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 5, 6\n",
      "        >>> mean, var, skew, kurt = betaprime.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(betaprime.ppf(0.01, a, b),\n",
      "        ...                 betaprime.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, betaprime.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='betaprime pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = betaprime(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = betaprime.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], betaprime.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = betaprime.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    binom = <scipy.stats._discrete_distns.binom_gen object>\n",
      "        A binomial discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `binom` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(n, p, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, n, p, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, n, p, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, n, p, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, n, p, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, n, p, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, n, p, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, n, p, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, n, p, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(n, p, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(n, p, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(n, p, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(n, p, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(n, p, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(n, p, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, n, p, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `binom` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           f(k) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
      "        \n",
      "        for :math:`k \\in \\{0, 1, \\dots, n\\}`, :math:`0 \\leq p \\leq 1`\n",
      "        \n",
      "        `binom` takes :math:`n` and :math:`p` as shape parameters,\n",
      "        where :math:`p` is the probability of a single success\n",
      "        and :math:`1-p` is the probability of a single failure.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``binom.pmf(k, n, p, loc)`` is identically\n",
      "        equivalent to ``binom.pmf(k - loc, n, p)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import binom\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> n, p = 5, 0.4\n",
      "        >>> mean, var, skew, kurt = binom.stats(n, p, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(binom.ppf(0.01, n, p),\n",
      "        ...               binom.ppf(0.99, n, p))\n",
      "        >>> ax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\n",
      "        >>> ax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = binom(n, p)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = binom.cdf(x, n, p)\n",
      "        >>> np.allclose(x, binom.ppf(prob, n, p))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = binom.rvs(n, p, size=1000)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        hypergeom, nbinom, nhypergeom\n",
      "    \n",
      "    boltzmann = <scipy.stats._discrete_distns.boltzmann_gen object>\n",
      "        A Boltzmann (Truncated Discrete Exponential) random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `boltzmann` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(lambda_, N, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, lambda_, N, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, lambda_, N, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, lambda_, N, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, lambda_, N, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, lambda_, N, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, lambda_, N, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, lambda_, N, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, lambda_, N, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(lambda_, N, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(lambda_, N, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(lambda_, N), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(lambda_, N, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(lambda_, N, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(lambda_, N, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(lambda_, N, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, lambda_, N, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `boltzmann` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) = (1-\\exp(-\\lambda)) \\exp(-\\lambda k) / (1-\\exp(-\\lambda N))\n",
      "        \n",
      "        for :math:`k = 0,..., N-1`.\n",
      "        \n",
      "        `boltzmann` takes :math:`\\lambda > 0` and :math:`N > 0` as shape parameters.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``boltzmann.pmf(k, lambda_, N, loc)`` is identically\n",
      "        equivalent to ``boltzmann.pmf(k - loc, lambda_, N)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import boltzmann\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> lambda_, N = 1.4, 19\n",
      "        >>> mean, var, skew, kurt = boltzmann.stats(lambda_, N, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(boltzmann.ppf(0.01, lambda_, N),\n",
      "        ...               boltzmann.ppf(0.99, lambda_, N))\n",
      "        >>> ax.plot(x, boltzmann.pmf(x, lambda_, N), 'bo', ms=8, label='boltzmann pmf')\n",
      "        >>> ax.vlines(x, 0, boltzmann.pmf(x, lambda_, N), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = boltzmann(lambda_, N)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = boltzmann.cdf(x, lambda_, N)\n",
      "        >>> np.allclose(x, boltzmann.ppf(prob, lambda_, N))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = boltzmann.rvs(lambda_, N, size=1000)\n",
      "    \n",
      "    bradford = <scipy.stats._continuous_distns.bradford_gen object>\n",
      "        A Bradford continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `bradford` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `bradford` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{c}{\\log(1+c) (1+cx)}\n",
      "        \n",
      "        for :math:`0 <= x <= 1` and :math:`c > 0`.\n",
      "        \n",
      "        `bradford` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``bradford.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``bradford.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import bradford\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.299\n",
      "        >>> mean, var, skew, kurt = bradford.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(bradford.ppf(0.01, c),\n",
      "        ...                 bradford.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, bradford.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='bradford pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = bradford(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = bradford.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], bradford.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = bradford.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    burr = <scipy.stats._continuous_distns.burr_gen object>\n",
      "        A Burr (Type III) continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `burr` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, d, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, d, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, d, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, d, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, d, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, d, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, d, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, d, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, d, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, d, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, d, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, d, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, d, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, d, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, d, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, d, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fisk : a special case of either `burr` or `burr12` with ``d=1``\n",
      "        burr12 : Burr Type XII distribution\n",
      "        mielke : Mielke Beta-Kappa / Dagum distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `burr` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c, d) = c d x^{-c - 1} / (1 + x^{-c})^{d + 1}\n",
      "        \n",
      "        for :math:`x >= 0` and :math:`c, d > 0`.\n",
      "        \n",
      "        `burr` takes :math:`c` and :math:`d` as shape parameters.\n",
      "        \n",
      "        This is the PDF corresponding to the third CDF given in Burr's list;\n",
      "        specifically, it is equation (11) in Burr's paper [1]_. The distribution\n",
      "        is also commonly referred to as the Dagum distribution [2]_. If the\n",
      "        parameter :math:`c < 1` then the mean of the distribution does not\n",
      "        exist and if :math:`c < 2` the variance does not exist [2]_.\n",
      "        The PDF is finite at the left endpoint :math:`x = 0` if :math:`c * d >= 1`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``burr.pdf(x, c, d, loc, scale)`` is identically\n",
      "        equivalent to ``burr.pdf(y, c, d) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burr, I. W. \"Cumulative frequency functions\", Annals of\n",
      "           Mathematical Statistics, 13(2), pp 215-232 (1942).\n",
      "        .. [2] https://en.wikipedia.org/wiki/Dagum_distribution\n",
      "        .. [3] Kleiber, Christian. \"A guide to the Dagum distributions.\"\n",
      "           Modeling Income Distributions and Lorenz Curves  pp 97-117 (2008).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import burr\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c, d = 10.5, 4.3\n",
      "        >>> mean, var, skew, kurt = burr.stats(c, d, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(burr.ppf(0.01, c, d),\n",
      "        ...                 burr.ppf(0.99, c, d), 100)\n",
      "        >>> ax.plot(x, burr.pdf(x, c, d),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='burr pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = burr(c, d)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = burr.ppf([0.001, 0.5, 0.999], c, d)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], burr.cdf(vals, c, d))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = burr.rvs(c, d, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    burr12 = <scipy.stats._continuous_distns.burr12_gen object>\n",
      "        A Burr (Type XII) continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `burr12` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, d, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, d, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, d, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, d, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, d, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, d, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, d, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, d, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, d, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, d, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, d, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, d, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, d, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, d, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, d, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, d, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fisk : a special case of either `burr` or `burr12` with ``d=1``\n",
      "        burr : Burr Type III distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `burr` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c, d) = c d x^{c-1} / (1 + x^c)^{d + 1}\n",
      "        \n",
      "        for :math:`x >= 0` and :math:`c, d > 0`.\n",
      "        \n",
      "        `burr12` takes ``c`` and ``d`` as shape parameters for :math:`c`\n",
      "        and :math:`d`.\n",
      "        \n",
      "        This is the PDF corresponding to the twelfth CDF given in Burr's list;\n",
      "        specifically, it is equation (20) in Burr's paper [1]_.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``burr12.pdf(x, c, d, loc, scale)`` is identically\n",
      "        equivalent to ``burr12.pdf(y, c, d) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        The Burr type 12 distribution is also sometimes referred to as\n",
      "        the Singh-Maddala distribution from NIST [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burr, I. W. \"Cumulative frequency functions\", Annals of\n",
      "           Mathematical Statistics, 13(2), pp 215-232 (1942).\n",
      "        \n",
      "        .. [2] https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/b12pdf.htm\n",
      "        \n",
      "        .. [3] \"Burr distribution\",\n",
      "           https://en.wikipedia.org/wiki/Burr_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import burr12\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c, d = 10, 4\n",
      "        >>> mean, var, skew, kurt = burr12.stats(c, d, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(burr12.ppf(0.01, c, d),\n",
      "        ...                 burr12.ppf(0.99, c, d), 100)\n",
      "        >>> ax.plot(x, burr12.pdf(x, c, d),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='burr12 pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = burr12(c, d)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = burr12.ppf([0.001, 0.5, 0.999], c, d)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], burr12.cdf(vals, c, d))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = burr12.rvs(c, d, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    cauchy = <scipy.stats._continuous_distns.cauchy_gen object>\n",
      "        A Cauchy continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `cauchy` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `cauchy` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\pi (1 + x^2)}\n",
      "        \n",
      "        for a real number :math:`x`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``cauchy.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``cauchy.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import cauchy\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = cauchy.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(cauchy.ppf(0.01),\n",
      "        ...                 cauchy.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, cauchy.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='cauchy pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = cauchy()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = cauchy.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], cauchy.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = cauchy.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    chi = <scipy.stats._continuous_distns.chi_gen object>\n",
      "        A chi continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `chi` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(df, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, df, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, df, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, df, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, df, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, df, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, df, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, df, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, df, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, df, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(df, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(df, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(df, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(df, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(df, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(df, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, df, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `chi` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, k) = \\frac{1}{2^{k/2-1} \\Gamma \\left( k/2 \\right)}\n",
      "                       x^{k-1} \\exp \\left( -x^2/2 \\right)\n",
      "        \n",
      "        for :math:`x >= 0` and :math:`k > 0` (degrees of freedom, denoted ``df``\n",
      "        in the implementation). :math:`\\Gamma` is the gamma function\n",
      "        (`scipy.special.gamma`).\n",
      "        \n",
      "        Special cases of `chi` are:\n",
      "        \n",
      "            - ``chi(1, loc, scale)`` is equivalent to `halfnorm`\n",
      "            - ``chi(2, 0, scale)`` is equivalent to `rayleigh`\n",
      "            - ``chi(3, 0, scale)`` is equivalent to `maxwell`\n",
      "        \n",
      "        `chi` takes ``df`` as a shape parameter.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``chi.pdf(x, df, loc, scale)`` is identically\n",
      "        equivalent to ``chi.pdf(y, df) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import chi\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> df = 78\n",
      "        >>> mean, var, skew, kurt = chi.stats(df, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(chi.ppf(0.01, df),\n",
      "        ...                 chi.ppf(0.99, df), 100)\n",
      "        >>> ax.plot(x, chi.pdf(x, df),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='chi pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = chi(df)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = chi.ppf([0.001, 0.5, 0.999], df)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], chi.cdf(vals, df))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = chi.rvs(df, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    chi2 = <scipy.stats._continuous_distns.chi2_gen object>\n",
      "        A chi-squared continuous random variable.\n",
      "        \n",
      "        For the noncentral chi-square distribution, see `ncx2`.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `chi2` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(df, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, df, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, df, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, df, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, df, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, df, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, df, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, df, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, df, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, df, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(df, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(df, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(df, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(df, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(df, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(df, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, df, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ncx2\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `chi2` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, k) = \\frac{1}{2^{k/2} \\Gamma \\left( k/2 \\right)}\n",
      "                       x^{k/2-1} \\exp \\left( -x/2 \\right)\n",
      "        \n",
      "        for :math:`x > 0`  and :math:`k > 0` (degrees of freedom, denoted ``df``\n",
      "        in the implementation).\n",
      "        \n",
      "        `chi2` takes ``df`` as a shape parameter.\n",
      "        \n",
      "        The chi-squared distribution is a special case of the gamma\n",
      "        distribution, with gamma parameters ``a = df/2``, ``loc = 0`` and\n",
      "        ``scale = 2``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``chi2.pdf(x, df, loc, scale)`` is identically\n",
      "        equivalent to ``chi2.pdf(y, df) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import chi2\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> df = 55\n",
      "        >>> mean, var, skew, kurt = chi2.stats(df, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(chi2.ppf(0.01, df),\n",
      "        ...                 chi2.ppf(0.99, df), 100)\n",
      "        >>> ax.plot(x, chi2.pdf(x, df),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='chi2 pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = chi2(df)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = chi2.ppf([0.001, 0.5, 0.999], df)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], chi2.cdf(vals, df))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = chi2.rvs(df, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    cosine = <scipy.stats._continuous_distns.cosine_gen object>\n",
      "        A cosine continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `cosine` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The cosine distribution is an approximation to the normal distribution.\n",
      "        The probability density function for `cosine` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{2\\pi} (1+\\cos(x))\n",
      "        \n",
      "        for :math:`-\\pi \\le x \\le \\pi`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``cosine.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``cosine.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import cosine\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = cosine.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(cosine.ppf(0.01),\n",
      "        ...                 cosine.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, cosine.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='cosine pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = cosine()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = cosine.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], cosine.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = cosine.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    crystalball = <scipy.stats._continuous_distns.crystalball_gen object>\n",
      "        Crystalball distribution\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `crystalball` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(beta, m, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, beta, m, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, beta, m, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, beta, m, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, beta, m, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, beta, m, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, beta, m, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, beta, m, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, beta, m, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, beta, m, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(beta, m, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(beta, m, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(beta, m), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(beta, m, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(beta, m, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(beta, m, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(beta, m, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, beta, m, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `crystalball` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\beta, m) =  \\begin{cases}\n",
      "                                N \\exp(-x^2 / 2),  &\\text{for } x > -\\beta\\\\\n",
      "                                N A (B - x)^{-m}  &\\text{for } x \\le -\\beta\n",
      "                              \\end{cases}\n",
      "        \n",
      "        where :math:`A = (m / |\\beta|)^m  \\exp(-\\beta^2 / 2)`,\n",
      "        :math:`B = m/|\\beta| - |\\beta|` and :math:`N` is a normalisation constant.\n",
      "        \n",
      "        `crystalball` takes :math:`\\beta > 0` and :math:`m > 1` as shape\n",
      "        parameters.  :math:`\\beta` defines the point where the pdf changes\n",
      "        from a power-law to a Gaussian distribution.  :math:`m` is the power\n",
      "        of the power-law tail.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Crystal Ball Function\",\n",
      "               https://en.wikipedia.org/wiki/Crystal_Ball_function\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``crystalball.pdf(x, beta, m, loc, scale)`` is identically\n",
      "        equivalent to ``crystalball.pdf(y, beta, m) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        .. versionadded:: 0.19.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import crystalball\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> beta, m = 2, 3\n",
      "        >>> mean, var, skew, kurt = crystalball.stats(beta, m, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(crystalball.ppf(0.01, beta, m),\n",
      "        ...                 crystalball.ppf(0.99, beta, m), 100)\n",
      "        >>> ax.plot(x, crystalball.pdf(x, beta, m),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='crystalball pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = crystalball(beta, m)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = crystalball.ppf([0.001, 0.5, 0.999], beta, m)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], crystalball.cdf(vals, beta, m))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = crystalball.rvs(beta, m, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    dgamma = <scipy.stats._continuous_distns.dgamma_gen object>\n",
      "        A double gamma continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `dgamma` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `dgamma` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a) = \\frac{1}{2\\Gamma(a)} |x|^{a-1} \\exp(-|x|)\n",
      "        \n",
      "        for a real number :math:`x` and :math:`a > 0`. :math:`\\Gamma` is the\n",
      "        gamma function (`scipy.special.gamma`).\n",
      "        \n",
      "        `dgamma` takes ``a`` as a shape parameter for :math:`a`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``dgamma.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``dgamma.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import dgamma\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 1.1\n",
      "        >>> mean, var, skew, kurt = dgamma.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(dgamma.ppf(0.01, a),\n",
      "        ...                 dgamma.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, dgamma.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='dgamma pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = dgamma(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = dgamma.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], dgamma.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = dgamma.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    dirichlet = <scipy.stats._multivariate.dirichlet_gen object>\n",
      "        A Dirichlet random variable.\n",
      "        \n",
      "        The `alpha` keyword specifies the concentration parameters of the\n",
      "        distribution.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pdf(x, alpha)``\n",
      "            Probability density function.\n",
      "        ``logpdf(x, alpha)``\n",
      "            Log of the probability density function.\n",
      "        ``rvs(alpha, size=1, random_state=None)``\n",
      "            Draw random samples from a Dirichlet distribution.\n",
      "        ``mean(alpha)``\n",
      "            The mean of the Dirichlet distribution\n",
      "        ``var(alpha)``\n",
      "            The variance of the Dirichlet distribution\n",
      "        ``entropy(alpha)``\n",
      "            Compute the differential entropy of the Dirichlet distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Quantiles, with the last axis of `x` denoting the components.\n",
      "        alpha : array_like\n",
      "            The concentration parameters. The number of entries determines the\n",
      "            dimensionality of the distribution.\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Alternatively, the object may be called (as a function) to fix\n",
      "        concentration parameters, returning a \"frozen\" Dirichlet\n",
      "        random variable:\n",
      "        \n",
      "        rv = dirichlet(alpha)\n",
      "            - Frozen object with the same methods but holding the given\n",
      "              concentration parameters fixed.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Each :math:`\\alpha` entry must be positive. The distribution has only\n",
      "        support on the simplex defined by\n",
      "        \n",
      "        .. math::\n",
      "            \\sum_{i=1}^{K} x_i = 1\n",
      "        \n",
      "        where 0 < x_i < 1.\n",
      "        \n",
      "        If the quantiles don't lie within the simplex, a ValueError is raised.\n",
      "        \n",
      "        The probability density function for `dirichlet` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\mathrm{B}(\\boldsymbol\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}\n",
      "        \n",
      "        where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\mathrm{B}(\\boldsymbol\\alpha) = \\frac{\\prod_{i=1}^K \\Gamma(\\alpha_i)}\n",
      "                                         {\\Gamma\\bigl(\\sum_{i=1}^K \\alpha_i\\bigr)}\n",
      "        \n",
      "        and :math:`\\boldsymbol\\alpha=(\\alpha_1,\\ldots,\\alpha_K)`, the\n",
      "        concentration parameters and :math:`K` is the dimension of the space\n",
      "        where :math:`x` takes values.\n",
      "        \n",
      "        Note that the dirichlet interface is somewhat inconsistent.\n",
      "        The array returned by the rvs function is transposed\n",
      "        with respect to the format expected by the pdf and logpdf.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import dirichlet\n",
      "        \n",
      "        Generate a dirichlet random variable\n",
      "        \n",
      "        >>> quantiles = np.array([0.2, 0.2, 0.6])  # specify quantiles\n",
      "        >>> alpha = np.array([0.4, 5, 15])  # specify concentration parameters\n",
      "        >>> dirichlet.pdf(quantiles, alpha)\n",
      "        0.2843831684937255\n",
      "        \n",
      "        The same PDF but following a log scale\n",
      "        \n",
      "        >>> dirichlet.logpdf(quantiles, alpha)\n",
      "        -1.2574327653159187\n",
      "        \n",
      "        Once we specify the dirichlet distribution\n",
      "        we can then calculate quantities of interest\n",
      "        \n",
      "        >>> dirichlet.mean(alpha)  # get the mean of the distribution\n",
      "        array([0.01960784, 0.24509804, 0.73529412])\n",
      "        >>> dirichlet.var(alpha) # get variance\n",
      "        array([0.00089829, 0.00864603, 0.00909517])\n",
      "        >>> dirichlet.entropy(alpha)  # calculate the differential entropy\n",
      "        -4.3280162474082715\n",
      "        \n",
      "        We can also return random samples from the distribution\n",
      "        \n",
      "        >>> dirichlet.rvs(alpha, size=1, random_state=1)\n",
      "        array([[0.00766178, 0.24670518, 0.74563305]])\n",
      "        >>> dirichlet.rvs(alpha, size=2, random_state=2)\n",
      "        array([[0.01639427, 0.1292273 , 0.85437844],\n",
      "               [0.00156917, 0.19033695, 0.80809388]])\n",
      "    \n",
      "    dlaplace = <scipy.stats._discrete_distns.dlaplace_gen object>\n",
      "        A  Laplacian discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `dlaplace` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, a, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, a, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, a, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, a, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, a, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, a, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(a, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `dlaplace` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) = \\tanh(a/2) \\exp(-a |k|)\n",
      "        \n",
      "        for integers :math:`k` and :math:`a > 0`.\n",
      "        \n",
      "        `dlaplace` takes :math:`a` as shape parameter.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``dlaplace.pmf(k, a, loc)`` is identically\n",
      "        equivalent to ``dlaplace.pmf(k - loc, a)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import dlaplace\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 0.8\n",
      "        >>> mean, var, skew, kurt = dlaplace.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(dlaplace.ppf(0.01, a),\n",
      "        ...               dlaplace.ppf(0.99, a))\n",
      "        >>> ax.plot(x, dlaplace.pmf(x, a), 'bo', ms=8, label='dlaplace pmf')\n",
      "        >>> ax.vlines(x, 0, dlaplace.pmf(x, a), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = dlaplace(a)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = dlaplace.cdf(x, a)\n",
      "        >>> np.allclose(x, dlaplace.ppf(prob, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = dlaplace.rvs(a, size=1000)\n",
      "    \n",
      "    dweibull = <scipy.stats._continuous_distns.dweibull_gen object>\n",
      "        A double Weibull continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `dweibull` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `dweibull` is given by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c / 2 |x|^{c-1} \\exp(-|x|^c)\n",
      "        \n",
      "        for a real number :math:`x` and :math:`c > 0`.\n",
      "        \n",
      "        `dweibull` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``dweibull.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``dweibull.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import dweibull\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 2.07\n",
      "        >>> mean, var, skew, kurt = dweibull.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(dweibull.ppf(0.01, c),\n",
      "        ...                 dweibull.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, dweibull.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='dweibull pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = dweibull(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = dweibull.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], dweibull.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = dweibull.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    erlang = <scipy.stats._continuous_distns.erlang_gen object>\n",
      "        An Erlang continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `erlang` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gamma\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Erlang distribution is a special case of the Gamma distribution, with\n",
      "        the shape parameter `a` an integer.  Note that this restriction is not\n",
      "        enforced by `erlang`. It will, however, generate a warning the first time\n",
      "        a non-integer value is used for the shape parameter.\n",
      "        \n",
      "        Refer to `gamma` for examples.\n",
      "    \n",
      "    expon = <scipy.stats._continuous_distns.expon_gen object>\n",
      "        An exponential continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `expon` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `expon` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\exp(-x)\n",
      "        \n",
      "        for :math:`x \\ge 0`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``expon.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``expon.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        A common parameterization for `expon` is in terms of the rate parameter\n",
      "        ``lambda``, such that ``pdf = lambda * exp(-lambda * x)``. This\n",
      "        parameterization corresponds to using ``scale = 1 / lambda``.\n",
      "        \n",
      "        The exponential distribution is a special case of the gamma\n",
      "        distributions, with gamma shape parameter ``a = 1``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import expon\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = expon.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(expon.ppf(0.01),\n",
      "        ...                 expon.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, expon.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='expon pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = expon()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = expon.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], expon.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = expon.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    exponnorm = <scipy.stats._continuous_distns.exponnorm_gen object>\n",
      "        An exponentially modified Normal continuous random variable.\n",
      "        \n",
      "        Also known as the exponentially modified Gaussian distribution [1]_.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `exponnorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(K, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, K, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, K, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, K, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, K, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, K, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, K, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, K, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, K, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, K, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(K, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(K, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(K,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(K, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(K, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(K, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(K, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, K, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `exponnorm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, K) = \\frac{1}{2K} \\exp\\left(\\frac{1}{2 K^2} - x / K \\right)\n",
      "                      \\text{erfc}\\left(-\\frac{x - 1/K}{\\sqrt{2}}\\right)\n",
      "        \n",
      "        where :math:`x` is a real number and :math:`K > 0`.\n",
      "        \n",
      "        It can be thought of as the sum of a standard normal random variable\n",
      "        and an independent exponentially distributed random variable with rate\n",
      "        ``1/K``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``exponnorm.pdf(x, K, loc, scale)`` is identically\n",
      "        equivalent to ``exponnorm.pdf(y, K) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        An alternative parameterization of this distribution (for example, in\n",
      "        the Wikpedia article [1]_) involves three parameters, :math:`\\mu`,\n",
      "        :math:`\\lambda` and :math:`\\sigma`.\n",
      "        \n",
      "        In the present parameterization this corresponds to having ``loc`` and\n",
      "        ``scale`` equal to :math:`\\mu` and :math:`\\sigma`, respectively, and\n",
      "        shape parameter :math:`K = 1/(\\sigma\\lambda)`.\n",
      "        \n",
      "        .. versionadded:: 0.16.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Exponentially modified Gaussian distribution, Wikipedia,\n",
      "               https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import exponnorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> K = 1.5\n",
      "        >>> mean, var, skew, kurt = exponnorm.stats(K, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(exponnorm.ppf(0.01, K),\n",
      "        ...                 exponnorm.ppf(0.99, K), 100)\n",
      "        >>> ax.plot(x, exponnorm.pdf(x, K),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='exponnorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = exponnorm(K)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = exponnorm.ppf([0.001, 0.5, 0.999], K)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], exponnorm.cdf(vals, K))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = exponnorm.rvs(K, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    exponpow = <scipy.stats._continuous_distns.exponpow_gen object>\n",
      "        An exponential power continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `exponpow` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `exponpow` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, b) = b x^{b-1} \\exp(1 + x^b - \\exp(x^b))\n",
      "        \n",
      "        for :math:`x \\ge 0`, :math:`b > 0`.  Note that this is a different\n",
      "        distribution from the exponential power distribution that is also known\n",
      "        under the names \"generalized normal\" or \"generalized Gaussian\".\n",
      "        \n",
      "        `exponpow` takes ``b`` as a shape parameter for :math:`b`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``exponpow.pdf(x, b, loc, scale)`` is identically\n",
      "        equivalent to ``exponpow.pdf(y, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Exponentialpower.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import exponpow\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> b = 2.7\n",
      "        >>> mean, var, skew, kurt = exponpow.stats(b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(exponpow.ppf(0.01, b),\n",
      "        ...                 exponpow.ppf(0.99, b), 100)\n",
      "        >>> ax.plot(x, exponpow.pdf(x, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='exponpow pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = exponpow(b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = exponpow.ppf([0.001, 0.5, 0.999], b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], exponpow.cdf(vals, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = exponpow.rvs(b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    exponweib = <scipy.stats._continuous_distns.exponweib_gen object>\n",
      "        An exponentiated Weibull continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `exponweib` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        weibull_min, numpy.random.Generator.weibull\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `exponweib` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, c) = a c [1-\\exp(-x^c)]^{a-1} \\exp(-x^c) x^{c-1}\n",
      "        \n",
      "        and its cumulative distribution function is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            F(x, a, c) = [1-\\exp(-x^c)]^a\n",
      "        \n",
      "        for :math:`x > 0`, :math:`a > 0`, :math:`c > 0`.\n",
      "        \n",
      "        `exponweib` takes :math:`a` and :math:`c` as shape parameters:\n",
      "        \n",
      "        * :math:`a` is the exponentiation parameter,\n",
      "          with the special case :math:`a=1` corresponding to the\n",
      "          (non-exponentiated) Weibull distribution `weibull_min`.\n",
      "        * :math:`c` is the shape parameter of the non-exponentiated Weibull law.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``exponweib.pdf(x, a, c, loc, scale)`` is identically\n",
      "        equivalent to ``exponweib.pdf(y, a, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        https://en.wikipedia.org/wiki/Exponentiated_Weibull_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import exponweib\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, c = 2.89, 1.95\n",
      "        >>> mean, var, skew, kurt = exponweib.stats(a, c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(exponweib.ppf(0.01, a, c),\n",
      "        ...                 exponweib.ppf(0.99, a, c), 100)\n",
      "        >>> ax.plot(x, exponweib.pdf(x, a, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='exponweib pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = exponweib(a, c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = exponweib.ppf([0.001, 0.5, 0.999], a, c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], exponweib.cdf(vals, a, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = exponweib.rvs(a, c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    f = <scipy.stats._continuous_distns.f_gen object>\n",
      "        An F continuous random variable.\n",
      "        \n",
      "        For the noncentral F distribution, see `ncf`.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `f` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(dfn, dfd, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, dfn, dfd, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, dfn, dfd, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, dfn, dfd, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, dfn, dfd, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, dfn, dfd, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, dfn, dfd, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, dfn, dfd, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, dfn, dfd, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, dfn, dfd, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(dfn, dfd, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(dfn, dfd, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(dfn, dfd), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(dfn, dfd, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(dfn, dfd, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(dfn, dfd, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(dfn, dfd, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, dfn, dfd, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ncf\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `f` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, df_1, df_2) = \\frac{df_2^{df_2/2} df_1^{df_1/2} x^{df_1 / 2-1}}\n",
      "                                    {(df_2+df_1 x)^{(df_1+df_2)/2}\n",
      "                                     B(df_1/2, df_2/2)}\n",
      "        \n",
      "        for :math:`x > 0`.\n",
      "        \n",
      "        `f` takes ``dfn`` and ``dfd`` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``f.pdf(x, dfn, dfd, loc, scale)`` is identically\n",
      "        equivalent to ``f.pdf(y, dfn, dfd) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import f\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> dfn, dfd = 29, 18\n",
      "        >>> mean, var, skew, kurt = f.stats(dfn, dfd, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(f.ppf(0.01, dfn, dfd),\n",
      "        ...                 f.ppf(0.99, dfn, dfd), 100)\n",
      "        >>> ax.plot(x, f.pdf(x, dfn, dfd),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='f pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = f(dfn, dfd)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = f.ppf([0.001, 0.5, 0.999], dfn, dfd)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], f.cdf(vals, dfn, dfd))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = f.rvs(dfn, dfd, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    fatiguelife = <scipy.stats._continuous_distns.fatiguelife_gen object>\n",
      "        A fatigue-life (Birnbaum-Saunders) continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `fatiguelife` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `fatiguelife` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{x+1}{2c\\sqrt{2\\pi x^3}} \\exp(-\\frac{(x-1)^2}{2x c^2})\n",
      "        \n",
      "        for :math:`x >= 0` and :math:`c > 0`.\n",
      "        \n",
      "        `fatiguelife` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``fatiguelife.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``fatiguelife.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Birnbaum-Saunders distribution\",\n",
      "               https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import fatiguelife\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 29\n",
      "        >>> mean, var, skew, kurt = fatiguelife.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(fatiguelife.ppf(0.01, c),\n",
      "        ...                 fatiguelife.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, fatiguelife.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='fatiguelife pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = fatiguelife(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = fatiguelife.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], fatiguelife.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = fatiguelife.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    fisk = <scipy.stats._continuous_distns.fisk_gen object>\n",
      "        A Fisk continuous random variable.\n",
      "        \n",
      "        The Fisk distribution is also known as the log-logistic distribution.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `fisk` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        burr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `fisk` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c x^{-c-1} (1 + x^{-c})^{-2}\n",
      "        \n",
      "        for :math:`x >= 0` and :math:`c > 0`.\n",
      "        \n",
      "        `fisk` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        `fisk` is a special case of `burr` or `burr12` with ``d=1``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``fisk.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``fisk.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import fisk\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 3.09\n",
      "        >>> mean, var, skew, kurt = fisk.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(fisk.ppf(0.01, c),\n",
      "        ...                 fisk.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, fisk.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='fisk pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = fisk(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = fisk.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], fisk.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = fisk.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    foldcauchy = <scipy.stats._continuous_distns.foldcauchy_gen object>\n",
      "        A folded Cauchy continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `foldcauchy` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `foldcauchy` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{1}{\\pi (1+(x-c)^2)} + \\frac{1}{\\pi (1+(x+c)^2)}\n",
      "        \n",
      "        for :math:`x \\ge 0`.\n",
      "        \n",
      "        `foldcauchy` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import foldcauchy\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 4.72\n",
      "        >>> mean, var, skew, kurt = foldcauchy.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(foldcauchy.ppf(0.01, c),\n",
      "        ...                 foldcauchy.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, foldcauchy.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='foldcauchy pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = foldcauchy(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = foldcauchy.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], foldcauchy.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = foldcauchy.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    foldnorm = <scipy.stats._continuous_distns.foldnorm_gen object>\n",
      "        A folded normal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `foldnorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `foldnorm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\sqrt{2/\\pi} cosh(c x) \\exp(-\\frac{x^2+c^2}{2})\n",
      "        \n",
      "        for :math:`c \\ge 0`.\n",
      "        \n",
      "        `foldnorm` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``foldnorm.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``foldnorm.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import foldnorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 1.95\n",
      "        >>> mean, var, skew, kurt = foldnorm.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(foldnorm.ppf(0.01, c),\n",
      "        ...                 foldnorm.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, foldnorm.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='foldnorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = foldnorm(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = foldnorm.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], foldnorm.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = foldnorm.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gamma = <scipy.stats._continuous_distns.gamma_gen object>\n",
      "        A gamma continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gamma` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        erlang, expon\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gamma` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a) = \\frac{x^{a-1} e^{-x}}{\\Gamma(a)}\n",
      "        \n",
      "        for :math:`x \\ge 0`, :math:`a > 0`. Here :math:`\\Gamma(a)` refers to the\n",
      "        gamma function.\n",
      "        \n",
      "        `gamma` takes ``a`` as a shape parameter for :math:`a`.\n",
      "        \n",
      "        When :math:`a` is an integer, `gamma` reduces to the Erlang\n",
      "        distribution, and when :math:`a=1` to the exponential distribution.\n",
      "        \n",
      "        Gamma distributions are sometimes parameterized with two variables,\n",
      "        with a probability density function of:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\alpha, \\beta) = \\frac{\\beta^\\alpha x^{\\alpha - 1} e^{-\\beta x }}{\\Gamma(\\alpha)}\n",
      "        \n",
      "        Note that this parameterization is equivalent to the above, with\n",
      "        ``scale = 1 / beta``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``gamma.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``gamma.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gamma\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 1.99\n",
      "        >>> mean, var, skew, kurt = gamma.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gamma.ppf(0.01, a),\n",
      "        ...                 gamma.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, gamma.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gamma pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gamma(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gamma.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gamma.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gamma.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gausshyper = <scipy.stats._continuous_distns.gausshyper_gen object>\n",
      "        A Gauss hypergeometric continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gausshyper` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, c, z, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, c, z, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, c, z, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, c, z, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, c, z, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, c, z, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, c, z, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, c, z, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, c, z, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, c, z, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, c, z, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, c, z, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b, c, z), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, c, z, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, c, z, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, c, z, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, c, z, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, c, z, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gausshyper` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b, c, z) = C x^{a-1} (1-x)^{b-1} (1+zx)^{-c}\n",
      "        \n",
      "        for :math:`0 \\le x \\le 1`, :math:`a > 0`, :math:`b > 0`, :math:`z > -1`,\n",
      "        and :math:`C = \\frac{1}{B(a, b) F[2, 1](c, a; a+b; -z)}`.\n",
      "        :math:`F[2, 1]` is the Gauss hypergeometric function\n",
      "        `scipy.special.hyp2f1`.\n",
      "        \n",
      "        `gausshyper` takes :math:`a`, :math:`b`, :math:`c` and :math:`z` as shape\n",
      "        parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``gausshyper.pdf(x, a, b, c, z, loc, scale)`` is identically\n",
      "        equivalent to ``gausshyper.pdf(y, a, b, c, z) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Armero, C., and M. J. Bayarri. \"Prior Assessments for Prediction in\n",
      "               Queues.\" *Journal of the Royal Statistical Society*. Series D (The\n",
      "               Statistician) 43, no. 1 (1994): 139-53. doi:10.2307/2348939\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gausshyper\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b, c, z = 13.8, 3.12, 2.51, 5.18\n",
      "        >>> mean, var, skew, kurt = gausshyper.stats(a, b, c, z, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gausshyper.ppf(0.01, a, b, c, z),\n",
      "        ...                 gausshyper.ppf(0.99, a, b, c, z), 100)\n",
      "        >>> ax.plot(x, gausshyper.pdf(x, a, b, c, z),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gausshyper pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gausshyper(a, b, c, z)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gausshyper.ppf([0.001, 0.5, 0.999], a, b, c, z)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gausshyper.cdf(vals, a, b, c, z))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gausshyper.rvs(a, b, c, z, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    genexpon = <scipy.stats._continuous_distns.genexpon_gen object>\n",
      "        A generalized exponential continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `genexpon` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `genexpon` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b, c) = (a + b (1 - \\exp(-c x)))\n",
      "                            \\exp(-a x - b x + \\frac{b}{c}  (1-\\exp(-c x)))\n",
      "        \n",
      "        for :math:`x \\ge 0`, :math:`a, b, c > 0`.\n",
      "        \n",
      "        `genexpon` takes :math:`a`, :math:`b` and :math:`c` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``genexpon.pdf(x, a, b, c, loc, scale)`` is identically\n",
      "        equivalent to ``genexpon.pdf(y, a, b, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        H.K. Ryu, \"An Extension of Marshall and Olkin's Bivariate Exponential\n",
      "        Distribution\", Journal of the American Statistical Association, 1993.\n",
      "        \n",
      "        N. Balakrishnan, \"The Exponential Distribution: Theory, Methods and\n",
      "        Applications\", Asit P. Basu.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import genexpon\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b, c = 9.13, 16.2, 3.28\n",
      "        >>> mean, var, skew, kurt = genexpon.stats(a, b, c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(genexpon.ppf(0.01, a, b, c),\n",
      "        ...                 genexpon.ppf(0.99, a, b, c), 100)\n",
      "        >>> ax.plot(x, genexpon.pdf(x, a, b, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='genexpon pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = genexpon(a, b, c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = genexpon.ppf([0.001, 0.5, 0.999], a, b, c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], genexpon.cdf(vals, a, b, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = genexpon.rvs(a, b, c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    genextreme = <scipy.stats._continuous_distns.genextreme_gen object>\n",
      "        A generalized extreme value continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `genextreme` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gumbel_r\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For :math:`c=0`, `genextreme` is equal to `gumbel_r`.\n",
      "        The probability density function for `genextreme` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\begin{cases}\n",
      "                        \\exp(-\\exp(-x)) \\exp(-x)              &\\text{for } c = 0\\\\\n",
      "                        \\exp(-(1-c x)^{1/c}) (1-c x)^{1/c-1}  &\\text{for }\n",
      "                                                                x \\le 1/c, c > 0\n",
      "                      \\end{cases}\n",
      "        \n",
      "        \n",
      "        Note that several sources and software packages use the opposite\n",
      "        convention for the sign of the shape parameter :math:`c`.\n",
      "        \n",
      "        `genextreme` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``genextreme.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``genextreme.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import genextreme\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = -0.1\n",
      "        >>> mean, var, skew, kurt = genextreme.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(genextreme.ppf(0.01, c),\n",
      "        ...                 genextreme.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, genextreme.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='genextreme pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = genextreme(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = genextreme.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], genextreme.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = genextreme.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gengamma = <scipy.stats._continuous_distns.gengamma_gen object>\n",
      "        A generalized gamma continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gengamma` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, c), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gamma, invgamma, weibull_min\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gengamma` is ([1]_):\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, c) = \\frac{|c| x^{c a-1} \\exp(-x^c)}{\\Gamma(a)}\n",
      "        \n",
      "        for :math:`x \\ge 0`, :math:`a > 0`, and :math:`c \\ne 0`.\n",
      "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
      "        \n",
      "        `gengamma` takes :math:`a` and :math:`c` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``gengamma.pdf(x, a, c, loc, scale)`` is identically\n",
      "        equivalent to ``gengamma.pdf(y, a, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] E.W. Stacy, \"A Generalization of the Gamma Distribution\",\n",
      "           Annals of Mathematical Statistics, Vol 33(3), pp. 1187--1192.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gengamma\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, c = 4.42, -3.12\n",
      "        >>> mean, var, skew, kurt = gengamma.stats(a, c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gengamma.ppf(0.01, a, c),\n",
      "        ...                 gengamma.ppf(0.99, a, c), 100)\n",
      "        >>> ax.plot(x, gengamma.pdf(x, a, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gengamma pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gengamma(a, c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gengamma.ppf([0.001, 0.5, 0.999], a, c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gengamma.cdf(vals, a, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gengamma.rvs(a, c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    genhalflogistic = <scipy.stats._continuous_distns.genhalflogistic_gen ...\n",
      "        A generalized half-logistic continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `genhalflogistic` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `genhalflogistic` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{2 (1 - c x)^{1/(c-1)}}{[1 + (1 - c x)^{1/c}]^2}\n",
      "        \n",
      "        for :math:`0 \\le x \\le 1/c`, and :math:`c > 0`.\n",
      "        \n",
      "        `genhalflogistic` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``genhalflogistic.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``genhalflogistic.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import genhalflogistic\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.773\n",
      "        >>> mean, var, skew, kurt = genhalflogistic.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(genhalflogistic.ppf(0.01, c),\n",
      "        ...                 genhalflogistic.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, genhalflogistic.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='genhalflogistic pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = genhalflogistic(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = genhalflogistic.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], genhalflogistic.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = genhalflogistic.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    genhyperbolic = <scipy.stats._continuous_distns.genhyperbolic_gen obje...\n",
      "        A generalized hyperbolic continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `genhyperbolic` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(p, a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, p, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, p, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, p, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, p, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, p, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, p, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, p, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, p, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, p, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(p, a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(p, a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(p, a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(p, a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(p, a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(p, a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(p, a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, p, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        t, norminvgauss, geninvgauss, laplace, cauchy\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `genhyperbolic` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, p, a, b) =\n",
      "                \\frac{(a^2 - b^2)^{p/2}}\n",
      "                {\\sqrt{2\\pi}a^{p-0.5}\n",
      "                K_p\\Big(\\sqrt{a^2 - b^2}\\Big)}\n",
      "                e^{bx} \\times \\frac{K_{p - 1/2}\n",
      "                (a \\sqrt{1 + x^2})}\n",
      "                {(\\sqrt{1 + x^2})^{1/2 - p}}\n",
      "        \n",
      "        for :math:`x, p \\in ( - \\infty; \\infty)`,\n",
      "        :math:`|b| < a` if :math:`p \\ge 0`,\n",
      "        :math:`|b| \\le a` if :math:`p < 0`.\n",
      "        :math:`K_{p}(.)` denotes the modified Bessel function of the second\n",
      "        kind and order :math:`p` (`scipy.special.kn`)\n",
      "        \n",
      "        `genhyperbolic` takes ``p`` as a tail parameter,\n",
      "        ``a`` as a shape parameter,\n",
      "        ``b`` as a skewness parameter.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``genhyperbolic.pdf(x, p, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``genhyperbolic.pdf(y, p, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        The original parameterization of the Generalized Hyperbolic Distribution\n",
      "        is found in [1]_ as follows\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\lambda, \\alpha, \\beta, \\delta, \\mu) =\n",
      "               \\frac{(\\gamma/\\delta)^\\lambda}{\\sqrt{2\\pi}K_\\lambda(\\delta \\gamma)}\n",
      "               e^{\\beta (x - \\mu)} \\times \\frac{K_{\\lambda - 1/2}\n",
      "               (\\alpha \\sqrt{\\delta^2 + (x - \\mu)^2})}\n",
      "               {(\\sqrt{\\delta^2 + (x - \\mu)^2} / \\alpha)^{1/2 - \\lambda}}\n",
      "        \n",
      "        for :math:`x \\in ( - \\infty; \\infty)`,\n",
      "        :math:`\\gamma := \\sqrt{\\alpha^2 - \\beta^2}`,\n",
      "        :math:`\\lambda, \\mu \\in ( - \\infty; \\infty)`,\n",
      "        :math:`\\delta \\ge 0, |\\beta| < \\alpha` if :math:`\\lambda \\ge 0`,\n",
      "        :math:`\\delta > 0, |\\beta| \\le \\alpha` if :math:`\\lambda < 0`.\n",
      "        \n",
      "        The location-scale-based parameterization implemented in\n",
      "        SciPy is based on [2]_, where :math:`a = \\alpha\\delta`,\n",
      "        :math:`b = \\beta\\delta`, :math:`p = \\lambda`,\n",
      "        :math:`scale=\\delta` and :math:`loc=\\mu`\n",
      "        \n",
      "        Moments are implemented based on [3]_ and [4]_.\n",
      "        \n",
      "        For the distributions that are a special case such as Student's t,\n",
      "        it is not recommended to rely on the implementation of genhyperbolic.\n",
      "        To avoid potential numerical problems and for performance reasons,\n",
      "        the methods of the specific distributions should be used.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] O. Barndorff-Nielsen, \"Hyperbolic Distributions and Distributions\n",
      "           on Hyperbolae\", Scandinavian Journal of Statistics, Vol. 5(3),\n",
      "           pp. 151-157, 1978. https://www.jstor.org/stable/4615705\n",
      "        \n",
      "        .. [2] Eberlein E., Prause K. (2002) The Generalized Hyperbolic Model:\n",
      "            Financial Derivatives and Risk Measures. In: Geman H., Madan D.,\n",
      "            Pliska S.R., Vorst T. (eds) Mathematical Finance - Bachelier\n",
      "            Congress 2000. Springer Finance. Springer, Berlin, Heidelberg.\n",
      "            :doi:`10.1007/978-3-662-12429-1_12`\n",
      "        \n",
      "        .. [3] Scott, David J, Würtz, Diethelm, Dong, Christine and Tran,\n",
      "           Thanh Tam, (2009), Moments of the generalized hyperbolic\n",
      "           distribution, MPRA Paper, University Library of Munich, Germany,\n",
      "           https://EconPapers.repec.org/RePEc:pra:mprapa:19081.\n",
      "        \n",
      "        .. [4] E. Eberlein and E. A. von Hammerstein. Generalized hyperbolic\n",
      "           and inverse Gaussian distributions: Limiting cases and approximation\n",
      "           of processes. FDM Preprint 80, April 2003. University of Freiburg.\n",
      "           https://freidok.uni-freiburg.de/fedora/objects/freidok:7974/datastreams/FILE1/content\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import genhyperbolic\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> p, a, b = 0.5, 1.5, -0.5\n",
      "        >>> mean, var, skew, kurt = genhyperbolic.stats(p, a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(genhyperbolic.ppf(0.01, p, a, b),\n",
      "        ...                 genhyperbolic.ppf(0.99, p, a, b), 100)\n",
      "        >>> ax.plot(x, genhyperbolic.pdf(x, p, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='genhyperbolic pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = genhyperbolic(p, a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = genhyperbolic.ppf([0.001, 0.5, 0.999], p, a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], genhyperbolic.cdf(vals, p, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = genhyperbolic.rvs(p, a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    geninvgauss = <scipy.stats._continuous_distns.geninvgauss_gen object>\n",
      "        A Generalized Inverse Gaussian continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `geninvgauss` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(p, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, p, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, p, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, p, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, p, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, p, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, p, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, p, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, p, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, p, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(p, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(p, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(p, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(p, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(p, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(p, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(p, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, p, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `geninvgauss` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, p, b) = x^{p-1} \\exp(-b (x + 1/x) / 2) / (2 K_p(b))\n",
      "        \n",
      "        where `x > 0`, and the parameters `p, b` satisfy `b > 0` ([1]_).\n",
      "        :math:`K_p` is the modified Bessel function of second kind of order `p`\n",
      "        (`scipy.special.kv`).\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``geninvgauss.pdf(x, p, b, loc, scale)`` is identically\n",
      "        equivalent to ``geninvgauss.pdf(y, p, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        The inverse Gaussian distribution `stats.invgauss(mu)` is a special case of\n",
      "        `geninvgauss` with `p = -1/2`, `b = 1 / mu` and `scale = mu`.\n",
      "        \n",
      "        Generating random variates is challenging for this distribution. The\n",
      "        implementation is based on [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] O. Barndorff-Nielsen, P. Blaesild, C. Halgreen, \"First hitting time\n",
      "           models for the generalized inverse gaussian distribution\",\n",
      "           Stochastic Processes and their Applications 7, pp. 49--54, 1978.\n",
      "        \n",
      "        .. [2] W. Hoermann and J. Leydold, \"Generating generalized inverse Gaussian\n",
      "           random variates\", Statistics and Computing, 24(4), p. 547--557, 2014.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import geninvgauss\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> p, b = 2.3, 1.5\n",
      "        >>> mean, var, skew, kurt = geninvgauss.stats(p, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(geninvgauss.ppf(0.01, p, b),\n",
      "        ...                 geninvgauss.ppf(0.99, p, b), 100)\n",
      "        >>> ax.plot(x, geninvgauss.pdf(x, p, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='geninvgauss pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = geninvgauss(p, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = geninvgauss.ppf([0.001, 0.5, 0.999], p, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], geninvgauss.cdf(vals, p, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = geninvgauss.rvs(p, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    genlogistic = <scipy.stats._continuous_distns.genlogistic_gen object>\n",
      "        A generalized logistic continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `genlogistic` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `genlogistic` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c \\frac{\\exp(-x)}\n",
      "                             {(1 + \\exp(-x))^{c+1}}\n",
      "        \n",
      "        for :math:`x >= 0`, :math:`c > 0`.\n",
      "        \n",
      "        `genlogistic` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``genlogistic.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``genlogistic.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import genlogistic\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.412\n",
      "        >>> mean, var, skew, kurt = genlogistic.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(genlogistic.ppf(0.01, c),\n",
      "        ...                 genlogistic.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, genlogistic.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='genlogistic pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = genlogistic(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = genlogistic.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], genlogistic.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = genlogistic.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gennorm = <scipy.stats._continuous_distns.gennorm_gen object>\n",
      "        A generalized normal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gennorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(beta, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, beta, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, beta, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, beta, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, beta, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, beta, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, beta, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, beta, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, beta, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, beta, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(beta, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(beta, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(beta, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(beta, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(beta, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(beta, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, beta, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        laplace : Laplace distribution\n",
      "        norm : normal distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gennorm` is [1]_:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\beta) = \\frac{\\beta}{2 \\Gamma(1/\\beta)} \\exp(-|x|^\\beta)\n",
      "        \n",
      "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
      "        \n",
      "        `gennorm` takes ``beta`` as a shape parameter for :math:`\\beta`.\n",
      "        For :math:`\\beta = 1`, it is identical to a Laplace distribution.\n",
      "        For :math:`\\beta = 2`, it is identical to a normal distribution\n",
      "        (with ``scale=1/sqrt(2)``).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] \"Generalized normal distribution, Version 1\",\n",
      "               https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gennorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> beta = 1.3\n",
      "        >>> mean, var, skew, kurt = gennorm.stats(beta, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gennorm.ppf(0.01, beta),\n",
      "        ...                 gennorm.ppf(0.99, beta), 100)\n",
      "        >>> ax.plot(x, gennorm.pdf(x, beta),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gennorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gennorm(beta)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gennorm.ppf([0.001, 0.5, 0.999], beta)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gennorm.cdf(vals, beta))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gennorm.rvs(beta, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    genpareto = <scipy.stats._continuous_distns.genpareto_gen object>\n",
      "        A generalized Pareto continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `genpareto` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `genpareto` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = (1 + c x)^{-1 - 1/c}\n",
      "        \n",
      "        defined for :math:`x \\ge 0` if :math:`c \\ge 0`, and for\n",
      "        :math:`0 \\le x \\le -1/c` if :math:`c < 0`.\n",
      "        \n",
      "        `genpareto` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        For :math:`c=0`, `genpareto` reduces to the exponential\n",
      "        distribution, `expon`:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, 0) = \\exp(-x)\n",
      "        \n",
      "        For :math:`c=-1`, `genpareto` is uniform on ``[0, 1]``:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, -1) = 1\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``genpareto.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``genpareto.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import genpareto\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.1\n",
      "        >>> mean, var, skew, kurt = genpareto.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(genpareto.ppf(0.01, c),\n",
      "        ...                 genpareto.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, genpareto.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='genpareto pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = genpareto(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = genpareto.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], genpareto.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = genpareto.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    geom = <scipy.stats._discrete_distns.geom_gen object>\n",
      "        A geometric discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `geom` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(p, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, p, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, p, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, p, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, p, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, p, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, p, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, p, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, p, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(p, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(p, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(p, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(p, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(p, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(p, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, p, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `geom` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) = (1-p)^{k-1} p\n",
      "        \n",
      "        for :math:`k \\ge 1`, :math:`0 < p \\leq 1`\n",
      "        \n",
      "        `geom` takes :math:`p` as shape parameter,\n",
      "        where :math:`p` is the probability of a single success\n",
      "        and :math:`1-p` is the probability of a single failure.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``geom.pmf(k, p, loc)`` is identically\n",
      "        equivalent to ``geom.pmf(k - loc, p)``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        planck\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import geom\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> p = 0.5\n",
      "        >>> mean, var, skew, kurt = geom.stats(p, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(geom.ppf(0.01, p),\n",
      "        ...               geom.ppf(0.99, p))\n",
      "        >>> ax.plot(x, geom.pmf(x, p), 'bo', ms=8, label='geom pmf')\n",
      "        >>> ax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = geom(p)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = geom.cdf(x, p)\n",
      "        >>> np.allclose(x, geom.ppf(prob, p))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = geom.rvs(p, size=1000)\n",
      "    \n",
      "    gilbrat = <scipy.stats._continuous_distns.gilbrat_gen object>\n",
      "        A Gilbrat continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gilbrat` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gilbrat` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{x \\sqrt{2\\pi}} \\exp(-\\frac{1}{2} (\\log(x))^2)\n",
      "        \n",
      "        `gilbrat` is a special case of `lognorm` with ``s=1``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``gilbrat.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``gilbrat.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gilbrat\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = gilbrat.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gilbrat.ppf(0.01),\n",
      "        ...                 gilbrat.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, gilbrat.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gilbrat pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gilbrat()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gilbrat.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gilbrat.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gilbrat.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gompertz = <scipy.stats._continuous_distns.gompertz_gen object>\n",
      "        A Gompertz (or truncated Gumbel) continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gompertz` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gompertz` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c \\exp(x) \\exp(-c (e^x-1))\n",
      "        \n",
      "        for :math:`x \\ge 0`, :math:`c > 0`.\n",
      "        \n",
      "        `gompertz` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``gompertz.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``gompertz.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gompertz\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.947\n",
      "        >>> mean, var, skew, kurt = gompertz.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gompertz.ppf(0.01, c),\n",
      "        ...                 gompertz.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, gompertz.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gompertz pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gompertz(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gompertz.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gompertz.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gompertz.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gumbel_l = <scipy.stats._continuous_distns.gumbel_l_gen object>\n",
      "        A left-skewed Gumbel continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gumbel_l` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gumbel_r, gompertz, genextreme\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gumbel_l` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\exp(x - e^x)\n",
      "        \n",
      "        The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett\n",
      "        distribution.  It is also related to the extreme value distribution,\n",
      "        log-Weibull and Gompertz distributions.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``gumbel_l.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``gumbel_l.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gumbel_l\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = gumbel_l.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gumbel_l.ppf(0.01),\n",
      "        ...                 gumbel_l.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, gumbel_l.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gumbel_l pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gumbel_l()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gumbel_l.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gumbel_l.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gumbel_l.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gumbel_r = <scipy.stats._continuous_distns.gumbel_r_gen object>\n",
      "        A right-skewed Gumbel continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `gumbel_r` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gumbel_l, gompertz, genextreme\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `gumbel_r` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\exp(-(x + e^{-x}))\n",
      "        \n",
      "        The Gumbel distribution is sometimes referred to as a type I Fisher-Tippett\n",
      "        distribution.  It is also related to the extreme value distribution,\n",
      "        log-Weibull and Gompertz distributions.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``gumbel_r.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``gumbel_r.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import gumbel_r\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = gumbel_r.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(gumbel_r.ppf(0.01),\n",
      "        ...                 gumbel_r.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, gumbel_r.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='gumbel_r pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = gumbel_r()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = gumbel_r.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], gumbel_r.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = gumbel_r.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    halfcauchy = <scipy.stats._continuous_distns.halfcauchy_gen object>\n",
      "        A Half-Cauchy continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `halfcauchy` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `halfcauchy` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{2}{\\pi (1 + x^2)}\n",
      "        \n",
      "        for :math:`x \\ge 0`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``halfcauchy.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``halfcauchy.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import halfcauchy\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = halfcauchy.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(halfcauchy.ppf(0.01),\n",
      "        ...                 halfcauchy.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, halfcauchy.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='halfcauchy pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = halfcauchy()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = halfcauchy.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], halfcauchy.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = halfcauchy.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    halfgennorm = <scipy.stats._continuous_distns.halfgennorm_gen object>\n",
      "        The upper half of a generalized normal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `halfgennorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(beta, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, beta, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, beta, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, beta, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, beta, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, beta, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, beta, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, beta, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, beta, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, beta, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(beta, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(beta, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(beta,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(beta, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(beta, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(beta, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(beta, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, beta, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gennorm : generalized normal distribution\n",
      "        expon : exponential distribution\n",
      "        halfnorm : half normal distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `halfgennorm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\beta) = \\frac{\\beta}{\\Gamma(1/\\beta)} \\exp(-|x|^\\beta)\n",
      "        \n",
      "        for :math:`x > 0`. :math:`\\Gamma` is the gamma function\n",
      "        (`scipy.special.gamma`).\n",
      "        \n",
      "        `gennorm` takes ``beta`` as a shape parameter for :math:`\\beta`.\n",
      "        For :math:`\\beta = 1`, it is identical to an exponential distribution.\n",
      "        For :math:`\\beta = 2`, it is identical to a half normal distribution\n",
      "        (with ``scale=1/sqrt(2)``).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] \"Generalized normal distribution, Version 1\",\n",
      "               https://en.wikipedia.org/wiki/Generalized_normal_distribution#Version_1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import halfgennorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> beta = 0.675\n",
      "        >>> mean, var, skew, kurt = halfgennorm.stats(beta, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(halfgennorm.ppf(0.01, beta),\n",
      "        ...                 halfgennorm.ppf(0.99, beta), 100)\n",
      "        >>> ax.plot(x, halfgennorm.pdf(x, beta),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='halfgennorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = halfgennorm(beta)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = halfgennorm.ppf([0.001, 0.5, 0.999], beta)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], halfgennorm.cdf(vals, beta))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = halfgennorm.rvs(beta, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    halflogistic = <scipy.stats._continuous_distns.halflogistic_gen object...\n",
      "        A half-logistic continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `halflogistic` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `halflogistic` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{ 2 e^{-x} }{ (1+e^{-x})^2 }\n",
      "                 = \\frac{1}{2} \\text{sech}(x/2)^2\n",
      "        \n",
      "        for :math:`x \\ge 0`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``halflogistic.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``halflogistic.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import halflogistic\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = halflogistic.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(halflogistic.ppf(0.01),\n",
      "        ...                 halflogistic.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, halflogistic.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='halflogistic pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = halflogistic()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = halflogistic.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], halflogistic.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = halflogistic.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    halfnorm = <scipy.stats._continuous_distns.halfnorm_gen object>\n",
      "        A half-normal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `halfnorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `halfnorm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\sqrt{2/\\pi} \\exp(-x^2 / 2)\n",
      "        \n",
      "        for :math:`x >= 0`.\n",
      "        \n",
      "        `halfnorm` is a special case of `chi` with ``df=1``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``halfnorm.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``halfnorm.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import halfnorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = halfnorm.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(halfnorm.ppf(0.01),\n",
      "        ...                 halfnorm.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, halfnorm.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='halfnorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = halfnorm()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = halfnorm.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], halfnorm.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = halfnorm.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    hypergeom = <scipy.stats._discrete_distns.hypergeom_gen object>\n",
      "        A hypergeometric discrete random variable.\n",
      "        \n",
      "        The hypergeometric distribution models drawing objects from a bin.\n",
      "        `M` is the total number of objects, `n` is total number of Type I objects.\n",
      "        The random variate represents the number of Type I objects in `N` drawn\n",
      "        without replacement from the total population.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `hypergeom` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(M, n, N, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, M, n, N, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, M, n, N, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, M, n, N, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, M, n, N, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, M, n, N, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, M, n, N, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, M, n, N, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, M, n, N, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(M, n, N, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(M, n, N, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(M, n, N), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(M, n, N, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(M, n, N, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(M, n, N, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(M, n, N, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, M, n, N, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The symbols used to denote the shape parameters (`M`, `n`, and `N`) are not\n",
      "        universally accepted.  See the Examples for a clarification of the\n",
      "        definitions used here.\n",
      "        \n",
      "        The probability mass function is defined as,\n",
      "        \n",
      "        .. math:: p(k, M, n, N) = \\frac{\\binom{n}{k} \\binom{M - n}{N - k}}\n",
      "                                       {\\binom{M}{N}}\n",
      "        \n",
      "        for :math:`k \\in [\\max(0, N - M + n), \\min(n, N)]`, where the binomial\n",
      "        coefficients are defined as,\n",
      "        \n",
      "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``hypergeom.pmf(k, M, n, N, loc)`` is identically\n",
      "        equivalent to ``hypergeom.pmf(k - loc, M, n, N)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import hypergeom\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Suppose we have a collection of 20 animals, of which 7 are dogs.  Then if\n",
      "        we want to know the probability of finding a given number of dogs if we\n",
      "        choose at random 12 of the 20 animals, we can initialize a frozen\n",
      "        distribution and plot the probability mass function:\n",
      "        \n",
      "        >>> [M, n, N] = [20, 7, 12]\n",
      "        >>> rv = hypergeom(M, n, N)\n",
      "        >>> x = np.arange(0, n+1)\n",
      "        >>> pmf_dogs = rv.pmf(x)\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, pmf_dogs, 'bo')\n",
      "        >>> ax.vlines(x, 0, pmf_dogs, lw=2)\n",
      "        >>> ax.set_xlabel('# of dogs in our group of chosen animals')\n",
      "        >>> ax.set_ylabel('hypergeom PMF')\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Instead of using a frozen distribution we can also use `hypergeom`\n",
      "        methods directly.  To for example obtain the cumulative distribution\n",
      "        function, use:\n",
      "        \n",
      "        >>> prb = hypergeom.cdf(x, M, n, N)\n",
      "        \n",
      "        And to generate random numbers:\n",
      "        \n",
      "        >>> R = hypergeom.rvs(M, n, N, size=10)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nhypergeom, binom, nbinom\n",
      "    \n",
      "    hypsecant = <scipy.stats._continuous_distns.hypsecant_gen object>\n",
      "        A hyperbolic secant continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `hypsecant` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `hypsecant` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\pi} \\text{sech}(x)\n",
      "        \n",
      "        for a real number :math:`x`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``hypsecant.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``hypsecant.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import hypsecant\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = hypsecant.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(hypsecant.ppf(0.01),\n",
      "        ...                 hypsecant.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, hypsecant.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='hypsecant pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = hypsecant()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = hypsecant.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], hypsecant.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = hypsecant.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    invgamma = <scipy.stats._continuous_distns.invgamma_gen object>\n",
      "        An inverted gamma continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `invgamma` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `invgamma` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a) = \\frac{x^{-a-1}}{\\Gamma(a)} \\exp(-\\frac{1}{x})\n",
      "        \n",
      "        for :math:`x >= 0`, :math:`a > 0`. :math:`\\Gamma` is the gamma function\n",
      "        (`scipy.special.gamma`).\n",
      "        \n",
      "        `invgamma` takes ``a`` as a shape parameter for :math:`a`.\n",
      "        \n",
      "        `invgamma` is a special case of `gengamma` with ``c=-1``, and it is a\n",
      "        different parameterization of the scaled inverse chi-squared distribution.\n",
      "        Specifically, if the scaled inverse chi-squared distribution is\n",
      "        parameterized with degrees of freedom :math:`\\nu` and scaling parameter\n",
      "        :math:`\\tau^2`, then it can be modeled using `invgamma` with\n",
      "        ``a=`` :math:`\\nu/2` and ``scale=`` :math:`\\nu \\tau^2/2`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``invgamma.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``invgamma.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import invgamma\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 4.07\n",
      "        >>> mean, var, skew, kurt = invgamma.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(invgamma.ppf(0.01, a),\n",
      "        ...                 invgamma.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, invgamma.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='invgamma pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = invgamma(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = invgamma.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], invgamma.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = invgamma.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    invgauss = <scipy.stats._continuous_distns.invgauss_gen object>\n",
      "        An inverse Gaussian continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `invgauss` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(mu, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, mu, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, mu, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, mu, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, mu, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, mu, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, mu, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, mu, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, mu, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, mu, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(mu, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(mu, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(mu, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(mu, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(mu, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(mu, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, mu, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `invgauss` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\mu) = \\frac{1}{\\sqrt{2 \\pi x^3}}\n",
      "                        \\exp(-\\frac{(x-\\mu)^2}{2 x \\mu^2})\n",
      "        \n",
      "        for :math:`x >= 0` and :math:`\\mu > 0`.\n",
      "        \n",
      "        `invgauss` takes ``mu`` as a shape parameter for :math:`\\mu`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``invgauss.pdf(x, mu, loc, scale)`` is identically\n",
      "        equivalent to ``invgauss.pdf(y, mu) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import invgauss\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> mu = 0.145\n",
      "        >>> mean, var, skew, kurt = invgauss.stats(mu, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(invgauss.ppf(0.01, mu),\n",
      "        ...                 invgauss.ppf(0.99, mu), 100)\n",
      "        >>> ax.plot(x, invgauss.pdf(x, mu),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='invgauss pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = invgauss(mu)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = invgauss.ppf([0.001, 0.5, 0.999], mu)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], invgauss.cdf(vals, mu))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = invgauss.rvs(mu, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    invweibull = <scipy.stats._continuous_distns.invweibull_gen object>\n",
      "        An inverted Weibull continuous random variable.\n",
      "        \n",
      "        This distribution is also known as the Fréchet distribution or the\n",
      "        type II extreme value distribution.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `invweibull` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `invweibull` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c x^{-c-1} \\exp(-x^{-c})\n",
      "        \n",
      "        for :math:`x > 0`, :math:`c > 0`.\n",
      "        \n",
      "        `invweibull` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``invweibull.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``invweibull.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        F.R.S. de Gusmao, E.M.M Ortega and G.M. Cordeiro, \"The generalized inverse\n",
      "        Weibull distribution\", Stat. Papers, vol. 52, pp. 591-619, 2011.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import invweibull\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 10.6\n",
      "        >>> mean, var, skew, kurt = invweibull.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(invweibull.ppf(0.01, c),\n",
      "        ...                 invweibull.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, invweibull.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='invweibull pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = invweibull(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = invweibull.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], invweibull.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = invweibull.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    invwishart = <scipy.stats._multivariate.invwishart_gen object>\n",
      "        An inverse Wishart random variable.\n",
      "        \n",
      "        The `df` keyword specifies the degrees of freedom. The `scale` keyword\n",
      "        specifies the scale matrix, which must be symmetric and positive definite.\n",
      "        In this context, the scale matrix is often interpreted in terms of a\n",
      "        multivariate normal covariance matrix.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pdf(x, df, scale)``\n",
      "            Probability density function.\n",
      "        ``logpdf(x, df, scale)``\n",
      "            Log of the probability density function.\n",
      "        ``rvs(df, scale, size=1, random_state=None)``\n",
      "            Draw random samples from an inverse Wishart distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Quantiles, with the last axis of `x` denoting the components.\n",
      "        df : int\n",
      "            Degrees of freedom, must be greater than or equal to dimension of the\n",
      "            scale matrix\n",
      "        scale : array_like\n",
      "            Symmetric positive definite scale matrix of the distribution\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Alternatively, the object may be called (as a function) to fix the degrees\n",
      "        of freedom and scale parameters, returning a \"frozen\" inverse Wishart\n",
      "        random variable:\n",
      "        \n",
      "        rv = invwishart(df=1, scale=1)\n",
      "            - Frozen object with the same methods but holding the given\n",
      "              degrees of freedom and scale fixed.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        wishart\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        The scale matrix `scale` must be a symmetric positive definite\n",
      "        matrix. Singular matrices, including the symmetric positive semi-definite\n",
      "        case, are not supported.\n",
      "        \n",
      "        The inverse Wishart distribution is often denoted\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            W_p^{-1}(\\nu, \\Psi)\n",
      "        \n",
      "        where :math:`\\nu` is the degrees of freedom and :math:`\\Psi` is the\n",
      "        :math:`p \\times p` scale matrix.\n",
      "        \n",
      "        The probability density function for `invwishart` has support over positive\n",
      "        definite matrices :math:`S`; if :math:`S \\sim W^{-1}_p(\\nu, \\Sigma)`,\n",
      "        then its PDF is given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(S) = \\frac{|\\Sigma|^\\frac{\\nu}{2}}{2^{ \\frac{\\nu p}{2} }\n",
      "                   |S|^{\\frac{\\nu + p + 1}{2}} \\Gamma_p \\left(\\frac{\\nu}{2} \\right)}\n",
      "                   \\exp\\left( -tr(\\Sigma S^{-1}) / 2 \\right)\n",
      "        \n",
      "        If :math:`S \\sim W_p^{-1}(\\nu, \\Psi)` (inverse Wishart) then\n",
      "        :math:`S^{-1} \\sim W_p(\\nu, \\Psi^{-1})` (Wishart).\n",
      "        \n",
      "        If the scale matrix is 1-dimensional and equal to one, then the inverse\n",
      "        Wishart distribution :math:`W_1(\\nu, 1)` collapses to the\n",
      "        inverse Gamma distribution with parameters shape = :math:`\\frac{\\nu}{2}`\n",
      "        and scale = :math:`\\frac{1}{2}`.\n",
      "        \n",
      "        .. versionadded:: 0.16.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] M.L. Eaton, \"Multivariate Statistics: A Vector Space Approach\",\n",
      "               Wiley, 1983.\n",
      "        .. [2] M.C. Jones, \"Generating Inverse Wishart Matrices\", Communications\n",
      "               in Statistics - Simulation and Computation, vol. 14.2, pp.511-514,\n",
      "               1985.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.stats import invwishart, invgamma\n",
      "        >>> x = np.linspace(0.01, 1, 100)\n",
      "        >>> iw = invwishart.pdf(x, df=6, scale=1)\n",
      "        >>> iw[:3]\n",
      "        array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])\n",
      "        >>> ig = invgamma.pdf(x, 6/2., scale=1./2)\n",
      "        >>> ig[:3]\n",
      "        array([  1.20546865e-15,   5.42497807e-06,   4.45813929e-03])\n",
      "        >>> plt.plot(x, iw)\n",
      "        \n",
      "        The input quantiles can be any shape of array, as long as the last\n",
      "        axis labels the components.\n",
      "    \n",
      "    johnsonsb = <scipy.stats._continuous_distns.johnsonsb_gen object>\n",
      "        A Johnson SB continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `johnsonsb` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        johnsonsu\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `johnsonsb` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b) = \\frac{b}{x(1-x)}  \\phi(a + b \\log \\frac{x}{1-x} )\n",
      "        \n",
      "        where :math:`x`, :math:`a`, and :math:`b` are real scalars; :math:`b > 0`\n",
      "        and :math:`x \\in [0,1]`.  :math:`\\phi` is the pdf of the normal\n",
      "        distribution.\n",
      "        \n",
      "        `johnsonsb` takes :math:`a` and :math:`b` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``johnsonsb.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``johnsonsb.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import johnsonsb\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 4.32, 3.18\n",
      "        >>> mean, var, skew, kurt = johnsonsb.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(johnsonsb.ppf(0.01, a, b),\n",
      "        ...                 johnsonsb.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, johnsonsb.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='johnsonsb pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = johnsonsb(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = johnsonsb.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], johnsonsb.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = johnsonsb.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    johnsonsu = <scipy.stats._continuous_distns.johnsonsu_gen object>\n",
      "        A Johnson SU continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `johnsonsu` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        johnsonsb\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `johnsonsu` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b) = \\frac{b}{\\sqrt{x^2 + 1}}\n",
      "                         \\phi(a + b \\log(x + \\sqrt{x^2 + 1}))\n",
      "        \n",
      "        where :math:`x`, :math:`a`, and :math:`b` are real scalars; :math:`b > 0`.\n",
      "        :math:`\\phi` is the pdf of the normal distribution.\n",
      "        \n",
      "        `johnsonsu` takes :math:`a` and :math:`b` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``johnsonsu.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``johnsonsu.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import johnsonsu\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 2.55, 2.25\n",
      "        >>> mean, var, skew, kurt = johnsonsu.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(johnsonsu.ppf(0.01, a, b),\n",
      "        ...                 johnsonsu.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, johnsonsu.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='johnsonsu pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = johnsonsu(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = johnsonsu.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], johnsonsu.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = johnsonsu.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    kappa3 = <scipy.stats._continuous_distns.kappa3_gen object>\n",
      "        Kappa 3 parameter distribution.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `kappa3` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `kappa3` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a) = a (a + x^a)^{-(a + 1)/a}\n",
      "        \n",
      "        for :math:`x > 0` and :math:`a > 0`.\n",
      "        \n",
      "        `kappa3` takes ``a`` as a shape parameter for :math:`a`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        P.W. Mielke and E.S. Johnson, \"Three-Parameter Kappa Distribution Maximum\n",
      "        Likelihood and Likelihood Ratio Tests\", Methods in Weather Research,\n",
      "        701-707, (September, 1973),\n",
      "        :doi:`10.1175/1520-0493(1973)101<0701:TKDMLE>2.3.CO;2`\n",
      "        \n",
      "        B. Kumphon, \"Maximum Entropy and Maximum Likelihood Estimation for the\n",
      "        Three-Parameter Kappa Distribution\", Open Journal of Statistics, vol 2,\n",
      "        415-419 (2012), :doi:`10.4236/ojs.2012.24050`\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``kappa3.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``kappa3.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import kappa3\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 1\n",
      "        >>> mean, var, skew, kurt = kappa3.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(kappa3.ppf(0.01, a),\n",
      "        ...                 kappa3.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, kappa3.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='kappa3 pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = kappa3(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = kappa3.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], kappa3.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = kappa3.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    kappa4 = <scipy.stats._continuous_distns.kappa4_gen object>\n",
      "        Kappa 4 parameter distribution.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `kappa4` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(h, k, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, h, k, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, h, k, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, h, k, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, h, k, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, h, k, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, h, k, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, h, k, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, h, k, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, h, k, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(h, k, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(h, k, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(h, k), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(h, k, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(h, k, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(h, k, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(h, k, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, h, k, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for kappa4 is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, h, k) = (1 - k x)^{1/k - 1} (1 - h (1 - k x)^{1/k})^{1/h-1}\n",
      "        \n",
      "        if :math:`h` and :math:`k` are not equal to 0.\n",
      "        \n",
      "        If :math:`h` or :math:`k` are zero then the pdf can be simplified:\n",
      "        \n",
      "        h = 0 and k != 0::\n",
      "        \n",
      "            kappa4.pdf(x, h, k) = (1.0 - k*x)**(1.0/k - 1.0)*\n",
      "                                  exp(-(1.0 - k*x)**(1.0/k))\n",
      "        \n",
      "        h != 0 and k = 0::\n",
      "        \n",
      "            kappa4.pdf(x, h, k) = exp(-x)*(1.0 - h*exp(-x))**(1.0/h - 1.0)\n",
      "        \n",
      "        h = 0 and k = 0::\n",
      "        \n",
      "            kappa4.pdf(x, h, k) = exp(-x)*exp(-exp(-x))\n",
      "        \n",
      "        kappa4 takes :math:`h` and :math:`k` as shape parameters.\n",
      "        \n",
      "        The kappa4 distribution returns other distributions when certain\n",
      "        :math:`h` and :math:`k` values are used.\n",
      "        \n",
      "        +------+-------------+----------------+------------------+\n",
      "        | h    | k=0.0       | k=1.0          | -inf<=k<=inf     |\n",
      "        +======+=============+================+==================+\n",
      "        | -1.0 | Logistic    |                | Generalized      |\n",
      "        |      |             |                | Logistic(1)      |\n",
      "        |      |             |                |                  |\n",
      "        |      | logistic(x) |                |                  |\n",
      "        +------+-------------+----------------+------------------+\n",
      "        |  0.0 | Gumbel      | Reverse        | Generalized      |\n",
      "        |      |             | Exponential(2) | Extreme Value    |\n",
      "        |      |             |                |                  |\n",
      "        |      | gumbel_r(x) |                | genextreme(x, k) |\n",
      "        +------+-------------+----------------+------------------+\n",
      "        |  1.0 | Exponential | Uniform        | Generalized      |\n",
      "        |      |             |                | Pareto           |\n",
      "        |      |             |                |                  |\n",
      "        |      | expon(x)    | uniform(x)     | genpareto(x, -k) |\n",
      "        +------+-------------+----------------+------------------+\n",
      "        \n",
      "        (1) There are at least five generalized logistic distributions.\n",
      "            Four are described here:\n",
      "            https://en.wikipedia.org/wiki/Generalized_logistic_distribution\n",
      "            The \"fifth\" one is the one kappa4 should match which currently\n",
      "            isn't implemented in scipy:\n",
      "            https://en.wikipedia.org/wiki/Talk:Generalized_logistic_distribution\n",
      "            https://www.mathwave.com/help/easyfit/html/analyses/distributions/gen_logistic.html\n",
      "        (2) This distribution is currently not in scipy.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        J.C. Finney, \"Optimization of a Skewed Logistic Distribution With Respect\n",
      "        to the Kolmogorov-Smirnov Test\", A Dissertation Submitted to the Graduate\n",
      "        Faculty of the Louisiana State University and Agricultural and Mechanical\n",
      "        College, (August, 2004),\n",
      "        https://digitalcommons.lsu.edu/gradschool_dissertations/3672\n",
      "        \n",
      "        J.R.M. Hosking, \"The four-parameter kappa distribution\". IBM J. Res.\n",
      "        Develop. 38 (3), 25 1-258 (1994).\n",
      "        \n",
      "        B. Kumphon, A. Kaew-Man, P. Seenoi, \"A Rainfall Distribution for the Lampao\n",
      "        Site in the Chi River Basin, Thailand\", Journal of Water Resource and\n",
      "        Protection, vol. 4, 866-869, (2012).\n",
      "        :doi:`10.4236/jwarp.2012.410101`\n",
      "        \n",
      "        C. Winchester, \"On Estimation of the Four-Parameter Kappa Distribution\", A\n",
      "        Thesis Submitted to Dalhousie University, Halifax, Nova Scotia, (March\n",
      "        2000).\n",
      "        http://www.nlc-bnc.ca/obj/s4/f2/dsk2/ftp01/MQ57336.pdf\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``kappa4.pdf(x, h, k, loc, scale)`` is identically\n",
      "        equivalent to ``kappa4.pdf(y, h, k) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import kappa4\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> h, k = 0.1, 0\n",
      "        >>> mean, var, skew, kurt = kappa4.stats(h, k, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(kappa4.ppf(0.01, h, k),\n",
      "        ...                 kappa4.ppf(0.99, h, k), 100)\n",
      "        >>> ax.plot(x, kappa4.pdf(x, h, k),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='kappa4 pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = kappa4(h, k)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = kappa4.ppf([0.001, 0.5, 0.999], h, k)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], kappa4.cdf(vals, h, k))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = kappa4.rvs(h, k, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    ksone = <scipy.stats._continuous_distns.ksone_gen object>\n",
      "        Kolmogorov-Smirnov one-sided test statistic distribution.\n",
      "        \n",
      "        This is the distribution of the one-sided Kolmogorov-Smirnov (KS)\n",
      "        statistics :math:`D_n^+` and :math:`D_n^-`\n",
      "        for a finite sample size ``n`` (the shape parameter).\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `ksone` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(n, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, n, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, n, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, n, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, n, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, n, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, n, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, n, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, n, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(n, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(n, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(n, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(n, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(n, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(n, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, n, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstwobign, kstwo, kstest\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :math:`D_n^+` and :math:`D_n^-` are given by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D_n^+ &= \\text{sup}_x (F_n(x) - F(x)),\\\\\n",
      "            D_n^- &= \\text{sup}_x (F(x) - F_n(x)),\\\\\n",
      "        \n",
      "        where :math:`F` is a continuous CDF and :math:`F_n` is an empirical CDF.\n",
      "        `ksone` describes the distribution under the null hypothesis of the KS test\n",
      "        that the empirical CDF corresponds to :math:`n` i.i.d. random variates\n",
      "        with CDF :math:`F`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``ksone.pdf(x, n, loc, scale)`` is identically\n",
      "        equivalent to ``ksone.pdf(y, n) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Birnbaum, Z. W. and Tingey, F.H. \"One-sided confidence contours\n",
      "           for probability distribution functions\", The Annals of Mathematical\n",
      "           Statistics, 22(4), pp 592-596 (1951).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import ksone\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> n = 1e+03\n",
      "        >>> mean, var, skew, kurt = ksone.stats(n, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(ksone.ppf(0.01, n),\n",
      "        ...                 ksone.ppf(0.99, n), 100)\n",
      "        >>> ax.plot(x, ksone.pdf(x, n),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='ksone pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = ksone(n)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = ksone.ppf([0.001, 0.5, 0.999], n)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], ksone.cdf(vals, n))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = ksone.rvs(n, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    kstwo = <scipy.stats._continuous_distns.kstwo_gen object>\n",
      "        Kolmogorov-Smirnov two-sided test statistic distribution.\n",
      "        \n",
      "        This is the distribution of the two-sided Kolmogorov-Smirnov (KS)\n",
      "        statistic :math:`D_n` for a finite sample size ``n``\n",
      "        (the shape parameter).\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `kstwo` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(n, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, n, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, n, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, n, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, n, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, n, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, n, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, n, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, n, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(n, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(n, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(n,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(n, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(n, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(n, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(n, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, n, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kstwobign, ksone, kstest\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :math:`D_n` is given by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D_n = \\text{sup}_x |F_n(x) - F(x)|\n",
      "        \n",
      "        where :math:`F` is a (continuous) CDF and :math:`F_n` is an empirical CDF.\n",
      "        `kstwo` describes the distribution under the null hypothesis of the KS test\n",
      "        that the empirical CDF corresponds to :math:`n` i.i.d. random variates\n",
      "        with CDF :math:`F`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``kstwo.pdf(x, n, loc, scale)`` is identically\n",
      "        equivalent to ``kstwo.pdf(y, n) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Simard, R., L'Ecuyer, P. \"Computing the Two-Sided\n",
      "           Kolmogorov-Smirnov Distribution\",  Journal of Statistical Software,\n",
      "           Vol 39, 11, 1-18 (2011).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import kstwo\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> n = 10\n",
      "        >>> mean, var, skew, kurt = kstwo.stats(n, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(kstwo.ppf(0.01, n),\n",
      "        ...                 kstwo.ppf(0.99, n), 100)\n",
      "        >>> ax.plot(x, kstwo.pdf(x, n),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='kstwo pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = kstwo(n)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = kstwo.ppf([0.001, 0.5, 0.999], n)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], kstwo.cdf(vals, n))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = kstwo.rvs(n, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    kstwobign = <scipy.stats._continuous_distns.kstwobign_gen object>\n",
      "        Limiting distribution of scaled Kolmogorov-Smirnov two-sided test statistic.\n",
      "        \n",
      "        This is the asymptotic distribution of the two-sided Kolmogorov-Smirnov\n",
      "        statistic :math:`\\sqrt{n} D_n` that measures the maximum absolute\n",
      "        distance of the theoretical (continuous) CDF from the empirical CDF.\n",
      "        (see `kstest`).\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `kstwobign` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ksone, kstwo, kstest\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :math:`\\sqrt{n} D_n` is given by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D_n = \\text{sup}_x |F_n(x) - F(x)|\n",
      "        \n",
      "        where :math:`F` is a continuous CDF and :math:`F_n` is an empirical CDF.\n",
      "        `kstwobign`  describes the asymptotic distribution (i.e. the limit of\n",
      "        :math:`\\sqrt{n} D_n`) under the null hypothesis of the KS test that the\n",
      "        empirical CDF corresponds to i.i.d. random variates with CDF :math:`F`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``kstwobign.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``kstwobign.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Feller, W. \"On the Kolmogorov-Smirnov Limit Theorems for Empirical\n",
      "           Distributions\",  Ann. Math. Statist. Vol 19, 177-189 (1948).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import kstwobign\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = kstwobign.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(kstwobign.ppf(0.01),\n",
      "        ...                 kstwobign.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, kstwobign.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='kstwobign pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = kstwobign()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = kstwobign.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], kstwobign.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = kstwobign.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    laplace = <scipy.stats._continuous_distns.laplace_gen object>\n",
      "        A Laplace continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `laplace` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `laplace` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{2} \\exp(-|x|)\n",
      "        \n",
      "        for a real number :math:`x`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``laplace.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``laplace.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import laplace\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = laplace.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(laplace.ppf(0.01),\n",
      "        ...                 laplace.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, laplace.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='laplace pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = laplace()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = laplace.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], laplace.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = laplace.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    laplace_asymmetric = <scipy.stats._continuous_distns.laplace_asymmetri...\n",
      "        An asymmetric Laplace continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `laplace_asymmetric` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, kappa, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, kappa, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, kappa, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, kappa, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, kappa, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, kappa, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, kappa, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, kappa, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, kappa, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(kappa, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(kappa, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(kappa, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(kappa, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(kappa, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(kappa, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, kappa, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        laplace : Laplace distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `laplace_asymmetric` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           f(x, \\kappa) &= \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(-x\\kappa),\\quad x\\ge0\\\\\n",
      "                        &= \\frac{1}{\\kappa+\\kappa^{-1}}\\exp(x/\\kappa),\\quad x<0\\\\\n",
      "        \n",
      "        for :math:`-\\infty < x < \\infty`, :math:`\\kappa > 0`.\n",
      "        \n",
      "        `laplace_asymmetric` takes ``kappa`` as a shape parameter for\n",
      "        :math:`\\kappa`. For :math:`\\kappa = 1`, it is identical to a\n",
      "        Laplace distribution.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``laplace_asymmetric.pdf(x, kappa, loc, scale)`` is identically\n",
      "        equivalent to ``laplace_asymmetric.pdf(y, kappa) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Asymmetric Laplace distribution\", Wikipedia\n",
      "                https://en.wikipedia.org/wiki/Asymmetric_Laplace_distribution\n",
      "        \n",
      "        .. [2] Kozubowski TJ and Podgórski K. A Multivariate and\n",
      "               Asymmetric Generalization of Laplace Distribution,\n",
      "               Computational Statistics 15, 531--540 (2000).\n",
      "               :doi:`10.1007/PL00022717`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import laplace_asymmetric\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> kappa = 2\n",
      "        >>> mean, var, skew, kurt = laplace_asymmetric.stats(kappa, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(laplace_asymmetric.ppf(0.01, kappa),\n",
      "        ...                 laplace_asymmetric.ppf(0.99, kappa), 100)\n",
      "        >>> ax.plot(x, laplace_asymmetric.pdf(x, kappa),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='laplace_asymmetric pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = laplace_asymmetric(kappa)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = laplace_asymmetric.ppf([0.001, 0.5, 0.999], kappa)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], laplace_asymmetric.cdf(vals, kappa))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = laplace_asymmetric.rvs(kappa, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    levy = <scipy.stats._continuous_distns.levy_gen object>\n",
      "        A Levy continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `levy` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        levy_stable, levy_l\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `levy` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\sqrt{2\\pi x^3}} \\exp\\left(-\\frac{1}{2x}\\right)\n",
      "        \n",
      "        for :math:`x >= 0`.\n",
      "        \n",
      "        This is the same as the Levy-stable distribution with :math:`a=1/2` and\n",
      "        :math:`b=1`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``levy.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``levy.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import levy\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = levy.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(levy.ppf(0.01),\n",
      "        ...                 levy.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, levy.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='levy pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = levy()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = levy.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], levy.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = levy.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    levy_l = <scipy.stats._continuous_distns.levy_l_gen object>\n",
      "        A left-skewed Levy continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `levy_l` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        levy, levy_stable\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `levy_l` is:\n",
      "        \n",
      "        .. math::\n",
      "            f(x) = \\frac{1}{|x| \\sqrt{2\\pi |x|}} \\exp{ \\left(-\\frac{1}{2|x|} \\right)}\n",
      "        \n",
      "        for :math:`x <= 0`.\n",
      "        \n",
      "        This is the same as the Levy-stable distribution with :math:`a=1/2` and\n",
      "        :math:`b=-1`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``levy_l.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``levy_l.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import levy_l\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = levy_l.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(levy_l.ppf(0.01),\n",
      "        ...                 levy_l.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, levy_l.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='levy_l pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = levy_l()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = levy_l.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], levy_l.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = levy_l.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    levy_stable = <scipy.stats._continuous_distns.levy_stable_gen object>\n",
      "        A Levy-stable continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `levy_stable` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(alpha, beta, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, alpha, beta, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, alpha, beta, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, alpha, beta, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, alpha, beta, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, alpha, beta, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, alpha, beta, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, alpha, beta, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, alpha, beta, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, alpha, beta, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(alpha, beta, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(alpha, beta, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(alpha, beta), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(alpha, beta, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(alpha, beta, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(alpha, beta, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(alpha, beta, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, alpha, beta, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        levy, levy_l\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The distribution for `levy_stable` has characteristic function:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\varphi(t, \\alpha, \\beta, c, \\mu) =\n",
      "            e^{it\\mu -|ct|^{\\alpha}(1-i\\beta \\operatorname{sign}(t)\\Phi(\\alpha, t))}\n",
      "        \n",
      "        where:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\Phi = \\begin{cases}\n",
      "                    \\tan \\left({\\frac {\\pi \\alpha }{2}}\\right)&\\alpha \\neq 1\\\\\n",
      "                    -{\\frac {2}{\\pi }}\\log |t|&\\alpha =1\n",
      "                    \\end{cases}\n",
      "        \n",
      "        The probability density function for `levy_stable` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^\\infty \\varphi(t)e^{-ixt}\\,dt\n",
      "        \n",
      "        where :math:`-\\infty < t < \\infty`. This integral does not have a known closed form.\n",
      "        \n",
      "        For evaluation of pdf we use either Zolotarev :math:`S_0` parameterization with integration,\n",
      "        direct integration of standard parameterization of characteristic function or FFT of\n",
      "        characteristic function. If set to other than None and if number of points is greater than\n",
      "        ``levy_stable.pdf_fft_min_points_threshold`` (defaults to None) we use FFT otherwise we use one\n",
      "        of the other methods.\n",
      "        \n",
      "        The default method is 'best' which uses Zolotarev's method if alpha = 1 and integration of\n",
      "        characteristic function otherwise. The default method can be changed by setting\n",
      "        ``levy_stable.pdf_default_method`` to either 'zolotarev', 'quadrature' or 'best'.\n",
      "        \n",
      "        To increase accuracy of FFT calculation one can specify ``levy_stable.pdf_fft_grid_spacing``\n",
      "        (defaults to 0.001) and ``pdf_fft_n_points_two_power`` (defaults to a value that covers the\n",
      "        input range * 4). Setting ``pdf_fft_n_points_two_power`` to 16 should be sufficiently accurate\n",
      "        in most cases at the expense of CPU time.\n",
      "        \n",
      "        For evaluation of cdf we use Zolatarev :math:`S_0` parameterization with integration or integral of\n",
      "        the pdf FFT interpolated spline. The settings affecting FFT calculation are the same as\n",
      "        for pdf calculation. Setting the threshold to ``None`` (default) will disable FFT. For cdf\n",
      "        calculations the Zolatarev method is superior in accuracy, so FFT is disabled by default.\n",
      "        \n",
      "        Fitting estimate uses quantile estimation method in [MC]. MLE estimation of parameters in\n",
      "        fit method uses this quantile estimate initially. Note that MLE doesn't always converge if\n",
      "        using FFT for pdf calculations; so it's best that ``pdf_fft_min_points_threshold`` is left unset.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            For pdf calculations implementation of Zolatarev is unstable for values where alpha = 1 and\n",
      "            beta != 0. In this case the quadrature method is recommended. FFT calculation is also\n",
      "            considered experimental.\n",
      "        \n",
      "            For cdf calculations FFT calculation is considered experimental. Use Zolatarev's method\n",
      "            instead (default).\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``levy_stable.pdf(x, alpha, beta, loc, scale)`` is identically\n",
      "        equivalent to ``levy_stable.pdf(y, alpha, beta) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [MC] McCulloch, J., 1986. Simple consistent estimators of stable distribution parameters.\n",
      "           Communications in Statistics - Simulation and Computation 15, 11091136.\n",
      "        .. [MS] Mittnik, S.T. Rachev, T. Doganoglu, D. Chenyao, 1999. Maximum likelihood estimation\n",
      "           of stable Paretian models, Mathematical and Computer Modelling, Volume 29, Issue 10,\n",
      "           1999, Pages 275-293.\n",
      "        .. [BS] Borak, S., Hardle, W., Rafal, W. 2005. Stable distributions, Economic Risk.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import levy_stable\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> alpha, beta = 1.8, -0.5\n",
      "        >>> mean, var, skew, kurt = levy_stable.stats(alpha, beta, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(levy_stable.ppf(0.01, alpha, beta),\n",
      "        ...                 levy_stable.ppf(0.99, alpha, beta), 100)\n",
      "        >>> ax.plot(x, levy_stable.pdf(x, alpha, beta),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='levy_stable pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = levy_stable(alpha, beta)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = levy_stable.ppf([0.001, 0.5, 0.999], alpha, beta)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], levy_stable.cdf(vals, alpha, beta))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = levy_stable.rvs(alpha, beta, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    loggamma = <scipy.stats._continuous_distns.loggamma_gen object>\n",
      "        A log gamma continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `loggamma` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `loggamma` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{\\exp(c x - \\exp(x))}\n",
      "                           {\\Gamma(c)}\n",
      "        \n",
      "        for all :math:`x, c > 0`. Here, :math:`\\Gamma` is the\n",
      "        gamma function (`scipy.special.gamma`).\n",
      "        \n",
      "        `loggamma` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``loggamma.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``loggamma.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import loggamma\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.414\n",
      "        >>> mean, var, skew, kurt = loggamma.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(loggamma.ppf(0.01, c),\n",
      "        ...                 loggamma.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, loggamma.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='loggamma pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = loggamma(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = loggamma.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], loggamma.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = loggamma.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    logistic = <scipy.stats._continuous_distns.logistic_gen object>\n",
      "        A logistic (or Sech-squared) continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `logistic` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `logistic` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{\\exp(-x)}\n",
      "                        {(1+\\exp(-x))^2}\n",
      "        \n",
      "        `logistic` is a special case of `genlogistic` with ``c=1``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``logistic.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``logistic.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import logistic\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = logistic.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(logistic.ppf(0.01),\n",
      "        ...                 logistic.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, logistic.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='logistic pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = logistic()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = logistic.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], logistic.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = logistic.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    loglaplace = <scipy.stats._continuous_distns.loglaplace_gen object>\n",
      "        A log-Laplace continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `loglaplace` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `loglaplace` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\begin{cases}\\frac{c}{2} x^{ c-1}  &\\text{for } 0 < x < 1\\\\\n",
      "                                   \\frac{c}{2} x^{-c-1}  &\\text{for } x \\ge 1\n",
      "                      \\end{cases}\n",
      "        \n",
      "        for :math:`c > 0`.\n",
      "        \n",
      "        `loglaplace` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``loglaplace.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``loglaplace.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        T.J. Kozubowski and K. Podgorski, \"A log-Laplace growth rate model\",\n",
      "        The Mathematical Scientist, vol. 28, pp. 49-60, 2003.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import loglaplace\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 3.25\n",
      "        >>> mean, var, skew, kurt = loglaplace.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(loglaplace.ppf(0.01, c),\n",
      "        ...                 loglaplace.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, loglaplace.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='loglaplace pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = loglaplace(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = loglaplace.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], loglaplace.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = loglaplace.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    lognorm = <scipy.stats._continuous_distns.lognorm_gen object>\n",
      "        A lognormal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `lognorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(s, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, s, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, s, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, s, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, s, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, s, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, s, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, s, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, s, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, s, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(s, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(s, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(s,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(s, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(s, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(s, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(s, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, s, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `lognorm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, s) = \\frac{1}{s x \\sqrt{2\\pi}}\n",
      "                      \\exp\\left(-\\frac{\\log^2(x)}{2s^2}\\right)\n",
      "        \n",
      "        for :math:`x > 0`, :math:`s > 0`.\n",
      "        \n",
      "        `lognorm` takes ``s`` as a shape parameter for :math:`s`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``lognorm.pdf(x, s, loc, scale)`` is identically\n",
      "        equivalent to ``lognorm.pdf(y, s) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        A common parametrization for a lognormal random variable ``Y`` is in\n",
      "        terms of the mean, ``mu``, and standard deviation, ``sigma``, of the\n",
      "        unique normally distributed random variable ``X`` such that exp(X) = Y.\n",
      "        This parametrization corresponds to setting ``s = sigma`` and ``scale =\n",
      "        exp(mu)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import lognorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> s = 0.954\n",
      "        >>> mean, var, skew, kurt = lognorm.stats(s, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(lognorm.ppf(0.01, s),\n",
      "        ...                 lognorm.ppf(0.99, s), 100)\n",
      "        >>> ax.plot(x, lognorm.pdf(x, s),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='lognorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = lognorm(s)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = lognorm.ppf([0.001, 0.5, 0.999], s)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], lognorm.cdf(vals, s))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = lognorm.rvs(s, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    logser = <scipy.stats._discrete_distns.logser_gen object>\n",
      "        A Logarithmic (Log-Series, Series) discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `logser` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(p, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, p, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, p, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, p, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, p, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, p, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, p, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, p, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, p, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(p, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(p, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(p,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(p, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(p, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(p, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(p, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, p, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `logser` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) = - \\frac{p^k}{k \\log(1-p)}\n",
      "        \n",
      "        for :math:`k \\ge 1`, :math:`0 < p < 1`\n",
      "        \n",
      "        `logser` takes :math:`p` as shape parameter,\n",
      "        where :math:`p` is the probability of a single success\n",
      "        and :math:`1-p` is the probability of a single failure.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``logser.pmf(k, p, loc)`` is identically\n",
      "        equivalent to ``logser.pmf(k - loc, p)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import logser\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> p = 0.6\n",
      "        >>> mean, var, skew, kurt = logser.stats(p, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(logser.ppf(0.01, p),\n",
      "        ...               logser.ppf(0.99, p))\n",
      "        >>> ax.plot(x, logser.pmf(x, p), 'bo', ms=8, label='logser pmf')\n",
      "        >>> ax.vlines(x, 0, logser.pmf(x, p), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = logser(p)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = logser.cdf(x, p)\n",
      "        >>> np.allclose(x, logser.ppf(prob, p))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = logser.rvs(p, size=1000)\n",
      "    \n",
      "    loguniform = <scipy.stats._continuous_distns.reciprocal_gen object>\n",
      "        A loguniform or reciprocal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `loguniform` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for this class is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b) = \\frac{1}{x \\log(b/a)}\n",
      "        \n",
      "        for :math:`a \\le x \\le b`, :math:`b > a > 0`. This class takes\n",
      "        :math:`a` and :math:`b` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``loguniform.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``loguniform.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import loguniform\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 0.01, 1.25\n",
      "        >>> mean, var, skew, kurt = loguniform.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(loguniform.ppf(0.01, a, b),\n",
      "        ...                 loguniform.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, loguniform.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='loguniform pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = loguniform(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = loguniform.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], loguniform.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = loguniform.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        \n",
      "        This doesn't show the equal probability of ``0.01``, ``0.1`` and\n",
      "        ``1``. This is best when the x-axis is log-scaled:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        >>> ax.hist(np.log10(r))\n",
      "        >>> ax.set_ylabel(\"Frequency\")\n",
      "        >>> ax.set_xlabel(\"Value of random variable\")\n",
      "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
      "        >>> ticks = [\"$10^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
      "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
      "        >>> plt.show()\n",
      "        \n",
      "        This random variable will be log-uniform regardless of the base chosen for\n",
      "        ``a`` and ``b``. Let's specify with base ``2`` instead:\n",
      "        \n",
      "        >>> rvs = loguniform(2**-2, 2**0).rvs(size=1000)\n",
      "        \n",
      "        Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random\n",
      "        variable.  Here's the histogram:\n",
      "        \n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        >>> ax.hist(np.log2(rvs))\n",
      "        >>> ax.set_ylabel(\"Frequency\")\n",
      "        >>> ax.set_xlabel(\"Value of random variable\")\n",
      "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
      "        >>> ticks = [\"$2^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
      "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
      "        >>> plt.show()\n",
      "    \n",
      "    lomax = <scipy.stats._continuous_distns.lomax_gen object>\n",
      "        A Lomax (Pareto of the second kind) continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `lomax` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `lomax` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{c}{(1+x)^{c+1}}\n",
      "        \n",
      "        for :math:`x \\ge 0`, :math:`c > 0`.\n",
      "        \n",
      "        `lomax` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        `lomax` is a special case of `pareto` with ``loc=-1.0``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``lomax.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``lomax.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import lomax\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 1.88\n",
      "        >>> mean, var, skew, kurt = lomax.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(lomax.ppf(0.01, c),\n",
      "        ...                 lomax.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, lomax.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='lomax pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = lomax(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = lomax.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], lomax.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = lomax.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    matrix_normal = <scipy.stats._multivariate.matrix_normal_gen object>\n",
      "        A matrix normal random variable.\n",
      "        \n",
      "        The `mean` keyword specifies the mean. The `rowcov` keyword specifies the\n",
      "        among-row covariance matrix. The 'colcov' keyword specifies the\n",
      "        among-column covariance matrix.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pdf(X, mean=None, rowcov=1, colcov=1)``\n",
      "            Probability density function.\n",
      "        ``logpdf(X, mean=None, rowcov=1, colcov=1)``\n",
      "            Log of the probability density function.\n",
      "        ``rvs(mean=None, rowcov=1, colcov=1, size=1, random_state=None)``\n",
      "            Draw random samples.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array_like\n",
      "            Quantiles, with the last two axes of `X` denoting the components.\n",
      "        mean : array_like, optional\n",
      "            Mean of the distribution (default: `None`)\n",
      "        rowcov : array_like, optional\n",
      "            Among-row covariance matrix of the distribution (default: `1`)\n",
      "        colcov : array_like, optional\n",
      "            Among-column covariance matrix of the distribution (default: `1`)\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Alternatively, the object may be called (as a function) to fix the mean\n",
      "        and covariance parameters, returning a \"frozen\" matrix normal\n",
      "        random variable:\n",
      "        \n",
      "        rv = matrix_normal(mean=None, rowcov=1, colcov=1)\n",
      "            - Frozen object with the same methods but holding the given\n",
      "              mean and covariance fixed.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If `mean` is set to `None` then a matrix of zeros is used for the mean.\n",
      "            The dimensions of this matrix are inferred from the shape of `rowcov` and\n",
      "            `colcov`, if these are provided, or set to `1` if ambiguous.\n",
      "        \n",
      "            `rowcov` and `colcov` can be two-dimensional array_likes specifying the\n",
      "            covariance matrices directly. Alternatively, a one-dimensional array will\n",
      "            be be interpreted as the entries of a diagonal matrix, and a scalar or\n",
      "            zero-dimensional array will be interpreted as this value times the\n",
      "            identity matrix.\n",
      "            \n",
      "        \n",
      "        The covariance matrices specified by `rowcov` and `colcov` must be\n",
      "        (symmetric) positive definite. If the samples in `X` are\n",
      "        :math:`m \\times n`, then `rowcov` must be :math:`m \\times m` and\n",
      "        `colcov` must be :math:`n \\times n`. `mean` must be the same shape as `X`.\n",
      "        \n",
      "        The probability density function for `matrix_normal` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(X) = (2 \\pi)^{-\\frac{mn}{2}}|U|^{-\\frac{n}{2}} |V|^{-\\frac{m}{2}}\n",
      "                   \\exp\\left( -\\frac{1}{2} \\mathrm{Tr}\\left[ U^{-1} (X-M) V^{-1}\n",
      "                   (X-M)^T \\right] \\right),\n",
      "        \n",
      "        where :math:`M` is the mean, :math:`U` the among-row covariance matrix,\n",
      "        :math:`V` the among-column covariance matrix.\n",
      "        \n",
      "        The `allow_singular` behaviour of the `multivariate_normal`\n",
      "        distribution is not currently supported. Covariance matrices must be\n",
      "        full rank.\n",
      "        \n",
      "        The `matrix_normal` distribution is closely related to the\n",
      "        `multivariate_normal` distribution. Specifically, :math:`\\mathrm{Vec}(X)`\n",
      "        (the vector formed by concatenating the columns  of :math:`X`) has a\n",
      "        multivariate normal distribution with mean :math:`\\mathrm{Vec}(M)`\n",
      "        and covariance :math:`V \\otimes U` (where :math:`\\otimes` is the Kronecker\n",
      "        product). Sampling and pdf evaluation are\n",
      "        :math:`\\mathcal{O}(m^3 + n^3 + m^2 n + m n^2)` for the matrix normal, but\n",
      "        :math:`\\mathcal{O}(m^3 n^3)` for the equivalent multivariate normal,\n",
      "        making this equivalent form algorithmically inefficient.\n",
      "        \n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from scipy.stats import matrix_normal\n",
      "        \n",
      "        >>> M = np.arange(6).reshape(3,2); M\n",
      "        array([[0, 1],\n",
      "               [2, 3],\n",
      "               [4, 5]])\n",
      "        >>> U = np.diag([1,2,3]); U\n",
      "        array([[1, 0, 0],\n",
      "               [0, 2, 0],\n",
      "               [0, 0, 3]])\n",
      "        >>> V = 0.3*np.identity(2); V\n",
      "        array([[ 0.3,  0. ],\n",
      "               [ 0. ,  0.3]])\n",
      "        >>> X = M + 0.1; X\n",
      "        array([[ 0.1,  1.1],\n",
      "               [ 2.1,  3.1],\n",
      "               [ 4.1,  5.1]])\n",
      "        >>> matrix_normal.pdf(X, mean=M, rowcov=U, colcov=V)\n",
      "        0.023410202050005054\n",
      "        \n",
      "        >>> # Equivalent multivariate normal\n",
      "        >>> from scipy.stats import multivariate_normal\n",
      "        >>> vectorised_X = X.T.flatten()\n",
      "        >>> equiv_mean = M.T.flatten()\n",
      "        >>> equiv_cov = np.kron(V,U)\n",
      "        >>> multivariate_normal.pdf(vectorised_X, mean=equiv_mean, cov=equiv_cov)\n",
      "        0.023410202050005054\n",
      "    \n",
      "    maxwell = <scipy.stats._continuous_distns.maxwell_gen object>\n",
      "        A Maxwell continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `maxwell` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A special case of a `chi` distribution,  with ``df=3``, ``loc=0.0``,\n",
      "        and given ``scale = a``, where ``a`` is the parameter used in the\n",
      "        Mathworld description [1]_.\n",
      "        \n",
      "        The probability density function for `maxwell` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\sqrt{2/\\pi}x^2 \\exp(-x^2/2)\n",
      "        \n",
      "        for :math:`x >= 0`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``maxwell.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``maxwell.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] http://mathworld.wolfram.com/MaxwellDistribution.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import maxwell\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = maxwell.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(maxwell.ppf(0.01),\n",
      "        ...                 maxwell.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, maxwell.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='maxwell pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = maxwell()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = maxwell.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], maxwell.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = maxwell.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    mielke = <scipy.stats._continuous_distns.mielke_gen object>\n",
      "        A Mielke Beta-Kappa / Dagum continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `mielke` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(k, s, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, k, s, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, k, s, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, k, s, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, k, s, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, k, s, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, k, s, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, k, s, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, k, s, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, k, s, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(k, s, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(k, s, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(k, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(k, s, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(k, s, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(k, s, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(k, s, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, k, s, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `mielke` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, k, s) = \\frac{k x^{k-1}}{(1+x^s)^{1+k/s}}\n",
      "        \n",
      "        for :math:`x > 0` and :math:`k, s > 0`. The distribution is sometimes\n",
      "        called Dagum distribution ([2]_). It was already defined in [3]_, called\n",
      "        a Burr Type III distribution (`burr` with parameters ``c=s`` and\n",
      "        ``d=k/s``).\n",
      "        \n",
      "        `mielke` takes ``k`` and ``s`` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``mielke.pdf(x, k, s, loc, scale)`` is identically\n",
      "        equivalent to ``mielke.pdf(y, k, s) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Mielke, P.W., 1973 \"Another Family of Distributions for Describing\n",
      "               and Analyzing Precipitation Data.\" J. Appl. Meteor., 12, 275-280\n",
      "        .. [2] Dagum, C., 1977 \"A new model for personal income distribution.\"\n",
      "               Economie Appliquee, 33, 327-367.\n",
      "        .. [3] Burr, I. W. \"Cumulative frequency functions\", Annals of\n",
      "               Mathematical Statistics, 13(2), pp 215-232 (1942).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import mielke\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> k, s = 10.4, 4.6\n",
      "        >>> mean, var, skew, kurt = mielke.stats(k, s, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(mielke.ppf(0.01, k, s),\n",
      "        ...                 mielke.ppf(0.99, k, s), 100)\n",
      "        >>> ax.plot(x, mielke.pdf(x, k, s),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='mielke pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = mielke(k, s)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = mielke.ppf([0.001, 0.5, 0.999], k, s)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], mielke.cdf(vals, k, s))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = mielke.rvs(k, s, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    moyal = <scipy.stats._continuous_distns.moyal_gen object>\n",
      "        A Moyal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `moyal` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `moyal` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\exp(-(x + \\exp(-x))/2) / \\sqrt{2\\pi}\n",
      "        \n",
      "        for a real number :math:`x`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``moyal.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``moyal.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        This distribution has utility in high-energy physics and radiation\n",
      "        detection. It describes the energy loss of a charged relativistic\n",
      "        particle due to ionization of the medium [1]_. It also provides an\n",
      "        approximation for the Landau distribution. For an in depth description\n",
      "        see [2]_. For additional description, see [3]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J.E. Moyal, \"XXX. Theory of ionization fluctuations\",\n",
      "               The London, Edinburgh, and Dublin Philosophical Magazine\n",
      "               and Journal of Science, vol 46, 263-280, (1955).\n",
      "               :doi:`10.1080/14786440308521076` (gated)\n",
      "        .. [2] G. Cordeiro et al., \"The beta Moyal: a useful skew distribution\",\n",
      "               International Journal of Research and Reviews in Applied Sciences,\n",
      "               vol 10, 171-192, (2012).\n",
      "               http://www.arpapress.com/Volumes/Vol10Issue2/IJRRAS_10_2_02.pdf\n",
      "        .. [3] C. Walck, \"Handbook on Statistical Distributions for\n",
      "               Experimentalists; International Report SUF-PFY/96-01\", Chapter 26,\n",
      "               University of Stockholm: Stockholm, Sweden, (2007).\n",
      "               http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf\n",
      "        \n",
      "        .. versionadded:: 1.1.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import moyal\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = moyal.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(moyal.ppf(0.01),\n",
      "        ...                 moyal.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, moyal.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='moyal pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = moyal()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = moyal.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], moyal.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = moyal.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    multinomial = <scipy.stats._multivariate.multinomial_gen object>\n",
      "        A multinomial random variable.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pmf(x, n, p)``\n",
      "            Probability mass function.\n",
      "        ``logpmf(x, n, p)``\n",
      "            Log of the probability mass function.\n",
      "        ``rvs(n, p, size=1, random_state=None)``\n",
      "            Draw random samples from a multinomial distribution.\n",
      "        ``entropy(n, p)``\n",
      "            Compute the entropy of the multinomial distribution.\n",
      "        ``cov(n, p)``\n",
      "            Compute the covariance matrix of the multinomial distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Quantiles, with the last axis of `x` denoting the components.\n",
      "        n : int\n",
      "            Number of trials\n",
      "        p : array_like\n",
      "            Probability of a trial falling into each category; should sum to 1\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `n` should be a positive integer. Each element of `p` should be in the\n",
      "        interval :math:`[0,1]` and the elements should sum to 1. If they do not sum to\n",
      "        1, the last element of the `p` array is not used and is replaced with the\n",
      "        remaining probability left over from the earlier elements.\n",
      "        \n",
      "        Alternatively, the object may be called (as a function) to fix the `n` and\n",
      "        `p` parameters, returning a \"frozen\" multinomial random variable:\n",
      "        \n",
      "        The probability mass function for `multinomial` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{n!}{x_1! \\cdots x_k!} p_1^{x_1} \\cdots p_k^{x_k},\n",
      "        \n",
      "        supported on :math:`x=(x_1, \\ldots, x_k)` where each :math:`x_i` is a\n",
      "        nonnegative integer and their sum is :math:`n`.\n",
      "        \n",
      "        .. versionadded:: 0.19.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from scipy.stats import multinomial\n",
      "        >>> rv = multinomial(8, [0.3, 0.2, 0.5])\n",
      "        >>> rv.pmf([1, 3, 4])\n",
      "        0.042000000000000072\n",
      "        \n",
      "        The multinomial distribution for :math:`k=2` is identical to the\n",
      "        corresponding binomial distribution (tiny numerical differences\n",
      "        notwithstanding):\n",
      "        \n",
      "        >>> from scipy.stats import binom\n",
      "        >>> multinomial.pmf([3, 4], n=7, p=[0.4, 0.6])\n",
      "        0.29030399999999973\n",
      "        >>> binom.pmf(3, 7, 0.4)\n",
      "        0.29030400000000012\n",
      "        \n",
      "        The functions ``pmf``, ``logpmf``, ``entropy``, and ``cov`` support\n",
      "        broadcasting, under the convention that the vector parameters (``x`` and\n",
      "        ``p``) are interpreted as if each row along the last axis is a single\n",
      "        object. For instance:\n",
      "        \n",
      "        >>> multinomial.pmf([[3, 4], [3, 5]], n=[7, 8], p=[.3, .7])\n",
      "        array([0.2268945,  0.25412184])\n",
      "        \n",
      "        Here, ``x.shape == (2, 2)``, ``n.shape == (2,)``, and ``p.shape == (2,)``,\n",
      "        but following the rules mentioned above they behave as if the rows\n",
      "        ``[3, 4]`` and ``[3, 5]`` in ``x`` and ``[.3, .7]`` in ``p`` were a single\n",
      "        object, and as if we had ``x.shape = (2,)``, ``n.shape = (2,)``, and\n",
      "        ``p.shape = ()``. To obtain the individual elements without broadcasting,\n",
      "        we would do this:\n",
      "        \n",
      "        >>> multinomial.pmf([3, 4], n=7, p=[.3, .7])\n",
      "        0.2268945\n",
      "        >>> multinomial.pmf([3, 5], 8, p=[.3, .7])\n",
      "        0.25412184\n",
      "        \n",
      "        This broadcasting also works for ``cov``, where the output objects are\n",
      "        square matrices of size ``p.shape[-1]``. For example:\n",
      "        \n",
      "        >>> multinomial.cov([4, 5], [[.3, .7], [.4, .6]])\n",
      "        array([[[ 0.84, -0.84],\n",
      "                [-0.84,  0.84]],\n",
      "               [[ 1.2 , -1.2 ],\n",
      "                [-1.2 ,  1.2 ]]])\n",
      "        \n",
      "        In this example, ``n.shape == (2,)`` and ``p.shape == (2, 2)``, and\n",
      "        following the rules above, these broadcast as if ``p.shape == (2,)``.\n",
      "        Thus the result should also be of shape ``(2,)``, but since each output is\n",
      "        a :math:`2 \\times 2` matrix, the result in fact has shape ``(2, 2, 2)``,\n",
      "        where ``result[0]`` is equal to ``multinomial.cov(n=4, p=[.3, .7])`` and\n",
      "        ``result[1]`` is equal to ``multinomial.cov(n=5, p=[.4, .6])``.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        scipy.stats.binom : The binomial distribution.\n",
      "        numpy.random.Generator.multinomial : Sampling from the multinomial distribution.\n",
      "        scipy.stats.multivariate_hypergeom :\n",
      "            The multivariate hypergeometric distribution.\n",
      "    \n",
      "    multivariate_hypergeom = <scipy.stats._multivariate.multivariate_hyper...\n",
      "        A multivariate hypergeometric random variable.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pmf(x, m, n)``\n",
      "            Probability mass function.\n",
      "        ``logpmf(x, m, n)``\n",
      "            Log of the probability mass function.\n",
      "        ``rvs(m, n, size=1, random_state=None)``\n",
      "            Draw random samples from a multivariate hypergeometric\n",
      "            distribution.\n",
      "        ``mean(m, n)``\n",
      "            Mean of the multivariate hypergeometric distribution.\n",
      "        ``var(m, n)``\n",
      "            Variance of the multivariate hypergeometric distribution.\n",
      "        ``cov(m, n)``\n",
      "            Compute the covariance matrix of the multivariate\n",
      "            hypergeometric distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : array_like\n",
      "            The number of each type of object in the population.\n",
      "            That is, :math:`m[i]` is the number of objects of\n",
      "            type :math:`i`.\n",
      "        n : array_like\n",
      "            The number of samples taken from the population.\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `m` must be an array of positive integers. If the quantile\n",
      "        :math:`i` contains values out of the range :math:`[0, m_i]`\n",
      "        where :math:`m_i` is the number of objects of type :math:`i`\n",
      "        in the population or if the parameters are inconsistent with one\n",
      "        another (e.g. ``x.sum() != n``), methods return the appropriate\n",
      "        value (e.g. ``0`` for ``pmf``). If `m` or `n` contain negative\n",
      "        values, the result will contain ``nan`` there.\n",
      "        \n",
      "        The probability mass function for `multivariate_hypergeom` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P(X_1 = x_1, X_2 = x_2, \\ldots, X_k = x_k) = \\frac{\\binom{m_1}{x_1}\n",
      "            \\binom{m_2}{x_2} \\cdots \\binom{m_k}{x_k}}{\\binom{M}{n}}, \\\\ \\quad\n",
      "            (x_1, x_2, \\ldots, x_k) \\in \\mathbb{N}^k \\text{ with }\n",
      "            \\sum_{i=1}^k x_i = n\n",
      "        \n",
      "        where :math:`m_i` are the number of objects of type :math:`i`, :math:`M`\n",
      "        is the total number of objects in the population (sum of all the\n",
      "        :math:`m_i`), and :math:`n` is the size of the sample to be taken\n",
      "        from the population.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        To evaluate the probability mass function of the multivariate\n",
      "        hypergeometric distribution, with a dichotomous population of size\n",
      "        :math:`10` and :math:`20`, at a sample of size :math:`12` with\n",
      "        :math:`8` objects of the first type and :math:`4` objects of the\n",
      "        second type, use:\n",
      "        \n",
      "        >>> from scipy.stats import multivariate_hypergeom\n",
      "        >>> multivariate_hypergeom.pmf(x=[8, 4], m=[10, 20], n=12)\n",
      "        0.0025207176631464523\n",
      "        \n",
      "        The `multivariate_hypergeom` distribution is identical to the\n",
      "        corresponding `hypergeom` distribution (tiny numerical differences\n",
      "        notwithstanding) when only two types (good and bad) of objects\n",
      "        are present in the population as in the example above. Consider\n",
      "        another example for a comparison with the hypergeometric distribution:\n",
      "        \n",
      "        >>> from scipy.stats import hypergeom\n",
      "        >>> multivariate_hypergeom.pmf(x=[3, 1], m=[10, 5], n=4)\n",
      "        0.4395604395604395\n",
      "        >>> hypergeom.pmf(k=3, M=15, n=4, N=10)\n",
      "        0.43956043956044005\n",
      "        \n",
      "        The functions ``pmf``, ``logpmf``, ``mean``, ``var``, ``cov``, and ``rvs``\n",
      "        support broadcasting, under the convention that the vector parameters\n",
      "        (``x``, ``m``, and ``n``) are interpreted as if each row along the last\n",
      "        axis is a single object. For instance, we can combine the previous two\n",
      "        calls to `multivariate_hypergeom` as\n",
      "        \n",
      "        >>> multivariate_hypergeom.pmf(x=[[8, 4], [3, 1]], m=[[10, 20], [10, 5]],\n",
      "        ...                            n=[12, 4])\n",
      "        array([0.00252072, 0.43956044])\n",
      "        \n",
      "        This broadcasting also works for ``cov``, where the output objects are\n",
      "        square matrices of size ``m.shape[-1]``. For example:\n",
      "        \n",
      "        >>> multivariate_hypergeom.cov(m=[[7, 9], [10, 15]], n=[8, 12])\n",
      "        array([[[ 1.05, -1.05],\n",
      "                [-1.05,  1.05]],\n",
      "               [[ 1.56, -1.56],\n",
      "                [-1.56,  1.56]]])\n",
      "        \n",
      "        That is, ``result[0]`` is equal to\n",
      "        ``multivariate_hypergeom.cov(m=[7, 9], n=8)`` and ``result[1]`` is equal\n",
      "        to ``multivariate_hypergeom.cov(m=[10, 15], n=12)``.\n",
      "        \n",
      "        Alternatively, the object may be called (as a function) to fix the `m`\n",
      "        and `n` parameters, returning a \"frozen\" multivariate hypergeometric\n",
      "        random variable.\n",
      "        \n",
      "        >>> rv = multivariate_hypergeom(m=[10, 20], n=12)\n",
      "        >>> rv.pmf(x=[8, 4])\n",
      "        0.0025207176631464523\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.stats.hypergeom : The hypergeometric distribution.\n",
      "        scipy.stats.multinomial : The multinomial distribution.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] The Multivariate Hypergeometric Distribution,\n",
      "               http://www.randomservices.org/random/urn/MultiHypergeometric.html\n",
      "        .. [2] Thomas J. Sargent and John Stachurski, 2020,\n",
      "               Multivariate Hypergeometric Distribution\n",
      "               https://python.quantecon.org/_downloads/pdf/multi_hyper.pdf\n",
      "    \n",
      "    multivariate_normal = <scipy.stats._multivariate.multivariate_normal_g...\n",
      "        A multivariate normal random variable.\n",
      "        \n",
      "        The `mean` keyword specifies the mean. The `cov` keyword specifies the\n",
      "        covariance matrix.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pdf(x, mean=None, cov=1, allow_singular=False)``\n",
      "            Probability density function.\n",
      "        ``logpdf(x, mean=None, cov=1, allow_singular=False)``\n",
      "            Log of the probability density function.\n",
      "        ``cdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5)``\n",
      "            Cumulative distribution function.\n",
      "        ``logcdf(x, mean=None, cov=1, allow_singular=False, maxpts=1000000*dim, abseps=1e-5, releps=1e-5)``\n",
      "            Log of the cumulative distribution function.\n",
      "        ``rvs(mean=None, cov=1, size=1, random_state=None)``\n",
      "            Draw random samples from a multivariate normal distribution.\n",
      "        ``entropy()``\n",
      "            Compute the differential entropy of the multivariate normal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Quantiles, with the last axis of `x` denoting the components.\n",
      "        mean : array_like, optional\n",
      "            Mean of the distribution (default zero)\n",
      "        cov : array_like, optional\n",
      "            Covariance matrix of the distribution (default one)\n",
      "        allow_singular : bool, optional\n",
      "            Whether to allow a singular covariance matrix.  (Default: False)\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Alternatively, the object may be called (as a function) to fix the mean\n",
      "        and covariance parameters, returning a \"frozen\" multivariate normal\n",
      "        random variable:\n",
      "        \n",
      "        rv = multivariate_normal(mean=None, cov=1, allow_singular=False)\n",
      "            - Frozen object with the same methods but holding the given\n",
      "              mean and covariance fixed.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Setting the parameter `mean` to `None` is equivalent to having `mean`\n",
      "            be the zero-vector. The parameter `cov` can be a scalar, in which case\n",
      "            the covariance matrix is the identity times that value, a vector of\n",
      "            diagonal entries for the covariance matrix, or a two-dimensional\n",
      "            array_like.\n",
      "            \n",
      "        \n",
      "        The covariance matrix `cov` must be a (symmetric) positive\n",
      "        semi-definite matrix. The determinant and inverse of `cov` are computed\n",
      "        as the pseudo-determinant and pseudo-inverse, respectively, so\n",
      "        that `cov` does not need to have full rank.\n",
      "        \n",
      "        The probability density function for `multivariate_normal` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\sqrt{(2 \\pi)^k \\det \\Sigma}}\n",
      "                   \\exp\\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right),\n",
      "        \n",
      "        where :math:`\\mu` is the mean, :math:`\\Sigma` the covariance matrix,\n",
      "        and :math:`k` is the dimension of the space where :math:`x` takes values.\n",
      "        \n",
      "        .. versionadded:: 0.14.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.stats import multivariate_normal\n",
      "        \n",
      "        >>> x = np.linspace(0, 5, 10, endpoint=False)\n",
      "        >>> y = multivariate_normal.pdf(x, mean=2.5, cov=0.5); y\n",
      "        array([ 0.00108914,  0.01033349,  0.05946514,  0.20755375,  0.43939129,\n",
      "                0.56418958,  0.43939129,  0.20755375,  0.05946514,  0.01033349])\n",
      "        >>> fig1 = plt.figure()\n",
      "        >>> ax = fig1.add_subplot(111)\n",
      "        >>> ax.plot(x, y)\n",
      "        \n",
      "        The input quantiles can be any shape of array, as long as the last\n",
      "        axis labels the components.  This allows us for instance to\n",
      "        display the frozen pdf for a non-isotropic random variable in 2D as\n",
      "        follows:\n",
      "        \n",
      "        >>> x, y = np.mgrid[-1:1:.01, -1:1:.01]\n",
      "        >>> pos = np.dstack((x, y))\n",
      "        >>> rv = multivariate_normal([0.5, -0.2], [[2.0, 0.3], [0.3, 0.5]])\n",
      "        >>> fig2 = plt.figure()\n",
      "        >>> ax2 = fig2.add_subplot(111)\n",
      "        >>> ax2.contourf(x, y, rv.pdf(pos))\n",
      "    \n",
      "    multivariate_t = <scipy.stats._multivariate.multivariate_t_gen object>\n",
      "        A multivariate t-distributed random variable.\n",
      "        \n",
      "        The `loc` parameter specifies the location. The `shape` parameter specifies\n",
      "        the positive semidefinite shape matrix. The `df` parameter specifies the\n",
      "        degrees of freedom.\n",
      "        \n",
      "        In addition to calling the methods below, the object itself may be called\n",
      "        as a function to fix the location, shape matrix, and degrees of freedom\n",
      "        parameters, returning a \"frozen\" multivariate t-distribution random.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pdf(x, loc=None, shape=1, df=1, allow_singular=False)``\n",
      "            Probability density function.\n",
      "        ``logpdf(x, loc=None, shape=1, df=1, allow_singular=False)``\n",
      "            Log of the probability density function.\n",
      "        ``rvs(loc=None, shape=1, df=1, size=1, random_state=None)``\n",
      "            Draw random samples from a multivariate t-distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Quantiles, with the last axis of `x` denoting the components.\n",
      "        \n",
      "        loc : array_like, optional\n",
      "            Location of the distribution. (default ``0``)\n",
      "        shape : array_like, optional\n",
      "            Positive semidefinite matrix of the distribution. (default ``1``)\n",
      "        df : float, optional\n",
      "            Degrees of freedom of the distribution; must be greater than zero.\n",
      "            If ``np.inf`` then results are multivariate normal. The default is ``1``.\n",
      "        allow_singular : bool, optional\n",
      "            Whether to allow a singular matrix. (default ``False``)\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Setting the parameter `loc` to ``None`` is equivalent to having `loc`\n",
      "        be the zero-vector. The parameter `shape` can be a scalar, in which case\n",
      "        the shape matrix is the identity times that value, a vector of\n",
      "        diagonal entries for the shape matrix, or a two-dimensional array_like.\n",
      "        The matrix `shape` must be a (symmetric) positive semidefinite matrix. The\n",
      "        determinant and inverse of `shape` are computed as the pseudo-determinant\n",
      "        and pseudo-inverse, respectively, so that `shape` does not need to have\n",
      "        full rank.\n",
      "        \n",
      "        The probability density function for `multivariate_t` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{\\Gamma(\\nu + p)/2}{\\Gamma(\\nu/2)\\nu^{p/2}\\pi^{p/2}|\\Sigma|^{1/2}}\n",
      "                   \\exp\\left[1 + \\frac{1}{\\nu} (\\mathbf{x} - \\boldsymbol{\\mu})^{\\top}\n",
      "                   \\boldsymbol{\\Sigma}^{-1}\n",
      "                   (\\mathbf{x} - \\boldsymbol{\\mu}) \\right]^{-(\\nu + p)/2},\n",
      "        \n",
      "        where :math:`p` is the dimension of :math:`\\mathbf{x}`,\n",
      "        :math:`\\boldsymbol{\\mu}` is the :math:`p`-dimensional location,\n",
      "        :math:`\\boldsymbol{\\Sigma}` the :math:`p \\times p`-dimensional shape\n",
      "        matrix, and :math:`\\nu` is the degrees of freedom.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.stats import multivariate_t\n",
      "        >>> x, y = np.mgrid[-1:3:.01, -2:1.5:.01]\n",
      "        >>> pos = np.dstack((x, y))\n",
      "        >>> rv = multivariate_t([1.0, -0.5], [[2.1, 0.3], [0.3, 1.5]], df=2)\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        >>> ax.set_aspect('equal')\n",
      "        >>> plt.contourf(x, y, rv.pdf(pos))\n",
      "    \n",
      "    nakagami = <scipy.stats._continuous_distns.nakagami_gen object>\n",
      "        A Nakagami continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `nakagami` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(nu, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, nu, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, nu, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, nu, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, nu, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, nu, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, nu, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, nu, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, nu, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, nu, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(nu, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(nu, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(nu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(nu, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(nu, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(nu, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(nu, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, nu, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `nakagami` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\nu) = \\frac{2 \\nu^\\nu}{\\Gamma(\\nu)} x^{2\\nu-1} \\exp(-\\nu x^2)\n",
      "        \n",
      "        for :math:`x >= 0`, :math:`\\nu > 0`.\n",
      "        \n",
      "        `nakagami` takes ``nu`` as a shape parameter for :math:`\\nu`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``nakagami.pdf(x, nu, loc, scale)`` is identically\n",
      "        equivalent to ``nakagami.pdf(y, nu) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import nakagami\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> nu = 4.97\n",
      "        >>> mean, var, skew, kurt = nakagami.stats(nu, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(nakagami.ppf(0.01, nu),\n",
      "        ...                 nakagami.ppf(0.99, nu), 100)\n",
      "        >>> ax.plot(x, nakagami.pdf(x, nu),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='nakagami pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = nakagami(nu)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = nakagami.ppf([0.001, 0.5, 0.999], nu)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], nakagami.cdf(vals, nu))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = nakagami.rvs(nu, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    nbinom = <scipy.stats._discrete_distns.nbinom_gen object>\n",
      "        A negative binomial discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `nbinom` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(n, p, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, n, p, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, n, p, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, n, p, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, n, p, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, n, p, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, n, p, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, n, p, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, n, p, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(n, p, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(n, p, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(n, p), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(n, p, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(n, p, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(n, p, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(n, p, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, n, p, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Negative binomial distribution describes a sequence of i.i.d. Bernoulli\n",
      "        trials, repeated until a predefined, non-random number of successes occurs.\n",
      "        \n",
      "        The probability mass function of the number of failures for `nbinom` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           f(k) = \\binom{k+n-1}{n-1} p^n (1-p)^k\n",
      "        \n",
      "        for :math:`k \\ge 0`, :math:`0 < p \\leq 1`\n",
      "        \n",
      "        `nbinom` takes :math:`n` and :math:`p` as shape parameters where n is the\n",
      "        number of successes, :math:`p` is the probability of a single success,\n",
      "        and :math:`1-p` is the probability of a single failure.\n",
      "        \n",
      "        Another common parameterization of the negative binomial distribution is\n",
      "        in terms of the mean number of failures :math:`\\mu` to achieve :math:`n`\n",
      "        successes. The mean :math:`\\mu` is related to the probability of success\n",
      "        as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           p = \\frac{n}{n + \\mu}\n",
      "        \n",
      "        The number of successes :math:`n` may also be specified in terms of a\n",
      "        \"dispersion\", \"heterogeneity\", or \"aggregation\" parameter :math:`\\alpha`,\n",
      "        which relates the mean :math:`\\mu` to the variance :math:`\\sigma^2`,\n",
      "        e.g. :math:`\\sigma^2 = \\mu + \\alpha \\mu^2`. Regardless of the convention\n",
      "        used for :math:`\\alpha`,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           p &= \\frac{\\mu}{\\sigma^2} \\\\\n",
      "           n &= \\frac{\\mu^2}{\\sigma^2 - \\mu}\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``nbinom.pmf(k, n, p, loc)`` is identically\n",
      "        equivalent to ``nbinom.pmf(k - loc, n, p)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import nbinom\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> n, p = 5, 0.5\n",
      "        >>> mean, var, skew, kurt = nbinom.stats(n, p, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(nbinom.ppf(0.01, n, p),\n",
      "        ...               nbinom.ppf(0.99, n, p))\n",
      "        >>> ax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=8, label='nbinom pmf')\n",
      "        >>> ax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = nbinom(n, p)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = nbinom.cdf(x, n, p)\n",
      "        >>> np.allclose(x, nbinom.ppf(prob, n, p))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = nbinom.rvs(n, p, size=1000)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        hypergeom, binom, nhypergeom\n",
      "    \n",
      "    ncf = <scipy.stats._continuous_distns.ncf_gen object>\n",
      "        A non-central F distribution continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `ncf` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(dfn, dfd, nc, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(dfn, dfd, nc, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(dfn, dfd, nc, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(dfn, dfd, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(dfn, dfd, nc, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(dfn, dfd, nc, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(dfn, dfd, nc, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(dfn, dfd, nc, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, dfn, dfd, nc, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.stats.f : Fisher distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `ncf` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, n_1, n_2, \\lambda) =\n",
      "                \\exp\\left(\\frac{\\lambda}{2} +\n",
      "                          \\lambda n_1 \\frac{x}{2(n_1 x + n_2)}\n",
      "                    \\right)\n",
      "                n_1^{n_1/2} n_2^{n_2/2} x^{n_1/2 - 1} \\\\\n",
      "                (n_2 + n_1 x)^{-(n_1 + n_2)/2}\n",
      "                \\gamma(n_1/2) \\gamma(1 + n_2/2) \\\\\n",
      "                \\frac{L^{\\frac{n_1}{2}-1}_{n_2/2}\n",
      "                    \\left(-\\lambda n_1 \\frac{x}{2(n_1 x + n_2)}\\right)}\n",
      "                {B(n_1/2, n_2/2)\n",
      "                    \\gamma\\left(\\frac{n_1 + n_2}{2}\\right)}\n",
      "        \n",
      "        for :math:`n_1, n_2 > 0`, :math:`\\lambda \\ge 0`.  Here :math:`n_1` is the\n",
      "        degrees of freedom in the numerator, :math:`n_2` the degrees of freedom in\n",
      "        the denominator, :math:`\\lambda` the non-centrality parameter,\n",
      "        :math:`\\gamma` is the logarithm of the Gamma function, :math:`L_n^k` is a\n",
      "        generalized Laguerre polynomial and :math:`B` is the beta function.\n",
      "        \n",
      "        `ncf` takes ``df1``, ``df2`` and ``nc`` as shape parameters. If ``nc=0``,\n",
      "        the distribution becomes equivalent to the Fisher distribution.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``ncf.pdf(x, dfn, dfd, nc, loc, scale)`` is identically\n",
      "        equivalent to ``ncf.pdf(y, dfn, dfd, nc) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import ncf\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> dfn, dfd, nc = 27, 27, 0.416\n",
      "        >>> mean, var, skew, kurt = ncf.stats(dfn, dfd, nc, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(ncf.ppf(0.01, dfn, dfd, nc),\n",
      "        ...                 ncf.ppf(0.99, dfn, dfd, nc), 100)\n",
      "        >>> ax.plot(x, ncf.pdf(x, dfn, dfd, nc),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='ncf pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = ncf(dfn, dfd, nc)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = ncf.ppf([0.001, 0.5, 0.999], dfn, dfd, nc)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], ncf.cdf(vals, dfn, dfd, nc))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = ncf.rvs(dfn, dfd, nc, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    nchypergeom_fisher = <scipy.stats._discrete_distns.nchypergeom_fisher_...\n",
      "        A Fisher's noncentral hypergeometric discrete random variable.\n",
      "        \n",
      "        Fisher's noncentral hypergeometric distribution models drawing objects of\n",
      "        two types from a bin. `M` is the total number of objects, `n` is the\n",
      "        number of Type I objects, and `odds` is the odds ratio: the odds of\n",
      "        selecting a Type I object rather than a Type II object when there is only\n",
      "        one object of each type.\n",
      "        The random variate represents the number of Type I objects drawn if we\n",
      "        take a handful of objects from the bin at once and find out afterwards\n",
      "        that we took `N` objects.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `nchypergeom_fisher` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(M, n, N, odds, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, M, n, N, odds, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, M, n, N, odds, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, M, n, N, odds, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, M, n, N, odds, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, M, n, N, odds, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, M, n, N, odds, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, M, n, N, odds, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, M, n, N, odds, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(M, n, N, odds, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(M, n, N, odds, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(M, n, N, odds), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(M, n, N, odds, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(M, n, N, odds, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(M, n, N, odds, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(M, n, N, odds, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, M, n, N, odds, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nchypergeom_wallenius, hypergeom, nhypergeom\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Let mathematical symbols :math:`N`, :math:`n`, and :math:`M` correspond\n",
      "        with parameters `N`, `n`, and `M` (respectively) as defined above.\n",
      "        \n",
      "        The probability mass function is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            p(x; M, n, N, \\omega) =\n",
      "            \\frac{\\binom{n}{x}\\binom{M - n}{N-x}\\omega^x}{P_0},\n",
      "        \n",
      "        for\n",
      "        :math:`x \\in [x_l, x_u]`,\n",
      "        :math:`M \\in {\\mathbb N}`,\n",
      "        :math:`n \\in [0, M]`,\n",
      "        :math:`N \\in [0, M]`,\n",
      "        :math:`\\omega > 0`,\n",
      "        where\n",
      "        :math:`x_l = \\max(0, N - (M - n))`,\n",
      "        :math:`x_u = \\min(N, n)`,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P_0 = \\sum_{y=x_l}^{x_u} \\binom{n}{y}\\binom{M - n}{N-y}\\omega^y,\n",
      "        \n",
      "        and the binomial coefficients are defined as\n",
      "        \n",
      "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
      "        \n",
      "        `nchypergeom_fisher` uses the BiasedUrn package by Agner Fog with\n",
      "        permission for it to be distributed under SciPy's license.\n",
      "        \n",
      "        The symbols used to denote the shape parameters (`N`, `n`, and `M`) are not\n",
      "        universally accepted; they are chosen for consistency with `hypergeom`.\n",
      "        \n",
      "        Note that Fisher's noncentral hypergeometric distribution is distinct\n",
      "        from Wallenius' noncentral hypergeometric distribution, which models\n",
      "        drawing a pre-determined `N` objects from a bin one by one.\n",
      "        When the odds ratio is unity, however, both distributions reduce to the\n",
      "        ordinary hypergeometric distribution.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``nchypergeom_fisher.pmf(k, M, n, N, odds, loc)`` is identically\n",
      "        equivalent to ``nchypergeom_fisher.pmf(k - loc, M, n, N, odds)``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Agner Fog, \"Biased Urn Theory\".\n",
      "               https://cran.r-project.org/web/packages/BiasedUrn/vignettes/UrnTheory.pdf\n",
      "        \n",
      "        .. [2] \"Fisher's noncentral hypergeometric distribution\", Wikipedia,\n",
      "               https://en.wikipedia.org/wiki/Fisher's_noncentral_hypergeometric_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import nchypergeom_fisher\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> M, n, N, odds = 140, 80, 60, 0.5\n",
      "        >>> mean, var, skew, kurt = nchypergeom_fisher.stats(M, n, N, odds, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(nchypergeom_fisher.ppf(0.01, M, n, N, odds),\n",
      "        ...               nchypergeom_fisher.ppf(0.99, M, n, N, odds))\n",
      "        >>> ax.plot(x, nchypergeom_fisher.pmf(x, M, n, N, odds), 'bo', ms=8, label='nchypergeom_fisher pmf')\n",
      "        >>> ax.vlines(x, 0, nchypergeom_fisher.pmf(x, M, n, N, odds), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = nchypergeom_fisher(M, n, N, odds)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = nchypergeom_fisher.cdf(x, M, n, N, odds)\n",
      "        >>> np.allclose(x, nchypergeom_fisher.ppf(prob, M, n, N, odds))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = nchypergeom_fisher.rvs(M, n, N, odds, size=1000)\n",
      "    \n",
      "    nchypergeom_wallenius = <scipy.stats._discrete_distns.nchypergeom_wall...\n",
      "        A Wallenius' noncentral hypergeometric discrete random variable.\n",
      "        \n",
      "        Wallenius' noncentral hypergeometric distribution models drawing objects of\n",
      "        two types from a bin. `M` is the total number of objects, `n` is the\n",
      "        number of Type I objects, and `odds` is the odds ratio: the odds of\n",
      "        selecting a Type I object rather than a Type II object when there is only\n",
      "        one object of each type.\n",
      "        The random variate represents the number of Type I objects drawn if we\n",
      "        draw a pre-determined `N` objects from a bin one by one.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `nchypergeom_wallenius` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(M, n, N, odds, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, M, n, N, odds, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, M, n, N, odds, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, M, n, N, odds, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, M, n, N, odds, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, M, n, N, odds, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, M, n, N, odds, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, M, n, N, odds, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, M, n, N, odds, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(M, n, N, odds, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(M, n, N, odds, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(M, n, N, odds), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(M, n, N, odds, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(M, n, N, odds, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(M, n, N, odds, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(M, n, N, odds, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, M, n, N, odds, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nchypergeom_fisher, hypergeom, nhypergeom\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Let mathematical symbols :math:`N`, :math:`n`, and :math:`M` correspond\n",
      "        with parameters `N`, `n`, and `M` (respectively) as defined above.\n",
      "        \n",
      "        The probability mass function is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            p(x; N, n, M) = \\binom{n}{x} \\binom{M - n}{N-x}\n",
      "            \\int_0^1 \\left(1-t^{\\omega/D}\\right)^x\\left(1-t^{1/D}\\right)^{N-x} dt\n",
      "        \n",
      "        for\n",
      "        :math:`x \\in [x_l, x_u]`,\n",
      "        :math:`M \\in {\\mathbb N}`,\n",
      "        :math:`n \\in [0, M]`,\n",
      "        :math:`N \\in [0, M]`,\n",
      "        :math:`\\omega > 0`,\n",
      "        where\n",
      "        :math:`x_l = \\max(0, N - (M - n))`,\n",
      "        :math:`x_u = \\min(N, n)`,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            D = \\omega(n - x) + ((M - n)-(N-x)),\n",
      "        \n",
      "        and the binomial coefficients are defined as\n",
      "        \n",
      "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
      "        \n",
      "        `nchypergeom_wallenius` uses the BiasedUrn package by Agner Fog with\n",
      "        permission for it to be distributed under SciPy's license.\n",
      "        \n",
      "        The symbols used to denote the shape parameters (`N`, `n`, and `M`) are not\n",
      "        universally accepted; they are chosen for consistency with `hypergeom`.\n",
      "        \n",
      "        Note that Wallenius' noncentral hypergeometric distribution is distinct\n",
      "        from Fisher's noncentral hypergeometric distribution, which models\n",
      "        take a handful of objects from the bin at once, finding out afterwards\n",
      "        that `N` objects were taken.\n",
      "        When the odds ratio is unity, however, both distributions reduce to the\n",
      "        ordinary hypergeometric distribution.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``nchypergeom_wallenius.pmf(k, M, n, N, odds, loc)`` is identically\n",
      "        equivalent to ``nchypergeom_wallenius.pmf(k - loc, M, n, N, odds)``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Agner Fog, \"Biased Urn Theory\".\n",
      "               https://cran.r-project.org/web/packages/BiasedUrn/vignettes/UrnTheory.pdf\n",
      "        \n",
      "        .. [2] \"Wallenius' noncentral hypergeometric distribution\", Wikipedia,\n",
      "               https://en.wikipedia.org/wiki/Wallenius'_noncentral_hypergeometric_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import nchypergeom_wallenius\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> M, n, N, odds = 140, 80, 60, 0.5\n",
      "        >>> mean, var, skew, kurt = nchypergeom_wallenius.stats(M, n, N, odds, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(nchypergeom_wallenius.ppf(0.01, M, n, N, odds),\n",
      "        ...               nchypergeom_wallenius.ppf(0.99, M, n, N, odds))\n",
      "        >>> ax.plot(x, nchypergeom_wallenius.pmf(x, M, n, N, odds), 'bo', ms=8, label='nchypergeom_wallenius pmf')\n",
      "        >>> ax.vlines(x, 0, nchypergeom_wallenius.pmf(x, M, n, N, odds), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = nchypergeom_wallenius(M, n, N, odds)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = nchypergeom_wallenius.cdf(x, M, n, N, odds)\n",
      "        >>> np.allclose(x, nchypergeom_wallenius.ppf(prob, M, n, N, odds))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = nchypergeom_wallenius.rvs(M, n, N, odds, size=1000)\n",
      "    \n",
      "    nct = <scipy.stats._continuous_distns.nct_gen object>\n",
      "        A non-central Student's t continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `nct` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(df, nc, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, df, nc, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, df, nc, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, df, nc, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, df, nc, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, df, nc, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, df, nc, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, df, nc, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, df, nc, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, df, nc, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(df, nc, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(df, nc, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(df, nc, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(df, nc, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(df, nc, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(df, nc, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, df, nc, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If :math:`Y` is a standard normal random variable and :math:`V` is\n",
      "        an independent chi-square random variable (`chi2`) with :math:`k` degrees\n",
      "        of freedom, then\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            X = \\frac{Y + c}{\\sqrt{V/k}}\n",
      "        \n",
      "        has a non-central Student's t distribution on the real line.\n",
      "        The degrees of freedom parameter :math:`k` (denoted ``df`` in the\n",
      "        implementation) satisfies :math:`k > 0` and the noncentrality parameter\n",
      "        :math:`c` (denoted ``nc`` in the implementation) is a real number.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``nct.pdf(x, df, nc, loc, scale)`` is identically\n",
      "        equivalent to ``nct.pdf(y, df, nc) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import nct\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> df, nc = 14, 0.24\n",
      "        >>> mean, var, skew, kurt = nct.stats(df, nc, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(nct.ppf(0.01, df, nc),\n",
      "        ...                 nct.ppf(0.99, df, nc), 100)\n",
      "        >>> ax.plot(x, nct.pdf(x, df, nc),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='nct pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = nct(df, nc)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = nct.ppf([0.001, 0.5, 0.999], df, nc)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], nct.cdf(vals, df, nc))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = nct.rvs(df, nc, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    ncx2 = <scipy.stats._continuous_distns.ncx2_gen object>\n",
      "        A non-central chi-squared continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `ncx2` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(df, nc, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, df, nc, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, df, nc, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, df, nc, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, df, nc, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, df, nc, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, df, nc, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, df, nc, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, df, nc, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, df, nc, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(df, nc, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(df, nc, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(df, nc), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(df, nc, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(df, nc, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(df, nc, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(df, nc, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, df, nc, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `ncx2` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, k, \\lambda) = \\frac{1}{2} \\exp(-(\\lambda+x)/2)\n",
      "                (x/\\lambda)^{(k-2)/4}  I_{(k-2)/2}(\\sqrt{\\lambda x})\n",
      "        \n",
      "        for :math:`x >= 0` and :math:`k, \\lambda > 0`. :math:`k` specifies the\n",
      "        degrees of freedom (denoted ``df`` in the implementation) and\n",
      "        :math:`\\lambda` is the non-centrality parameter (denoted ``nc`` in the\n",
      "        implementation). :math:`I_\\nu` denotes the modified Bessel function of\n",
      "        first order of degree :math:`\\nu` (`scipy.special.iv`).\n",
      "        \n",
      "        `ncx2` takes ``df`` and ``nc`` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``ncx2.pdf(x, df, nc, loc, scale)`` is identically\n",
      "        equivalent to ``ncx2.pdf(y, df, nc) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import ncx2\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> df, nc = 21, 1.06\n",
      "        >>> mean, var, skew, kurt = ncx2.stats(df, nc, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(ncx2.ppf(0.01, df, nc),\n",
      "        ...                 ncx2.ppf(0.99, df, nc), 100)\n",
      "        >>> ax.plot(x, ncx2.pdf(x, df, nc),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='ncx2 pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = ncx2(df, nc)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = ncx2.ppf([0.001, 0.5, 0.999], df, nc)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], ncx2.cdf(vals, df, nc))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = ncx2.rvs(df, nc, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    nhypergeom = <scipy.stats._discrete_distns.nhypergeom_gen object>\n",
      "        A negative hypergeometric discrete random variable.\n",
      "        \n",
      "        Consider a box containing :math:`M` balls:, :math:`n` red and\n",
      "        :math:`M-n` blue. We randomly sample balls from the box, one\n",
      "        at a time and *without* replacement, until we have picked :math:`r`\n",
      "        blue balls. `nhypergeom` is the distribution of the number of\n",
      "        red balls :math:`k` we have picked.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `nhypergeom` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(M, n, r, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, M, n, r, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, M, n, r, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, M, n, r, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, M, n, r, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, M, n, r, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, M, n, r, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, M, n, r, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, M, n, r, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(M, n, r, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(M, n, r, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(M, n, r), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(M, n, r, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(M, n, r, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(M, n, r, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(M, n, r, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, M, n, r, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The symbols used to denote the shape parameters (`M`, `n`, and `r`) are not\n",
      "        universally accepted. See the Examples for a clarification of the\n",
      "        definitions used here.\n",
      "        \n",
      "        The probability mass function is defined as,\n",
      "        \n",
      "        .. math:: f(k; M, n, r) = \\frac{{{k+r-1}\\choose{k}}{{M-r-k}\\choose{n-k}}}\n",
      "                                       {{M \\choose n}}\n",
      "        \n",
      "        for :math:`k \\in [0, n]`, :math:`n \\in [0, M]`, :math:`r \\in [0, M-n]`,\n",
      "        and the binomial coefficient is:\n",
      "        \n",
      "        .. math:: \\binom{n}{k} \\equiv \\frac{n!}{k! (n - k)!}.\n",
      "        \n",
      "        It is equivalent to observing :math:`k` successes in :math:`k+r-1`\n",
      "        samples with :math:`k+r`'th sample being a failure. The former\n",
      "        can be modelled as a hypergeometric distribution. The probability\n",
      "        of the latter is simply the number of failures remaining\n",
      "        :math:`M-n-(r-1)` divided by the size of the remaining population\n",
      "        :math:`M-(k+r-1)`. This relationship can be shown as:\n",
      "        \n",
      "        .. math:: NHG(k;M,n,r) = HG(k;M,n,k+r-1)\\frac{(M-n-(r-1))}{(M-(k+r-1))}\n",
      "        \n",
      "        where :math:`NHG` is probability mass function (PMF) of the\n",
      "        negative hypergeometric distribution and :math:`HG` is the\n",
      "        PMF of the hypergeometric distribution.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``nhypergeom.pmf(k, M, n, r, loc)`` is identically\n",
      "        equivalent to ``nhypergeom.pmf(k - loc, M, n, r)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import nhypergeom\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Suppose we have a collection of 20 animals, of which 7 are dogs.\n",
      "        Then if we want to know the probability of finding a given number\n",
      "        of dogs (successes) in a sample with exactly 12 animals that\n",
      "        aren't dogs (failures), we can initialize a frozen distribution\n",
      "        and plot the probability mass function:\n",
      "        \n",
      "        >>> M, n, r = [20, 7, 12]\n",
      "        >>> rv = nhypergeom(M, n, r)\n",
      "        >>> x = np.arange(0, n+2)\n",
      "        >>> pmf_dogs = rv.pmf(x)\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, pmf_dogs, 'bo')\n",
      "        >>> ax.vlines(x, 0, pmf_dogs, lw=2)\n",
      "        >>> ax.set_xlabel('# of dogs in our group with given 12 failures')\n",
      "        >>> ax.set_ylabel('nhypergeom PMF')\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Instead of using a frozen distribution we can also use `nhypergeom`\n",
      "        methods directly.  To for example obtain the probability mass\n",
      "        function, use:\n",
      "        \n",
      "        >>> prb = nhypergeom.pmf(x, M, n, r)\n",
      "        \n",
      "        And to generate random numbers:\n",
      "        \n",
      "        >>> R = nhypergeom.rvs(M, n, r, size=10)\n",
      "        \n",
      "        To verify the relationship between `hypergeom` and `nhypergeom`, use:\n",
      "        \n",
      "        >>> from scipy.stats import hypergeom, nhypergeom\n",
      "        >>> M, n, r = 45, 13, 8\n",
      "        >>> k = 6\n",
      "        >>> nhypergeom.pmf(k, M, n, r)\n",
      "        0.06180776620271643\n",
      "        >>> hypergeom.pmf(k, M, n, k+r-1) * (M - n - (r-1)) / (M - (k+r-1))\n",
      "        0.06180776620271644\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        hypergeom, binom, nbinom\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Negative Hypergeometric Distribution on Wikipedia\n",
      "               https://en.wikipedia.org/wiki/Negative_hypergeometric_distribution\n",
      "        \n",
      "        .. [2] Negative Hypergeometric Distribution from\n",
      "               http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Negativehypergeometric.pdf\n",
      "    \n",
      "    norm = <scipy.stats._continuous_distns.norm_gen object>\n",
      "        A normal continuous random variable.\n",
      "        \n",
      "        The location (``loc``) keyword specifies the mean.\n",
      "        The scale (``scale``) keyword specifies the standard deviation.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `norm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `norm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\n",
      "        \n",
      "        for a real number :math:`x`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``norm.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``norm.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import norm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = norm.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(norm.ppf(0.01),\n",
      "        ...                 norm.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, norm.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='norm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = norm()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = norm.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], norm.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = norm.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    norminvgauss = <scipy.stats._continuous_distns.norminvgauss_gen object...\n",
      "        A Normal Inverse Gaussian continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `norminvgauss` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `norminvgauss` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b) = \\frac{a \\, K_1(a \\sqrt{1 + x^2})}{\\pi \\sqrt{1 + x^2}} \\,\n",
      "                         \\exp(\\sqrt{a^2 - b^2} + b x)\n",
      "        \n",
      "        where :math:`x` is a real number, the parameter :math:`a` is the tail\n",
      "        heaviness and :math:`b` is the asymmetry parameter satisfying\n",
      "        :math:`a > 0` and :math:`|b| <= a`.\n",
      "        :math:`K_1` is the modified Bessel function of second kind\n",
      "        (`scipy.special.k1`).\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``norminvgauss.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``norminvgauss.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        A normal inverse Gaussian random variable `Y` with parameters `a` and `b`\n",
      "        can be expressed as a normal mean-variance mixture:\n",
      "        `Y = b * V + sqrt(V) * X` where `X` is `norm(0,1)` and `V` is\n",
      "        `invgauss(mu=1/sqrt(a**2 - b**2))`. This representation is used\n",
      "        to generate random variates.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        O. Barndorff-Nielsen, \"Hyperbolic Distributions and Distributions on\n",
      "        Hyperbolae\", Scandinavian Journal of Statistics, Vol. 5(3),\n",
      "        pp. 151-157, 1978.\n",
      "        \n",
      "        O. Barndorff-Nielsen, \"Normal Inverse Gaussian Distributions and Stochastic\n",
      "        Volatility Modelling\", Scandinavian Journal of Statistics, Vol. 24,\n",
      "        pp. 1-13, 1997.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import norminvgauss\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 1.25, 0.5\n",
      "        >>> mean, var, skew, kurt = norminvgauss.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(norminvgauss.ppf(0.01, a, b),\n",
      "        ...                 norminvgauss.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, norminvgauss.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='norminvgauss pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = norminvgauss(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = norminvgauss.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], norminvgauss.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = norminvgauss.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    ortho_group = <scipy.stats._multivariate.ortho_group_gen object>\n",
      "    pareto = <scipy.stats._continuous_distns.pareto_gen object>\n",
      "        A Pareto continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `pareto` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `pareto` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, b) = \\frac{b}{x^{b+1}}\n",
      "        \n",
      "        for :math:`x \\ge 1`, :math:`b > 0`.\n",
      "        \n",
      "        `pareto` takes ``b`` as a shape parameter for :math:`b`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``pareto.pdf(x, b, loc, scale)`` is identically\n",
      "        equivalent to ``pareto.pdf(y, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import pareto\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> b = 2.62\n",
      "        >>> mean, var, skew, kurt = pareto.stats(b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(pareto.ppf(0.01, b),\n",
      "        ...                 pareto.ppf(0.99, b), 100)\n",
      "        >>> ax.plot(x, pareto.pdf(x, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='pareto pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = pareto(b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = pareto.ppf([0.001, 0.5, 0.999], b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], pareto.cdf(vals, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = pareto.rvs(b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    pearson3 = <scipy.stats._continuous_distns.pearson3_gen object>\n",
      "        A pearson type III continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `pearson3` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(skew, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, skew, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, skew, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, skew, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, skew, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, skew, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, skew, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, skew, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, skew, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, skew, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(skew, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(skew, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(skew,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(skew, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(skew, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(skew, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(skew, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, skew, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `pearson3` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\kappa) = \\frac{|\\beta|}{\\Gamma(\\alpha)}\n",
      "                           (\\beta (x - \\zeta))^{\\alpha - 1}\n",
      "                           \\exp(-\\beta (x - \\zeta))\n",
      "        \n",
      "        where:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "                \\beta = \\frac{2}{\\kappa}\n",
      "        \n",
      "                \\alpha = \\beta^2 = \\frac{4}{\\kappa^2}\n",
      "        \n",
      "                \\zeta = -\\frac{\\alpha}{\\beta} = -\\beta\n",
      "        \n",
      "        :math:`\\Gamma` is the gamma function (`scipy.special.gamma`).\n",
      "        Pass the skew :math:`\\kappa` into `pearson3` as the shape parameter\n",
      "        ``skew``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``pearson3.pdf(x, skew, loc, scale)`` is identically\n",
      "        equivalent to ``pearson3.pdf(y, skew) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import pearson3\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> skew = 0.1\n",
      "        >>> mean, var, skew, kurt = pearson3.stats(skew, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(pearson3.ppf(0.01, skew),\n",
      "        ...                 pearson3.ppf(0.99, skew), 100)\n",
      "        >>> ax.plot(x, pearson3.pdf(x, skew),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='pearson3 pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = pearson3(skew)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = pearson3.ppf([0.001, 0.5, 0.999], skew)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], pearson3.cdf(vals, skew))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = pearson3.rvs(skew, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        R.W. Vogel and D.E. McMartin, \"Probability Plot Goodness-of-Fit and\n",
      "        Skewness Estimation Procedures for the Pearson Type 3 Distribution\", Water\n",
      "        Resources Research, Vol.27, 3149-3158 (1991).\n",
      "        \n",
      "        L.R. Salvosa, \"Tables of Pearson's Type III Function\", Ann. Math. Statist.,\n",
      "        Vol.1, 191-198 (1930).\n",
      "        \n",
      "        \"Using Modern Computing Tools to Fit the Pearson Type III Distribution to\n",
      "        Aviation Loads Data\", Office of Aviation Research (2003).\n",
      "    \n",
      "    planck = <scipy.stats._discrete_distns.planck_gen object>\n",
      "        A Planck discrete exponential random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `planck` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(lambda_, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, lambda_, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, lambda_, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, lambda_, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, lambda_, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, lambda_, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, lambda_, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, lambda_, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, lambda_, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(lambda_, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(lambda_, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(lambda_,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(lambda_, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(lambda_, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(lambda_, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(lambda_, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, lambda_, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `planck` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) = (1-\\exp(-\\lambda)) \\exp(-\\lambda k)\n",
      "        \n",
      "        for :math:`k \\ge 0` and :math:`\\lambda > 0`.\n",
      "        \n",
      "        `planck` takes :math:`\\lambda` as shape parameter. The Planck distribution\n",
      "        can be written as a geometric distribution (`geom`) with\n",
      "        :math:`p = 1 - \\exp(-\\lambda)` shifted by ``loc = -1``.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``planck.pmf(k, lambda_, loc)`` is identically\n",
      "        equivalent to ``planck.pmf(k - loc, lambda_)``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        geom\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import planck\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> lambda_ = 0.51\n",
      "        >>> mean, var, skew, kurt = planck.stats(lambda_, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(planck.ppf(0.01, lambda_),\n",
      "        ...               planck.ppf(0.99, lambda_))\n",
      "        >>> ax.plot(x, planck.pmf(x, lambda_), 'bo', ms=8, label='planck pmf')\n",
      "        >>> ax.vlines(x, 0, planck.pmf(x, lambda_), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = planck(lambda_)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = planck.cdf(x, lambda_)\n",
      "        >>> np.allclose(x, planck.ppf(prob, lambda_))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = planck.rvs(lambda_, size=1000)\n",
      "    \n",
      "    poisson = <scipy.stats._discrete_distns.poisson_gen object>\n",
      "        A Poisson discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `poisson` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(mu, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, mu, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, mu, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, mu, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, mu, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, mu, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, mu, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, mu, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, mu, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(mu, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(mu, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(mu,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(mu, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(mu, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(mu, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(mu, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, mu, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `poisson` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) = \\exp(-\\mu) \\frac{\\mu^k}{k!}\n",
      "        \n",
      "        for :math:`k \\ge 0`.\n",
      "        \n",
      "        `poisson` takes :math:`\\mu \\geq 0` as shape parameter.\n",
      "        When :math:`\\mu = 0`, the ``pmf`` method\n",
      "        returns ``1.0`` at quantile :math:`k = 0`.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``poisson.pmf(k, mu, loc)`` is identically\n",
      "        equivalent to ``poisson.pmf(k - loc, mu)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import poisson\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> mu = 0.6\n",
      "        >>> mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(poisson.ppf(0.01, mu),\n",
      "        ...               poisson.ppf(0.99, mu))\n",
      "        >>> ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n",
      "        >>> ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = poisson(mu)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = poisson.cdf(x, mu)\n",
      "        >>> np.allclose(x, poisson.ppf(prob, mu))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = poisson.rvs(mu, size=1000)\n",
      "    \n",
      "    powerlaw = <scipy.stats._continuous_distns.powerlaw_gen object>\n",
      "        A power-function continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `powerlaw` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `powerlaw` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a) = a x^{a-1}\n",
      "        \n",
      "        for :math:`0 \\le x \\le 1`, :math:`a > 0`.\n",
      "        \n",
      "        `powerlaw` takes ``a`` as a shape parameter for :math:`a`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``powerlaw.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``powerlaw.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        `powerlaw` is a special case of `beta` with ``b=1``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import powerlaw\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 1.66\n",
      "        >>> mean, var, skew, kurt = powerlaw.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(powerlaw.ppf(0.01, a),\n",
      "        ...                 powerlaw.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, powerlaw.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='powerlaw pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = powerlaw(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = powerlaw.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], powerlaw.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = powerlaw.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    powerlognorm = <scipy.stats._continuous_distns.powerlognorm_gen object...\n",
      "        A power log-normal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `powerlognorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, s, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, s, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, s, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, s, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, s, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, s, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, s, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, s, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, s, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, s, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, s, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, s, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c, s), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, s, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, s, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, s, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, s, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, s, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `powerlognorm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c, s) = \\frac{c}{x s} \\phi(\\log(x)/s)\n",
      "                         (\\Phi(-\\log(x)/s))^{c-1}\n",
      "        \n",
      "        where :math:`\\phi` is the normal pdf, and :math:`\\Phi` is the normal cdf,\n",
      "        and :math:`x > 0`, :math:`s, c > 0`.\n",
      "        \n",
      "        `powerlognorm` takes :math:`c` and :math:`s` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``powerlognorm.pdf(x, c, s, loc, scale)`` is identically\n",
      "        equivalent to ``powerlognorm.pdf(y, c, s) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import powerlognorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c, s = 2.14, 0.446\n",
      "        >>> mean, var, skew, kurt = powerlognorm.stats(c, s, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(powerlognorm.ppf(0.01, c, s),\n",
      "        ...                 powerlognorm.ppf(0.99, c, s), 100)\n",
      "        >>> ax.plot(x, powerlognorm.pdf(x, c, s),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='powerlognorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = powerlognorm(c, s)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = powerlognorm.ppf([0.001, 0.5, 0.999], c, s)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], powerlognorm.cdf(vals, c, s))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = powerlognorm.rvs(c, s, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    powernorm = <scipy.stats._continuous_distns.powernorm_gen object>\n",
      "        A power normal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `powernorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `powernorm` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c \\phi(x) (\\Phi(-x))^{c-1}\n",
      "        \n",
      "        where :math:`\\phi` is the normal pdf, and :math:`\\Phi` is the normal cdf,\n",
      "        and :math:`x >= 0`, :math:`c > 0`.\n",
      "        \n",
      "        `powernorm` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``powernorm.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``powernorm.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import powernorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 4.45\n",
      "        >>> mean, var, skew, kurt = powernorm.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(powernorm.ppf(0.01, c),\n",
      "        ...                 powernorm.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, powernorm.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='powernorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = powernorm(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = powernorm.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], powernorm.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = powernorm.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    randint = <scipy.stats._discrete_distns.randint_gen object>\n",
      "        A uniform discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `randint` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(low, high, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, low, high, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, low, high, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, low, high, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, low, high, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, low, high, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, low, high, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, low, high, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, low, high, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(low, high, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(low, high, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(low, high), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(low, high, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(low, high, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(low, high, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(low, high, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, low, high, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `randint` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) = \\frac{1}{\\texttt{high} - \\texttt{low}}\n",
      "        \n",
      "        for :math:`k \\in \\{\\texttt{low}, \\dots, \\texttt{high} - 1\\}`.\n",
      "        \n",
      "        `randint` takes :math:`\\texttt{low}` and :math:`\\texttt{high}` as shape\n",
      "        parameters.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``randint.pmf(k, low, high, loc)`` is identically\n",
      "        equivalent to ``randint.pmf(k - loc, low, high)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import randint\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> low, high = 7, 31\n",
      "        >>> mean, var, skew, kurt = randint.stats(low, high, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(randint.ppf(0.01, low, high),\n",
      "        ...               randint.ppf(0.99, low, high))\n",
      "        >>> ax.plot(x, randint.pmf(x, low, high), 'bo', ms=8, label='randint pmf')\n",
      "        >>> ax.vlines(x, 0, randint.pmf(x, low, high), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = randint(low, high)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = randint.cdf(x, low, high)\n",
      "        >>> np.allclose(x, randint.ppf(prob, low, high))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = randint.rvs(low, high, size=1000)\n",
      "    \n",
      "    random_correlation = <scipy.stats._multivariate.random_correlation_gen...\n",
      "    rayleigh = <scipy.stats._continuous_distns.rayleigh_gen object>\n",
      "        A Rayleigh continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `rayleigh` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `rayleigh` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = x \\exp(-x^2/2)\n",
      "        \n",
      "        for :math:`x \\ge 0`.\n",
      "        \n",
      "        `rayleigh` is a special case of `chi` with ``df=2``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``rayleigh.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``rayleigh.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import rayleigh\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = rayleigh.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(rayleigh.ppf(0.01),\n",
      "        ...                 rayleigh.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, rayleigh.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='rayleigh pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = rayleigh()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = rayleigh.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], rayleigh.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = rayleigh.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    rdist = <scipy.stats._continuous_distns.rdist_gen object>\n",
      "        An R-distributed (symmetric beta) continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `rdist` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `rdist` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{(1-x^2)^{c/2-1}}{B(1/2, c/2)}\n",
      "        \n",
      "        for :math:`-1 \\le x \\le 1`, :math:`c > 0`. `rdist` is also called the\n",
      "        symmetric beta distribution: if B has a `beta` distribution with\n",
      "        parameters (c/2, c/2), then X = 2*B - 1 follows a R-distribution with\n",
      "        parameter c.\n",
      "        \n",
      "        `rdist` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        This distribution includes the following distribution kernels as\n",
      "        special cases::\n",
      "        \n",
      "            c = 2:  uniform\n",
      "            c = 3:  `semicircular`\n",
      "            c = 4:  Epanechnikov (parabolic)\n",
      "            c = 6:  quartic (biweight)\n",
      "            c = 8:  triweight\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``rdist.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``rdist.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import rdist\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 1.6\n",
      "        >>> mean, var, skew, kurt = rdist.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(rdist.ppf(0.01, c),\n",
      "        ...                 rdist.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, rdist.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='rdist pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = rdist(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = rdist.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], rdist.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = rdist.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    recipinvgauss = <scipy.stats._continuous_distns.recipinvgauss_gen obje...\n",
      "        A reciprocal inverse Gaussian continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `recipinvgauss` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(mu, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, mu, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, mu, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, mu, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, mu, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, mu, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, mu, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, mu, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, mu, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, mu, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(mu, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(mu, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(mu,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(mu, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(mu, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(mu, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(mu, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, mu, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `recipinvgauss` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\mu) = \\frac{1}{\\sqrt{2\\pi x}}\n",
      "                        \\exp\\left(\\frac{-(1-\\mu x)^2}{2\\mu^2x}\\right)\n",
      "        \n",
      "        for :math:`x \\ge 0`.\n",
      "        \n",
      "        `recipinvgauss` takes ``mu`` as a shape parameter for :math:`\\mu`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``recipinvgauss.pdf(x, mu, loc, scale)`` is identically\n",
      "        equivalent to ``recipinvgauss.pdf(y, mu) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import recipinvgauss\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> mu = 0.63\n",
      "        >>> mean, var, skew, kurt = recipinvgauss.stats(mu, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(recipinvgauss.ppf(0.01, mu),\n",
      "        ...                 recipinvgauss.ppf(0.99, mu), 100)\n",
      "        >>> ax.plot(x, recipinvgauss.pdf(x, mu),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='recipinvgauss pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = recipinvgauss(mu)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = recipinvgauss.ppf([0.001, 0.5, 0.999], mu)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], recipinvgauss.cdf(vals, mu))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = recipinvgauss.rvs(mu, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    reciprocal = <scipy.stats._continuous_distns.reciprocal_gen object>\n",
      "        A loguniform or reciprocal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `reciprocal` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for this class is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, a, b) = \\frac{1}{x \\log(b/a)}\n",
      "        \n",
      "        for :math:`a \\le x \\le b`, :math:`b > a > 0`. This class takes\n",
      "        :math:`a` and :math:`b` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``reciprocal.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``reciprocal.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import reciprocal\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 0.01, 1.25\n",
      "        >>> mean, var, skew, kurt = reciprocal.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(reciprocal.ppf(0.01, a, b),\n",
      "        ...                 reciprocal.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, reciprocal.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='reciprocal pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = reciprocal(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = reciprocal.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], reciprocal.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = reciprocal.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        \n",
      "        This doesn't show the equal probability of ``0.01``, ``0.1`` and\n",
      "        ``1``. This is best when the x-axis is log-scaled:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        >>> ax.hist(np.log10(r))\n",
      "        >>> ax.set_ylabel(\"Frequency\")\n",
      "        >>> ax.set_xlabel(\"Value of random variable\")\n",
      "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
      "        >>> ticks = [\"$10^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
      "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
      "        >>> plt.show()\n",
      "        \n",
      "        This random variable will be log-uniform regardless of the base chosen for\n",
      "        ``a`` and ``b``. Let's specify with base ``2`` instead:\n",
      "        \n",
      "        >>> rvs = reciprocal(2**-2, 2**0).rvs(size=1000)\n",
      "        \n",
      "        Values of ``1/4``, ``1/2`` and ``1`` are equally likely with this random\n",
      "        variable.  Here's the histogram:\n",
      "        \n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        >>> ax.hist(np.log2(rvs))\n",
      "        >>> ax.set_ylabel(\"Frequency\")\n",
      "        >>> ax.set_xlabel(\"Value of random variable\")\n",
      "        >>> ax.xaxis.set_major_locator(plt.FixedLocator([-2, -1, 0]))\n",
      "        >>> ticks = [\"$2^{{ {} }}$\".format(i) for i in [-2, -1, 0]]\n",
      "        >>> ax.set_xticklabels(ticks)  # doctest: +SKIP\n",
      "        >>> plt.show()\n",
      "    \n",
      "    rice = <scipy.stats._continuous_distns.rice_gen object>\n",
      "        A Rice continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `rice` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `rice` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, b) = x \\exp(- \\frac{x^2 + b^2}{2}) I_0(x b)\n",
      "        \n",
      "        for :math:`x >= 0`, :math:`b > 0`. :math:`I_0` is the modified Bessel\n",
      "        function of order zero (`scipy.special.i0`).\n",
      "        \n",
      "        `rice` takes ``b`` as a shape parameter for :math:`b`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``rice.pdf(x, b, loc, scale)`` is identically\n",
      "        equivalent to ``rice.pdf(y, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        The Rice distribution describes the length, :math:`r`, of a 2-D vector with\n",
      "        components :math:`(U+u, V+v)`, where :math:`U, V` are constant, :math:`u,\n",
      "        v` are independent Gaussian random variables with standard deviation\n",
      "        :math:`s`.  Let :math:`R = \\sqrt{U^2 + V^2}`. Then the pdf of :math:`r` is\n",
      "        ``rice.pdf(x, R/s, scale=s)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import rice\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> b = 0.775\n",
      "        >>> mean, var, skew, kurt = rice.stats(b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(rice.ppf(0.01, b),\n",
      "        ...                 rice.ppf(0.99, b), 100)\n",
      "        >>> ax.plot(x, rice.pdf(x, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='rice pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = rice(b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = rice.ppf([0.001, 0.5, 0.999], b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], rice.cdf(vals, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = rice.rvs(b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    semicircular = <scipy.stats._continuous_distns.semicircular_gen object...\n",
      "        A semicircular continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `semicircular` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rdist\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `semicircular` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{2}{\\pi} \\sqrt{1-x^2}\n",
      "        \n",
      "        for :math:`-1 \\le x \\le 1`.\n",
      "        \n",
      "        The distribution is a special case of `rdist` with `c = 3`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``semicircular.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``semicircular.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Wigner semicircle distribution\",\n",
      "               https://en.wikipedia.org/wiki/Wigner_semicircle_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import semicircular\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = semicircular.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(semicircular.ppf(0.01),\n",
      "        ...                 semicircular.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, semicircular.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='semicircular pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = semicircular()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = semicircular.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], semicircular.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = semicircular.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    skellam = <scipy.stats._discrete_distns.skellam_gen object>\n",
      "        A  Skellam discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `skellam` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(mu1, mu2, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, mu1, mu2, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, mu1, mu2, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, mu1, mu2, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, mu1, mu2, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, mu1, mu2, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, mu1, mu2, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, mu1, mu2, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, mu1, mu2, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(mu1, mu2, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(mu1, mu2, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(mu1, mu2), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(mu1, mu2, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(mu1, mu2, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(mu1, mu2, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(mu1, mu2, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, mu1, mu2, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Probability distribution of the difference of two correlated or\n",
      "        uncorrelated Poisson random variables.\n",
      "        \n",
      "        Let :math:`k_1` and :math:`k_2` be two Poisson-distributed r.v. with\n",
      "        expected values :math:`\\lambda_1` and :math:`\\lambda_2`. Then,\n",
      "        :math:`k_1 - k_2` follows a Skellam distribution with parameters\n",
      "        :math:`\\mu_1 = \\lambda_1 - \\rho \\sqrt{\\lambda_1 \\lambda_2}` and\n",
      "        :math:`\\mu_2 = \\lambda_2 - \\rho \\sqrt{\\lambda_1 \\lambda_2}`, where\n",
      "        :math:`\\rho` is the correlation coefficient between :math:`k_1` and\n",
      "        :math:`k_2`. If the two Poisson-distributed r.v. are independent then\n",
      "        :math:`\\rho = 0`.\n",
      "        \n",
      "        Parameters :math:`\\mu_1` and :math:`\\mu_2` must be strictly positive.\n",
      "        \n",
      "        For details see: https://en.wikipedia.org/wiki/Skellam_distribution\n",
      "        \n",
      "        `skellam` takes :math:`\\mu_1` and :math:`\\mu_2` as shape parameters.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``skellam.pmf(k, mu1, mu2, loc)`` is identically\n",
      "        equivalent to ``skellam.pmf(k - loc, mu1, mu2)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import skellam\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> mu1, mu2 = 15, 8\n",
      "        >>> mean, var, skew, kurt = skellam.stats(mu1, mu2, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(skellam.ppf(0.01, mu1, mu2),\n",
      "        ...               skellam.ppf(0.99, mu1, mu2))\n",
      "        >>> ax.plot(x, skellam.pmf(x, mu1, mu2), 'bo', ms=8, label='skellam pmf')\n",
      "        >>> ax.vlines(x, 0, skellam.pmf(x, mu1, mu2), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = skellam(mu1, mu2)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = skellam.cdf(x, mu1, mu2)\n",
      "        >>> np.allclose(x, skellam.ppf(prob, mu1, mu2))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = skellam.rvs(mu1, mu2, size=1000)\n",
      "    \n",
      "    skewcauchy = <scipy.stats._continuous_distns.skewcauchy_gen object>\n",
      "        A skewed Cauchy random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `skewcauchy` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cauchy : Cauchy distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        The probability density function for `skewcauchy` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\pi \\left(\\frac{x^2}{\\left(a\\, \\text{sign}(x) + 1\n",
      "                                                       \\right)^2} + 1 \\right)}\n",
      "        \n",
      "        for a real number :math:`x` and skewness parameter :math:`-1 < a < 1`.\n",
      "        \n",
      "        When :math:`a=0`, the distribution reduces to the usual Cauchy\n",
      "        distribution.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``skewcauchy.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``skewcauchy.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Skewed generalized *t* distribution\", Wikipedia\n",
      "           https://en.wikipedia.org/wiki/Skewed_generalized_t_distribution#Skewed_Cauchy_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import skewcauchy\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 0.5\n",
      "        >>> mean, var, skew, kurt = skewcauchy.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(skewcauchy.ppf(0.01, a),\n",
      "        ...                 skewcauchy.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, skewcauchy.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='skewcauchy pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = skewcauchy(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = skewcauchy.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], skewcauchy.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = skewcauchy.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    skewnorm = <scipy.stats._continuous_distns.skew_norm_gen object>\n",
      "        A skew-normal random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `skewnorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The pdf is::\n",
      "        \n",
      "            skewnorm.pdf(x, a) = 2 * norm.pdf(x) * norm.cdf(a*x)\n",
      "        \n",
      "        `skewnorm` takes a real number :math:`a` as a skewness parameter\n",
      "        When ``a = 0`` the distribution is identical to a normal distribution\n",
      "        (`norm`). `rvs` implements the method of [1]_.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``skewnorm.pdf(x, a, loc, scale)`` is identically\n",
      "        equivalent to ``skewnorm.pdf(y, a) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import skewnorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 4\n",
      "        >>> mean, var, skew, kurt = skewnorm.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(skewnorm.ppf(0.01, a),\n",
      "        ...                 skewnorm.ppf(0.99, a), 100)\n",
      "        >>> ax.plot(x, skewnorm.pdf(x, a),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='skewnorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = skewnorm(a)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = skewnorm.ppf([0.001, 0.5, 0.999], a)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], skewnorm.cdf(vals, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = skewnorm.rvs(a, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] A. Azzalini and A. Capitanio (1999). Statistical applications of the\n",
      "            multivariate skew-normal distribution. J. Roy. Statist. Soc., B 61, 579-602.\n",
      "            :arxiv:`0911.2093`\n",
      "    \n",
      "    special_ortho_group = <scipy.stats._multivariate.special_ortho_group_g...\n",
      "    studentized_range = <scipy.stats._continuous_distns.studentized_range_...\n",
      "        A studentized range continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `studentized_range` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(k, df, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, k, df, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, k, df, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, k, df, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, k, df, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, k, df, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, k, df, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, k, df, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, k, df, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, k, df, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(k, df, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(k, df, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(k, df), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(k, df, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(k, df, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(k, df, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(k, df, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, k, df, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        t: Student's t distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `studentized_range` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "             f(x; k, \\nu) = \\frac{k(k-1)\\nu^{\\nu/2}}{\\Gamma(\\nu/2)\n",
      "                            2^{\\nu/2-1}} \\int_{0}^{\\infty} \\int_{-\\infty}^{\\infty}\n",
      "                            s^{\\nu} e^{-\\nu s^2/2} \\phi(z) \\phi(sx + z)\n",
      "                            [\\Phi(sx + z) - \\Phi(z)]^{k-2} \\,dz \\,ds\n",
      "        \n",
      "        for :math:`x ≥ 0`, :math:`k > 1`, and :math:`\\nu > 0`.\n",
      "        \n",
      "        `studentized_range` takes ``k`` for :math:`k` and ``df`` for :math:`\\nu`\n",
      "        as shape parameters.\n",
      "        \n",
      "        When :math:`\\nu` exceeds 100,000, an asymptotic approximation (infinite\n",
      "        degrees of freedom) is used to compute the cumulative distribution\n",
      "        function [4]_.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``studentized_range.pdf(x, k, df, loc, scale)`` is identically\n",
      "        equivalent to ``studentized_range.pdf(y, k, df) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] \"Studentized range distribution\",\n",
      "               https://en.wikipedia.org/wiki/Studentized_range_distribution\n",
      "        .. [2] Batista, Ben Dêivide, et al. \"Externally Studentized Normal Midrange\n",
      "               Distribution.\" Ciência e Agrotecnologia, vol. 41, no. 4, 2017, pp.\n",
      "               378-389., doi:10.1590/1413-70542017414047716.\n",
      "        .. [3] Harter, H. Leon. \"Tables of Range and Studentized Range.\" The Annals\n",
      "               of Mathematical Statistics, vol. 31, no. 4, 1960, pp. 1122-1147.\n",
      "               JSTOR, www.jstor.org/stable/2237810. Accessed 18 Feb. 2021.\n",
      "        .. [4] Lund, R. E., and J. R. Lund. \"Algorithm AS 190: Probabilities and\n",
      "               Upper Quantiles for the Studentized Range.\" Journal of the Royal\n",
      "               Statistical Society. Series C (Applied Statistics), vol. 32, no. 2,\n",
      "               1983, pp. 204-210. JSTOR, www.jstor.org/stable/2347300. Accessed 18\n",
      "               Feb. 2021.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import studentized_range\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> k, df = 3, 10\n",
      "        >>> mean, var, skew, kurt = studentized_range.stats(k, df, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(studentized_range.ppf(0.01, k, df),\n",
      "        ...                 studentized_range.ppf(0.99, k, df), 100)\n",
      "        >>> ax.plot(x, studentized_range.pdf(x, k, df),\n",
      "        ...         'r-', lw=5, alpha=0.6, label='studentized_range pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = studentized_range(k, df)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = studentized_range.ppf([0.001, 0.5, 0.999], k, df)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], studentized_range.cdf(vals, k, df))\n",
      "        True\n",
      "        \n",
      "        Rather than using (``studentized_range.rvs``) to generate random variates,\n",
      "        which is very slow for this distribution, we can approximate the inverse\n",
      "        CDF using an interpolator, and then perform inverse transform sampling\n",
      "        with this approximate inverse CDF.\n",
      "        \n",
      "        This distribution has an infinite but thin right tail, so we focus our\n",
      "        attention on the leftmost 99.9 percent.\n",
      "        \n",
      "        >>> a, b = studentized_range.ppf([0, .999], k, df)\n",
      "        >>> a, b\n",
      "        0, 7.41058083802274\n",
      "        \n",
      "        >>> from scipy.interpolate import interp1d\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> xs = np.linspace(a, b, 50)\n",
      "        >>> cdf = studentized_range.cdf(xs, k, df)\n",
      "        # Create an interpolant of the inverse CDF\n",
      "        >>> ppf = interp1d(cdf, xs, fill_value='extrapolate')\n",
      "        # Perform inverse transform sampling using the interpolant\n",
      "        >>> r = ppf(rng.uniform(size=1000))\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    t = <scipy.stats._continuous_distns.t_gen object>\n",
      "        A Student's t continuous random variable.\n",
      "        \n",
      "        For the noncentral t distribution, see `nct`.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `t` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(df, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, df, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, df, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, df, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, df, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, df, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, df, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, df, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, df, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, df, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(df, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(df, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(df, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(df, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(df, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(df, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, df, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nct\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `t` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\nu) = \\frac{\\Gamma((\\nu+1)/2)}\n",
      "                            {\\sqrt{\\pi \\nu} \\Gamma(\\nu/2)}\n",
      "                        (1+x^2/\\nu)^{-(\\nu+1)/2}\n",
      "        \n",
      "        where :math:`x` is a real number and the degrees of freedom parameter\n",
      "        :math:`\\nu` (denoted ``df`` in the implementation) satisfies\n",
      "        :math:`\\nu > 0`. :math:`\\Gamma` is the gamma function\n",
      "        (`scipy.special.gamma`).\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``t.pdf(x, df, loc, scale)`` is identically\n",
      "        equivalent to ``t.pdf(y, df) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import t\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> df = 2.74\n",
      "        >>> mean, var, skew, kurt = t.stats(df, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(t.ppf(0.01, df),\n",
      "        ...                 t.ppf(0.99, df), 100)\n",
      "        >>> ax.plot(x, t.pdf(x, df),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='t pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = t(df)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = t.ppf([0.001, 0.5, 0.999], df)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = t.rvs(df, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    trapezoid = <scipy.stats._continuous_distns.trapezoid_gen object>\n",
      "        A trapezoidal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `trapezoid` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, d, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, d, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, d, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, d, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, d, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, d, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, d, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, d, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, d, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, d, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, d, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, d, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c, d), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, d, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, d, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, d, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, d, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, d, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The trapezoidal distribution can be represented with an up-sloping line\n",
      "        from ``loc`` to ``(loc + c*scale)``, then constant to ``(loc + d*scale)``\n",
      "        and then downsloping from ``(loc + d*scale)`` to ``(loc+scale)``.  This\n",
      "        defines the trapezoid base from ``loc`` to ``(loc+scale)`` and the flat\n",
      "        top from ``c`` to ``d`` proportional to the position along the base\n",
      "        with ``0 <= c <= d <= 1``.  When ``c=d``, this is equivalent to `triang`\n",
      "        with the same values for `loc`, `scale` and `c`.\n",
      "        The method of [1]_ is used for computing moments.\n",
      "        \n",
      "        `trapezoid` takes :math:`c` and :math:`d` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``trapezoid.pdf(x, c, d, loc, scale)`` is identically\n",
      "        equivalent to ``trapezoid.pdf(y, c, d) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        The standard form is in the range [0, 1] with c the mode.\n",
      "        The location parameter shifts the start to `loc`.\n",
      "        The scale parameter changes the width from 1 to `scale`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import trapezoid\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c, d = 0.2, 0.8\n",
      "        >>> mean, var, skew, kurt = trapezoid.stats(c, d, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(trapezoid.ppf(0.01, c, d),\n",
      "        ...                 trapezoid.ppf(0.99, c, d), 100)\n",
      "        >>> ax.plot(x, trapezoid.pdf(x, c, d),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='trapezoid pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = trapezoid(c, d)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = trapezoid.ppf([0.001, 0.5, 0.999], c, d)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], trapezoid.cdf(vals, c, d))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = trapezoid.rvs(c, d, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Kacker, R.N. and Lawrence, J.F. (2007). Trapezoidal and triangular\n",
      "           distributions for Type B evaluation of standard uncertainty.\n",
      "           Metrologia 44, 117-127. :doi:`10.1088/0026-1394/44/2/003`\n",
      "    \n",
      "    trapz = <scipy.stats._continuous_distns.trapezoid_gen object>\n",
      "        trapz is an alias for `trapezoid`\n",
      "    \n",
      "    triang = <scipy.stats._continuous_distns.triang_gen object>\n",
      "        A triangular continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `triang` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The triangular distribution can be represented with an up-sloping line from\n",
      "        ``loc`` to ``(loc + c*scale)`` and then downsloping for ``(loc + c*scale)``\n",
      "        to ``(loc + scale)``.\n",
      "        \n",
      "        `triang` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``triang.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``triang.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        The standard form is in the range [0, 1] with c the mode.\n",
      "        The location parameter shifts the start to `loc`.\n",
      "        The scale parameter changes the width from 1 to `scale`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import triang\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.158\n",
      "        >>> mean, var, skew, kurt = triang.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(triang.ppf(0.01, c),\n",
      "        ...                 triang.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, triang.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='triang pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = triang(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = triang.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], triang.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = triang.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    truncexpon = <scipy.stats._continuous_distns.truncexpon_gen object>\n",
      "        A truncated exponential continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `truncexpon` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(b,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `truncexpon` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, b) = \\frac{\\exp(-x)}{1 - \\exp(-b)}\n",
      "        \n",
      "        for :math:`0 <= x <= b`.\n",
      "        \n",
      "        `truncexpon` takes ``b`` as a shape parameter for :math:`b`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``truncexpon.pdf(x, b, loc, scale)`` is identically\n",
      "        equivalent to ``truncexpon.pdf(y, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import truncexpon\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> b = 4.69\n",
      "        >>> mean, var, skew, kurt = truncexpon.stats(b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(truncexpon.ppf(0.01, b),\n",
      "        ...                 truncexpon.ppf(0.99, b), 100)\n",
      "        >>> ax.plot(x, truncexpon.pdf(x, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='truncexpon pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = truncexpon(b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = truncexpon.ppf([0.001, 0.5, 0.999], b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], truncexpon.cdf(vals, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = truncexpon.rvs(b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    truncnorm = <scipy.stats._continuous_distns.truncnorm_gen object>\n",
      "        A truncated normal continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `truncnorm` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, b, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, a, b, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, a, b, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, a, b, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, a, b, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, a, b, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, b, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, b, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, a, b, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(a, b, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, b, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(a, b), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, b, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(a, b, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(a, b, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(a, b, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, b, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The standard form of this distribution is a standard normal truncated to\n",
      "        the range [a, b] --- notice that a and b are defined over the domain of the\n",
      "        standard normal.  To convert clip values for a specific mean and standard\n",
      "        deviation, use::\n",
      "        \n",
      "            a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std\n",
      "        \n",
      "        `truncnorm` takes :math:`a` and :math:`b` as shape parameters.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``truncnorm.pdf(x, a, b, loc, scale)`` is identically\n",
      "        equivalent to ``truncnorm.pdf(y, a, b) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import truncnorm\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, b = 0.1, 2\n",
      "        >>> mean, var, skew, kurt = truncnorm.stats(a, b, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(truncnorm.ppf(0.01, a, b),\n",
      "        ...                 truncnorm.ppf(0.99, a, b), 100)\n",
      "        >>> ax.plot(x, truncnorm.pdf(x, a, b),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='truncnorm pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = truncnorm(a, b)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = truncnorm.ppf([0.001, 0.5, 0.999], a, b)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], truncnorm.cdf(vals, a, b))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = truncnorm.rvs(a, b, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    tukeylambda = <scipy.stats._continuous_distns.tukeylambda_gen object>\n",
      "        A Tukey-Lamdba continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `tukeylambda` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(lam, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, lam, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, lam, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, lam, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, lam, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, lam, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, lam, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, lam, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, lam, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, lam, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(lam, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(lam, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(lam,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(lam, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(lam, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(lam, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(lam, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, lam, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A flexible distribution, able to represent and interpolate between the\n",
      "        following distributions:\n",
      "        \n",
      "        - Cauchy                (:math:`lambda = -1`)\n",
      "        - logistic              (:math:`lambda = 0`)\n",
      "        - approx Normal         (:math:`lambda = 0.14`)\n",
      "        - uniform from -1 to 1  (:math:`lambda = 1`)\n",
      "        \n",
      "        `tukeylambda` takes a real number :math:`lambda` (denoted ``lam``\n",
      "        in the implementation) as a shape parameter.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``tukeylambda.pdf(x, lam, loc, scale)`` is identically\n",
      "        equivalent to ``tukeylambda.pdf(y, lam) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import tukeylambda\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> lam = 3.13\n",
      "        >>> mean, var, skew, kurt = tukeylambda.stats(lam, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(tukeylambda.ppf(0.01, lam),\n",
      "        ...                 tukeylambda.ppf(0.99, lam), 100)\n",
      "        >>> ax.plot(x, tukeylambda.pdf(x, lam),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='tukeylambda pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = tukeylambda(lam)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = tukeylambda.ppf([0.001, 0.5, 0.999], lam)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], tukeylambda.cdf(vals, lam))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = tukeylambda.rvs(lam, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    uniform = <scipy.stats._continuous_distns.uniform_gen object>\n",
      "        A uniform continuous random variable.\n",
      "        \n",
      "        In the standard form, the distribution is uniform on ``[0, 1]``. Using\n",
      "        the parameters ``loc`` and ``scale``, one obtains the uniform distribution\n",
      "        on ``[loc, loc + scale]``.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `uniform` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import uniform\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = uniform.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(uniform.ppf(0.01),\n",
      "        ...                 uniform.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, uniform.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='uniform pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = uniform()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = uniform.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], uniform.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = uniform.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    unitary_group = <scipy.stats._multivariate.unitary_group_gen object>\n",
      "    vonmises = <scipy.stats._continuous_distns.vonmises_gen object>\n",
      "        A Von Mises continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `vonmises` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, kappa, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, kappa, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, kappa, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, kappa, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, kappa, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, kappa, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, kappa, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, kappa, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, kappa, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(kappa, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(kappa, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(kappa, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(kappa, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(kappa, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(kappa, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, kappa, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `vonmises` and `vonmises_line` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\kappa) = \\frac{ \\exp(\\kappa \\cos(x)) }{ 2 \\pi I_0(\\kappa) }\n",
      "        \n",
      "        for :math:`-\\pi \\le x \\le \\pi`, :math:`\\kappa > 0`. :math:`I_0` is the\n",
      "        modified Bessel function of order zero (`scipy.special.i0`).\n",
      "        \n",
      "        `vonmises` is a circular distribution which does not restrict the\n",
      "        distribution to a fixed interval. Currently, there is no circular\n",
      "        distribution framework in scipy. The ``cdf`` is implemented such that\n",
      "        ``cdf(x + 2*np.pi) == cdf(x) + 1``.\n",
      "        \n",
      "        `vonmises_line` is the same distribution, defined on :math:`[-\\pi, \\pi]`\n",
      "        on the real line. This is a regular (i.e. non-circular) distribution.\n",
      "        \n",
      "        `vonmises` and `vonmises_line` take ``kappa`` as a shape parameter.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``vonmises.pdf(x, kappa, loc, scale)`` is identically\n",
      "        equivalent to ``vonmises.pdf(y, kappa) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import vonmises\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> kappa = 3.99\n",
      "        >>> mean, var, skew, kurt = vonmises.stats(kappa, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(vonmises.ppf(0.01, kappa),\n",
      "        ...                 vonmises.ppf(0.99, kappa), 100)\n",
      "        >>> ax.plot(x, vonmises.pdf(x, kappa),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='vonmises pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = vonmises(kappa)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = vonmises.ppf([0.001, 0.5, 0.999], kappa)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], vonmises.cdf(vals, kappa))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = vonmises.rvs(kappa, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    vonmises_line = <scipy.stats._continuous_distns.vonmises_gen object>\n",
      "        A Von Mises continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `vonmises_line` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(kappa, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, kappa, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, kappa, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, kappa, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, kappa, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, kappa, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, kappa, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, kappa, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, kappa, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, kappa, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(kappa, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(kappa, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(kappa,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(kappa, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(kappa, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(kappa, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(kappa, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, kappa, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `vonmises` and `vonmises_line` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, \\kappa) = \\frac{ \\exp(\\kappa \\cos(x)) }{ 2 \\pi I_0(\\kappa) }\n",
      "        \n",
      "        for :math:`-\\pi \\le x \\le \\pi`, :math:`\\kappa > 0`. :math:`I_0` is the\n",
      "        modified Bessel function of order zero (`scipy.special.i0`).\n",
      "        \n",
      "        `vonmises` is a circular distribution which does not restrict the\n",
      "        distribution to a fixed interval. Currently, there is no circular\n",
      "        distribution framework in scipy. The ``cdf`` is implemented such that\n",
      "        ``cdf(x + 2*np.pi) == cdf(x) + 1``.\n",
      "        \n",
      "        `vonmises_line` is the same distribution, defined on :math:`[-\\pi, \\pi]`\n",
      "        on the real line. This is a regular (i.e. non-circular) distribution.\n",
      "        \n",
      "        `vonmises` and `vonmises_line` take ``kappa`` as a shape parameter.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``vonmises_line.pdf(x, kappa, loc, scale)`` is identically\n",
      "        equivalent to ``vonmises_line.pdf(y, kappa) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import vonmises_line\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> kappa = 3.99\n",
      "        >>> mean, var, skew, kurt = vonmises_line.stats(kappa, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(vonmises_line.ppf(0.01, kappa),\n",
      "        ...                 vonmises_line.ppf(0.99, kappa), 100)\n",
      "        >>> ax.plot(x, vonmises_line.pdf(x, kappa),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='vonmises_line pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = vonmises_line(kappa)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = vonmises_line.ppf([0.001, 0.5, 0.999], kappa)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], vonmises_line.cdf(vals, kappa))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = vonmises_line.rvs(kappa, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    wald = <scipy.stats._continuous_distns.wald_gen object>\n",
      "        A Wald continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `wald` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `wald` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x) = \\frac{1}{\\sqrt{2\\pi x^3}} \\exp(- \\frac{ (x-1)^2 }{ 2x })\n",
      "        \n",
      "        for :math:`x >= 0`.\n",
      "        \n",
      "        `wald` is a special case of `invgauss` with ``mu=1``.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``wald.pdf(x, loc, scale)`` is identically\n",
      "        equivalent to ``wald.pdf(y) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import wald\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        \n",
      "        >>> mean, var, skew, kurt = wald.stats(moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(wald.ppf(0.01),\n",
      "        ...                 wald.ppf(0.99), 100)\n",
      "        >>> ax.plot(x, wald.pdf(x),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='wald pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = wald()\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = wald.ppf([0.001, 0.5, 0.999])\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], wald.cdf(vals))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = wald.rvs(size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    weibull_max = <scipy.stats._continuous_distns.weibull_max_gen object>\n",
      "        Weibull maximum continuous random variable.\n",
      "        \n",
      "        The Weibull Maximum Extreme Value distribution, from extreme value theory\n",
      "        (Fisher-Gnedenko theorem), is the limiting distribution of rescaled\n",
      "        maximum of iid random variables. This is the distribution of -X\n",
      "        if X is from the `weibull_min` function.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `weibull_max` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        weibull_min\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `weibull_max` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c (-x)^{c-1} \\exp(-(-x)^c)\n",
      "        \n",
      "        for :math:`x < 0`, :math:`c > 0`.\n",
      "        \n",
      "        `weibull_max` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``weibull_max.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``weibull_max.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        https://en.wikipedia.org/wiki/Weibull_distribution\n",
      "        \n",
      "        https://en.wikipedia.org/wiki/Fisher-Tippett-Gnedenko_theorem\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import weibull_max\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 2.87\n",
      "        >>> mean, var, skew, kurt = weibull_max.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(weibull_max.ppf(0.01, c),\n",
      "        ...                 weibull_max.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, weibull_max.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='weibull_max pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = weibull_max(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = weibull_max.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], weibull_max.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = weibull_max.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    weibull_min = <scipy.stats._continuous_distns.weibull_min_gen object>\n",
      "        Weibull minimum continuous random variable.\n",
      "        \n",
      "        The Weibull Minimum Extreme Value distribution, from extreme value theory\n",
      "        (Fisher-Gnedenko theorem), is also often simply called the Weibull\n",
      "        distribution. It arises as the limiting distribution of the rescaled\n",
      "        minimum of iid random variables.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `weibull_min` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        weibull_max, numpy.random.Generator.weibull, exponweib\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `weibull_min` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = c x^{c-1} \\exp(-x^c)\n",
      "        \n",
      "        for :math:`x > 0`, :math:`c > 0`.\n",
      "        \n",
      "        `weibull_min` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        (named :math:`k` in Wikipedia article and :math:`a` in\n",
      "        ``numpy.random.weibull``).  Special shape values are :math:`c=1` and\n",
      "        :math:`c=2` where Weibull distribution reduces to the `expon` and\n",
      "        `rayleigh` distributions respectively.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``weibull_min.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``weibull_min.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        https://en.wikipedia.org/wiki/Weibull_distribution\n",
      "        \n",
      "        https://en.wikipedia.org/wiki/Fisher-Tippett-Gnedenko_theorem\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import weibull_min\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 1.79\n",
      "        >>> mean, var, skew, kurt = weibull_min.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(weibull_min.ppf(0.01, c),\n",
      "        ...                 weibull_min.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, weibull_min.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='weibull_min pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = weibull_min(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = weibull_min.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], weibull_min.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = weibull_min.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    wishart = <scipy.stats._multivariate.wishart_gen object>\n",
      "        A Wishart random variable.\n",
      "        \n",
      "        The `df` keyword specifies the degrees of freedom. The `scale` keyword\n",
      "        specifies the scale matrix, which must be symmetric and positive definite.\n",
      "        In this context, the scale matrix is often interpreted in terms of a\n",
      "        multivariate normal precision matrix (the inverse of the covariance\n",
      "        matrix). These arguments must satisfy the relationship\n",
      "        ``df > scale.ndim - 1``, but see notes on using the `rvs` method with\n",
      "        ``df < scale.ndim``.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        ``pdf(x, df, scale)``\n",
      "            Probability density function.\n",
      "        ``logpdf(x, df, scale)``\n",
      "            Log of the probability density function.\n",
      "        ``rvs(df, scale, size=1, random_state=None)``\n",
      "            Draw random samples from a Wishart distribution.\n",
      "        ``entropy()``\n",
      "            Compute the differential entropy of the Wishart distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Quantiles, with the last axis of `x` denoting the components.\n",
      "        df : int\n",
      "            Degrees of freedom, must be greater than or equal to dimension of the\n",
      "            scale matrix\n",
      "        scale : array_like\n",
      "            Symmetric positive definite scale matrix of the distribution\n",
      "        random_state : {None, int, `numpy.random.Generator`,\n",
      "                        `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "        \n",
      "        Alternatively, the object may be called (as a function) to fix the degrees\n",
      "        of freedom and scale parameters, returning a \"frozen\" Wishart random\n",
      "        variable:\n",
      "        \n",
      "        rv = wishart(df=1, scale=1)\n",
      "            - Frozen object with the same methods but holding the given\n",
      "              degrees of freedom and scale fixed.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        invwishart, chi2\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        The scale matrix `scale` must be a symmetric positive definite\n",
      "        matrix. Singular matrices, including the symmetric positive semi-definite\n",
      "        case, are not supported.\n",
      "        \n",
      "        The Wishart distribution is often denoted\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            W_p(\\nu, \\Sigma)\n",
      "        \n",
      "        where :math:`\\nu` is the degrees of freedom and :math:`\\Sigma` is the\n",
      "        :math:`p \\times p` scale matrix.\n",
      "        \n",
      "        The probability density function for `wishart` has support over positive\n",
      "        definite matrices :math:`S`; if :math:`S \\sim W_p(\\nu, \\Sigma)`, then\n",
      "        its PDF is given by:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(S) = \\frac{|S|^{\\frac{\\nu - p - 1}{2}}}{2^{ \\frac{\\nu p}{2} }\n",
      "                   |\\Sigma|^\\frac{\\nu}{2} \\Gamma_p \\left ( \\frac{\\nu}{2} \\right )}\n",
      "                   \\exp\\left( -tr(\\Sigma^{-1} S) / 2 \\right)\n",
      "        \n",
      "        If :math:`S \\sim W_p(\\nu, \\Sigma)` (Wishart) then\n",
      "        :math:`S^{-1} \\sim W_p^{-1}(\\nu, \\Sigma^{-1})` (inverse Wishart).\n",
      "        \n",
      "        If the scale matrix is 1-dimensional and equal to one, then the Wishart\n",
      "        distribution :math:`W_1(\\nu, 1)` collapses to the :math:`\\chi^2(\\nu)`\n",
      "        distribution.\n",
      "        \n",
      "        The algorithm [2]_ implemented by the `rvs` method may\n",
      "        produce numerically singular matrices with :math:`p - 1 < \\nu < p`; the\n",
      "        user may wish to check for this condition and generate replacement samples\n",
      "        as necessary.\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 0.16.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] M.L. Eaton, \"Multivariate Statistics: A Vector Space Approach\",\n",
      "               Wiley, 1983.\n",
      "        .. [2] W.B. Smith and R.R. Hocking, \"Algorithm AS 53: Wishart Variate\n",
      "               Generator\", Applied Statistics, vol. 21, pp. 341-345, 1972.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.stats import wishart, chi2\n",
      "        >>> x = np.linspace(1e-5, 8, 100)\n",
      "        >>> w = wishart.pdf(x, df=3, scale=1); w[:5]\n",
      "        array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])\n",
      "        >>> c = chi2.pdf(x, 3); c[:5]\n",
      "        array([ 0.00126156,  0.10892176,  0.14793434,  0.17400548,  0.1929669 ])\n",
      "        >>> plt.plot(x, w)\n",
      "        \n",
      "        The input quantiles can be any shape of array, as long as the last\n",
      "        axis labels the components.\n",
      "    \n",
      "    wrapcauchy = <scipy.stats._continuous_distns.wrapcauchy_gen object>\n",
      "        A wrapped Cauchy continuous random variable.\n",
      "        \n",
      "        As an instance of the `rv_continuous` class, `wrapcauchy` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(c, loc=0, scale=1, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pdf(x, c, loc=0, scale=1)\n",
      "            Probability density function.\n",
      "        logpdf(x, c, loc=0, scale=1)\n",
      "            Log of the probability density function.\n",
      "        cdf(x, c, loc=0, scale=1)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(x, c, loc=0, scale=1)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(x, c, loc=0, scale=1)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(x, c, loc=0, scale=1)\n",
      "            Log of the survival function.\n",
      "        ppf(q, c, loc=0, scale=1)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, c, loc=0, scale=1)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        moment(n, c, loc=0, scale=1)\n",
      "            Non-central moment of order n\n",
      "        stats(c, loc=0, scale=1, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(c, loc=0, scale=1)\n",
      "            (Differential) entropy of the RV.\n",
      "        fit(data)\n",
      "            Parameter estimates for generic data.\n",
      "            See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
      "            keyword arguments.\n",
      "        expect(func, args=(c,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(c, loc=0, scale=1)\n",
      "            Median of the distribution.\n",
      "        mean(c, loc=0, scale=1)\n",
      "            Mean of the distribution.\n",
      "        var(c, loc=0, scale=1)\n",
      "            Variance of the distribution.\n",
      "        std(c, loc=0, scale=1)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, c, loc=0, scale=1)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability density function for `wrapcauchy` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(x, c) = \\frac{1-c^2}{2\\pi (1+c^2 - 2c \\cos(x))}\n",
      "        \n",
      "        for :math:`0 \\le x \\le 2\\pi`, :math:`0 < c < 1`.\n",
      "        \n",
      "        `wrapcauchy` takes ``c`` as a shape parameter for :math:`c`.\n",
      "        \n",
      "        The probability density above is defined in the \"standardized\" form. To shift\n",
      "        and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
      "        Specifically, ``wrapcauchy.pdf(x, c, loc, scale)`` is identically\n",
      "        equivalent to ``wrapcauchy.pdf(y, c) / scale`` with\n",
      "        ``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
      "        does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
      "        some distributions are available in separate classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import wrapcauchy\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> c = 0.0311\n",
      "        >>> mean, var, skew, kurt = wrapcauchy.stats(c, moments='mvsk')\n",
      "        \n",
      "        Display the probability density function (``pdf``):\n",
      "        \n",
      "        >>> x = np.linspace(wrapcauchy.ppf(0.01, c),\n",
      "        ...                 wrapcauchy.ppf(0.99, c), 100)\n",
      "        >>> ax.plot(x, wrapcauchy.pdf(x, c),\n",
      "        ...        'r-', lw=5, alpha=0.6, label='wrapcauchy pdf')\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
      "        RV object holding the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pdf``:\n",
      "        \n",
      "        >>> rv = wrapcauchy(c)\n",
      "        >>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> vals = wrapcauchy.ppf([0.001, 0.5, 0.999], c)\n",
      "        >>> np.allclose([0.001, 0.5, 0.999], wrapcauchy.cdf(vals, c))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = wrapcauchy.rvs(c, size=1000)\n",
      "        \n",
      "        And compare the histogram:\n",
      "        \n",
      "        >>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    yulesimon = <scipy.stats._discrete_distns.yulesimon_gen object>\n",
      "        A Yule-Simon discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `yulesimon` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(alpha, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, alpha, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, alpha, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, alpha, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, alpha, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, alpha, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, alpha, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, alpha, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, alpha, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(alpha, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(alpha, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(alpha,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(alpha, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(alpha, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(alpha, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(alpha, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, alpha, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        The probability mass function for the `yulesimon` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k) =  \\alpha B(k, \\alpha+1)\n",
      "        \n",
      "        for :math:`k=1,2,3,...`, where :math:`\\alpha>0`.\n",
      "        Here :math:`B` refers to the `scipy.special.beta` function.\n",
      "        \n",
      "        The sampling of random variates is based on pg 553, Section 6.3 of [1]_.\n",
      "        Our notation maps to the referenced logic via :math:`\\alpha=a-1`.\n",
      "        \n",
      "        For details see the wikipedia entry [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Devroye, Luc. \"Non-uniform Random Variate Generation\",\n",
      "             (1986) Springer, New York.\n",
      "        \n",
      "        .. [2] https://en.wikipedia.org/wiki/Yule-Simon_distribution\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``yulesimon.pmf(k, alpha, loc)`` is identically\n",
      "        equivalent to ``yulesimon.pmf(k - loc, alpha)``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import yulesimon\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> alpha = 11\n",
      "        >>> mean, var, skew, kurt = yulesimon.stats(alpha, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(yulesimon.ppf(0.01, alpha),\n",
      "        ...               yulesimon.ppf(0.99, alpha))\n",
      "        >>> ax.plot(x, yulesimon.pmf(x, alpha), 'bo', ms=8, label='yulesimon pmf')\n",
      "        >>> ax.vlines(x, 0, yulesimon.pmf(x, alpha), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = yulesimon(alpha)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = yulesimon.cdf(x, alpha)\n",
      "        >>> np.allclose(x, yulesimon.ppf(prob, alpha))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = yulesimon.rvs(alpha, size=1000)\n",
      "    \n",
      "    zipf = <scipy.stats._discrete_distns.zipf_gen object>\n",
      "        A Zipf (Zeta) discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `zipf` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, a, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, a, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, a, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, a, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, a, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, a, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(a, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(a,), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(a, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(a, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(a, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zipfian\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `zipf` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k, a) = \\frac{1}{\\zeta(a) k^a}\n",
      "        \n",
      "        for :math:`k \\ge 1`, :math:`a > 1`.\n",
      "        \n",
      "        `zipf` takes :math:`a > 1` as shape parameter. :math:`\\zeta` is the\n",
      "        Riemann zeta function (`scipy.special.zeta`)\n",
      "        \n",
      "        The Zipf distribution is also known as the zeta distribution, which is\n",
      "        a special case of the Zipfian distribution (`zipfian`).\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``zipf.pmf(k, a, loc)`` is identically\n",
      "        equivalent to ``zipf.pmf(k - loc, a)``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Zeta Distribution\", Wikipedia,\n",
      "               https://en.wikipedia.org/wiki/Zeta_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import zipf\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a = 6.5\n",
      "        >>> mean, var, skew, kurt = zipf.stats(a, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(zipf.ppf(0.01, a),\n",
      "        ...               zipf.ppf(0.99, a))\n",
      "        >>> ax.plot(x, zipf.pmf(x, a), 'bo', ms=8, label='zipf pmf')\n",
      "        >>> ax.vlines(x, 0, zipf.pmf(x, a), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = zipf(a)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = zipf.cdf(x, a)\n",
      "        >>> np.allclose(x, zipf.ppf(prob, a))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = zipf.rvs(a, size=1000)\n",
      "        \n",
      "        Confirm that `zipf` is the large `n` limit of `zipfian`.\n",
      "        \n",
      "        >>> from scipy.stats import zipfian\n",
      "        >>> k = np.arange(11)\n",
      "        >>> np.allclose(zipf.pmf(k, a), zipfian.pmf(k, a, n=10000000))\n",
      "        True\n",
      "    \n",
      "    zipfian = <scipy.stats._discrete_distns.zipfian_gen object>\n",
      "        A Zipfian discrete random variable.\n",
      "        \n",
      "        As an instance of the `rv_discrete` class, `zipfian` object inherits from it\n",
      "        a collection of generic methods (see below for the full list),\n",
      "        and completes them with details specific for this particular distribution.\n",
      "        \n",
      "        Methods\n",
      "        -------\n",
      "        rvs(a, n, loc=0, size=1, random_state=None)\n",
      "            Random variates.\n",
      "        pmf(k, a, n, loc=0)\n",
      "            Probability mass function.\n",
      "        logpmf(k, a, n, loc=0)\n",
      "            Log of the probability mass function.\n",
      "        cdf(k, a, n, loc=0)\n",
      "            Cumulative distribution function.\n",
      "        logcdf(k, a, n, loc=0)\n",
      "            Log of the cumulative distribution function.\n",
      "        sf(k, a, n, loc=0)\n",
      "            Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
      "        logsf(k, a, n, loc=0)\n",
      "            Log of the survival function.\n",
      "        ppf(q, a, n, loc=0)\n",
      "            Percent point function (inverse of ``cdf`` --- percentiles).\n",
      "        isf(q, a, n, loc=0)\n",
      "            Inverse survival function (inverse of ``sf``).\n",
      "        stats(a, n, loc=0, moments='mv')\n",
      "            Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
      "        entropy(a, n, loc=0)\n",
      "            (Differential) entropy of the RV.\n",
      "        expect(func, args=(a, n), loc=0, lb=None, ub=None, conditional=False)\n",
      "            Expected value of a function (of one argument) with respect to the distribution.\n",
      "        median(a, n, loc=0)\n",
      "            Median of the distribution.\n",
      "        mean(a, n, loc=0)\n",
      "            Mean of the distribution.\n",
      "        var(a, n, loc=0)\n",
      "            Variance of the distribution.\n",
      "        std(a, n, loc=0)\n",
      "            Standard deviation of the distribution.\n",
      "        interval(alpha, a, n, loc=0)\n",
      "            Endpoints of the range that contains fraction alpha [0, 1] of the\n",
      "            distribution\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zipf\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The probability mass function for `zipfian` is:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            f(k, a, n) = \\frac{1}{H_{n,a} k^a}\n",
      "        \n",
      "        for :math:`k \\in \\{1, 2, \\dots, n-1, n\\}`, :math:`a \\ge 0`,\n",
      "        :math:`n \\in \\{1, 2, 3, \\dots\\}`.\n",
      "        \n",
      "        `zipfian` takes :math:`a` and :math:`n` as shape parameters.\n",
      "        :math:`H_{n,a}` is the :math:`n`:sup:`th` generalized harmonic\n",
      "        number of order :math:`a`.\n",
      "        \n",
      "        The Zipfian distribution reduces to the Zipf (zeta) distribution as\n",
      "        :math:`n \\rightarrow \\infty`.\n",
      "        \n",
      "        The probability mass function above is defined in the \"standardized\" form.\n",
      "        To shift distribution use the ``loc`` parameter.\n",
      "        Specifically, ``zipfian.pmf(k, a, n, loc)`` is identically\n",
      "        equivalent to ``zipfian.pmf(k - loc, a, n)``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] \"Zipf's Law\", Wikipedia, https://en.wikipedia.org/wiki/Zipf's_law\n",
      "        .. [2] Larry Leemis, \"Zipf Distribution\", Univariate Distribution\n",
      "               Relationships. http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Zipf.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.stats import zipfian\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> fig, ax = plt.subplots(1, 1)\n",
      "        \n",
      "        Calculate the first four moments:\n",
      "        \n",
      "        >>> a, n = 1.25, 10\n",
      "        >>> mean, var, skew, kurt = zipfian.stats(a, n, moments='mvsk')\n",
      "        \n",
      "        Display the probability mass function (``pmf``):\n",
      "        \n",
      "        >>> x = np.arange(zipfian.ppf(0.01, a, n),\n",
      "        ...               zipfian.ppf(0.99, a, n))\n",
      "        >>> ax.plot(x, zipfian.pmf(x, a, n), 'bo', ms=8, label='zipfian pmf')\n",
      "        >>> ax.vlines(x, 0, zipfian.pmf(x, a, n), colors='b', lw=5, alpha=0.5)\n",
      "        \n",
      "        Alternatively, the distribution object can be called (as a function)\n",
      "        to fix the shape and location. This returns a \"frozen\" RV object holding\n",
      "        the given parameters fixed.\n",
      "        \n",
      "        Freeze the distribution and display the frozen ``pmf``:\n",
      "        \n",
      "        >>> rv = zipfian(a, n)\n",
      "        >>> ax.vlines(x, 0, rv.pmf(x), colors='k', linestyles='-', lw=1,\n",
      "        ...         label='frozen pmf')\n",
      "        >>> ax.legend(loc='best', frameon=False)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Check accuracy of ``cdf`` and ``ppf``:\n",
      "        \n",
      "        >>> prob = zipfian.cdf(x, a, n)\n",
      "        >>> np.allclose(x, zipfian.ppf(prob, a, n))\n",
      "        True\n",
      "        \n",
      "        Generate random numbers:\n",
      "        \n",
      "        >>> r = zipfian.rvs(a, n, size=1000)\n",
      "        \n",
      "        Confirm that `zipfian` reduces to `zipf` for large `n`, `a > 1`.\n",
      "        \n",
      "        >>> from scipy.stats import zipf\n",
      "        >>> k = np.arange(11)\n",
      "        >>> np.allclose(zipfian.pmf(k, a=3.5, n=10000000), zipf.pmf(k, a=3.5))\n",
      "        True\n",
      "\n",
      "FILE\n",
      "    c:\\users\\amant\\anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fb1e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F_onewayBadInputSizesWarning',\n",
       " 'F_onewayConstantInputWarning',\n",
       " 'NumericalInverseHermite',\n",
       " 'PearsonRConstantInputWarning',\n",
       " 'PearsonRNearConstantInputWarning',\n",
       " 'SpearmanRConstantInputWarning',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_binned_statistic',\n",
       " '_binomtest',\n",
       " '_boost',\n",
       " '_bootstrap',\n",
       " '_common',\n",
       " '_constants',\n",
       " '_continuous_distns',\n",
       " '_crosstab',\n",
       " '_discrete_distns',\n",
       " '_distn_infrastructure',\n",
       " '_distr_params',\n",
       " '_entropy',\n",
       " '_hypotests',\n",
       " '_ksstats',\n",
       " '_mannwhitneyu',\n",
       " '_multivariate',\n",
       " '_page_trend_test',\n",
       " '_qmc',\n",
       " '_qmc_cy',\n",
       " '_relative_risk',\n",
       " '_rvs_sampling',\n",
       " '_sobol',\n",
       " '_stats',\n",
       " '_stats_mstats_common',\n",
       " '_tukeylambda_stats',\n",
       " '_wilcoxon_data',\n",
       " 'alexandergovern',\n",
       " 'alpha',\n",
       " 'anderson',\n",
       " 'anderson_ksamp',\n",
       " 'anglit',\n",
       " 'ansari',\n",
       " 'arcsine',\n",
       " 'argus',\n",
       " 'barnard_exact',\n",
       " 'bartlett',\n",
       " 'bayes_mvs',\n",
       " 'bernoulli',\n",
       " 'beta',\n",
       " 'betabinom',\n",
       " 'betaprime',\n",
       " 'biasedurn',\n",
       " 'binned_statistic',\n",
       " 'binned_statistic_2d',\n",
       " 'binned_statistic_dd',\n",
       " 'binom',\n",
       " 'binom_test',\n",
       " 'binomtest',\n",
       " 'boltzmann',\n",
       " 'bootstrap',\n",
       " 'boschloo_exact',\n",
       " 'boxcox',\n",
       " 'boxcox_llf',\n",
       " 'boxcox_normmax',\n",
       " 'boxcox_normplot',\n",
       " 'bradford',\n",
       " 'brunnermunzel',\n",
       " 'burr',\n",
       " 'burr12',\n",
       " 'cauchy',\n",
       " 'chi',\n",
       " 'chi2',\n",
       " 'chi2_contingency',\n",
       " 'chisquare',\n",
       " 'circmean',\n",
       " 'circstd',\n",
       " 'circvar',\n",
       " 'combine_pvalues',\n",
       " 'contingency',\n",
       " 'cosine',\n",
       " 'cramervonmises',\n",
       " 'cramervonmises_2samp',\n",
       " 'crystalball',\n",
       " 'cumfreq',\n",
       " 'describe',\n",
       " 'dgamma',\n",
       " 'differential_entropy',\n",
       " 'dirichlet',\n",
       " 'distributions',\n",
       " 'dlaplace',\n",
       " 'dweibull',\n",
       " 'energy_distance',\n",
       " 'entropy',\n",
       " 'epps_singleton_2samp',\n",
       " 'erlang',\n",
       " 'expon',\n",
       " 'exponnorm',\n",
       " 'exponpow',\n",
       " 'exponweib',\n",
       " 'f',\n",
       " 'f_oneway',\n",
       " 'fatiguelife',\n",
       " 'find_repeats',\n",
       " 'fisher_exact',\n",
       " 'fisk',\n",
       " 'fligner',\n",
       " 'foldcauchy',\n",
       " 'foldnorm',\n",
       " 'friedmanchisquare',\n",
       " 'gamma',\n",
       " 'gausshyper',\n",
       " 'gaussian_kde',\n",
       " 'genexpon',\n",
       " 'genextreme',\n",
       " 'gengamma',\n",
       " 'genhalflogistic',\n",
       " 'genhyperbolic',\n",
       " 'geninvgauss',\n",
       " 'genlogistic',\n",
       " 'gennorm',\n",
       " 'genpareto',\n",
       " 'geom',\n",
       " 'gilbrat',\n",
       " 'gmean',\n",
       " 'gompertz',\n",
       " 'gstd',\n",
       " 'gumbel_l',\n",
       " 'gumbel_r',\n",
       " 'halfcauchy',\n",
       " 'halfgennorm',\n",
       " 'halflogistic',\n",
       " 'halfnorm',\n",
       " 'hmean',\n",
       " 'hypergeom',\n",
       " 'hypsecant',\n",
       " 'invgamma',\n",
       " 'invgauss',\n",
       " 'invweibull',\n",
       " 'invwishart',\n",
       " 'iqr',\n",
       " 'itemfreq',\n",
       " 'jarque_bera',\n",
       " 'johnsonsb',\n",
       " 'johnsonsu',\n",
       " 'kappa3',\n",
       " 'kappa4',\n",
       " 'kde',\n",
       " 'kendalltau',\n",
       " 'kruskal',\n",
       " 'ks_1samp',\n",
       " 'ks_2samp',\n",
       " 'ksone',\n",
       " 'kstat',\n",
       " 'kstatvar',\n",
       " 'kstest',\n",
       " 'kstwo',\n",
       " 'kstwobign',\n",
       " 'kurtosis',\n",
       " 'kurtosistest',\n",
       " 'laplace',\n",
       " 'laplace_asymmetric',\n",
       " 'levene',\n",
       " 'levy',\n",
       " 'levy_l',\n",
       " 'levy_stable',\n",
       " 'linregress',\n",
       " 'loggamma',\n",
       " 'logistic',\n",
       " 'loglaplace',\n",
       " 'lognorm',\n",
       " 'logser',\n",
       " 'loguniform',\n",
       " 'lomax',\n",
       " 'mannwhitneyu',\n",
       " 'matrix_normal',\n",
       " 'maxwell',\n",
       " 'median_abs_deviation',\n",
       " 'median_absolute_deviation',\n",
       " 'median_test',\n",
       " 'mielke',\n",
       " 'mode',\n",
       " 'moment',\n",
       " 'mood',\n",
       " 'morestats',\n",
       " 'moyal',\n",
       " 'mstats',\n",
       " 'mstats_basic',\n",
       " 'mstats_extras',\n",
       " 'multinomial',\n",
       " 'multiscale_graphcorr',\n",
       " 'multivariate_hypergeom',\n",
       " 'multivariate_normal',\n",
       " 'multivariate_t',\n",
       " 'mvn',\n",
       " 'mvsdist',\n",
       " 'nakagami',\n",
       " 'nbinom',\n",
       " 'ncf',\n",
       " 'nchypergeom_fisher',\n",
       " 'nchypergeom_wallenius',\n",
       " 'nct',\n",
       " 'ncx2',\n",
       " 'nhypergeom',\n",
       " 'norm',\n",
       " 'normaltest',\n",
       " 'norminvgauss',\n",
       " 'obrientransform',\n",
       " 'ortho_group',\n",
       " 'page_trend_test',\n",
       " 'pareto',\n",
       " 'pearson3',\n",
       " 'pearsonr',\n",
       " 'percentileofscore',\n",
       " 'planck',\n",
       " 'pointbiserialr',\n",
       " 'poisson',\n",
       " 'power_divergence',\n",
       " 'powerlaw',\n",
       " 'powerlognorm',\n",
       " 'powernorm',\n",
       " 'ppcc_max',\n",
       " 'ppcc_plot',\n",
       " 'probplot',\n",
       " 'qmc',\n",
       " 'randint',\n",
       " 'random_correlation',\n",
       " 'rankdata',\n",
       " 'ranksums',\n",
       " 'rayleigh',\n",
       " 'rdist',\n",
       " 'recipinvgauss',\n",
       " 'reciprocal',\n",
       " 'relfreq',\n",
       " 'rice',\n",
       " 'rv_continuous',\n",
       " 'rv_discrete',\n",
       " 'rv_histogram',\n",
       " 'rvs_ratio_uniforms',\n",
       " 'scoreatpercentile',\n",
       " 'sem',\n",
       " 'semicircular',\n",
       " 'shapiro',\n",
       " 'siegelslopes',\n",
       " 'sigmaclip',\n",
       " 'skellam',\n",
       " 'skew',\n",
       " 'skewcauchy',\n",
       " 'skewnorm',\n",
       " 'skewtest',\n",
       " 'somersd',\n",
       " 'spearmanr',\n",
       " 'special_ortho_group',\n",
       " 'statlib',\n",
       " 'stats',\n",
       " 'studentized_range',\n",
       " 't',\n",
       " 'test',\n",
       " 'theilslopes',\n",
       " 'tiecorrect',\n",
       " 'tmax',\n",
       " 'tmean',\n",
       " 'tmin',\n",
       " 'trapezoid',\n",
       " 'trapz',\n",
       " 'triang',\n",
       " 'trim1',\n",
       " 'trim_mean',\n",
       " 'trimboth',\n",
       " 'truncexpon',\n",
       " 'truncnorm',\n",
       " 'tsem',\n",
       " 'tstd',\n",
       " 'ttest_1samp',\n",
       " 'ttest_ind',\n",
       " 'ttest_ind_from_stats',\n",
       " 'ttest_rel',\n",
       " 'tukeylambda',\n",
       " 'tvar',\n",
       " 'uniform',\n",
       " 'unitary_group',\n",
       " 'variation',\n",
       " 'vonmises',\n",
       " 'vonmises_line',\n",
       " 'wald',\n",
       " 'wasserstein_distance',\n",
       " 'weibull_max',\n",
       " 'weibull_min',\n",
       " 'weightedtau',\n",
       " 'wilcoxon',\n",
       " 'wishart',\n",
       " 'wrapcauchy',\n",
       " 'yeojohnson',\n",
       " 'yeojohnson_llf',\n",
       " 'yeojohnson_normmax',\n",
       " 'yeojohnson_normplot',\n",
       " 'yulesimon',\n",
       " 'zipf',\n",
       " 'zipfian',\n",
       " 'zmap',\n",
       " 'zscore']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09eb6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b691d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SpecialFunctionError',\n",
       " 'SpecialFunctionWarning',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_basic',\n",
       " '_comb',\n",
       " '_ellip_harm',\n",
       " '_ellip_harm_2',\n",
       " '_lambertw',\n",
       " '_logsumexp',\n",
       " '_spherical_bessel',\n",
       " '_ufuncs',\n",
       " '_ufuncs_cxx',\n",
       " 'agm',\n",
       " 'ai_zeros',\n",
       " 'airy',\n",
       " 'airye',\n",
       " 'assoc_laguerre',\n",
       " 'bdtr',\n",
       " 'bdtrc',\n",
       " 'bdtri',\n",
       " 'bdtrik',\n",
       " 'bdtrin',\n",
       " 'bei',\n",
       " 'bei_zeros',\n",
       " 'beip',\n",
       " 'beip_zeros',\n",
       " 'ber',\n",
       " 'ber_zeros',\n",
       " 'bernoulli',\n",
       " 'berp',\n",
       " 'berp_zeros',\n",
       " 'besselpoly',\n",
       " 'beta',\n",
       " 'betainc',\n",
       " 'betaincinv',\n",
       " 'betaln',\n",
       " 'bi_zeros',\n",
       " 'binom',\n",
       " 'boxcox',\n",
       " 'boxcox1p',\n",
       " 'btdtr',\n",
       " 'btdtri',\n",
       " 'btdtria',\n",
       " 'btdtrib',\n",
       " 'c_roots',\n",
       " 'cbrt',\n",
       " 'cg_roots',\n",
       " 'chdtr',\n",
       " 'chdtrc',\n",
       " 'chdtri',\n",
       " 'chdtriv',\n",
       " 'chebyc',\n",
       " 'chebys',\n",
       " 'chebyt',\n",
       " 'chebyu',\n",
       " 'chndtr',\n",
       " 'chndtridf',\n",
       " 'chndtrinc',\n",
       " 'chndtrix',\n",
       " 'clpmn',\n",
       " 'comb',\n",
       " 'cosdg',\n",
       " 'cosm1',\n",
       " 'cotdg',\n",
       " 'cython_special',\n",
       " 'dawsn',\n",
       " 'digamma',\n",
       " 'diric',\n",
       " 'ellip_harm',\n",
       " 'ellip_harm_2',\n",
       " 'ellip_normal',\n",
       " 'ellipe',\n",
       " 'ellipeinc',\n",
       " 'ellipj',\n",
       " 'ellipk',\n",
       " 'ellipkinc',\n",
       " 'ellipkm1',\n",
       " 'entr',\n",
       " 'erf',\n",
       " 'erf_zeros',\n",
       " 'erfc',\n",
       " 'erfcinv',\n",
       " 'erfcx',\n",
       " 'erfi',\n",
       " 'erfinv',\n",
       " 'errstate',\n",
       " 'euler',\n",
       " 'eval_chebyc',\n",
       " 'eval_chebys',\n",
       " 'eval_chebyt',\n",
       " 'eval_chebyu',\n",
       " 'eval_gegenbauer',\n",
       " 'eval_genlaguerre',\n",
       " 'eval_hermite',\n",
       " 'eval_hermitenorm',\n",
       " 'eval_jacobi',\n",
       " 'eval_laguerre',\n",
       " 'eval_legendre',\n",
       " 'eval_sh_chebyt',\n",
       " 'eval_sh_chebyu',\n",
       " 'eval_sh_jacobi',\n",
       " 'eval_sh_legendre',\n",
       " 'exp1',\n",
       " 'exp10',\n",
       " 'exp2',\n",
       " 'expi',\n",
       " 'expit',\n",
       " 'expm1',\n",
       " 'expn',\n",
       " 'exprel',\n",
       " 'factorial',\n",
       " 'factorial2',\n",
       " 'factorialk',\n",
       " 'fdtr',\n",
       " 'fdtrc',\n",
       " 'fdtri',\n",
       " 'fdtridfd',\n",
       " 'fresnel',\n",
       " 'fresnel_zeros',\n",
       " 'fresnelc_zeros',\n",
       " 'fresnels_zeros',\n",
       " 'gamma',\n",
       " 'gammainc',\n",
       " 'gammaincc',\n",
       " 'gammainccinv',\n",
       " 'gammaincinv',\n",
       " 'gammaln',\n",
       " 'gammasgn',\n",
       " 'gdtr',\n",
       " 'gdtrc',\n",
       " 'gdtria',\n",
       " 'gdtrib',\n",
       " 'gdtrix',\n",
       " 'gegenbauer',\n",
       " 'genlaguerre',\n",
       " 'geterr',\n",
       " 'h1vp',\n",
       " 'h2vp',\n",
       " 'h_roots',\n",
       " 'hankel1',\n",
       " 'hankel1e',\n",
       " 'hankel2',\n",
       " 'hankel2e',\n",
       " 'he_roots',\n",
       " 'hermite',\n",
       " 'hermitenorm',\n",
       " 'huber',\n",
       " 'hyp0f1',\n",
       " 'hyp1f1',\n",
       " 'hyp2f1',\n",
       " 'hyperu',\n",
       " 'i0',\n",
       " 'i0e',\n",
       " 'i1',\n",
       " 'i1e',\n",
       " 'inv_boxcox',\n",
       " 'inv_boxcox1p',\n",
       " 'it2i0k0',\n",
       " 'it2j0y0',\n",
       " 'it2struve0',\n",
       " 'itairy',\n",
       " 'iti0k0',\n",
       " 'itj0y0',\n",
       " 'itmodstruve0',\n",
       " 'itstruve0',\n",
       " 'iv',\n",
       " 'ive',\n",
       " 'ivp',\n",
       " 'j0',\n",
       " 'j1',\n",
       " 'j_roots',\n",
       " 'jacobi',\n",
       " 'jn',\n",
       " 'jn_zeros',\n",
       " 'jnjnp_zeros',\n",
       " 'jnp_zeros',\n",
       " 'jnyn_zeros',\n",
       " 'js_roots',\n",
       " 'jv',\n",
       " 'jve',\n",
       " 'jvp',\n",
       " 'k0',\n",
       " 'k0e',\n",
       " 'k1',\n",
       " 'k1e',\n",
       " 'kei',\n",
       " 'kei_zeros',\n",
       " 'keip',\n",
       " 'keip_zeros',\n",
       " 'kelvin',\n",
       " 'kelvin_zeros',\n",
       " 'ker',\n",
       " 'ker_zeros',\n",
       " 'kerp',\n",
       " 'kerp_zeros',\n",
       " 'kl_div',\n",
       " 'kn',\n",
       " 'kolmogi',\n",
       " 'kolmogorov',\n",
       " 'kv',\n",
       " 'kve',\n",
       " 'kvp',\n",
       " 'l_roots',\n",
       " 'la_roots',\n",
       " 'laguerre',\n",
       " 'lambertw',\n",
       " 'legendre',\n",
       " 'lmbda',\n",
       " 'log1p',\n",
       " 'log_ndtr',\n",
       " 'log_softmax',\n",
       " 'loggamma',\n",
       " 'logit',\n",
       " 'logsumexp',\n",
       " 'lpmn',\n",
       " 'lpmv',\n",
       " 'lpn',\n",
       " 'lqmn',\n",
       " 'lqn',\n",
       " 'mathieu_a',\n",
       " 'mathieu_b',\n",
       " 'mathieu_cem',\n",
       " 'mathieu_even_coef',\n",
       " 'mathieu_modcem1',\n",
       " 'mathieu_modcem2',\n",
       " 'mathieu_modsem1',\n",
       " 'mathieu_modsem2',\n",
       " 'mathieu_odd_coef',\n",
       " 'mathieu_sem',\n",
       " 'modfresnelm',\n",
       " 'modfresnelp',\n",
       " 'modstruve',\n",
       " 'multigammaln',\n",
       " 'nbdtr',\n",
       " 'nbdtrc',\n",
       " 'nbdtri',\n",
       " 'nbdtrik',\n",
       " 'nbdtrin',\n",
       " 'ncfdtr',\n",
       " 'ncfdtri',\n",
       " 'ncfdtridfd',\n",
       " 'ncfdtridfn',\n",
       " 'ncfdtrinc',\n",
       " 'nctdtr',\n",
       " 'nctdtridf',\n",
       " 'nctdtrinc',\n",
       " 'nctdtrit',\n",
       " 'ndtr',\n",
       " 'ndtri',\n",
       " 'ndtri_exp',\n",
       " 'nrdtrimn',\n",
       " 'nrdtrisd',\n",
       " 'obl_ang1',\n",
       " 'obl_ang1_cv',\n",
       " 'obl_cv',\n",
       " 'obl_cv_seq',\n",
       " 'obl_rad1',\n",
       " 'obl_rad1_cv',\n",
       " 'obl_rad2',\n",
       " 'obl_rad2_cv',\n",
       " 'orthogonal',\n",
       " 'owens_t',\n",
       " 'p_roots',\n",
       " 'pbdn_seq',\n",
       " 'pbdv',\n",
       " 'pbdv_seq',\n",
       " 'pbvv',\n",
       " 'pbvv_seq',\n",
       " 'pbwa',\n",
       " 'pdtr',\n",
       " 'pdtrc',\n",
       " 'pdtri',\n",
       " 'pdtrik',\n",
       " 'perm',\n",
       " 'poch',\n",
       " 'polygamma',\n",
       " 'pro_ang1',\n",
       " 'pro_ang1_cv',\n",
       " 'pro_cv',\n",
       " 'pro_cv_seq',\n",
       " 'pro_rad1',\n",
       " 'pro_rad1_cv',\n",
       " 'pro_rad2',\n",
       " 'pro_rad2_cv',\n",
       " 'ps_roots',\n",
       " 'pseudo_huber',\n",
       " 'psi',\n",
       " 'radian',\n",
       " 'rel_entr',\n",
       " 'rgamma',\n",
       " 'riccati_jn',\n",
       " 'riccati_yn',\n",
       " 'roots_chebyc',\n",
       " 'roots_chebys',\n",
       " 'roots_chebyt',\n",
       " 'roots_chebyu',\n",
       " 'roots_gegenbauer',\n",
       " 'roots_genlaguerre',\n",
       " 'roots_hermite',\n",
       " 'roots_hermitenorm',\n",
       " 'roots_jacobi',\n",
       " 'roots_laguerre',\n",
       " 'roots_legendre',\n",
       " 'roots_sh_chebyt',\n",
       " 'roots_sh_chebyu',\n",
       " 'roots_sh_jacobi',\n",
       " 'roots_sh_legendre',\n",
       " 'round',\n",
       " 's_roots',\n",
       " 'seterr',\n",
       " 'sf_error',\n",
       " 'sh_chebyt',\n",
       " 'sh_chebyu',\n",
       " 'sh_jacobi',\n",
       " 'sh_legendre',\n",
       " 'shichi',\n",
       " 'sici',\n",
       " 'sinc',\n",
       " 'sindg',\n",
       " 'smirnov',\n",
       " 'smirnovi',\n",
       " 'softmax',\n",
       " 'specfun',\n",
       " 'spence',\n",
       " 'spfun_stats',\n",
       " 'sph_harm',\n",
       " 'spherical_in',\n",
       " 'spherical_jn',\n",
       " 'spherical_kn',\n",
       " 'spherical_yn',\n",
       " 'stdtr',\n",
       " 'stdtridf',\n",
       " 'stdtrit',\n",
       " 'struve',\n",
       " 't_roots',\n",
       " 'tandg',\n",
       " 'test',\n",
       " 'tklmbda',\n",
       " 'ts_roots',\n",
       " 'u_roots',\n",
       " 'us_roots',\n",
       " 'voigt_profile',\n",
       " 'wofz',\n",
       " 'wright_bessel',\n",
       " 'wrightomega',\n",
       " 'xlog1py',\n",
       " 'xlogy',\n",
       " 'y0',\n",
       " 'y0_zeros',\n",
       " 'y1',\n",
       " 'y1_zeros',\n",
       " 'y1p_zeros',\n",
       " 'yn',\n",
       " 'yn_zeros',\n",
       " 'ynp_zeros',\n",
       " 'yv',\n",
       " 'yve',\n",
       " 'yvp',\n",
       " 'zeta',\n",
       " 'zetac']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abbdb1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.special in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.special\n",
      "\n",
      "DESCRIPTION\n",
      "    ========================================\n",
      "    Special functions (:mod:`scipy.special`)\n",
      "    ========================================\n",
      "    \n",
      "    .. currentmodule:: scipy.special\n",
      "    \n",
      "    Nearly all of the functions below are universal functions and follow\n",
      "    broadcasting and automatic array-looping rules.\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "       `scipy.special.cython_special` -- Typed Cython versions of special functions\n",
      "    \n",
      "    \n",
      "    Error handling\n",
      "    ==============\n",
      "    \n",
      "    Errors are handled by returning NaNs or other appropriate values.\n",
      "    Some of the special function routines can emit warnings or raise\n",
      "    exceptions when an error occurs. By default this is disabled; to\n",
      "    query and control the current error handling state the following\n",
      "    functions are provided.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       geterr                 -- Get the current way of handling special-function errors.\n",
      "       seterr                 -- Set how special-function errors are handled.\n",
      "       errstate               -- Context manager for special-function error handling.\n",
      "       SpecialFunctionWarning -- Warning that can be emitted by special functions.\n",
      "       SpecialFunctionError   -- Exception that can be raised by special functions.\n",
      "    \n",
      "    Available functions\n",
      "    ===================\n",
      "    \n",
      "    Airy functions\n",
      "    --------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       airy     -- Airy functions and their derivatives.\n",
      "       airye    -- Exponentially scaled Airy functions and their derivatives.\n",
      "       ai_zeros -- Compute `nt` zeros and values of the Airy function Ai and its derivative.\n",
      "       bi_zeros -- Compute `nt` zeros and values of the Airy function Bi and its derivative.\n",
      "       itairy   -- Integrals of Airy functions\n",
      "    \n",
      "    \n",
      "    Elliptic functions and integrals\n",
      "    --------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ellipj    -- Jacobian elliptic functions.\n",
      "       ellipk    -- Complete elliptic integral of the first kind.\n",
      "       ellipkm1  -- Complete elliptic integral of the first kind around `m` = 1.\n",
      "       ellipkinc -- Incomplete elliptic integral of the first kind.\n",
      "       ellipe    -- Complete elliptic integral of the second kind.\n",
      "       ellipeinc -- Incomplete elliptic integral of the second kind.\n",
      "    \n",
      "    Bessel functions\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       jv            -- Bessel function of the first kind of real order and                     complex argument.\n",
      "       jve           -- Exponentially scaled Bessel function of order `v`.\n",
      "       yn            -- Bessel function of the second kind of integer order and                     real argument.\n",
      "       yv            -- Bessel function of the second kind of real order and                     complex argument.\n",
      "       yve           -- Exponentially scaled Bessel function of the second kind                     of real order.\n",
      "       kn            -- Modified Bessel function of the second kind of integer                     order `n`\n",
      "       kv            -- Modified Bessel function of the second kind of real order                     `v`\n",
      "       kve           -- Exponentially scaled modified Bessel function of the                     second kind.\n",
      "       iv            -- Modified Bessel function of the first kind of real order.\n",
      "       ive           -- Exponentially scaled modified Bessel function of the                     first kind.\n",
      "       hankel1       -- Hankel function of the first kind.\n",
      "       hankel1e      -- Exponentially scaled Hankel function of the first kind.\n",
      "       hankel2       -- Hankel function of the second kind.\n",
      "       hankel2e      -- Exponentially scaled Hankel function of the second kind.\n",
      "       wright_bessel -- Wright's generalized Bessel function.\n",
      "    \n",
      "    The following is not a universal function:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       lmbda -- Jahnke-Emden Lambda function, Lambdav(x).\n",
      "    \n",
      "    Zeros of Bessel functions\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    These are not universal functions:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       jnjnp_zeros -- Compute zeros of integer-order Bessel functions Jn and Jn'.\n",
      "       jnyn_zeros  -- Compute nt zeros of Bessel functions Jn(x), Jn'(x), Yn(x), and Yn'(x).\n",
      "       jn_zeros    -- Compute zeros of integer-order Bessel function Jn(x).\n",
      "       jnp_zeros   -- Compute zeros of integer-order Bessel function derivative Jn'(x).\n",
      "       yn_zeros    -- Compute zeros of integer-order Bessel function Yn(x).\n",
      "       ynp_zeros   -- Compute zeros of integer-order Bessel function derivative Yn'(x).\n",
      "       y0_zeros    -- Compute nt zeros of Bessel function Y0(z), and derivative at each zero.\n",
      "       y1_zeros    -- Compute nt zeros of Bessel function Y1(z), and derivative at each zero.\n",
      "       y1p_zeros   -- Compute nt zeros of Bessel derivative Y1'(z), and value at each zero.\n",
      "    \n",
      "    Faster versions of common Bessel functions\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       j0  -- Bessel function of the first kind of order 0.\n",
      "       j1  -- Bessel function of the first kind of order 1.\n",
      "       y0  -- Bessel function of the second kind of order 0.\n",
      "       y1  -- Bessel function of the second kind of order 1.\n",
      "       i0  -- Modified Bessel function of order 0.\n",
      "       i0e -- Exponentially scaled modified Bessel function of order 0.\n",
      "       i1  -- Modified Bessel function of order 1.\n",
      "       i1e -- Exponentially scaled modified Bessel function of order 1.\n",
      "       k0  -- Modified Bessel function of the second kind of order 0, :math:`K_0`.\n",
      "       k0e -- Exponentially scaled modified Bessel function K of order 0\n",
      "       k1  -- Modified Bessel function of the second kind of order 1, :math:`K_1(x)`.\n",
      "       k1e -- Exponentially scaled modified Bessel function K of order 1.\n",
      "    \n",
      "    Integrals of Bessel functions\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       itj0y0     -- Integrals of Bessel functions of order 0.\n",
      "       it2j0y0    -- Integrals related to Bessel functions of order 0.\n",
      "       iti0k0     -- Integrals of modified Bessel functions of order 0.\n",
      "       it2i0k0    -- Integrals related to modified Bessel functions of order 0.\n",
      "       besselpoly -- Weighted integral of a Bessel function.\n",
      "    \n",
      "    Derivatives of Bessel functions\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       jvp  -- Compute nth derivative of Bessel function Jv(z) with respect to `z`.\n",
      "       yvp  -- Compute nth derivative of Bessel function Yv(z) with respect to `z`.\n",
      "       kvp  -- Compute nth derivative of real-order modified Bessel function Kv(z)\n",
      "       ivp  -- Compute nth derivative of modified Bessel function Iv(z) with respect to `z`.\n",
      "       h1vp -- Compute nth derivative of Hankel function H1v(z) with respect to `z`.\n",
      "       h2vp -- Compute nth derivative of Hankel function H2v(z) with respect to `z`.\n",
      "    \n",
      "    Spherical Bessel functions\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       spherical_jn -- Spherical Bessel function of the first kind or its derivative.\n",
      "       spherical_yn -- Spherical Bessel function of the second kind or its derivative.\n",
      "       spherical_in -- Modified spherical Bessel function of the first kind or its derivative.\n",
      "       spherical_kn -- Modified spherical Bessel function of the second kind or its derivative.\n",
      "    \n",
      "    Riccati-Bessel functions\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    These are not universal functions:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       riccati_jn -- Compute Ricatti-Bessel function of the first kind and its derivative.\n",
      "       riccati_yn -- Compute Ricatti-Bessel function of the second kind and its derivative.\n",
      "    \n",
      "    Struve functions\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       struve       -- Struve function.\n",
      "       modstruve    -- Modified Struve function.\n",
      "       itstruve0    -- Integral of the Struve function of order 0.\n",
      "       it2struve0   -- Integral related to the Struve function of order 0.\n",
      "       itmodstruve0 -- Integral of the modified Struve function of order 0.\n",
      "    \n",
      "    \n",
      "    Raw statistical functions\n",
      "    -------------------------\n",
      "    \n",
      "    .. seealso:: :mod:`scipy.stats`: Friendly versions of these functions.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       bdtr         -- Binomial distribution cumulative distribution function.\n",
      "       bdtrc        -- Binomial distribution survival function.\n",
      "       bdtri        -- Inverse function to `bdtr` with respect to `p`.\n",
      "       bdtrik       -- Inverse function to `bdtr` with respect to `k`.\n",
      "       bdtrin       -- Inverse function to `bdtr` with respect to `n`.\n",
      "       btdtr        -- Cumulative distribution function of the beta distribution.\n",
      "       btdtri       -- The `p`-th quantile of the beta distribution.\n",
      "       btdtria      -- Inverse of `btdtr` with respect to `a`.\n",
      "       btdtrib      -- btdtria(a, p, x).\n",
      "       fdtr         -- F cumulative distribution function.\n",
      "       fdtrc        -- F survival function.\n",
      "       fdtri        -- The `p`-th quantile of the F-distribution.\n",
      "       fdtridfd     -- Inverse to `fdtr` vs dfd.\n",
      "       gdtr         -- Gamma distribution cumulative distribution function.\n",
      "       gdtrc        -- Gamma distribution survival function.\n",
      "       gdtria       -- Inverse of `gdtr` vs a.\n",
      "       gdtrib       -- Inverse of `gdtr` vs b.\n",
      "       gdtrix       -- Inverse of `gdtr` vs x.\n",
      "       nbdtr        -- Negative binomial cumulative distribution function.\n",
      "       nbdtrc       -- Negative binomial survival function.\n",
      "       nbdtri       -- Inverse of `nbdtr` vs `p`.\n",
      "       nbdtrik      -- Inverse of `nbdtr` vs `k`.\n",
      "       nbdtrin      -- Inverse of `nbdtr` vs `n`.\n",
      "       ncfdtr       -- Cumulative distribution function of the non-central F distribution.\n",
      "       ncfdtridfd   -- Calculate degrees of freedom (denominator) for the noncentral F-distribution.\n",
      "       ncfdtridfn   -- Calculate degrees of freedom (numerator) for the noncentral F-distribution.\n",
      "       ncfdtri      -- Inverse cumulative distribution function of the non-central F distribution.\n",
      "       ncfdtrinc    -- Calculate non-centrality parameter for non-central F distribution.\n",
      "       nctdtr       -- Cumulative distribution function of the non-central `t` distribution.\n",
      "       nctdtridf    -- Calculate degrees of freedom for non-central t distribution.\n",
      "       nctdtrit     -- Inverse cumulative distribution function of the non-central t distribution.\n",
      "       nctdtrinc    -- Calculate non-centrality parameter for non-central t distribution.\n",
      "       nrdtrimn     -- Calculate mean of normal distribution given other params.\n",
      "       nrdtrisd     -- Calculate standard deviation of normal distribution given other params.\n",
      "       pdtr         -- Poisson cumulative distribution function.\n",
      "       pdtrc        -- Poisson survival function.\n",
      "       pdtri        -- Inverse to `pdtr` vs m.\n",
      "       pdtrik       -- Inverse to `pdtr` vs k.\n",
      "       stdtr        -- Student t distribution cumulative distribution function.\n",
      "       stdtridf     -- Inverse of `stdtr` vs df.\n",
      "       stdtrit      -- Inverse of `stdtr` vs `t`.\n",
      "       chdtr        -- Chi square cumulative distribution function.\n",
      "       chdtrc       -- Chi square survival function.\n",
      "       chdtri       -- Inverse to `chdtrc`.\n",
      "       chdtriv      -- Inverse to `chdtr` vs `v`.\n",
      "       ndtr         -- Gaussian cumulative distribution function.\n",
      "       log_ndtr     -- Logarithm of Gaussian cumulative distribution function.\n",
      "       ndtri        -- Inverse of `ndtr` vs x.\n",
      "       ndtri_exp    -- Inverse of `log_ndtr` vs x.\n",
      "       chndtr       -- Non-central chi square cumulative distribution function.\n",
      "       chndtridf    -- Inverse to `chndtr` vs `df`.\n",
      "       chndtrinc    -- Inverse to `chndtr` vs `nc`.\n",
      "       chndtrix     -- Inverse to `chndtr` vs `x`.\n",
      "       smirnov      -- Kolmogorov-Smirnov complementary cumulative distribution function.\n",
      "       smirnovi     -- Inverse to `smirnov`.\n",
      "       kolmogorov   -- Complementary cumulative distribution function of Kolmogorov distribution.\n",
      "       kolmogi      -- Inverse function to `kolmogorov`.\n",
      "       tklmbda      -- Tukey-Lambda cumulative distribution function.\n",
      "       logit        -- Logit ufunc for ndarrays.\n",
      "       expit        -- Expit ufunc for ndarrays.\n",
      "       boxcox       -- Compute the Box-Cox transformation.\n",
      "       boxcox1p     -- Compute the Box-Cox transformation of 1 + `x`.\n",
      "       inv_boxcox   -- Compute the inverse of the Box-Cox transformation.\n",
      "       inv_boxcox1p -- Compute the inverse of the Box-Cox transformation.\n",
      "       owens_t      -- Owen's T Function.\n",
      "    \n",
      "    \n",
      "    Information Theory functions\n",
      "    ----------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       entr         -- Elementwise function for computing entropy.\n",
      "       rel_entr     -- Elementwise function for computing relative entropy.\n",
      "       kl_div       -- Elementwise function for computing Kullback-Leibler divergence.\n",
      "       huber        -- Huber loss function.\n",
      "       pseudo_huber -- Pseudo-Huber loss function.\n",
      "    \n",
      "    \n",
      "    Gamma and related functions\n",
      "    ---------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       gamma        -- Gamma function.\n",
      "       gammaln      -- Logarithm of the absolute value of the Gamma function for real inputs.\n",
      "       loggamma     -- Principal branch of the logarithm of the Gamma function.\n",
      "       gammasgn     -- Sign of the gamma function.\n",
      "       gammainc     -- Regularized lower incomplete gamma function.\n",
      "       gammaincinv  -- Inverse to `gammainc`.\n",
      "       gammaincc    -- Regularized upper incomplete gamma function.\n",
      "       gammainccinv -- Inverse to `gammaincc`.\n",
      "       beta         -- Beta function.\n",
      "       betaln       -- Natural logarithm of absolute value of beta function.\n",
      "       betainc      -- Incomplete beta integral.\n",
      "       betaincinv   -- Inverse function to beta integral.\n",
      "       psi          -- The digamma function.\n",
      "       rgamma       -- Gamma function inverted.\n",
      "       polygamma    -- Polygamma function n.\n",
      "       multigammaln -- Returns the log of multivariate gamma, also sometimes called the generalized gamma.\n",
      "       digamma      -- psi(x[, out]).\n",
      "       poch         -- Rising factorial (z)_m.\n",
      "    \n",
      "    \n",
      "    Error function and Fresnel integrals\n",
      "    ------------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       erf           -- Returns the error function of complex argument.\n",
      "       erfc          -- Complementary error function, ``1 - erf(x)``.\n",
      "       erfcx         -- Scaled complementary error function, ``exp(x**2) * erfc(x)``.\n",
      "       erfi          -- Imaginary error function, ``-i erf(i z)``.\n",
      "       erfinv        -- Inverse function for erf.\n",
      "       erfcinv       -- Inverse function for erfc.\n",
      "       wofz          -- Faddeeva function.\n",
      "       dawsn         -- Dawson's integral.\n",
      "       fresnel       -- Fresnel sin and cos integrals.\n",
      "       fresnel_zeros -- Compute nt complex zeros of sine and cosine Fresnel integrals S(z) and C(z).\n",
      "       modfresnelp   -- Modified Fresnel positive integrals.\n",
      "       modfresnelm   -- Modified Fresnel negative integrals.\n",
      "       voigt_profile -- Voigt profile.\n",
      "    \n",
      "    These are not universal functions:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       erf_zeros      -- Compute nt complex zeros of error function erf(z).\n",
      "       fresnelc_zeros -- Compute nt complex zeros of cosine Fresnel integral C(z).\n",
      "       fresnels_zeros -- Compute nt complex zeros of sine Fresnel integral S(z).\n",
      "    \n",
      "    Legendre functions\n",
      "    ------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       lpmv     -- Associated Legendre function of integer order and real degree.\n",
      "       sph_harm -- Compute spherical harmonics.\n",
      "    \n",
      "    These are not universal functions:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       clpmn -- Associated Legendre function of the first kind for complex arguments.\n",
      "       lpn   -- Legendre function of the first kind.\n",
      "       lqn   -- Legendre function of the second kind.\n",
      "       lpmn  -- Sequence of associated Legendre functions of the first kind.\n",
      "       lqmn  -- Sequence of associated Legendre functions of the second kind.\n",
      "    \n",
      "    Ellipsoidal harmonics\n",
      "    ---------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ellip_harm   -- Ellipsoidal harmonic functions E^p_n(l).\n",
      "       ellip_harm_2 -- Ellipsoidal harmonic functions F^p_n(l).\n",
      "       ellip_normal -- Ellipsoidal harmonic normalization constants gamma^p_n.\n",
      "    \n",
      "    Orthogonal polynomials\n",
      "    ----------------------\n",
      "    \n",
      "    The following functions evaluate values of orthogonal polynomials:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       assoc_laguerre   -- Compute the generalized (associated) Laguerre polynomial of degree n and order k.\n",
      "       eval_legendre    -- Evaluate Legendre polynomial at a point.\n",
      "       eval_chebyt      -- Evaluate Chebyshev polynomial of the first kind at a point.\n",
      "       eval_chebyu      -- Evaluate Chebyshev polynomial of the second kind at a point.\n",
      "       eval_chebyc      -- Evaluate Chebyshev polynomial of the first kind on [-2, 2] at a point.\n",
      "       eval_chebys      -- Evaluate Chebyshev polynomial of the second kind on [-2, 2] at a point.\n",
      "       eval_jacobi      -- Evaluate Jacobi polynomial at a point.\n",
      "       eval_laguerre    -- Evaluate Laguerre polynomial at a point.\n",
      "       eval_genlaguerre -- Evaluate generalized Laguerre polynomial at a point.\n",
      "       eval_hermite     -- Evaluate physicist's Hermite polynomial at a point.\n",
      "       eval_hermitenorm -- Evaluate probabilist's (normalized) Hermite polynomial at a point.\n",
      "       eval_gegenbauer  -- Evaluate Gegenbauer polynomial at a point.\n",
      "       eval_sh_legendre -- Evaluate shifted Legendre polynomial at a point.\n",
      "       eval_sh_chebyt   -- Evaluate shifted Chebyshev polynomial of the first kind at a point.\n",
      "       eval_sh_chebyu   -- Evaluate shifted Chebyshev polynomial of the second kind at a point.\n",
      "       eval_sh_jacobi   -- Evaluate shifted Jacobi polynomial at a point.\n",
      "    \n",
      "    The following functions compute roots and quadrature weights for\n",
      "    orthogonal polynomials:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       roots_legendre    -- Gauss-Legendre quadrature.\n",
      "       roots_chebyt      -- Gauss-Chebyshev (first kind) quadrature.\n",
      "       roots_chebyu      -- Gauss-Chebyshev (second kind) quadrature.\n",
      "       roots_chebyc      -- Gauss-Chebyshev (first kind) quadrature.\n",
      "       roots_chebys      -- Gauss-Chebyshev (second kind) quadrature.\n",
      "       roots_jacobi      -- Gauss-Jacobi quadrature.\n",
      "       roots_laguerre    -- Gauss-Laguerre quadrature.\n",
      "       roots_genlaguerre -- Gauss-generalized Laguerre quadrature.\n",
      "       roots_hermite     -- Gauss-Hermite (physicst's) quadrature.\n",
      "       roots_hermitenorm -- Gauss-Hermite (statistician's) quadrature.\n",
      "       roots_gegenbauer  -- Gauss-Gegenbauer quadrature.\n",
      "       roots_sh_legendre -- Gauss-Legendre (shifted) quadrature.\n",
      "       roots_sh_chebyt   -- Gauss-Chebyshev (first kind, shifted) quadrature.\n",
      "       roots_sh_chebyu   -- Gauss-Chebyshev (second kind, shifted) quadrature.\n",
      "       roots_sh_jacobi   -- Gauss-Jacobi (shifted) quadrature.\n",
      "    \n",
      "    The functions below, in turn, return the polynomial coefficients in\n",
      "    ``orthopoly1d`` objects, which function similarly as `numpy.poly1d`.\n",
      "    The ``orthopoly1d`` class also has an attribute ``weights``, which returns\n",
      "    the roots, weights, and total weights for the appropriate form of Gaussian\n",
      "    quadrature. These are returned in an ``n x 3`` array with roots in the first\n",
      "    column, weights in the second column, and total weights in the final column.\n",
      "    Note that ``orthopoly1d`` objects are converted to `~numpy.poly1d` when doing\n",
      "    arithmetic, and lose information of the original orthogonal polynomial.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       legendre    -- Legendre polynomial.\n",
      "       chebyt      -- Chebyshev polynomial of the first kind.\n",
      "       chebyu      -- Chebyshev polynomial of the second kind.\n",
      "       chebyc      -- Chebyshev polynomial of the first kind on :math:`[-2, 2]`.\n",
      "       chebys      -- Chebyshev polynomial of the second kind on :math:`[-2, 2]`.\n",
      "       jacobi      -- Jacobi polynomial.\n",
      "       laguerre    -- Laguerre polynomial.\n",
      "       genlaguerre -- Generalized (associated) Laguerre polynomial.\n",
      "       hermite     -- Physicist's Hermite polynomial.\n",
      "       hermitenorm -- Normalized (probabilist's) Hermite polynomial.\n",
      "       gegenbauer  -- Gegenbauer (ultraspherical) polynomial.\n",
      "       sh_legendre -- Shifted Legendre polynomial.\n",
      "       sh_chebyt   -- Shifted Chebyshev polynomial of the first kind.\n",
      "       sh_chebyu   -- Shifted Chebyshev polynomial of the second kind.\n",
      "       sh_jacobi   -- Shifted Jacobi polynomial.\n",
      "    \n",
      "    .. warning::\n",
      "    \n",
      "       Computing values of high-order polynomials (around ``order > 20``) using\n",
      "       polynomial coefficients is numerically unstable. To evaluate polynomial\n",
      "       values, the ``eval_*`` functions should be used instead.\n",
      "    \n",
      "    \n",
      "    Hypergeometric functions\n",
      "    ------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       hyp2f1 -- Gauss hypergeometric function 2F1(a, b; c; z).\n",
      "       hyp1f1 -- Confluent hypergeometric function 1F1(a, b; x).\n",
      "       hyperu -- Confluent hypergeometric function U(a, b, x) of the second kind.\n",
      "       hyp0f1 -- Confluent hypergeometric limit function 0F1.\n",
      "    \n",
      "    \n",
      "    Parabolic cylinder functions\n",
      "    ----------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       pbdv -- Parabolic cylinder function D.\n",
      "       pbvv -- Parabolic cylinder function V.\n",
      "       pbwa -- Parabolic cylinder function W.\n",
      "    \n",
      "    These are not universal functions:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       pbdv_seq -- Parabolic cylinder functions Dv(x) and derivatives.\n",
      "       pbvv_seq -- Parabolic cylinder functions Vv(x) and derivatives.\n",
      "       pbdn_seq -- Parabolic cylinder functions Dn(z) and derivatives.\n",
      "    \n",
      "    Mathieu and related functions\n",
      "    -----------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       mathieu_a -- Characteristic value of even Mathieu functions.\n",
      "       mathieu_b -- Characteristic value of odd Mathieu functions.\n",
      "    \n",
      "    These are not universal functions:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       mathieu_even_coef -- Fourier coefficients for even Mathieu and modified Mathieu functions.\n",
      "       mathieu_odd_coef  -- Fourier coefficients for even Mathieu and modified Mathieu functions.\n",
      "    \n",
      "    The following return both function and first derivative:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       mathieu_cem     -- Even Mathieu function and its derivative.\n",
      "       mathieu_sem     -- Odd Mathieu function and its derivative.\n",
      "       mathieu_modcem1 -- Even modified Mathieu function of the first kind and its derivative.\n",
      "       mathieu_modcem2 -- Even modified Mathieu function of the second kind and its derivative.\n",
      "       mathieu_modsem1 -- Odd modified Mathieu function of the first kind and its derivative.\n",
      "       mathieu_modsem2 -- Odd modified Mathieu function of the second kind and its derivative.\n",
      "    \n",
      "    Spheroidal wave functions\n",
      "    -------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       pro_ang1   -- Prolate spheroidal angular function of the first kind and its derivative.\n",
      "       pro_rad1   -- Prolate spheroidal radial function of the first kind and its derivative.\n",
      "       pro_rad2   -- Prolate spheroidal radial function of the secon kind and its derivative.\n",
      "       obl_ang1   -- Oblate spheroidal angular function of the first kind and its derivative.\n",
      "       obl_rad1   -- Oblate spheroidal radial function of the first kind and its derivative.\n",
      "       obl_rad2   -- Oblate spheroidal radial function of the second kind and its derivative.\n",
      "       pro_cv     -- Characteristic value of prolate spheroidal function.\n",
      "       obl_cv     -- Characteristic value of oblate spheroidal function.\n",
      "       pro_cv_seq -- Characteristic values for prolate spheroidal wave functions.\n",
      "       obl_cv_seq -- Characteristic values for oblate spheroidal wave functions.\n",
      "    \n",
      "    The following functions require pre-computed characteristic value:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       pro_ang1_cv -- Prolate spheroidal angular function pro_ang1 for precomputed characteristic value.\n",
      "       pro_rad1_cv -- Prolate spheroidal radial function pro_rad1 for precomputed characteristic value.\n",
      "       pro_rad2_cv -- Prolate spheroidal radial function pro_rad2 for precomputed characteristic value.\n",
      "       obl_ang1_cv -- Oblate spheroidal angular function obl_ang1 for precomputed characteristic value.\n",
      "       obl_rad1_cv -- Oblate spheroidal radial function obl_rad1 for precomputed characteristic value.\n",
      "       obl_rad2_cv -- Oblate spheroidal radial function obl_rad2 for precomputed characteristic value.\n",
      "    \n",
      "    Kelvin functions\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       kelvin       -- Kelvin functions as complex numbers.\n",
      "       kelvin_zeros -- Compute nt zeros of all Kelvin functions.\n",
      "       ber          -- Kelvin function ber.\n",
      "       bei          -- Kelvin function bei\n",
      "       berp         -- Derivative of the Kelvin function `ber`.\n",
      "       beip         -- Derivative of the Kelvin function `bei`.\n",
      "       ker          -- Kelvin function ker.\n",
      "       kei          -- Kelvin function ker.\n",
      "       kerp         -- Derivative of the Kelvin function ker.\n",
      "       keip         -- Derivative of the Kelvin function kei.\n",
      "    \n",
      "    These are not universal functions:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       ber_zeros  -- Compute nt zeros of the Kelvin function ber(x).\n",
      "       bei_zeros  -- Compute nt zeros of the Kelvin function bei(x).\n",
      "       berp_zeros -- Compute nt zeros of the Kelvin function ber'(x).\n",
      "       beip_zeros -- Compute nt zeros of the Kelvin function bei'(x).\n",
      "       ker_zeros  -- Compute nt zeros of the Kelvin function ker(x).\n",
      "       kei_zeros  -- Compute nt zeros of the Kelvin function kei(x).\n",
      "       kerp_zeros -- Compute nt zeros of the Kelvin function ker'(x).\n",
      "       keip_zeros -- Compute nt zeros of the Kelvin function kei'(x).\n",
      "    \n",
      "    Combinatorics\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       comb -- The number of combinations of N things taken k at a time.\n",
      "       perm -- Permutations of N things taken k at a time, i.e., k-permutations of N.\n",
      "    \n",
      "    Lambert W and related functions\n",
      "    -------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       lambertw    -- Lambert W function.\n",
      "       wrightomega -- Wright Omega function.\n",
      "    \n",
      "    Other special functions\n",
      "    -----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       agm         -- Arithmetic, Geometric Mean.\n",
      "       bernoulli   -- Bernoulli numbers B0..Bn (inclusive).\n",
      "       binom       -- Binomial coefficient\n",
      "       diric       -- Periodic sinc function, also called the Dirichlet function.\n",
      "       euler       -- Euler numbers E0..En (inclusive).\n",
      "       expn        -- Exponential integral E_n.\n",
      "       exp1        -- Exponential integral E_1 of complex argument z.\n",
      "       expi        -- Exponential integral Ei.\n",
      "       factorial   -- The factorial of a number or array of numbers.\n",
      "       factorial2  -- Double factorial.\n",
      "       factorialk  -- Multifactorial of n of order k, n(!!...!).\n",
      "       shichi      -- Hyperbolic sine and cosine integrals.\n",
      "       sici        -- Sine and cosine integrals.\n",
      "       softmax     -- Softmax function.\n",
      "       log_softmax -- Logarithm of softmax function.\n",
      "       spence      -- Spence's function, also known as the dilogarithm.\n",
      "       zeta        -- Riemann zeta function.\n",
      "       zetac       -- Riemann zeta function minus 1.\n",
      "    \n",
      "    Convenience functions\n",
      "    ---------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       cbrt      -- Cube root of `x`.\n",
      "       exp10     -- 10**x.\n",
      "       exp2      -- 2**x.\n",
      "       radian    -- Convert from degrees to radians.\n",
      "       cosdg     -- Cosine of the angle `x` given in degrees.\n",
      "       sindg     -- Sine of angle given in degrees.\n",
      "       tandg     -- Tangent of angle x given in degrees.\n",
      "       cotdg     -- Cotangent of the angle `x` given in degrees.\n",
      "       log1p     -- Calculates log(1+x) for use when `x` is near zero.\n",
      "       expm1     -- exp(x) - 1 for use when `x` is near zero.\n",
      "       cosm1     -- cos(x) - 1 for use when `x` is near zero.\n",
      "       round     -- Round to nearest integer.\n",
      "       xlogy     -- Compute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n",
      "       xlog1py   -- Compute ``x*log1p(y)`` so that the result is 0 if ``x = 0``.\n",
      "       logsumexp -- Compute the log of the sum of exponentials of input elements.\n",
      "       exprel    -- Relative error exponential, (exp(x)-1)/x, for use when `x` is near zero.\n",
      "       sinc      -- Return the sinc function.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _basic\n",
      "    _comb\n",
      "    _ellip_harm\n",
      "    _ellip_harm_2\n",
      "    _generate_pyx\n",
      "    _lambertw\n",
      "    _logsumexp\n",
      "    _mptestutils\n",
      "    _precompute (package)\n",
      "    _spherical_bessel\n",
      "    _test_round\n",
      "    _testutils\n",
      "    _ufuncs\n",
      "    _ufuncs_cxx\n",
      "    add_newdocs\n",
      "    basic\n",
      "    cython_special\n",
      "    orthogonal\n",
      "    setup\n",
      "    sf_error\n",
      "    specfun\n",
      "    spfun_stats\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        scipy.special.sf_error.SpecialFunctionError\n",
      "    builtins.Warning(builtins.Exception)\n",
      "        scipy.special.sf_error.SpecialFunctionWarning\n",
      "    builtins.object\n",
      "        scipy.special._ufuncs.errstate\n",
      "    \n",
      "    class SpecialFunctionError(builtins.Exception)\n",
      "     |  Exception that can be raised by special functions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpecialFunctionError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class SpecialFunctionWarning(builtins.Warning)\n",
      "     |  Warning that can be emitted by special functions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SpecialFunctionWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Warning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Warning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class errstate(builtins.object)\n",
      "     |  errstate(**kwargs)\n",
      "     |  \n",
      "     |  Context manager for special-function error handling.\n",
      "     |  \n",
      "     |  Using an instance of `errstate` as a context manager allows\n",
      "     |  statements in that context to execute with a known error handling\n",
      "     |  behavior. Upon entering the context the error handling is set with\n",
      "     |  `seterr`, and upon exiting it is restored to what it was before.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  kwargs : {all, singular, underflow, overflow, slow, loss, no_result, domain, arg, other}\n",
      "     |      Keyword arguments. The valid keywords are possible\n",
      "     |      special-function errors. Each keyword should have a string\n",
      "     |      value that defines the treatement for the particular type of\n",
      "     |      error. Values must be 'ignore', 'warn', or 'other'. See\n",
      "     |      `seterr` for details.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  geterr : get the current way of handling special-function errors\n",
      "     |  seterr : set how special-function errors are handled\n",
      "     |  numpy.errstate : similar numpy function for floating-point errors\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import scipy.special as sc\n",
      "     |  >>> from pytest import raises\n",
      "     |  >>> sc.gammaln(0)\n",
      "     |  inf\n",
      "     |  >>> with sc.errstate(singular='raise'):\n",
      "     |  ...     with raises(sc.SpecialFunctionError):\n",
      "     |  ...         sc.gammaln(0)\n",
      "     |  ...\n",
      "     |  >>> sc.gammaln(0)\n",
      "     |  inf\n",
      "     |  \n",
      "     |  We can also raise on every category except one.\n",
      "     |  \n",
      "     |  >>> with sc.errstate(all='raise', singular='ignore'):\n",
      "     |  ...     sc.gammaln(0)\n",
      "     |  ...     with raises(sc.SpecialFunctionError):\n",
      "     |  ...         sc.spence(-1)\n",
      "     |  ...\n",
      "     |  inf\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, exc_type, exc_value, traceback)\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    ai_zeros(nt)\n",
      "        Compute `nt` zeros and values of the Airy function Ai and its derivative.\n",
      "        \n",
      "        Computes the first `nt` zeros, `a`, of the Airy function Ai(x);\n",
      "        first `nt` zeros, `ap`, of the derivative of the Airy function Ai'(x);\n",
      "        the corresponding values Ai(a');\n",
      "        and the corresponding values Ai'(a).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        a : ndarray\n",
      "            First `nt` zeros of Ai(x)\n",
      "        ap : ndarray\n",
      "            First `nt` zeros of Ai'(x)\n",
      "        ai : ndarray\n",
      "            Values of Ai(x) evaluated at first `nt` zeros of Ai'(x)\n",
      "        aip : ndarray\n",
      "            Values of Ai'(x) evaluated at first `nt` zeros of Ai(x)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> a, ap, ai, aip = special.ai_zeros(3)\n",
      "        >>> a\n",
      "        array([-2.33810741, -4.08794944, -5.52055983])\n",
      "        >>> ap\n",
      "        array([-1.01879297, -3.24819758, -4.82009921])\n",
      "        >>> ai\n",
      "        array([ 0.53565666, -0.41901548,  0.38040647])\n",
      "        >>> aip\n",
      "        array([ 0.70121082, -0.80311137,  0.86520403])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    assoc_laguerre(x, n, k=0.0)\n",
      "        Compute the generalized (associated) Laguerre polynomial of degree n and order k.\n",
      "        \n",
      "        The polynomial :math:`L^{(k)}_n(x)` is orthogonal over ``[0, inf)``,\n",
      "        with weighting function ``exp(-x) * x**k`` with ``k > -1``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `assoc_laguerre` is a simple wrapper around `eval_genlaguerre`, with\n",
      "        reversed argument order ``(x, n, k=0.0) --> (n, k, x)``.\n",
      "    \n",
      "    bei_zeros(nt)\n",
      "        Compute nt zeros of the Kelvin function bei.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bei\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    beip_zeros(nt)\n",
      "        Compute nt zeros of the derivative of the Kelvin function bei.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the derivative of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bei, beip\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    ber_zeros(nt)\n",
      "        Compute nt zeros of the Kelvin function ber.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ber\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    bernoulli(n)\n",
      "        Bernoulli numbers B0..Bn (inclusive).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Indicated the number of terms in the Bernoulli series to generate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            The Bernoulli numbers ``[B(0), B(1), ..., B(n)]``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] \"Bernoulli number\", Wikipedia, https://en.wikipedia.org/wiki/Bernoulli_number\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import bernoulli, zeta\n",
      "        >>> bernoulli(4)\n",
      "        array([ 1.        , -0.5       ,  0.16666667,  0.        , -0.03333333])\n",
      "        \n",
      "        The Wikipedia article ([2]_) points out the relationship between the\n",
      "        Bernoulli numbers and the zeta function, ``B_n^+ = -n * zeta(1 - n)``\n",
      "        for ``n > 0``:\n",
      "        \n",
      "        >>> n = np.arange(1, 5)\n",
      "        >>> -n * zeta(1 - n)\n",
      "        array([ 0.5       ,  0.16666667, -0.        , -0.03333333])\n",
      "        \n",
      "        Note that, in the notation used in the wikipedia article,\n",
      "        `bernoulli` computes ``B_n^-`` (i.e. it used the convention that\n",
      "        ``B_1`` is -1/2).  The relation given above is for ``B_n^+``, so the\n",
      "        sign of 0.5 does not match the output of ``bernoulli(4)``.\n",
      "    \n",
      "    berp_zeros(nt)\n",
      "        Compute nt zeros of the derivative of the Kelvin function ber.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the derivative of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ber, berp\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    bi_zeros(nt)\n",
      "        Compute `nt` zeros and values of the Airy function Bi and its derivative.\n",
      "        \n",
      "        Computes the first `nt` zeros, b, of the Airy function Bi(x);\n",
      "        first `nt` zeros, b', of the derivative of the Airy function Bi'(x);\n",
      "        the corresponding values Bi(b');\n",
      "        and the corresponding values Bi'(b).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        b : ndarray\n",
      "            First `nt` zeros of Bi(x)\n",
      "        bp : ndarray\n",
      "            First `nt` zeros of Bi'(x)\n",
      "        bi : ndarray\n",
      "            Values of Bi(x) evaluated at first `nt` zeros of Bi'(x)\n",
      "        bip : ndarray\n",
      "            Values of Bi'(x) evaluated at first `nt` zeros of Bi(x)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> b, bp, bi, bip = special.bi_zeros(3)\n",
      "        >>> b\n",
      "        array([-1.17371322, -3.2710933 , -4.83073784])\n",
      "        >>> bp\n",
      "        array([-2.29443968, -4.07315509, -5.51239573])\n",
      "        >>> bi\n",
      "        array([-0.45494438,  0.39652284, -0.36796916])\n",
      "        >>> bip\n",
      "        array([ 0.60195789, -0.76031014,  0.83699101])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    c_roots = roots_chebyc(n, mu=False)\n",
      "        Gauss-Chebyshev (first kind) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the first kind, :math:`C_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-2, 2]`\n",
      "        with weight function :math:`w(x) = 1 / \\sqrt{1 - (x/2)^2}`. See\n",
      "        22.2.6 in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    cg_roots = roots_gegenbauer(n, alpha, mu=False)\n",
      "        Gauss-Gegenbauer quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Gegenbauer\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Gegenbauer polynomial, :math:`C^{\\alpha}_n(x)`. These sample\n",
      "        points and weights correctly integrate polynomials of degree\n",
      "        :math:`2n - 1` or less over the interval :math:`[-1, 1]` with\n",
      "        weight function :math:`w(x) = (1 - x^2)^{\\alpha - 1/2}`. See\n",
      "        22.2.3 in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        alpha : float\n",
      "            alpha must be > -0.5\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    chebyc(n, monic=False)\n",
      "        Chebyshev polynomial of the first kind on :math:`[-2, 2]`.\n",
      "        \n",
      "        Defined as :math:`C_n(x) = 2T_n(x/2)`, where :math:`T_n` is the\n",
      "        nth Chebychev polynomial of the first kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : orthopoly1d\n",
      "            Chebyshev polynomial of the first kind on :math:`[-2, 2]`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`C_n(x)` are orthogonal over :math:`[-2, 2]`\n",
      "        with weight function :math:`1/\\sqrt{1 - (x/2)^2}`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chebyt : Chebyshev polynomial of the first kind.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Abramowitz and Stegun, \"Handbook of Mathematical Functions\"\n",
      "               Section 22. National Bureau of Standards, 1972.\n",
      "    \n",
      "    chebys(n, monic=False)\n",
      "        Chebyshev polynomial of the second kind on :math:`[-2, 2]`.\n",
      "        \n",
      "        Defined as :math:`S_n(x) = U_n(x/2)` where :math:`U_n` is the\n",
      "        nth Chebychev polynomial of the second kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        S : orthopoly1d\n",
      "            Chebyshev polynomial of the second kind on :math:`[-2, 2]`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`S_n(x)` are orthogonal over :math:`[-2, 2]`\n",
      "        with weight function :math:`\\sqrt{1 - (x/2)}^2`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chebyu : Chebyshev polynomial of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Abramowitz and Stegun, \"Handbook of Mathematical Functions\"\n",
      "               Section 22. National Bureau of Standards, 1972.\n",
      "    \n",
      "    chebyt(n, monic=False)\n",
      "        Chebyshev polynomial of the first kind.\n",
      "        \n",
      "        Defined to be the solution of\n",
      "        \n",
      "        .. math::\n",
      "            (1 - x^2)\\frac{d^2}{dx^2}T_n - x\\frac{d}{dx}T_n + n^2T_n = 0;\n",
      "        \n",
      "        :math:`T_n` is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        T : orthopoly1d\n",
      "            Chebyshev polynomial of the first kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`T_n` are orthogonal over :math:`[-1, 1]`\n",
      "        with weight function :math:`(1 - x^2)^{-1/2}`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chebyu : Chebyshev polynomial of the second kind.\n",
      "    \n",
      "    chebyu(n, monic=False)\n",
      "        Chebyshev polynomial of the second kind.\n",
      "        \n",
      "        Defined to be the solution of\n",
      "        \n",
      "        .. math::\n",
      "            (1 - x^2)\\frac{d^2}{dx^2}U_n - 3x\\frac{d}{dx}U_n\n",
      "              + n(n + 2)U_n = 0;\n",
      "        \n",
      "        :math:`U_n` is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        U : orthopoly1d\n",
      "            Chebyshev polynomial of the second kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`U_n` are orthogonal over :math:`[-1, 1]`\n",
      "        with weight function :math:`(1 - x^2)^{1/2}`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chebyt : Chebyshev polynomial of the first kind.\n",
      "    \n",
      "    clpmn(m, n, z, type=3)\n",
      "        Associated Legendre function of the first kind for complex arguments.\n",
      "        \n",
      "        Computes the associated Legendre function of the first kind of order m and\n",
      "        degree n, ``Pmn(z)`` = :math:`P_n^m(z)`, and its derivative, ``Pmn'(z)``.\n",
      "        Returns two arrays of size ``(m+1, n+1)`` containing ``Pmn(z)`` and\n",
      "        ``Pmn'(z)`` for all orders from ``0..m`` and degrees from ``0..n``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : int\n",
      "           ``|m| <= n``; the order of the Legendre function.\n",
      "        n : int\n",
      "           where ``n >= 0``; the degree of the Legendre function.  Often\n",
      "           called ``l`` (lower case L) in descriptions of the associated\n",
      "           Legendre function\n",
      "        z : float or complex\n",
      "            Input value.\n",
      "        type : int, optional\n",
      "           takes values 2 or 3\n",
      "           2: cut on the real axis ``|x| > 1``\n",
      "           3: cut on the real axis ``-1 < x < 1`` (default)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Pmn_z : (m+1, n+1) array\n",
      "           Values for all orders ``0..m`` and degrees ``0..n``\n",
      "        Pmn_d_z : (m+1, n+1) array\n",
      "           Derivatives for all orders ``0..m`` and degrees ``0..n``\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lpmn: associated Legendre functions of the first kind for real z\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        By default, i.e. for ``type=3``, phase conventions are chosen according\n",
      "        to [1]_ such that the function is analytic. The cut lies on the interval\n",
      "        (-1, 1). Approaching the cut from above or below in general yields a phase\n",
      "        factor with respect to Ferrer's function of the first kind\n",
      "        (cf. `lpmn`).\n",
      "        \n",
      "        For ``type=2`` a cut at ``|x| > 1`` is chosen. Approaching the real values\n",
      "        on the interval (-1, 1) in the complex plane yields Ferrer's function\n",
      "        of the first kind.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/14.21\n",
      "    \n",
      "    comb(N, k, exact=False, repetition=False)\n",
      "        The number of combinations of N things taken k at a time.\n",
      "        \n",
      "        This is often expressed as \"N choose k\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        N : int, ndarray\n",
      "            Number of things.\n",
      "        k : int, ndarray\n",
      "            Number of elements taken.\n",
      "        exact : bool, optional\n",
      "            If `exact` is False, then floating point precision is used, otherwise\n",
      "            exact long integer is computed.\n",
      "        repetition : bool, optional\n",
      "            If `repetition` is True, then the number of combinations with\n",
      "            repetition is computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        val : int, float, ndarray\n",
      "            The total number of combinations.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        binom : Binomial coefficient ufunc\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        - Array arguments accepted only for exact=False case.\n",
      "        - If N < 0, or k < 0, then 0 is returned.\n",
      "        - If k > N and repetition=False, then 0 is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import comb\n",
      "        >>> k = np.array([3, 4])\n",
      "        >>> n = np.array([10, 10])\n",
      "        >>> comb(n, k, exact=False)\n",
      "        array([ 120.,  210.])\n",
      "        >>> comb(10, 3, exact=True)\n",
      "        120\n",
      "        >>> comb(10, 3, exact=True, repetition=True)\n",
      "        220\n",
      "    \n",
      "    diric(x, n)\n",
      "        Periodic sinc function, also called the Dirichlet function.\n",
      "        \n",
      "        The Dirichlet function is defined as::\n",
      "        \n",
      "            diric(x, n) = sin(x * n/2) / (n * sin(x / 2)),\n",
      "        \n",
      "        where `n` is a positive integer.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input data\n",
      "        n : int\n",
      "            Integer defining the periodicity.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        diric : ndarray\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> x = np.linspace(-8*np.pi, 8*np.pi, num=201)\n",
      "        >>> plt.figure(figsize=(8, 8));\n",
      "        >>> for idx, n in enumerate([2, 3, 4, 9]):\n",
      "        ...     plt.subplot(2, 2, idx+1)\n",
      "        ...     plt.plot(x, special.diric(x, n))\n",
      "        ...     plt.title('diric, n={}'.format(n))\n",
      "        >>> plt.show()\n",
      "        \n",
      "        The following example demonstrates that `diric` gives the magnitudes\n",
      "        (modulo the sign and scaling) of the Fourier coefficients of a\n",
      "        rectangular pulse.\n",
      "        \n",
      "        Suppress output of values that are effectively 0:\n",
      "        \n",
      "        >>> np.set_printoptions(suppress=True)\n",
      "        \n",
      "        Create a signal `x` of length `m` with `k` ones:\n",
      "        \n",
      "        >>> m = 8\n",
      "        >>> k = 3\n",
      "        >>> x = np.zeros(m)\n",
      "        >>> x[:k] = 1\n",
      "        \n",
      "        Use the FFT to compute the Fourier transform of `x`, and\n",
      "        inspect the magnitudes of the coefficients:\n",
      "        \n",
      "        >>> np.abs(np.fft.fft(x))\n",
      "        array([ 3.        ,  2.41421356,  1.        ,  0.41421356,  1.        ,\n",
      "                0.41421356,  1.        ,  2.41421356])\n",
      "        \n",
      "        Now find the same values (up to sign) using `diric`. We multiply\n",
      "        by `k` to account for the different scaling conventions of\n",
      "        `numpy.fft.fft` and `diric`:\n",
      "        \n",
      "        >>> theta = np.linspace(0, 2*np.pi, m, endpoint=False)\n",
      "        >>> k * special.diric(theta, k)\n",
      "        array([ 3.        ,  2.41421356,  1.        , -0.41421356, -1.        ,\n",
      "               -0.41421356,  1.        ,  2.41421356])\n",
      "    \n",
      "    ellip_harm(h2, k2, n, p, s, signm=1, signn=1)\n",
      "        Ellipsoidal harmonic functions E^p_n(l)\n",
      "        \n",
      "        These are also known as Lame functions of the first kind, and are\n",
      "        solutions to the Lame equation:\n",
      "        \n",
      "        .. math:: (s^2 - h^2)(s^2 - k^2)E''(s) + s(2s^2 - h^2 - k^2)E'(s) + (a - q s^2)E(s) = 0\n",
      "        \n",
      "        where :math:`q = (n+1)n` and :math:`a` is the eigenvalue (not\n",
      "        returned) corresponding to the solutions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        h2 : float\n",
      "            ``h**2``\n",
      "        k2 : float\n",
      "            ``k**2``; should be larger than ``h**2``\n",
      "        n : int\n",
      "            Degree\n",
      "        s : float\n",
      "            Coordinate\n",
      "        p : int\n",
      "            Order, can range between [1,2n+1]\n",
      "        signm : {1, -1}, optional\n",
      "            Sign of prefactor of functions. Can be +/-1. See Notes.\n",
      "        signn : {1, -1}, optional\n",
      "            Sign of prefactor of functions. Can be +/-1. See Notes.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        E : float\n",
      "            the harmonic :math:`E^p_n(s)`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellip_harm_2, ellip_normal\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The geometric interpretation of the ellipsoidal functions is\n",
      "        explained in [2]_, [3]_, [4]_. The `signm` and `signn` arguments control the\n",
      "        sign of prefactors for functions according to their type::\n",
      "        \n",
      "            K : +1\n",
      "            L : signm\n",
      "            M : signn\n",
      "            N : signm*signn\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Digital Library of Mathematical Functions 29.12\n",
      "           https://dlmf.nist.gov/29.12\n",
      "        .. [2] Bardhan and Knepley, \"Computational science and\n",
      "           re-discovery: open-source implementations of\n",
      "           ellipsoidal harmonics for problems in potential theory\",\n",
      "           Comput. Sci. Disc. 5, 014006 (2012)\n",
      "           :doi:`10.1088/1749-4699/5/1/014006`.\n",
      "        .. [3] David J.and Dechambre P, \"Computation of Ellipsoidal\n",
      "           Gravity Field Harmonics for small solar system bodies\"\n",
      "           pp. 30-36, 2000\n",
      "        .. [4] George Dassios, \"Ellipsoidal Harmonics: Theory and Applications\"\n",
      "           pp. 418, 2012\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import ellip_harm\n",
      "        >>> w = ellip_harm(5,8,1,1,2.5)\n",
      "        >>> w\n",
      "        2.5\n",
      "        \n",
      "        Check that the functions indeed are solutions to the Lame equation:\n",
      "        \n",
      "        >>> from scipy.interpolate import UnivariateSpline\n",
      "        >>> def eigenvalue(f, df, ddf):\n",
      "        ...     r = ((s**2 - h**2)*(s**2 - k**2)*ddf + s*(2*s**2 - h**2 - k**2)*df - n*(n+1)*s**2*f)/f\n",
      "        ...     return -r.mean(), r.std()\n",
      "        >>> s = np.linspace(0.1, 10, 200)\n",
      "        >>> k, h, n, p = 8.0, 2.2, 3, 2\n",
      "        >>> E = ellip_harm(h**2, k**2, n, p, s)\n",
      "        >>> E_spl = UnivariateSpline(s, E)\n",
      "        >>> a, a_err = eigenvalue(E_spl(s), E_spl(s,1), E_spl(s,2))\n",
      "        >>> a, a_err\n",
      "        (583.44366156701483, 6.4580890640310646e-11)\n",
      "    \n",
      "    ellip_harm_2(h2, k2, n, p, s)\n",
      "        Ellipsoidal harmonic functions F^p_n(l)\n",
      "        \n",
      "        These are also known as Lame functions of the second kind, and are\n",
      "        solutions to the Lame equation:\n",
      "        \n",
      "        .. math:: (s^2 - h^2)(s^2 - k^2)F''(s) + s(2s^2 - h^2 - k^2)F'(s) + (a - q s^2)F(s) = 0\n",
      "        \n",
      "        where :math:`q = (n+1)n` and :math:`a` is the eigenvalue (not\n",
      "        returned) corresponding to the solutions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        h2 : float\n",
      "            ``h**2``\n",
      "        k2 : float\n",
      "            ``k**2``; should be larger than ``h**2``\n",
      "        n : int\n",
      "            Degree.\n",
      "        p : int\n",
      "            Order, can range between [1,2n+1].\n",
      "        s : float\n",
      "            Coordinate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        F : float\n",
      "            The harmonic :math:`F^p_n(s)`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Lame functions of the second kind are related to the functions of the first kind:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           F^p_n(s)=(2n + 1)E^p_n(s)\\int_{0}^{1/s}\\frac{du}{(E^p_n(1/u))^2\\sqrt{(1-u^2k^2)(1-u^2h^2)}}\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellip_harm, ellip_normal\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import ellip_harm_2\n",
      "        >>> w = ellip_harm_2(5,8,2,1,10)\n",
      "        >>> w\n",
      "        0.00108056853382\n",
      "    \n",
      "    ellip_normal(h2, k2, n, p)\n",
      "        Ellipsoidal harmonic normalization constants gamma^p_n\n",
      "        \n",
      "        The normalization constant is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\gamma^p_n=8\\int_{0}^{h}dx\\int_{h}^{k}dy\\frac{(y^2-x^2)(E^p_n(y)E^p_n(x))^2}{\\sqrt((k^2-y^2)(y^2-h^2)(h^2-x^2)(k^2-x^2)}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        h2 : float\n",
      "            ``h**2``\n",
      "        k2 : float\n",
      "            ``k**2``; should be larger than ``h**2``\n",
      "        n : int\n",
      "            Degree.\n",
      "        p : int\n",
      "            Order, can range between [1,2n+1].\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        gamma : float\n",
      "            The normalization constant :math:`\\gamma^p_n`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellip_harm, ellip_harm_2\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import ellip_normal\n",
      "        >>> w = ellip_normal(5,8,3,7)\n",
      "        >>> w\n",
      "        1723.38796997\n",
      "    \n",
      "    erf_zeros(nt)\n",
      "        Compute the first nt zero in the first quadrant, ordered by absolute value.\n",
      "        \n",
      "        Zeros in the other quadrants can be obtained by using the symmetries erf(-z) = erf(z) and\n",
      "        erf(conj(z)) = conj(erf(z)).\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            The number of zeros to compute\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        The locations of the zeros of erf : ndarray (complex)\n",
      "            Complex values at which zeros of erf(z)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> special.erf_zeros(1)\n",
      "        array([1.45061616+1.880943j])\n",
      "        \n",
      "        Check that erf is (close to) zero for the value returned by erf_zeros\n",
      "        \n",
      "        >>> special.erf(special.erf_zeros(1))\n",
      "        array([4.95159469e-14-1.16407394e-16j])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    euler(n)\n",
      "        Euler numbers E(0), E(1), ..., E(n).\n",
      "        \n",
      "        The Euler numbers [1]_ are also known as the secant numbers.\n",
      "        \n",
      "        Because ``euler(n)`` returns floating point values, it does not give\n",
      "        exact values for large `n`.  The first inexact value is E(22).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            The highest index of the Euler number to be returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            The Euler numbers [E(0), E(1), ..., E(n)].\n",
      "            The odd Euler numbers, which are all zero, are included.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Sequence A122045, The On-Line Encyclopedia of Integer Sequences,\n",
      "               https://oeis.org/A122045\n",
      "        .. [2] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import euler\n",
      "        >>> euler(6)\n",
      "        array([  1.,   0.,  -1.,   0.,   5.,   0., -61.])\n",
      "        \n",
      "        >>> euler(13).astype(np.int64)\n",
      "        array([      1,       0,      -1,       0,       5,       0,     -61,\n",
      "                     0,    1385,       0,  -50521,       0, 2702765,       0])\n",
      "        \n",
      "        >>> euler(22)[-1]  # Exact value of E(22) is -69348874393137901.\n",
      "        -69348874393137976.0\n",
      "    \n",
      "    factorial(n, exact=False)\n",
      "        The factorial of a number or array of numbers.\n",
      "        \n",
      "        The factorial of non-negative integer `n` is the product of all\n",
      "        positive integers less than or equal to `n`::\n",
      "        \n",
      "            n! = n * (n - 1) * (n - 2) * ... * 1\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int or array_like of ints\n",
      "            Input values.  If ``n < 0``, the return value is 0.\n",
      "        exact : bool, optional\n",
      "            If True, calculate the answer exactly using long integer arithmetic.\n",
      "            If False, result is approximated in floating point rapidly using the\n",
      "            `gamma` function.\n",
      "            Default is False.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nf : float or int or ndarray\n",
      "            Factorial of `n`, as integer or float depending on `exact`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For arrays with ``exact=True``, the factorial is computed only once, for\n",
      "        the largest input, with each other result computed in the process.\n",
      "        The output dtype is increased to ``int64`` or ``object`` if necessary.\n",
      "        \n",
      "        With ``exact=False`` the factorial is approximated using the gamma\n",
      "        function:\n",
      "        \n",
      "        .. math:: n! = \\Gamma(n+1)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import factorial\n",
      "        >>> arr = np.array([3, 4, 5])\n",
      "        >>> factorial(arr, exact=False)\n",
      "        array([   6.,   24.,  120.])\n",
      "        >>> factorial(arr, exact=True)\n",
      "        array([  6,  24, 120])\n",
      "        >>> factorial(5, exact=True)\n",
      "        120\n",
      "    \n",
      "    factorial2(n, exact=False)\n",
      "        Double factorial.\n",
      "        \n",
      "        This is the factorial with every second value skipped.  E.g., ``7!! = 7 * 5\n",
      "        * 3 * 1``.  It can be approximated numerically as::\n",
      "        \n",
      "          n!! = special.gamma(n/2+1)*2**((m+1)/2)/sqrt(pi)  n odd\n",
      "              = 2**(n/2) * (n/2)!                           n even\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int or array_like\n",
      "            Calculate ``n!!``.  Arrays are only supported with `exact` set\n",
      "            to False.  If ``n < 0``, the return value is 0.\n",
      "        exact : bool, optional\n",
      "            The result can be approximated rapidly using the gamma-formula\n",
      "            above (default).  If `exact` is set to True, calculate the\n",
      "            answer exactly using integer arithmetic.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nff : float or int\n",
      "            Double factorial of `n`, as an int or a float depending on\n",
      "            `exact`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import factorial2\n",
      "        >>> factorial2(7, exact=False)\n",
      "        array(105.00000000000001)\n",
      "        >>> factorial2(7, exact=True)\n",
      "        105\n",
      "    \n",
      "    factorialk(n, k, exact=True)\n",
      "        Multifactorial of n of order k, n(!!...!).\n",
      "        \n",
      "        This is the multifactorial of n skipping k values.  For example,\n",
      "        \n",
      "          factorialk(17, 4) = 17!!!! = 17 * 13 * 9 * 5 * 1\n",
      "        \n",
      "        In particular, for any integer ``n``, we have\n",
      "        \n",
      "          factorialk(n, 1) = factorial(n)\n",
      "        \n",
      "          factorialk(n, 2) = factorial2(n)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Calculate multifactorial. If `n` < 0, the return value is 0.\n",
      "        k : int\n",
      "            Order of multifactorial.\n",
      "        exact : bool, optional\n",
      "            If exact is set to True, calculate the answer exactly using\n",
      "            integer arithmetic.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        val : int\n",
      "            Multifactorial of `n`.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NotImplementedError\n",
      "            Raises when exact is False\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import factorialk\n",
      "        >>> factorialk(5, 1, exact=True)\n",
      "        120\n",
      "        >>> factorialk(5, 3, exact=True)\n",
      "        10\n",
      "    \n",
      "    fresnel_zeros(nt)\n",
      "        Compute nt complex zeros of sine and cosine Fresnel integrals S(z) and C(z).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    fresnelc_zeros(nt)\n",
      "        Compute nt complex zeros of cosine Fresnel integral C(z).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    fresnels_zeros(nt)\n",
      "        Compute nt complex zeros of sine Fresnel integral S(z).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    gegenbauer(n, alpha, monic=False)\n",
      "        Gegenbauer (ultraspherical) polynomial.\n",
      "        \n",
      "        Defined to be the solution of\n",
      "        \n",
      "        .. math::\n",
      "            (1 - x^2)\\frac{d^2}{dx^2}C_n^{(\\alpha)}\n",
      "              - (2\\alpha + 1)x\\frac{d}{dx}C_n^{(\\alpha)}\n",
      "              + n(n + 2\\alpha)C_n^{(\\alpha)} = 0\n",
      "        \n",
      "        for :math:`\\alpha > -1/2`; :math:`C_n^{(\\alpha)}` is a polynomial\n",
      "        of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        alpha : float\n",
      "            Parameter, must be greater than -0.5.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : orthopoly1d\n",
      "            Gegenbauer polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`C_n^{(\\alpha)}` are orthogonal over\n",
      "        :math:`[-1,1]` with weight function :math:`(1 - x^2)^{(\\alpha -\n",
      "        1/2)}`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        We can initialize a variable ``p`` as a Gegenbauer polynomial using the\n",
      "        `gegenbauer` function and evaluate at a point ``x = 1``.\n",
      "        \n",
      "        >>> p = special.gegenbauer(3, 0.5, monic=False)\n",
      "        >>> p\n",
      "        poly1d([ 2.5,  0. , -1.5,  0. ])\n",
      "        >>> p(1)\n",
      "        1.0\n",
      "        \n",
      "        To evaluate ``p`` at various points ``x`` in the interval ``(-3, 3)``,\n",
      "        simply pass an array ``x`` to ``p`` as follows:\n",
      "        \n",
      "        >>> x = np.linspace(-3, 3, 400)\n",
      "        >>> y = p(x)\n",
      "        \n",
      "        We can then visualize ``x, y`` using `matplotlib.pyplot`.\n",
      "        \n",
      "        >>> fig, ax = plt.subplots()\n",
      "        >>> ax.plot(x, y)\n",
      "        >>> ax.set_title(\"Gegenbauer (ultraspherical) polynomial of degree 3\")\n",
      "        >>> ax.set_xlabel(\"x\")\n",
      "        >>> ax.set_ylabel(\"G_3(x)\")\n",
      "        >>> plt.show()\n",
      "    \n",
      "    genlaguerre(n, alpha, monic=False)\n",
      "        Generalized (associated) Laguerre polynomial.\n",
      "        \n",
      "        Defined to be the solution of\n",
      "        \n",
      "        .. math::\n",
      "            x\\frac{d^2}{dx^2}L_n^{(\\alpha)}\n",
      "              + (\\alpha + 1 - x)\\frac{d}{dx}L_n^{(\\alpha)}\n",
      "              + nL_n^{(\\alpha)} = 0,\n",
      "        \n",
      "        where :math:`\\alpha > -1`; :math:`L_n^{(\\alpha)}` is a polynomial\n",
      "        of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        alpha : float\n",
      "            Parameter, must be greater than -1.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        L : orthopoly1d\n",
      "            Generalized Laguerre polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For fixed :math:`\\alpha`, the polynomials :math:`L_n^{(\\alpha)}`\n",
      "        are orthogonal over :math:`[0, \\infty)` with weight function\n",
      "        :math:`e^{-x}x^\\alpha`.\n",
      "        \n",
      "        The Laguerre polynomials are the special case where :math:`\\alpha\n",
      "        = 0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        laguerre : Laguerre polynomial.\n",
      "    \n",
      "    geterr(...)\n",
      "        Get the current way of handling special-function errors.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        err : dict\n",
      "            A dictionary with keys \"singular\", \"underflow\", \"overflow\",\n",
      "            \"slow\", \"loss\", \"no_result\", \"domain\", \"arg\", and \"other\",\n",
      "            whose values are from the strings \"ignore\", \"warn\", and\n",
      "            \"raise\". The keys represent possible special-function errors,\n",
      "            and the values define how these errors are handled.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        seterr : set how special-function errors are handled\n",
      "        errstate : context manager for special-function error handling\n",
      "        numpy.geterr : similar numpy function for floating-point errors\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For complete documentation of the types of special-function errors\n",
      "        and treatment options, see `seterr`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        By default all errors are ignored.\n",
      "        \n",
      "        >>> import scipy.special as sc\n",
      "        >>> for key, value in sorted(sc.geterr().items()):\n",
      "        ...     print(\"{}: {}\".format(key, value))\n",
      "        ...\n",
      "        arg: ignore\n",
      "        domain: ignore\n",
      "        loss: ignore\n",
      "        no_result: ignore\n",
      "        other: ignore\n",
      "        overflow: ignore\n",
      "        singular: ignore\n",
      "        slow: ignore\n",
      "        underflow: ignore\n",
      "    \n",
      "    h1vp(v, z, n=1)\n",
      "        Compute nth derivative of Hankel function H1v(z) with respect to `z`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order of Hankel function\n",
      "        z : array_like\n",
      "            Argument at which to evaluate the derivative. Can be real or\n",
      "            complex.\n",
      "        n : int, default 1\n",
      "            Order of derivative\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the derivative of the Hankel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The derivative is computed using the relation DLFM 10.6.7 [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.6.E7\n",
      "    \n",
      "    h2vp(v, z, n=1)\n",
      "        Compute nth derivative of Hankel function H2v(z) with respect to `z`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order of Hankel function\n",
      "        z : array_like\n",
      "            Argument at which to evaluate the derivative. Can be real or\n",
      "            complex.\n",
      "        n : int, default 1\n",
      "            Order of derivative\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the derivative of the Hankel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The derivative is computed using the relation DLFM 10.6.7 [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.6.E7\n",
      "    \n",
      "    h_roots = roots_hermite(n, mu=False)\n",
      "        Gauss-Hermite (physicist's) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Hermite\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Hermite polynomial, :math:`H_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[-\\infty, \\infty]` with weight\n",
      "        function :math:`w(x) = e^{-x^2}`. See 22.2.14 in [AS]_ for\n",
      "        details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For small n up to 150 a modified version of the Golub-Welsch\n",
      "        algorithm is used. Nodes are computed from the eigenvalue\n",
      "        problem and improved by one step of a Newton iteration.\n",
      "        The weights are computed from the well-known analytical formula.\n",
      "        \n",
      "        For n larger than 150 an optimal asymptotic algorithm is applied\n",
      "        which computes nodes and weights in a numerically stable manner.\n",
      "        The algorithm has linear runtime making computation for very\n",
      "        large n (several thousand or more) feasible.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.hermite.hermgauss\n",
      "        roots_hermitenorm\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [townsend.trogdon.olver-2014]\n",
      "            Townsend, A. and Trogdon, T. and Olver, S. (2014)\n",
      "            *Fast computation of Gauss quadrature nodes and\n",
      "            weights on the whole real line*. :arXiv:`1410.5286`.\n",
      "        .. [townsend.trogdon.olver-2015]\n",
      "            Townsend, A. and Trogdon, T. and Olver, S. (2015)\n",
      "            *Fast computation of Gauss quadrature nodes and\n",
      "            weights on the whole real line*.\n",
      "            IMA Journal of Numerical Analysis\n",
      "            :doi:`10.1093/imanum/drv002`.\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    he_roots = roots_hermitenorm(n, mu=False)\n",
      "        Gauss-Hermite (statistician's) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Hermite\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Hermite polynomial, :math:`He_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[-\\infty, \\infty]` with weight\n",
      "        function :math:`w(x) = e^{-x^2/2}`. See 22.2.15 in [AS]_ for more\n",
      "        details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For small n up to 150 a modified version of the Golub-Welsch\n",
      "        algorithm is used. Nodes are computed from the eigenvalue\n",
      "        problem and improved by one step of a Newton iteration.\n",
      "        The weights are computed from the well-known analytical formula.\n",
      "        \n",
      "        For n larger than 150 an optimal asymptotic algorithm is used\n",
      "        which computes nodes and weights in a numerical stable manner.\n",
      "        The algorithm has linear runtime making computation for very\n",
      "        large n (several thousand or more) feasible.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.hermite_e.hermegauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    hermite(n, monic=False)\n",
      "        Physicist's Hermite polynomial.\n",
      "        \n",
      "        Defined by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            H_n(x) = (-1)^ne^{x^2}\\frac{d^n}{dx^n}e^{-x^2};\n",
      "        \n",
      "        :math:`H_n` is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        H : orthopoly1d\n",
      "            Hermite polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`H_n` are orthogonal over :math:`(-\\infty,\n",
      "        \\infty)` with weight function :math:`e^{-x^2}`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> import numpy as np\n",
      "        \n",
      "        >>> p_monic = special.hermite(3, monic=True)\n",
      "        >>> p_monic\n",
      "        poly1d([ 1. ,  0. , -1.5,  0. ])\n",
      "        >>> p_monic(1)\n",
      "        -0.49999999999999983\n",
      "        >>> x = np.linspace(-3, 3, 400)\n",
      "        >>> y = p_monic(x)\n",
      "        >>> plt.plot(x, y)\n",
      "        >>> plt.title(\"Monic Hermite polynomial of degree 3\")\n",
      "        >>> plt.xlabel(\"x\")\n",
      "        >>> plt.ylabel(\"H_3(x)\")\n",
      "        >>> plt.show()\n",
      "    \n",
      "    hermitenorm(n, monic=False)\n",
      "        Normalized (probabilist's) Hermite polynomial.\n",
      "        \n",
      "        Defined by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            He_n(x) = (-1)^ne^{x^2/2}\\frac{d^n}{dx^n}e^{-x^2/2};\n",
      "        \n",
      "        :math:`He_n` is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        He : orthopoly1d\n",
      "            Hermite polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        The polynomials :math:`He_n` are orthogonal over :math:`(-\\infty,\n",
      "        \\infty)` with weight function :math:`e^{-x^2/2}`.\n",
      "    \n",
      "    ivp(v, z, n=1)\n",
      "        Compute derivatives of modified Bessel functions of the first kind.\n",
      "        \n",
      "        Compute the nth derivative of the modified Bessel function `Iv`\n",
      "        with respect to `z`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order of Bessel function\n",
      "        z : array_like\n",
      "            Argument at which to evaluate the derivative; can be real or\n",
      "            complex.\n",
      "        n : int, default 1\n",
      "            Order of derivative\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            nth derivative of the modified Bessel function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        iv\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The derivative is computed using the relation DLFM 10.29.5 [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 6.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.29.E5\n",
      "    \n",
      "    j_roots = roots_jacobi(n, alpha, beta, mu=False)\n",
      "        Gauss-Jacobi quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Jacobi\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Jacobi polynomial, :math:`P^{\\alpha, \\beta}_n(x)`. These sample\n",
      "        points and weights correctly integrate polynomials of degree\n",
      "        :math:`2n - 1` or less over the interval :math:`[-1, 1]` with\n",
      "        weight function :math:`w(x) = (1 - x)^{\\alpha} (1 +\n",
      "        x)^{\\beta}`. See 22.2.1 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        alpha : float\n",
      "            alpha must be > -1\n",
      "        beta : float\n",
      "            beta must be > -1\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    jacobi(n, alpha, beta, monic=False)\n",
      "        Jacobi polynomial.\n",
      "        \n",
      "        Defined to be the solution of\n",
      "        \n",
      "        .. math::\n",
      "            (1 - x^2)\\frac{d^2}{dx^2}P_n^{(\\alpha, \\beta)}\n",
      "              + (\\beta - \\alpha - (\\alpha + \\beta + 2)x)\n",
      "                \\frac{d}{dx}P_n^{(\\alpha, \\beta)}\n",
      "              + n(n + \\alpha + \\beta + 1)P_n^{(\\alpha, \\beta)} = 0\n",
      "        \n",
      "        for :math:`\\alpha, \\beta > -1`; :math:`P_n^{(\\alpha, \\beta)}` is a\n",
      "        polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        alpha : float\n",
      "            Parameter, must be greater than -1.\n",
      "        beta : float\n",
      "            Parameter, must be greater than -1.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        P : orthopoly1d\n",
      "            Jacobi polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For fixed :math:`\\alpha, \\beta`, the polynomials\n",
      "        :math:`P_n^{(\\alpha, \\beta)}` are orthogonal over :math:`[-1, 1]`\n",
      "        with weight function :math:`(1 - x)^\\alpha(1 + x)^\\beta`.\n",
      "    \n",
      "    jn_zeros(n, nt)\n",
      "        Compute zeros of integer-order Bessel functions Jn.\n",
      "        \n",
      "        Compute `nt` zeros of the Bessel functions :math:`J_n(x)` on the\n",
      "        interval :math:`(0, \\infty)`. The zeros are returned in ascending\n",
      "        order. Note that this interval excludes the zero at :math:`x = 0`\n",
      "        that exists for :math:`n > 0`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Order of Bessel function\n",
      "        nt : int\n",
      "            Number of zeros to return\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `n` zeros of the Bessel function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        jv\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        We can check that we are getting approximations of the zeros by\n",
      "        evaluating them with `jv`.\n",
      "        \n",
      "        >>> n = 1\n",
      "        >>> x = sc.jn_zeros(n, 3)\n",
      "        >>> x\n",
      "        array([ 3.83170597,  7.01558667, 10.17346814])\n",
      "        >>> sc.jv(n, x)\n",
      "        array([-0.00000000e+00,  1.72975330e-16,  2.89157291e-16])\n",
      "        \n",
      "        Note that the zero at ``x = 0`` for ``n > 0`` is not included.\n",
      "        \n",
      "        >>> sc.jv(1, 0)\n",
      "        0.0\n",
      "    \n",
      "    jnjnp_zeros(nt)\n",
      "        Compute zeros of integer-order Bessel functions Jn and Jn'.\n",
      "        \n",
      "        Results are arranged in order of the magnitudes of the zeros.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number (<=1200) of zeros to compute\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        zo[l-1] : ndarray\n",
      "            Value of the lth zero of Jn(x) and Jn'(x). Of length `nt`.\n",
      "        n[l-1] : ndarray\n",
      "            Order of the Jn(x) or Jn'(x) associated with lth zero. Of length `nt`.\n",
      "        m[l-1] : ndarray\n",
      "            Serial number of the zeros of Jn(x) or Jn'(x) associated\n",
      "            with lth zero. Of length `nt`.\n",
      "        t[l-1] : ndarray\n",
      "            0 if lth zero in zo is zero of Jn(x), 1 if it is a zero of Jn'(x). Of\n",
      "            length `nt`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        jn_zeros, jnp_zeros : to get separated arrays of zeros.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    jnp_zeros(n, nt)\n",
      "        Compute zeros of integer-order Bessel function derivatives Jn'.\n",
      "        \n",
      "        Compute `nt` zeros of the functions :math:`J_n'(x)` on the\n",
      "        interval :math:`(0, \\infty)`. The zeros are returned in ascending\n",
      "        order. Note that this interval excludes the zero at :math:`x = 0`\n",
      "        that exists for :math:`n > 1`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Order of Bessel function\n",
      "        nt : int\n",
      "            Number of zeros to return\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `n` zeros of the Bessel function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        jvp, jv\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        We can check that we are getting approximations of the zeros by\n",
      "        evaluating them with `jvp`.\n",
      "        \n",
      "        >>> n = 2\n",
      "        >>> x = sc.jnp_zeros(n, 3)\n",
      "        >>> x\n",
      "        array([3.05423693, 6.70613319, 9.96946782])\n",
      "        >>> sc.jvp(n, x)\n",
      "        array([ 2.77555756e-17,  2.08166817e-16, -3.01841885e-16])\n",
      "        \n",
      "        Note that the zero at ``x = 0`` for ``n > 1`` is not included.\n",
      "        \n",
      "        >>> sc.jvp(n, 0)\n",
      "        0.0\n",
      "    \n",
      "    jnyn_zeros(n, nt)\n",
      "        Compute nt zeros of Bessel functions Jn(x), Jn'(x), Yn(x), and Yn'(x).\n",
      "        \n",
      "        Returns 4 arrays of length `nt`, corresponding to the first `nt`\n",
      "        zeros of Jn(x), Jn'(x), Yn(x), and Yn'(x), respectively. The zeros\n",
      "        are returned in ascending order.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Order of the Bessel functions\n",
      "        nt : int\n",
      "            Number (<=1200) of zeros to compute\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Jn : ndarray\n",
      "            First `nt` zeros of Jn\n",
      "        Jnp : ndarray\n",
      "            First `nt` zeros of Jn'\n",
      "        Yn : ndarray\n",
      "            First `nt` zeros of Yn\n",
      "        Ynp : ndarray\n",
      "            First `nt` zeros of Yn'\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        jn_zeros, jnp_zeros, yn_zeros, ynp_zeros\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    js_roots = roots_sh_jacobi(n, p1, q1, mu=False)\n",
      "        Gauss-Jacobi (shifted) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Jacobi (shifted)\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Jacobi polynomial, :math:`G^{p,q}_n(x)`. These sample\n",
      "        points and weights correctly integrate polynomials of degree\n",
      "        :math:`2n - 1` or less over the interval :math:`[0, 1]` with\n",
      "        weight function :math:`w(x) = (1 - x)^{p-q} x^{q-1}`. See 22.2.2\n",
      "        in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        p1 : float\n",
      "            (p1 - q1) must be > -1\n",
      "        q1 : float\n",
      "            q1 must be > 0\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    jvp(v, z, n=1)\n",
      "        Compute derivatives of Bessel functions of the first kind.\n",
      "        \n",
      "        Compute the nth derivative of the Bessel function `Jv` with\n",
      "        respect to `z`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : float\n",
      "            Order of Bessel function\n",
      "        z : complex\n",
      "            Argument at which to evaluate the derivative; can be real or\n",
      "            complex.\n",
      "        n : int, default 1\n",
      "            Order of derivative\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the derivative of the Bessel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The derivative is computed using the relation DLFM 10.6.7 [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.6.E7\n",
      "    \n",
      "    kei_zeros(nt)\n",
      "        Compute nt zeros of the Kelvin function kei.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kei\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    keip_zeros(nt)\n",
      "        Compute nt zeros of the derivative of the Kelvin function kei.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the derivative of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kei, keip\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    kelvin_zeros(nt)\n",
      "        Compute nt zeros of all Kelvin functions.\n",
      "        \n",
      "        Returned in a length-8 tuple of arrays of length nt.  The tuple contains\n",
      "        the arrays of zeros of (ber, bei, ker, kei, ber', bei', ker', kei').\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    ker_zeros(nt)\n",
      "        Compute nt zeros of the Kelvin function ker.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ker\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    kerp_zeros(nt)\n",
      "        Compute nt zeros of the derivative of the Kelvin function ker.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to compute. Must be positive.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `nt` zeros of the derivative of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ker, kerp\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    kvp(v, z, n=1)\n",
      "        Compute nth derivative of real-order modified Bessel function Kv(z)\n",
      "        \n",
      "        Kv(z) is the modified Bessel function of the second kind.\n",
      "        Derivative is calculated with respect to `z`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like of float\n",
      "            Order of Bessel function\n",
      "        z : array_like of complex\n",
      "            Argument at which to evaluate the derivative\n",
      "        n : int\n",
      "            Order of derivative.  Default is first derivative.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            The results\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Calculate multiple values at order 5:\n",
      "        \n",
      "        >>> from scipy.special import kvp\n",
      "        >>> kvp(5, (1, 2, 3+5j))\n",
      "        array([-1.84903536e+03+0.j        , -2.57735387e+01+0.j        ,\n",
      "               -3.06627741e-02+0.08750845j])\n",
      "        \n",
      "        \n",
      "        Calculate for a single value at multiple orders:\n",
      "        \n",
      "        >>> kvp((4, 4.5, 5), 1)\n",
      "        array([ -184.0309,  -568.9585, -1849.0354])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The derivative is computed using the relation DLFM 10.29.5 [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 6.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.29.E5\n",
      "    \n",
      "    l_roots = roots_laguerre(n, mu=False)\n",
      "        Gauss-Laguerre quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Laguerre\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Laguerre polynomial, :math:`L_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[0, \\infty]` with weight function\n",
      "        :math:`w(x) = e^{-x}`. See 22.2.13 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.laguerre.laggauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    la_roots = roots_genlaguerre(n, alpha, mu=False)\n",
      "        Gauss-generalized Laguerre quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-generalized\n",
      "        Laguerre quadrature. The sample points are the roots of the nth\n",
      "        degree generalized Laguerre polynomial, :math:`L^{\\alpha}_n(x)`.\n",
      "        These sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[0,\n",
      "        \\infty]` with weight function :math:`w(x) = x^{\\alpha}\n",
      "        e^{-x}`. See 22.3.9 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        alpha : float\n",
      "            alpha must be > -1\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    laguerre(n, monic=False)\n",
      "        Laguerre polynomial.\n",
      "        \n",
      "        Defined to be the solution of\n",
      "        \n",
      "        .. math::\n",
      "            x\\frac{d^2}{dx^2}L_n + (1 - x)\\frac{d}{dx}L_n + nL_n = 0;\n",
      "        \n",
      "        :math:`L_n` is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        L : orthopoly1d\n",
      "            Laguerre Polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`L_n` are orthogonal over :math:`[0,\n",
      "        \\infty)` with weight function :math:`e^{-x}`.\n",
      "    \n",
      "    lambertw(z, k=0, tol=1e-08)\n",
      "        lambertw(z, k=0, tol=1e-8)\n",
      "        \n",
      "        Lambert W function.\n",
      "        \n",
      "        The Lambert W function `W(z)` is defined as the inverse function\n",
      "        of ``w * exp(w)``. In other words, the value of ``W(z)`` is\n",
      "        such that ``z = W(z) * exp(W(z))`` for any complex number\n",
      "        ``z``.\n",
      "        \n",
      "        The Lambert W function is a multivalued function with infinitely\n",
      "        many branches. Each branch gives a separate solution of the\n",
      "        equation ``z = w exp(w)``. Here, the branches are indexed by the\n",
      "        integer `k`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Input argument.\n",
      "        k : int, optional\n",
      "            Branch index.\n",
      "        tol : float, optional\n",
      "            Evaluation tolerance.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        w : array\n",
      "            `w` will have the same shape as `z`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All branches are supported by `lambertw`:\n",
      "        \n",
      "        * ``lambertw(z)`` gives the principal solution (branch 0)\n",
      "        * ``lambertw(z, k)`` gives the solution on branch `k`\n",
      "        \n",
      "        The Lambert W function has two partially real branches: the\n",
      "        principal branch (`k = 0`) is real for real ``z > -1/e``, and the\n",
      "        ``k = -1`` branch is real for ``-1/e < z < 0``. All branches except\n",
      "        ``k = 0`` have a logarithmic singularity at ``z = 0``.\n",
      "        \n",
      "        **Possible issues**\n",
      "        \n",
      "        The evaluation can become inaccurate very close to the branch point\n",
      "        at ``-1/e``. In some corner cases, `lambertw` might currently\n",
      "        fail to converge, or can end up on the wrong branch.\n",
      "        \n",
      "        **Algorithm**\n",
      "        \n",
      "        Halley's iteration is used to invert ``w * exp(w)``, using a first-order\n",
      "        asymptotic approximation (O(log(w)) or `O(w)`) as the initial estimate.\n",
      "        \n",
      "        The definition, implementation and choice of branches is based on [2]_.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        wrightomega : the Wright Omega function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Lambert_W_function\n",
      "        .. [2] Corless et al, \"On the Lambert W function\", Adv. Comp. Math. 5\n",
      "           (1996) 329-359.\n",
      "           https://cs.uwaterloo.ca/research/tr/1993/03/W.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The Lambert W function is the inverse of ``w exp(w)``:\n",
      "        \n",
      "        >>> from scipy.special import lambertw\n",
      "        >>> w = lambertw(1)\n",
      "        >>> w\n",
      "        (0.56714329040978384+0j)\n",
      "        >>> w * np.exp(w)\n",
      "        (1.0+0j)\n",
      "        \n",
      "        Any branch gives a valid inverse:\n",
      "        \n",
      "        >>> w = lambertw(1, k=3)\n",
      "        >>> w\n",
      "        (-2.8535817554090377+17.113535539412148j)\n",
      "        >>> w*np.exp(w)\n",
      "        (1.0000000000000002+1.609823385706477e-15j)\n",
      "        \n",
      "        **Applications to equation-solving**\n",
      "        \n",
      "        The Lambert W function may be used to solve various kinds of\n",
      "        equations, such as finding the value of the infinite power\n",
      "        tower :math:`z^{z^{z^{\\ldots}}}`:\n",
      "        \n",
      "        >>> def tower(z, n):\n",
      "        ...     if n == 0:\n",
      "        ...         return z\n",
      "        ...     return z ** tower(z, n-1)\n",
      "        ...\n",
      "        >>> tower(0.5, 100)\n",
      "        0.641185744504986\n",
      "        >>> -lambertw(-np.log(0.5)) / np.log(0.5)\n",
      "        (0.64118574450498589+0j)\n",
      "    \n",
      "    legendre(n, monic=False)\n",
      "        Legendre polynomial.\n",
      "        \n",
      "        Defined to be the solution of\n",
      "        \n",
      "        .. math::\n",
      "            \\frac{d}{dx}\\left[(1 - x^2)\\frac{d}{dx}P_n(x)\\right]\n",
      "              + n(n + 1)P_n(x) = 0;\n",
      "        \n",
      "        :math:`P_n(x)` is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        P : orthopoly1d\n",
      "            Legendre polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`P_n` are orthogonal over :math:`[-1, 1]`\n",
      "        with weight function 1.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Generate the 3rd-order Legendre polynomial 1/2*(5x^3 + 0x^2 - 3x + 0):\n",
      "        \n",
      "        >>> from scipy.special import legendre\n",
      "        >>> legendre(3)\n",
      "        poly1d([ 2.5,  0. , -1.5,  0. ])\n",
      "    \n",
      "    lmbda(v, x)\n",
      "        Jahnke-Emden Lambda function, Lambdav(x).\n",
      "        \n",
      "        This function is defined as [2]_,\n",
      "        \n",
      "        .. math:: \\Lambda_v(x) = \\Gamma(v+1) \\frac{J_v(x)}{(x/2)^v},\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function and :math:`J_v` is the\n",
      "        Bessel function of the first kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : float\n",
      "            Order of the Lambda function\n",
      "        x : float\n",
      "            Value at which to evaluate the function and derivatives\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        vl : ndarray\n",
      "            Values of Lambda_vi(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.\n",
      "        dl : ndarray\n",
      "            Derivatives Lambda_vi'(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] Jahnke, E. and Emde, F. \"Tables of Functions with Formulae and\n",
      "               Curves\" (4th ed.), Dover, 1945\n",
      "    \n",
      "    log_softmax(x, axis=None)\n",
      "        Logarithm of softmax function::\n",
      "        \n",
      "            log_softmax(x) = log(softmax(x))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        axis : int or tuple of ints, optional\n",
      "            Axis to compute values along. Default is None and softmax will be\n",
      "            computed over the entire array `x`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s : ndarray or scalar\n",
      "            An array with the same shape as `x`. Exponential of the result will\n",
      "            sum to 1 along the specified axis. If `x` is a scalar, a scalar is\n",
      "            returned.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `log_softmax` is more accurate than ``np.log(softmax(x))`` with inputs that\n",
      "        make `softmax` saturate (see examples below).\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import log_softmax\n",
      "        >>> from scipy.special import softmax\n",
      "        >>> np.set_printoptions(precision=5)\n",
      "        \n",
      "        >>> x = np.array([1000.0, 1.0])\n",
      "        \n",
      "        >>> y = log_softmax(x)\n",
      "        >>> y\n",
      "        array([   0., -999.])\n",
      "        \n",
      "        >>> with np.errstate(divide='ignore'):\n",
      "        ...   y = np.log(softmax(x))\n",
      "        ...\n",
      "        >>> y\n",
      "        array([  0., -inf])\n",
      "    \n",
      "    logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False)\n",
      "        Compute the log of the sum of exponentials of input elements.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Input array.\n",
      "        axis : None or int or tuple of ints, optional\n",
      "            Axis or axes over which the sum is taken. By default `axis` is None,\n",
      "            and all elements are summed.\n",
      "        \n",
      "            .. versionadded:: 0.11.0\n",
      "        keepdims : bool, optional\n",
      "            If this is set to True, the axes which are reduced are left in the\n",
      "            result as dimensions with size one. With this option, the result\n",
      "            will broadcast correctly against the original array.\n",
      "        \n",
      "            .. versionadded:: 0.15.0\n",
      "        b : array-like, optional\n",
      "            Scaling factor for exp(`a`) must be of the same shape as `a` or\n",
      "            broadcastable to `a`. These values may be negative in order to\n",
      "            implement subtraction.\n",
      "        \n",
      "            .. versionadded:: 0.12.0\n",
      "        return_sign : bool, optional\n",
      "            If this is set to True, the result will be a pair containing sign\n",
      "            information; if False, results that are negative will be returned\n",
      "            as NaN. Default is False (no sign information).\n",
      "        \n",
      "            .. versionadded:: 0.16.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : ndarray\n",
      "            The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically\n",
      "            more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``\n",
      "            is returned.\n",
      "        sgn : ndarray\n",
      "            If return_sign is True, this will be an array of floating-point\n",
      "            numbers matching res and +1, 0, or -1 depending on the sign\n",
      "            of the result. If False, only one result is returned.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        numpy.logaddexp, numpy.logaddexp2\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        NumPy has a logaddexp function which is very similar to `logsumexp`, but\n",
      "        only handles two arguments. `logaddexp.reduce` is similar to this\n",
      "        function, but may be less stable.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import logsumexp\n",
      "        >>> a = np.arange(10)\n",
      "        >>> np.log(np.sum(np.exp(a)))\n",
      "        9.4586297444267107\n",
      "        >>> logsumexp(a)\n",
      "        9.4586297444267107\n",
      "        \n",
      "        With weights\n",
      "        \n",
      "        >>> a = np.arange(10)\n",
      "        >>> b = np.arange(10, 0, -1)\n",
      "        >>> logsumexp(a, b=b)\n",
      "        9.9170178533034665\n",
      "        >>> np.log(np.sum(b*np.exp(a)))\n",
      "        9.9170178533034647\n",
      "        \n",
      "        Returning a sign flag\n",
      "        \n",
      "        >>> logsumexp([1,2],b=[1,-1],return_sign=True)\n",
      "        (1.5413248546129181, -1.0)\n",
      "        \n",
      "        Notice that `logsumexp` does not directly support masked arrays. To use it\n",
      "        on a masked array, convert the mask into zero weights:\n",
      "        \n",
      "        >>> a = np.ma.array([np.log(2), 2, np.log(3)],\n",
      "        ...                  mask=[False, True, False])\n",
      "        >>> b = (~a.mask).astype(int)\n",
      "        >>> logsumexp(a.data, b=b), np.log(5)\n",
      "        1.6094379124341005, 1.6094379124341005\n",
      "    \n",
      "    lpmn(m, n, z)\n",
      "        Sequence of associated Legendre functions of the first kind.\n",
      "        \n",
      "        Computes the associated Legendre function of the first kind of order m and\n",
      "        degree n, ``Pmn(z)`` = :math:`P_n^m(z)`, and its derivative, ``Pmn'(z)``.\n",
      "        Returns two arrays of size ``(m+1, n+1)`` containing ``Pmn(z)`` and\n",
      "        ``Pmn'(z)`` for all orders from ``0..m`` and degrees from ``0..n``.\n",
      "        \n",
      "        This function takes a real argument ``z``. For complex arguments ``z``\n",
      "        use clpmn instead.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : int\n",
      "           ``|m| <= n``; the order of the Legendre function.\n",
      "        n : int\n",
      "           where ``n >= 0``; the degree of the Legendre function.  Often\n",
      "           called ``l`` (lower case L) in descriptions of the associated\n",
      "           Legendre function\n",
      "        z : float\n",
      "            Input value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Pmn_z : (m+1, n+1) array\n",
      "           Values for all orders 0..m and degrees 0..n\n",
      "        Pmn_d_z : (m+1, n+1) array\n",
      "           Derivatives for all orders 0..m and degrees 0..n\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        clpmn: associated Legendre functions of the first kind for complex z\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In the interval (-1, 1), Ferrer's function of the first kind is\n",
      "        returned. The phase convention used for the intervals (1, inf)\n",
      "        and (-inf, -1) is such that the result is always real.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/14.3\n",
      "    \n",
      "    lpn(n, z)\n",
      "        Legendre function of the first kind.\n",
      "        \n",
      "        Compute sequence of Legendre functions of the first kind (polynomials),\n",
      "        Pn(z) and derivatives for all degrees from 0 to n (inclusive).\n",
      "        \n",
      "        See also special.legendre for polynomial class.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    lqmn(m, n, z)\n",
      "        Sequence of associated Legendre functions of the second kind.\n",
      "        \n",
      "        Computes the associated Legendre function of the second kind of order m and\n",
      "        degree n, ``Qmn(z)`` = :math:`Q_n^m(z)`, and its derivative, ``Qmn'(z)``.\n",
      "        Returns two arrays of size ``(m+1, n+1)`` containing ``Qmn(z)`` and\n",
      "        ``Qmn'(z)`` for all orders from ``0..m`` and degrees from ``0..n``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : int\n",
      "           ``|m| <= n``; the order of the Legendre function.\n",
      "        n : int\n",
      "           where ``n >= 0``; the degree of the Legendre function.  Often\n",
      "           called ``l`` (lower case L) in descriptions of the associated\n",
      "           Legendre function\n",
      "        z : complex\n",
      "            Input value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Qmn_z : (m+1, n+1) array\n",
      "           Values for all orders 0..m and degrees 0..n\n",
      "        Qmn_d_z : (m+1, n+1) array\n",
      "           Derivatives for all orders 0..m and degrees 0..n\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    lqn(n, z)\n",
      "        Legendre function of the second kind.\n",
      "        \n",
      "        Compute sequence of Legendre functions of the second kind, Qn(z) and\n",
      "        derivatives for all degrees from 0 to n (inclusive).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    mathieu_even_coef(m, q)\n",
      "        Fourier coefficients for even Mathieu and modified Mathieu functions.\n",
      "        \n",
      "        The Fourier series of the even solutions of the Mathieu differential\n",
      "        equation are of the form\n",
      "        \n",
      "        .. math:: \\mathrm{ce}_{2n}(z, q) = \\sum_{k=0}^{\\infty} A_{(2n)}^{(2k)} \\cos 2kz\n",
      "        \n",
      "        .. math:: \\mathrm{ce}_{2n+1}(z, q) = \\sum_{k=0}^{\\infty} A_{(2n+1)}^{(2k+1)} \\cos (2k+1)z\n",
      "        \n",
      "        This function returns the coefficients :math:`A_{(2n)}^{(2k)}` for even\n",
      "        input m=2n, and the coefficients :math:`A_{(2n+1)}^{(2k+1)}` for odd input\n",
      "        m=2n+1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : int\n",
      "            Order of Mathieu functions.  Must be non-negative.\n",
      "        q : float (>=0)\n",
      "            Parameter of Mathieu functions.  Must be non-negative.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Ak : ndarray\n",
      "            Even or odd Fourier coefficients, corresponding to even or odd m.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/28.4#i\n",
      "    \n",
      "    mathieu_odd_coef(m, q)\n",
      "        Fourier coefficients for even Mathieu and modified Mathieu functions.\n",
      "        \n",
      "        The Fourier series of the odd solutions of the Mathieu differential\n",
      "        equation are of the form\n",
      "        \n",
      "        .. math:: \\mathrm{se}_{2n+1}(z, q) = \\sum_{k=0}^{\\infty} B_{(2n+1)}^{(2k+1)} \\sin (2k+1)z\n",
      "        \n",
      "        .. math:: \\mathrm{se}_{2n+2}(z, q) = \\sum_{k=0}^{\\infty} B_{(2n+2)}^{(2k+2)} \\sin (2k+2)z\n",
      "        \n",
      "        This function returns the coefficients :math:`B_{(2n+2)}^{(2k+2)}` for even\n",
      "        input m=2n+2, and the coefficients :math:`B_{(2n+1)}^{(2k+1)}` for odd\n",
      "        input m=2n+1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : int\n",
      "            Order of Mathieu functions.  Must be non-negative.\n",
      "        q : float (>=0)\n",
      "            Parameter of Mathieu functions.  Must be non-negative.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Bk : ndarray\n",
      "            Even or odd Fourier coefficients, corresponding to even or odd m.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    multigammaln(a, d)\n",
      "        Returns the log of multivariate gamma, also sometimes called the\n",
      "        generalized gamma.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : ndarray\n",
      "            The multivariate gamma is computed for each item of `a`.\n",
      "        d : int\n",
      "            The dimension of the space of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : ndarray\n",
      "            The values of the log multivariate gamma at the given points `a`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The formal definition of the multivariate gamma of dimension d for a real\n",
      "        `a` is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\Gamma_d(a) = \\int_{A>0} e^{-tr(A)} |A|^{a - (d+1)/2} dA\n",
      "        \n",
      "        with the condition :math:`a > (d-1)/2`, and :math:`A > 0` being the set of\n",
      "        all the positive definite matrices of dimension `d`.  Note that `a` is a\n",
      "        scalar: the integrand only is multivariate, the argument is not (the\n",
      "        function is defined over a subset of the real set).\n",
      "        \n",
      "        This can be proven to be equal to the much friendlier equation\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\Gamma_d(a) = \\pi^{d(d-1)/4} \\prod_{i=1}^{d} \\Gamma(a - (i-1)/2).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        R. J. Muirhead, Aspects of multivariate statistical theory (Wiley Series in\n",
      "        probability and mathematical statistics).\n",
      "    \n",
      "    obl_cv_seq(m, n, c)\n",
      "        Characteristic values for oblate spheroidal wave functions.\n",
      "        \n",
      "        Compute a sequence of characteristic values for the oblate\n",
      "        spheroidal wave functions for mode m and n'=m..n and spheroidal\n",
      "        parameter c.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    p_roots = roots_legendre(n, mu=False)\n",
      "        Gauss-Legendre quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Legendre\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Legendre polynomial :math:`P_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[-1, 1]` with weight function\n",
      "        :math:`w(x) = 1.0`. See 2.2.10 in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.legendre.leggauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    pbdn_seq(n, z)\n",
      "        Parabolic cylinder functions Dn(z) and derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Order of the parabolic cylinder function\n",
      "        z : complex\n",
      "            Value at which to evaluate the function and derivatives\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dv : ndarray\n",
      "            Values of D_i(z), for i=0, ..., i=n.\n",
      "        dp : ndarray\n",
      "            Derivatives D_i'(z), for i=0, ..., i=n.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 13.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    pbdv_seq(v, x)\n",
      "        Parabolic cylinder functions Dv(x) and derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : float\n",
      "            Order of the parabolic cylinder function\n",
      "        x : float\n",
      "            Value at which to evaluate the function and derivatives\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dv : ndarray\n",
      "            Values of D_vi(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.\n",
      "        dp : ndarray\n",
      "            Derivatives D_vi'(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 13.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    pbvv_seq(v, x)\n",
      "        Parabolic cylinder functions Vv(x) and derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : float\n",
      "            Order of the parabolic cylinder function\n",
      "        x : float\n",
      "            Value at which to evaluate the function and derivatives\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dv : ndarray\n",
      "            Values of V_vi(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.\n",
      "        dp : ndarray\n",
      "            Derivatives V_vi'(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 13.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    perm(N, k, exact=False)\n",
      "        Permutations of N things taken k at a time, i.e., k-permutations of N.\n",
      "        \n",
      "        It's also known as \"partial permutations\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        N : int, ndarray\n",
      "            Number of things.\n",
      "        k : int, ndarray\n",
      "            Number of elements taken.\n",
      "        exact : bool, optional\n",
      "            If `exact` is False, then floating point precision is used, otherwise\n",
      "            exact long integer is computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        val : int, ndarray\n",
      "            The number of k-permutations of N.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        - Array arguments accepted only for exact=False case.\n",
      "        - If k > N, N < 0, or k < 0, then a 0 is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import perm\n",
      "        >>> k = np.array([3, 4])\n",
      "        >>> n = np.array([10, 10])\n",
      "        >>> perm(n, k)\n",
      "        array([  720.,  5040.])\n",
      "        >>> perm(10, 3, exact=True)\n",
      "        720\n",
      "    \n",
      "    polygamma(n, x)\n",
      "        Polygamma functions.\n",
      "        \n",
      "        Defined as :math:`\\psi^{(n)}(x)` where :math:`\\psi` is the\n",
      "        `digamma` function. See [dlmf]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            The order of the derivative of the digamma function; must be\n",
      "            integral\n",
      "        x : array_like\n",
      "            Real valued input\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            Function results\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        digamma\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/5.15\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> x = [2, 3, 25.5]\n",
      "        >>> special.polygamma(1, x)\n",
      "        array([ 0.64493407,  0.39493407,  0.03999467])\n",
      "        >>> special.polygamma(0, x) == special.psi(x)\n",
      "        array([ True,  True,  True], dtype=bool)\n",
      "    \n",
      "    pro_cv_seq(m, n, c)\n",
      "        Characteristic values for prolate spheroidal wave functions.\n",
      "        \n",
      "        Compute a sequence of characteristic values for the prolate\n",
      "        spheroidal wave functions for mode m and n'=m..n and spheroidal\n",
      "        parameter c.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    ps_roots = roots_sh_legendre(n, mu=False)\n",
      "        Gauss-Legendre (shifted) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Legendre\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Legendre polynomial :math:`P^*_n(x)`. These sample points\n",
      "        and weights correctly integrate polynomials of degree :math:`2n -\n",
      "        1` or less over the interval :math:`[0, 1]` with weight function\n",
      "        :math:`w(x) = 1.0`. See 2.2.11 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    riccati_jn(n, x)\n",
      "        Compute Ricatti-Bessel function of the first kind and its derivative.\n",
      "        \n",
      "        The Ricatti-Bessel function of the first kind is defined as :math:`x\n",
      "        j_n(x)`, where :math:`j_n` is the spherical Bessel function of the first\n",
      "        kind of order :math:`n`.\n",
      "        \n",
      "        This function computes the value and first derivative of the\n",
      "        Ricatti-Bessel function for all orders up to and including `n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Maximum order of function to compute\n",
      "        x : float\n",
      "            Argument at which to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        jn : ndarray\n",
      "            Value of j0(x), ..., jn(x)\n",
      "        jnp : ndarray\n",
      "            First derivative j0'(x), ..., jn'(x)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The computation is carried out via backward recurrence, using the\n",
      "        relation DLMF 10.51.1 [2]_.\n",
      "        \n",
      "        Wrapper for a Fortran routine created by Shanjie Zhang and Jianming\n",
      "        Jin [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.51.E1\n",
      "    \n",
      "    riccati_yn(n, x)\n",
      "        Compute Ricatti-Bessel function of the second kind and its derivative.\n",
      "        \n",
      "        The Ricatti-Bessel function of the second kind is defined as :math:`x\n",
      "        y_n(x)`, where :math:`y_n` is the spherical Bessel function of the second\n",
      "        kind of order :math:`n`.\n",
      "        \n",
      "        This function computes the value and first derivative of the function for\n",
      "        all orders up to and including `n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Maximum order of function to compute\n",
      "        x : float\n",
      "            Argument at which to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        yn : ndarray\n",
      "            Value of y0(x), ..., yn(x)\n",
      "        ynp : ndarray\n",
      "            First derivative y0'(x), ..., yn'(x)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The computation is carried out via ascending recurrence, using the\n",
      "        relation DLMF 10.51.1 [2]_.\n",
      "        \n",
      "        Wrapper for a Fortran routine created by Shanjie Zhang and Jianming\n",
      "        Jin [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.51.E1\n",
      "    \n",
      "    roots_chebyc(n, mu=False)\n",
      "        Gauss-Chebyshev (first kind) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the first kind, :math:`C_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-2, 2]`\n",
      "        with weight function :math:`w(x) = 1 / \\sqrt{1 - (x/2)^2}`. See\n",
      "        22.2.6 in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_chebys(n, mu=False)\n",
      "        Gauss-Chebyshev (second kind) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the second kind, :math:`S_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-2, 2]`\n",
      "        with weight function :math:`w(x) = \\sqrt{1 - (x/2)^2}`. See 22.2.7\n",
      "        in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_chebyt(n, mu=False)\n",
      "        Gauss-Chebyshev (first kind) quadrature.\n",
      "        \n",
      "        Computes the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the first kind, :math:`T_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-1, 1]`\n",
      "        with weight function :math:`w(x) = 1/\\sqrt{1 - x^2}`. See 22.2.4\n",
      "        in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.chebyshev.chebgauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_chebyu(n, mu=False)\n",
      "        Gauss-Chebyshev (second kind) quadrature.\n",
      "        \n",
      "        Computes the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the second kind, :math:`U_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-1, 1]`\n",
      "        with weight function :math:`w(x) = \\sqrt{1 - x^2}`. See 22.2.5 in\n",
      "        [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_gegenbauer(n, alpha, mu=False)\n",
      "        Gauss-Gegenbauer quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Gegenbauer\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Gegenbauer polynomial, :math:`C^{\\alpha}_n(x)`. These sample\n",
      "        points and weights correctly integrate polynomials of degree\n",
      "        :math:`2n - 1` or less over the interval :math:`[-1, 1]` with\n",
      "        weight function :math:`w(x) = (1 - x^2)^{\\alpha - 1/2}`. See\n",
      "        22.2.3 in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        alpha : float\n",
      "            alpha must be > -0.5\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_genlaguerre(n, alpha, mu=False)\n",
      "        Gauss-generalized Laguerre quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-generalized\n",
      "        Laguerre quadrature. The sample points are the roots of the nth\n",
      "        degree generalized Laguerre polynomial, :math:`L^{\\alpha}_n(x)`.\n",
      "        These sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[0,\n",
      "        \\infty]` with weight function :math:`w(x) = x^{\\alpha}\n",
      "        e^{-x}`. See 22.3.9 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        alpha : float\n",
      "            alpha must be > -1\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_hermite(n, mu=False)\n",
      "        Gauss-Hermite (physicist's) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Hermite\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Hermite polynomial, :math:`H_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[-\\infty, \\infty]` with weight\n",
      "        function :math:`w(x) = e^{-x^2}`. See 22.2.14 in [AS]_ for\n",
      "        details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For small n up to 150 a modified version of the Golub-Welsch\n",
      "        algorithm is used. Nodes are computed from the eigenvalue\n",
      "        problem and improved by one step of a Newton iteration.\n",
      "        The weights are computed from the well-known analytical formula.\n",
      "        \n",
      "        For n larger than 150 an optimal asymptotic algorithm is applied\n",
      "        which computes nodes and weights in a numerically stable manner.\n",
      "        The algorithm has linear runtime making computation for very\n",
      "        large n (several thousand or more) feasible.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.hermite.hermgauss\n",
      "        roots_hermitenorm\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [townsend.trogdon.olver-2014]\n",
      "            Townsend, A. and Trogdon, T. and Olver, S. (2014)\n",
      "            *Fast computation of Gauss quadrature nodes and\n",
      "            weights on the whole real line*. :arXiv:`1410.5286`.\n",
      "        .. [townsend.trogdon.olver-2015]\n",
      "            Townsend, A. and Trogdon, T. and Olver, S. (2015)\n",
      "            *Fast computation of Gauss quadrature nodes and\n",
      "            weights on the whole real line*.\n",
      "            IMA Journal of Numerical Analysis\n",
      "            :doi:`10.1093/imanum/drv002`.\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_hermitenorm(n, mu=False)\n",
      "        Gauss-Hermite (statistician's) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Hermite\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Hermite polynomial, :math:`He_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[-\\infty, \\infty]` with weight\n",
      "        function :math:`w(x) = e^{-x^2/2}`. See 22.2.15 in [AS]_ for more\n",
      "        details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For small n up to 150 a modified version of the Golub-Welsch\n",
      "        algorithm is used. Nodes are computed from the eigenvalue\n",
      "        problem and improved by one step of a Newton iteration.\n",
      "        The weights are computed from the well-known analytical formula.\n",
      "        \n",
      "        For n larger than 150 an optimal asymptotic algorithm is used\n",
      "        which computes nodes and weights in a numerical stable manner.\n",
      "        The algorithm has linear runtime making computation for very\n",
      "        large n (several thousand or more) feasible.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.hermite_e.hermegauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_jacobi(n, alpha, beta, mu=False)\n",
      "        Gauss-Jacobi quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Jacobi\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Jacobi polynomial, :math:`P^{\\alpha, \\beta}_n(x)`. These sample\n",
      "        points and weights correctly integrate polynomials of degree\n",
      "        :math:`2n - 1` or less over the interval :math:`[-1, 1]` with\n",
      "        weight function :math:`w(x) = (1 - x)^{\\alpha} (1 +\n",
      "        x)^{\\beta}`. See 22.2.1 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        alpha : float\n",
      "            alpha must be > -1\n",
      "        beta : float\n",
      "            beta must be > -1\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_laguerre(n, mu=False)\n",
      "        Gauss-Laguerre quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Laguerre\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Laguerre polynomial, :math:`L_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[0, \\infty]` with weight function\n",
      "        :math:`w(x) = e^{-x}`. See 22.2.13 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.laguerre.laggauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_legendre(n, mu=False)\n",
      "        Gauss-Legendre quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Legendre\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Legendre polynomial :math:`P_n(x)`. These sample points and\n",
      "        weights correctly integrate polynomials of degree :math:`2n - 1`\n",
      "        or less over the interval :math:`[-1, 1]` with weight function\n",
      "        :math:`w(x) = 1.0`. See 2.2.10 in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.legendre.leggauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_sh_chebyt(n, mu=False)\n",
      "        Gauss-Chebyshev (first kind, shifted) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Chebyshev polynomial of the first kind, :math:`T_n(x)`.\n",
      "        These sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[0, 1]`\n",
      "        with weight function :math:`w(x) = 1/\\sqrt{x - x^2}`. See 22.2.8\n",
      "        in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_sh_chebyu(n, mu=False)\n",
      "        Gauss-Chebyshev (second kind, shifted) quadrature.\n",
      "        \n",
      "        Computes the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Chebyshev polynomial of the second kind, :math:`U_n(x)`.\n",
      "        These sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[0, 1]`\n",
      "        with weight function :math:`w(x) = \\sqrt{x - x^2}`. See 22.2.9 in\n",
      "        [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_sh_jacobi(n, p1, q1, mu=False)\n",
      "        Gauss-Jacobi (shifted) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Jacobi (shifted)\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Jacobi polynomial, :math:`G^{p,q}_n(x)`. These sample\n",
      "        points and weights correctly integrate polynomials of degree\n",
      "        :math:`2n - 1` or less over the interval :math:`[0, 1]` with\n",
      "        weight function :math:`w(x) = (1 - x)^{p-q} x^{q-1}`. See 22.2.2\n",
      "        in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        p1 : float\n",
      "            (p1 - q1) must be > -1\n",
      "        q1 : float\n",
      "            q1 must be > 0\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    roots_sh_legendre(n, mu=False)\n",
      "        Gauss-Legendre (shifted) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Legendre\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Legendre polynomial :math:`P^*_n(x)`. These sample points\n",
      "        and weights correctly integrate polynomials of degree :math:`2n -\n",
      "        1` or less over the interval :math:`[0, 1]` with weight function\n",
      "        :math:`w(x) = 1.0`. See 2.2.11 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    s_roots = roots_chebys(n, mu=False)\n",
      "        Gauss-Chebyshev (second kind) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the second kind, :math:`S_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-2, 2]`\n",
      "        with weight function :math:`w(x) = \\sqrt{1 - (x/2)^2}`. See 22.2.7\n",
      "        in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    seterr(...)\n",
      "        Set how special-function errors are handled.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        all : {'ignore', 'warn' 'raise'}, optional\n",
      "            Set treatment for all type of special-function errors at\n",
      "            once. The options are:\n",
      "        \n",
      "            - 'ignore' Take no action when the error occurs\n",
      "            - 'warn' Print a `SpecialFunctionWarning` when the error\n",
      "              occurs (via the Python `warnings` module)\n",
      "            - 'raise' Raise a `SpecialFunctionError` when the error\n",
      "              occurs.\n",
      "        \n",
      "            The default is to not change the current behavior. If\n",
      "            behaviors for additional categories of special-function errors\n",
      "            are specified, then ``all`` is applied first, followed by the\n",
      "            additional categories.\n",
      "        singular : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for singularities.\n",
      "        underflow : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for underflow.\n",
      "        overflow : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for overflow.\n",
      "        slow : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for slow convergence.\n",
      "        loss : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for loss of accuracy.\n",
      "        no_result : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for failing to find a result.\n",
      "        domain : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for an invalid argument to a function.\n",
      "        arg : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for an invalid parameter to a function.\n",
      "        other : {'ignore', 'warn', 'raise'}, optional\n",
      "            Treatment for an unknown error.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        olderr : dict\n",
      "            Dictionary containing the old settings.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        geterr : get the current way of handling special-function errors\n",
      "        errstate : context manager for special-function error handling\n",
      "        numpy.seterr : similar numpy function for floating-point errors\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        >>> from pytest import raises\n",
      "        >>> sc.gammaln(0)\n",
      "        inf\n",
      "        >>> olderr = sc.seterr(singular='raise')\n",
      "        >>> with raises(sc.SpecialFunctionError):\n",
      "        ...     sc.gammaln(0)\n",
      "        ...\n",
      "        >>> _ = sc.seterr(**olderr)\n",
      "        \n",
      "        We can also raise for every category except one.\n",
      "        \n",
      "        >>> olderr = sc.seterr(all='raise', singular='ignore')\n",
      "        >>> sc.gammaln(0)\n",
      "        inf\n",
      "        >>> with raises(sc.SpecialFunctionError):\n",
      "        ...     sc.spence(-1)\n",
      "        ...\n",
      "        >>> _ = sc.seterr(**olderr)\n",
      "    \n",
      "    sh_chebyt(n, monic=False)\n",
      "        Shifted Chebyshev polynomial of the first kind.\n",
      "        \n",
      "        Defined as :math:`T^*_n(x) = T_n(2x - 1)` for :math:`T_n` the nth\n",
      "        Chebyshev polynomial of the first kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        T : orthopoly1d\n",
      "            Shifted Chebyshev polynomial of the first kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`T^*_n` are orthogonal over :math:`[0, 1]`\n",
      "        with weight function :math:`(x - x^2)^{-1/2}`.\n",
      "    \n",
      "    sh_chebyu(n, monic=False)\n",
      "        Shifted Chebyshev polynomial of the second kind.\n",
      "        \n",
      "        Defined as :math:`U^*_n(x) = U_n(2x - 1)` for :math:`U_n` the nth\n",
      "        Chebyshev polynomial of the second kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        U : orthopoly1d\n",
      "            Shifted Chebyshev polynomial of the second kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`U^*_n` are orthogonal over :math:`[0, 1]`\n",
      "        with weight function :math:`(x - x^2)^{1/2}`.\n",
      "    \n",
      "    sh_jacobi(n, p, q, monic=False)\n",
      "        Shifted Jacobi polynomial.\n",
      "        \n",
      "        Defined by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            G_n^{(p, q)}(x)\n",
      "              = \\binom{2n + p - 1}{n}^{-1}P_n^{(p - q, q - 1)}(2x - 1),\n",
      "        \n",
      "        where :math:`P_n^{(\\cdot, \\cdot)}` is the nth Jacobi polynomial.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        p : float\n",
      "            Parameter, must have :math:`p > q - 1`.\n",
      "        q : float\n",
      "            Parameter, must be greater than 0.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        G : orthopoly1d\n",
      "            Shifted Jacobi polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For fixed :math:`p, q`, the polynomials :math:`G_n^{(p, q)}` are\n",
      "        orthogonal over :math:`[0, 1]` with weight function :math:`(1 -\n",
      "        x)^{p - q}x^{q - 1}`.\n",
      "    \n",
      "    sh_legendre(n, monic=False)\n",
      "        Shifted Legendre polynomial.\n",
      "        \n",
      "        Defined as :math:`P^*_n(x) = P_n(2x - 1)` for :math:`P_n` the nth\n",
      "        Legendre polynomial.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial.\n",
      "        monic : bool, optional\n",
      "            If `True`, scale the leading coefficient to be 1. Default is\n",
      "            `False`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        P : orthopoly1d\n",
      "            Shifted Legendre polynomial.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The polynomials :math:`P^*_n` are orthogonal over :math:`[0, 1]`\n",
      "        with weight function 1.\n",
      "    \n",
      "    sinc(x)\n",
      "        Return the normalized sinc function.\n",
      "        \n",
      "        The sinc function is :math:`\\sin(\\pi x)/(\\pi x)`.\n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            Note the normalization factor of ``pi`` used in the definition.\n",
      "            This is the most commonly used definition in signal processing.\n",
      "            Use ``sinc(x / np.pi)`` to obtain the unnormalized sinc function\n",
      "            :math:`\\sin(x)/(x)` that is more common in mathematics.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Array (possibly multi-dimensional) of values for which to to\n",
      "            calculate ``sinc(x)``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            ``sinc(x)``, which has the same shape as the input.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        ``sinc(0)`` is the limit value 1.\n",
      "        \n",
      "        The name sinc is short for \"sine cardinal\" or \"sinus cardinalis\".\n",
      "        \n",
      "        The sinc function is used in various signal processing applications,\n",
      "        including in anti-aliasing, in the construction of a Lanczos resampling\n",
      "        filter, and in interpolation.\n",
      "        \n",
      "        For bandlimited interpolation of discrete-time signals, the ideal\n",
      "        interpolation kernel is proportional to the sinc function.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Weisstein, Eric W. \"Sinc Function.\" From MathWorld--A Wolfram Web\n",
      "               Resource. http://mathworld.wolfram.com/SincFunction.html\n",
      "        .. [2] Wikipedia, \"Sinc function\",\n",
      "               https://en.wikipedia.org/wiki/Sinc_function\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(-4, 4, 41)\n",
      "        >>> np.sinc(x)\n",
      "         array([-3.89804309e-17,  -4.92362781e-02,  -8.40918587e-02, # may vary\n",
      "                -8.90384387e-02,  -5.84680802e-02,   3.89804309e-17,\n",
      "                6.68206631e-02,   1.16434881e-01,   1.26137788e-01,\n",
      "                8.50444803e-02,  -3.89804309e-17,  -1.03943254e-01,\n",
      "                -1.89206682e-01,  -2.16236208e-01,  -1.55914881e-01,\n",
      "                3.89804309e-17,   2.33872321e-01,   5.04551152e-01,\n",
      "                7.56826729e-01,   9.35489284e-01,   1.00000000e+00,\n",
      "                9.35489284e-01,   7.56826729e-01,   5.04551152e-01,\n",
      "                2.33872321e-01,   3.89804309e-17,  -1.55914881e-01,\n",
      "               -2.16236208e-01,  -1.89206682e-01,  -1.03943254e-01,\n",
      "               -3.89804309e-17,   8.50444803e-02,   1.26137788e-01,\n",
      "                1.16434881e-01,   6.68206631e-02,   3.89804309e-17,\n",
      "                -5.84680802e-02,  -8.90384387e-02,  -8.40918587e-02,\n",
      "                -4.92362781e-02,  -3.89804309e-17])\n",
      "        \n",
      "        >>> plt.plot(x, np.sinc(x))\n",
      "        [<matplotlib.lines.Line2D object at 0x...>]\n",
      "        >>> plt.title(\"Sinc Function\")\n",
      "        Text(0.5, 1.0, 'Sinc Function')\n",
      "        >>> plt.ylabel(\"Amplitude\")\n",
      "        Text(0, 0.5, 'Amplitude')\n",
      "        >>> plt.xlabel(\"X\")\n",
      "        Text(0.5, 0, 'X')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    softmax(x, axis=None)\n",
      "        Softmax function\n",
      "        \n",
      "        The softmax function transforms each element of a collection by\n",
      "        computing the exponential of each element divided by the sum of the\n",
      "        exponentials of all the elements. That is, if `x` is a one-dimensional\n",
      "        numpy array::\n",
      "        \n",
      "            softmax(x) = np.exp(x)/sum(np.exp(x))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Input array.\n",
      "        axis : int or tuple of ints, optional\n",
      "            Axis to compute values along. Default is None and softmax will be\n",
      "            computed over the entire array `x`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s : ndarray\n",
      "            An array the same shape as `x`. The result will sum to 1 along the\n",
      "            specified axis.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The formula for the softmax function :math:`\\sigma(x)` for a vector\n",
      "        :math:`x = \\{x_0, x_1, ..., x_{n-1}\\}` is\n",
      "        \n",
      "        .. math:: \\sigma(x)_j = \\frac{e^{x_j}}{\\sum_k e^{x_k}}\n",
      "        \n",
      "        The `softmax` function is the gradient of `logsumexp`.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import softmax\n",
      "        >>> np.set_printoptions(precision=5)\n",
      "        \n",
      "        >>> x = np.array([[1, 0.5, 0.2, 3],\n",
      "        ...               [1,  -1,   7, 3],\n",
      "        ...               [2,  12,  13, 3]])\n",
      "        ...\n",
      "        \n",
      "        Compute the softmax transformation over the entire array.\n",
      "        \n",
      "        >>> m = softmax(x)\n",
      "        >>> m\n",
      "        array([[  4.48309e-06,   2.71913e-06,   2.01438e-06,   3.31258e-05],\n",
      "               [  4.48309e-06,   6.06720e-07,   1.80861e-03,   3.31258e-05],\n",
      "               [  1.21863e-05,   2.68421e-01,   7.29644e-01,   3.31258e-05]])\n",
      "        \n",
      "        >>> m.sum()\n",
      "        1.0000000000000002\n",
      "        \n",
      "        Compute the softmax transformation along the first axis (i.e., the\n",
      "        columns).\n",
      "        \n",
      "        >>> m = softmax(x, axis=0)\n",
      "        \n",
      "        >>> m\n",
      "        array([[  2.11942e-01,   1.01300e-05,   2.75394e-06,   3.33333e-01],\n",
      "               [  2.11942e-01,   2.26030e-06,   2.47262e-03,   3.33333e-01],\n",
      "               [  5.76117e-01,   9.99988e-01,   9.97525e-01,   3.33333e-01]])\n",
      "        \n",
      "        >>> m.sum(axis=0)\n",
      "        array([ 1.,  1.,  1.,  1.])\n",
      "        \n",
      "        Compute the softmax transformation along the second axis (i.e., the rows).\n",
      "        \n",
      "        >>> m = softmax(x, axis=1)\n",
      "        >>> m\n",
      "        array([[  1.05877e-01,   6.42177e-02,   4.75736e-02,   7.82332e-01],\n",
      "               [  2.42746e-03,   3.28521e-04,   9.79307e-01,   1.79366e-02],\n",
      "               [  1.22094e-05,   2.68929e-01,   7.31025e-01,   3.31885e-05]])\n",
      "        \n",
      "        >>> m.sum(axis=1)\n",
      "        array([ 1.,  1.,  1.])\n",
      "    \n",
      "    spherical_in(n, z, derivative=False)\n",
      "        Modified spherical Bessel function of the first kind or its derivative.\n",
      "        \n",
      "        Defined as [1]_,\n",
      "        \n",
      "        .. math:: i_n(z) = \\sqrt{\\frac{\\pi}{2z}} I_{n + 1/2}(z),\n",
      "        \n",
      "        where :math:`I_n` is the modified Bessel function of the first kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, array_like\n",
      "            Order of the Bessel function (n >= 0).\n",
      "        z : complex or float, array_like\n",
      "            Argument of the Bessel function.\n",
      "        derivative : bool, optional\n",
      "            If True, the value of the derivative (rather than the function\n",
      "            itself) is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        in : ndarray\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is computed using its definitional relation to the\n",
      "        modified cylindrical Bessel function of the first kind.\n",
      "        \n",
      "        The derivative is computed using the relations [2]_,\n",
      "        \n",
      "        .. math::\n",
      "            i_n' = i_{n-1} - \\frac{n + 1}{z} i_n.\n",
      "        \n",
      "            i_1' = i_0\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://dlmf.nist.gov/10.47.E7\n",
      "        .. [2] https://dlmf.nist.gov/10.51.E5\n",
      "    \n",
      "    spherical_jn(n, z, derivative=False)\n",
      "        Spherical Bessel function of the first kind or its derivative.\n",
      "        \n",
      "        Defined as [1]_,\n",
      "        \n",
      "        .. math:: j_n(z) = \\sqrt{\\frac{\\pi}{2z}} J_{n + 1/2}(z),\n",
      "        \n",
      "        where :math:`J_n` is the Bessel function of the first kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, array_like\n",
      "            Order of the Bessel function (n >= 0).\n",
      "        z : complex or float, array_like\n",
      "            Argument of the Bessel function.\n",
      "        derivative : bool, optional\n",
      "            If True, the value of the derivative (rather than the function\n",
      "            itself) is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        jn : ndarray\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For real arguments greater than the order, the function is computed\n",
      "        using the ascending recurrence [2]_. For small real or complex\n",
      "        arguments, the definitional relation to the cylindrical Bessel function\n",
      "        of the first kind is used.\n",
      "        \n",
      "        The derivative is computed using the relations [3]_,\n",
      "        \n",
      "        .. math::\n",
      "            j_n'(z) = j_{n-1}(z) - \\frac{n + 1}{z} j_n(z).\n",
      "        \n",
      "            j_0'(z) = -j_1(z)\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://dlmf.nist.gov/10.47.E3\n",
      "        .. [2] https://dlmf.nist.gov/10.51.E1\n",
      "        .. [3] https://dlmf.nist.gov/10.51.E2\n",
      "    \n",
      "    spherical_kn(n, z, derivative=False)\n",
      "        Modified spherical Bessel function of the second kind or its derivative.\n",
      "        \n",
      "        Defined as [1]_,\n",
      "        \n",
      "        .. math:: k_n(z) = \\sqrt{\\frac{\\pi}{2z}} K_{n + 1/2}(z),\n",
      "        \n",
      "        where :math:`K_n` is the modified Bessel function of the second kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, array_like\n",
      "            Order of the Bessel function (n >= 0).\n",
      "        z : complex or float, array_like\n",
      "            Argument of the Bessel function.\n",
      "        derivative : bool, optional\n",
      "            If True, the value of the derivative (rather than the function\n",
      "            itself) is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kn : ndarray\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is computed using its definitional relation to the\n",
      "        modified cylindrical Bessel function of the second kind.\n",
      "        \n",
      "        The derivative is computed using the relations [2]_,\n",
      "        \n",
      "        .. math::\n",
      "            k_n' = -k_{n-1} - \\frac{n + 1}{z} k_n.\n",
      "        \n",
      "            k_0' = -k_1\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://dlmf.nist.gov/10.47.E9\n",
      "        .. [2] https://dlmf.nist.gov/10.51.E5\n",
      "    \n",
      "    spherical_yn(n, z, derivative=False)\n",
      "        Spherical Bessel function of the second kind or its derivative.\n",
      "        \n",
      "        Defined as [1]_,\n",
      "        \n",
      "        .. math:: y_n(z) = \\sqrt{\\frac{\\pi}{2z}} Y_{n + 1/2}(z),\n",
      "        \n",
      "        where :math:`Y_n` is the Bessel function of the second kind.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, array_like\n",
      "            Order of the Bessel function (n >= 0).\n",
      "        z : complex or float, array_like\n",
      "            Argument of the Bessel function.\n",
      "        derivative : bool, optional\n",
      "            If True, the value of the derivative (rather than the function\n",
      "            itself) is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        yn : ndarray\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For real arguments, the function is computed using the ascending\n",
      "        recurrence [2]_.  For complex arguments, the definitional relation to\n",
      "        the cylindrical Bessel function of the second kind is used.\n",
      "        \n",
      "        The derivative is computed using the relations [3]_,\n",
      "        \n",
      "        .. math::\n",
      "            y_n' = y_{n-1} - \\frac{n + 1}{z} y_n.\n",
      "        \n",
      "            y_0' = -y_1\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://dlmf.nist.gov/10.47.E4\n",
      "        .. [2] https://dlmf.nist.gov/10.51.E1\n",
      "        .. [3] https://dlmf.nist.gov/10.51.E2\n",
      "    \n",
      "    t_roots = roots_chebyt(n, mu=False)\n",
      "        Gauss-Chebyshev (first kind) quadrature.\n",
      "        \n",
      "        Computes the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the first kind, :math:`T_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-1, 1]`\n",
      "        with weight function :math:`w(x) = 1/\\sqrt{1 - x^2}`. See 22.2.4\n",
      "        in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        numpy.polynomial.chebyshev.chebgauss\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    ts_roots = roots_sh_chebyt(n, mu=False)\n",
      "        Gauss-Chebyshev (first kind, shifted) quadrature.\n",
      "        \n",
      "        Compute the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Chebyshev polynomial of the first kind, :math:`T_n(x)`.\n",
      "        These sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[0, 1]`\n",
      "        with weight function :math:`w(x) = 1/\\sqrt{x - x^2}`. See 22.2.8\n",
      "        in [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    u_roots = roots_chebyu(n, mu=False)\n",
      "        Gauss-Chebyshev (second kind) quadrature.\n",
      "        \n",
      "        Computes the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        Chebyshev polynomial of the second kind, :math:`U_n(x)`. These\n",
      "        sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[-1, 1]`\n",
      "        with weight function :math:`w(x) = \\sqrt{1 - x^2}`. See 22.2.5 in\n",
      "        [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    us_roots = roots_sh_chebyu(n, mu=False)\n",
      "        Gauss-Chebyshev (second kind, shifted) quadrature.\n",
      "        \n",
      "        Computes the sample points and weights for Gauss-Chebyshev\n",
      "        quadrature. The sample points are the roots of the nth degree\n",
      "        shifted Chebyshev polynomial of the second kind, :math:`U_n(x)`.\n",
      "        These sample points and weights correctly integrate polynomials of\n",
      "        degree :math:`2n - 1` or less over the interval :math:`[0, 1]`\n",
      "        with weight function :math:`w(x) = \\sqrt{x - x^2}`. See 22.2.9 in\n",
      "        [AS]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            quadrature order\n",
      "        mu : bool, optional\n",
      "            If True, return the sum of the weights, optional.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Sample points\n",
      "        w : ndarray\n",
      "            Weights\n",
      "        mu : float\n",
      "            Sum of the weights\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.integrate.quadrature\n",
      "        scipy.integrate.fixed_quad\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    y0_zeros(nt, complex=False)\n",
      "        Compute nt zeros of Bessel function Y0(z), and derivative at each zero.\n",
      "        \n",
      "        The derivatives are given by Y0'(z0) = -Y1(z0) at each zero z0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to return\n",
      "        complex : bool, default False\n",
      "            Set to False to return only the real zeros; set to True to return only\n",
      "            the complex zeros with negative real part and positive imaginary part.\n",
      "            Note that the complex conjugates of the latter are also zeros of the\n",
      "            function, but are not returned by this routine.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z0n : ndarray\n",
      "            Location of nth zero of Y0(z)\n",
      "        y0pz0n : ndarray\n",
      "            Value of derivative Y0'(z0) for nth zero\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    y1_zeros(nt, complex=False)\n",
      "        Compute nt zeros of Bessel function Y1(z), and derivative at each zero.\n",
      "        \n",
      "        The derivatives are given by Y1'(z1) = Y0(z1) at each zero z1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to return\n",
      "        complex : bool, default False\n",
      "            Set to False to return only the real zeros; set to True to return only\n",
      "            the complex zeros with negative real part and positive imaginary part.\n",
      "            Note that the complex conjugates of the latter are also zeros of the\n",
      "            function, but are not returned by this routine.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z1n : ndarray\n",
      "            Location of nth zero of Y1(z)\n",
      "        y1pz1n : ndarray\n",
      "            Value of derivative Y1'(z1) for nth zero\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    y1p_zeros(nt, complex=False)\n",
      "        Compute nt zeros of Bessel derivative Y1'(z), and value at each zero.\n",
      "        \n",
      "        The values are given by Y1(z1) at each z1 where Y1'(z1)=0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nt : int\n",
      "            Number of zeros to return\n",
      "        complex : bool, default False\n",
      "            Set to False to return only the real zeros; set to True to return only\n",
      "            the complex zeros with negative real part and positive imaginary part.\n",
      "            Note that the complex conjugates of the latter are also zeros of the\n",
      "            function, but are not returned by this routine.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z1pn : ndarray\n",
      "            Location of nth zero of Y1'(z)\n",
      "        y1z1pn : ndarray\n",
      "            Value of derivative Y1(z1) for nth zero\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    yn_zeros(n, nt)\n",
      "        Compute zeros of integer-order Bessel function Yn(x).\n",
      "        \n",
      "        Compute `nt` zeros of the functions :math:`Y_n(x)` on the interval\n",
      "        :math:`(0, \\infty)`. The zeros are returned in ascending order.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Order of Bessel function\n",
      "        nt : int\n",
      "            Number of zeros to return\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            First `n` zeros of the Bessel function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        yn, yv\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        We can check that we are getting approximations of the zeros by\n",
      "        evaluating them with `yn`.\n",
      "        \n",
      "        >>> n = 2\n",
      "        >>> x = sc.yn_zeros(n, 3)\n",
      "        >>> x\n",
      "        array([ 3.38424177,  6.79380751, 10.02347798])\n",
      "        >>> sc.yn(n, x)\n",
      "        array([-1.94289029e-16,  8.32667268e-17, -1.52655666e-16])\n",
      "    \n",
      "    ynp_zeros(n, nt)\n",
      "        Compute zeros of integer-order Bessel function derivatives Yn'(x).\n",
      "        \n",
      "        Compute `nt` zeros of the functions :math:`Y_n'(x)` on the\n",
      "        interval :math:`(0, \\infty)`. The zeros are returned in ascending\n",
      "        order.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Order of Bessel function\n",
      "        nt : int\n",
      "            Number of zeros to return\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        yvp\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        We can check that we are getting approximations of the zeros by\n",
      "        evaluating them with `yvp`.\n",
      "        \n",
      "        >>> n = 2\n",
      "        >>> x = sc.ynp_zeros(n, 3)\n",
      "        >>> x\n",
      "        array([ 5.00258293,  8.3507247 , 11.57419547])\n",
      "        >>> sc.yvp(n, x)\n",
      "        array([ 2.22044605e-16, -3.33066907e-16,  2.94902991e-16])\n",
      "    \n",
      "    yvp(v, z, n=1)\n",
      "        Compute derivatives of Bessel functions of the second kind.\n",
      "        \n",
      "        Compute the nth derivative of the Bessel function `Yv` with\n",
      "        respect to `z`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : float\n",
      "            Order of Bessel function\n",
      "        z : complex\n",
      "            Argument at which to evaluate the derivative\n",
      "        n : int, default 1\n",
      "            Order of derivative\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            nth derivative of the Bessel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The derivative is computed using the relation DLFM 10.6.7 [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996, chapter 5.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "        .. [2] NIST Digital Library of Mathematical Functions.\n",
      "               https://dlmf.nist.gov/10.6.E7\n",
      "    \n",
      "    zeta(x, q=None, out=None)\n",
      "        Riemann or Hurwitz zeta function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like of float\n",
      "            Input data, must be real\n",
      "        q : array_like of float, optional\n",
      "            Input data, must be real.  Defaults to Riemann zeta.\n",
      "        out : ndarray, optional\n",
      "            Output array for the computed values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : array_like\n",
      "            Values of zeta(x).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The two-argument version is the Hurwitz zeta function\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\zeta(x, q) = \\sum_{k=0}^{\\infty} \\frac{1}{(k + q)^x};\n",
      "        \n",
      "        see [dlmf]_ for details. The Riemann zeta function corresponds to\n",
      "        the case when ``q = 1``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zetac\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/25.11#i\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import zeta, polygamma, factorial\n",
      "        \n",
      "        Some specific values:\n",
      "        \n",
      "        >>> zeta(2), np.pi**2/6\n",
      "        (1.6449340668482266, 1.6449340668482264)\n",
      "        \n",
      "        >>> zeta(4), np.pi**4/90\n",
      "        (1.0823232337111381, 1.082323233711138)\n",
      "        \n",
      "        Relation to the `polygamma` function:\n",
      "        \n",
      "        >>> m = 3\n",
      "        >>> x = 1.25\n",
      "        >>> polygamma(m, x)\n",
      "        array(2.782144009188397)\n",
      "        >>> (-1)**(m+1) * factorial(m) * zeta(m+1, x)\n",
      "        2.7821440091883969\n",
      "\n",
      "DATA\n",
      "    __all__ = ['agm', 'airy', 'airye', 'bdtr', 'bdtrc', 'bdtri', 'bdtrik',...\n",
      "    agm = <ufunc 'agm'>\n",
      "        agm(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        agm(a, b)\n",
      "        \n",
      "        Compute the arithmetic-geometric mean of `a` and `b`.\n",
      "        \n",
      "        Start with a_0 = a and b_0 = b and iteratively compute::\n",
      "        \n",
      "            a_{n+1} = (a_n + b_n)/2\n",
      "            b_{n+1} = sqrt(a_n*b_n)\n",
      "        \n",
      "        a_n and b_n converge to the same limit as n increases; their common\n",
      "        limit is agm(a, b).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array_like\n",
      "            Real values only. If the values are both negative, the result\n",
      "            is negative. If one value is negative and the other is positive,\n",
      "            `nan` is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            The arithmetic-geometric mean of `a` and `b`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import agm\n",
      "        >>> a, b = 24.0, 6.0\n",
      "        >>> agm(a, b)\n",
      "        13.458171481725614\n",
      "        \n",
      "        Compare that result to the iteration:\n",
      "        \n",
      "        >>> while a != b:\n",
      "        ...     a, b = (a + b)/2, np.sqrt(a*b)\n",
      "        ...     print(\"a = %19.16f  b=%19.16f\" % (a, b))\n",
      "        ...\n",
      "        a = 15.0000000000000000  b=12.0000000000000000\n",
      "        a = 13.5000000000000000  b=13.4164078649987388\n",
      "        a = 13.4582039324993694  b=13.4581390309909850\n",
      "        a = 13.4581714817451772  b=13.4581714817060547\n",
      "        a = 13.4581714817256159  b=13.4581714817256159\n",
      "        \n",
      "        When array-like arguments are given, broadcasting applies:\n",
      "        \n",
      "        >>> a = np.array([[1.5], [3], [6]])  # a has shape (3, 1).\n",
      "        >>> b = np.array([6, 12, 24, 48])    # b has shape (4,).\n",
      "        >>> agm(a, b)\n",
      "        array([[  3.36454287,   5.42363427,   9.05798751,  15.53650756],\n",
      "               [  4.37037309,   6.72908574,  10.84726853,  18.11597502],\n",
      "               [  6.        ,   8.74074619,  13.45817148,  21.69453707]])\n",
      "    \n",
      "    airy = <ufunc 'airy'>\n",
      "        airy(x[, out1, out2, out3, out4], / [, out=(None, None, None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        airy(z)\n",
      "        \n",
      "        Airy functions and their derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex argument.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Ai, Aip, Bi, Bip : ndarrays\n",
      "            Airy functions Ai and Bi, and their derivatives Aip and Bip.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The Airy functions Ai and Bi are two independent solutions of\n",
      "        \n",
      "        .. math:: y''(x) = x y(x).\n",
      "        \n",
      "        For real `z` in [-10, 10], the computation is carried out by calling\n",
      "        the Cephes [1]_ `airy` routine, which uses power series summation\n",
      "        for small `z` and rational minimax approximations for large `z`.\n",
      "        \n",
      "        Outside this range, the AMOS [2]_ `zairy` and `zbiry` routines are\n",
      "        employed.  They are computed using power series for :math:`|z| < 1` and\n",
      "        the following relations to modified Bessel functions for larger `z`\n",
      "        (where :math:`t \\equiv 2 z^{3/2}/3`):\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            Ai(z) = \\frac{1}{\\pi \\sqrt{3}} K_{1/3}(t)\n",
      "        \n",
      "            Ai'(z) = -\\frac{z}{\\pi \\sqrt{3}} K_{2/3}(t)\n",
      "        \n",
      "            Bi(z) = \\sqrt{\\frac{z}{3}} \\left(I_{-1/3}(t) + I_{1/3}(t) \\right)\n",
      "        \n",
      "            Bi'(z) = \\frac{z}{\\sqrt{3}} \\left(I_{-2/3}(t) + I_{2/3}(t)\\right)\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        airye : exponentially scaled Airy functions.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "        .. [2] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Compute the Airy functions on the interval [-15, 5].\n",
      "        \n",
      "        >>> from scipy import special\n",
      "        >>> x = np.linspace(-15, 5, 201)\n",
      "        >>> ai, aip, bi, bip = special.airy(x)\n",
      "        \n",
      "        Plot Ai(x) and Bi(x).\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(x, ai, 'r', label='Ai(x)')\n",
      "        >>> plt.plot(x, bi, 'b--', label='Bi(x)')\n",
      "        >>> plt.ylim(-0.5, 1.0)\n",
      "        >>> plt.grid()\n",
      "        >>> plt.legend(loc='upper left')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    airye = <ufunc 'airye'>\n",
      "        airye(x[, out1, out2, out3, out4], / [, out=(None, None, None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        airye(z)\n",
      "        \n",
      "        Exponentially scaled Airy functions and their derivatives.\n",
      "        \n",
      "        Scaling::\n",
      "        \n",
      "            eAi  = Ai  * exp(2.0/3.0*z*sqrt(z))\n",
      "            eAip = Aip * exp(2.0/3.0*z*sqrt(z))\n",
      "            eBi  = Bi  * exp(-abs(2.0/3.0*(z*sqrt(z)).real))\n",
      "            eBip = Bip * exp(-abs(2.0/3.0*(z*sqrt(z)).real))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex argument.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        eAi, eAip, eBi, eBip : array_like\n",
      "            Exponentially scaled Airy functions eAi and eBi, and their derivatives\n",
      "            eAip and eBip\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the AMOS [1]_ routines `zairy` and `zbiry`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        airy\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We can compute exponentially scaled Airy functions and their derivatives:\n",
      "        \n",
      "        >>> from scipy.special import airye\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> z = np.linspace(0, 50, 500)\n",
      "        >>> eAi, eAip, eBi, eBip = airye(z)\n",
      "        >>> f, ax = plt.subplots(2, 1, sharex=True)\n",
      "        >>> for ind, data in enumerate([[eAi, eAip, [\"eAi\", \"eAip\"]],\n",
      "        ...                             [eBi, eBip, [\"eBi\", \"eBip\"]]]):\n",
      "        ...     ax[ind].plot(z, data[0], \"-r\", z, data[1], \"-b\")\n",
      "        ...     ax[ind].legend(data[2])\n",
      "        ...     ax[ind].grid(True)\n",
      "        >>> plt.show()\n",
      "        \n",
      "        We can compute these using usual non-scaled Airy functions by:\n",
      "        \n",
      "        >>> from scipy.special import airy\n",
      "        >>> Ai, Aip, Bi, Bip = airy(z)\n",
      "        >>> np.allclose(eAi, Ai * np.exp(2.0 / 3.0 * z * np.sqrt(z)))\n",
      "        True\n",
      "        >>> np.allclose(eAip, Aip * np.exp(2.0 / 3.0 * z * np.sqrt(z)))\n",
      "        True\n",
      "        >>> np.allclose(eBi, Bi * np.exp(-abs(np.real(2.0 / 3.0 * z * np.sqrt(z)))))\n",
      "        True\n",
      "        >>> np.allclose(eBip, Bip * np.exp(-abs(np.real(2.0 / 3.0 * z * np.sqrt(z)))))\n",
      "        True\n",
      "        \n",
      "        Comparing non-scaled and exponentially scaled ones, the usual non-scaled\n",
      "        function quickly underflows for large values, whereas the exponentially\n",
      "        scaled function does not.\n",
      "        \n",
      "        >>> airy(200)\n",
      "        (0.0, 0.0, nan, nan)\n",
      "        >>> airye(200)\n",
      "        (0.07501041684381093, -1.0609012305109042, 0.15003188417418148, 2.1215836725571093)\n",
      "    \n",
      "    bdtr = <ufunc 'bdtr'>\n",
      "        bdtr(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        bdtr(k, n, p)\n",
      "        \n",
      "        Binomial distribution cumulative distribution function.\n",
      "        \n",
      "        Sum of the terms 0 through `floor(k)` of the Binomial probability density.\n",
      "        \n",
      "        .. math::\n",
      "            \\mathrm{bdtr}(k, n, p) = \\sum_{j=0}^{\\lfloor k \\rfloor} {{n}\\choose{j}} p^j (1-p)^{n-j}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            Number of successes (double), rounded down to the nearest integer.\n",
      "        n : array_like\n",
      "            Number of events (int).\n",
      "        p : array_like\n",
      "            Probability of success in a single event (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : ndarray\n",
      "            Probability of `floor(k)` or fewer successes in `n` independent events with\n",
      "            success probabilities of `p`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The terms are not summed directly; instead the regularized incomplete beta\n",
      "        function is employed, according to the formula,\n",
      "        \n",
      "        .. math::\n",
      "            \\mathrm{bdtr}(k, n, p) = I_{1 - p}(n - \\lfloor k \\rfloor, \\lfloor k \\rfloor + 1).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `bdtr`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    bdtrc = <ufunc 'bdtrc'>\n",
      "        bdtrc(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        bdtrc(k, n, p)\n",
      "        \n",
      "        Binomial distribution survival function.\n",
      "        \n",
      "        Sum of the terms `floor(k) + 1` through `n` of the binomial probability\n",
      "        density,\n",
      "        \n",
      "        .. math::\n",
      "            \\mathrm{bdtrc}(k, n, p) = \\sum_{j=\\lfloor k \\rfloor +1}^n {{n}\\choose{j}} p^j (1-p)^{n-j}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            Number of successes (double), rounded down to nearest integer.\n",
      "        n : array_like\n",
      "            Number of events (int)\n",
      "        p : array_like\n",
      "            Probability of success in a single event.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : ndarray\n",
      "            Probability of `floor(k) + 1` or more successes in `n` independent\n",
      "            events with success probabilities of `p`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        bdtr\n",
      "        betainc\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The terms are not summed directly; instead the regularized incomplete beta\n",
      "        function is employed, according to the formula,\n",
      "        \n",
      "        .. math::\n",
      "            \\mathrm{bdtrc}(k, n, p) = I_{p}(\\lfloor k \\rfloor + 1, n - \\lfloor k \\rfloor).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `bdtrc`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    bdtri = <ufunc 'bdtri'>\n",
      "        bdtri(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        bdtri(k, n, y)\n",
      "        \n",
      "        Inverse function to `bdtr` with respect to `p`.\n",
      "        \n",
      "        Finds the event probability `p` such that the sum of the terms 0 through\n",
      "        `k` of the binomial probability density is equal to the given cumulative\n",
      "        probability `y`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            Number of successes (float), rounded down to the nearest integer.\n",
      "        n : array_like\n",
      "            Number of events (float)\n",
      "        y : array_like\n",
      "            Cumulative probability (probability of `k` or fewer successes in `n`\n",
      "            events).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        p : ndarray\n",
      "            The event probability such that `bdtr(\\lfloor k \\rfloor, n, p) = y`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        bdtr\n",
      "        betaincinv\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The computation is carried out using the inverse beta integral function\n",
      "        and the relation,::\n",
      "        \n",
      "            1 - p = betaincinv(n - k, k + 1, y).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `bdtri`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    bdtrik = <ufunc 'bdtrik'>\n",
      "        bdtrik(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        bdtrik(y, n, p)\n",
      "        \n",
      "        Inverse function to `bdtr` with respect to `k`.\n",
      "        \n",
      "        Finds the number of successes `k` such that the sum of the terms 0 through\n",
      "        `k` of the Binomial probability density for `n` events with probability\n",
      "        `p` is equal to the given cumulative probability `y`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Cumulative probability (probability of `k` or fewer successes in `n`\n",
      "            events).\n",
      "        n : array_like\n",
      "            Number of events (float).\n",
      "        p : array_like\n",
      "            Success probability (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        k : ndarray\n",
      "            The number of successes `k` such that `bdtr(k, n, p) = y`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        bdtr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Formula 26.5.24 of [1]_ is used to reduce the binomial distribution to the\n",
      "        cumulative incomplete beta distribution.\n",
      "        \n",
      "        Computation of `k` involves a search for a value that produces the desired\n",
      "        value of `y`. The search relies on the monotonicity of `y` with `k`.\n",
      "        \n",
      "        Wrapper for the CDFLIB [2]_ Fortran routine `cdfbin`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "        .. [2] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "    \n",
      "    bdtrin = <ufunc 'bdtrin'>\n",
      "        bdtrin(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        bdtrin(k, y, p)\n",
      "        \n",
      "        Inverse function to `bdtr` with respect to `n`.\n",
      "        \n",
      "        Finds the number of events `n` such that the sum of the terms 0 through\n",
      "        `k` of the Binomial probability density for events with probability `p` is\n",
      "        equal to the given cumulative probability `y`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            Number of successes (float).\n",
      "        y : array_like\n",
      "            Cumulative probability (probability of `k` or fewer successes in `n`\n",
      "            events).\n",
      "        p : array_like\n",
      "            Success probability (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        n : ndarray\n",
      "            The number of events `n` such that `bdtr(k, n, p) = y`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        bdtr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Formula 26.5.24 of [1]_ is used to reduce the binomial distribution to the\n",
      "        cumulative incomplete beta distribution.\n",
      "        \n",
      "        Computation of `n` involves a search for a value that produces the desired\n",
      "        value of `y`. The search relies on the monotonicity of `y` with `n`.\n",
      "        \n",
      "        Wrapper for the CDFLIB [2]_ Fortran routine `cdfbin`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "        .. [2] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "    \n",
      "    bei = <ufunc 'bei'>\n",
      "        bei(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        bei(x, out=None)\n",
      "        \n",
      "        Kelvin function bei.\n",
      "        \n",
      "        Defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\mathrm{bei}(x) = \\Im[J_0(x e^{3 \\pi i / 4})]\n",
      "        \n",
      "        where :math:`J_0` is the Bessel function of the first kind of\n",
      "        order zero (see `jv`). See [dlmf]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ber : the corresponding real part\n",
      "        beip : the derivative of bei\n",
      "        jv : Bessel function of the first kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10.61\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        It can be expressed using Bessel functions.\n",
      "        \n",
      "        >>> import scipy.special as sc\n",
      "        >>> x = np.array([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> sc.jv(0, x * np.exp(3 * np.pi * 1j / 4)).imag\n",
      "        array([0.24956604, 0.97229163, 1.93758679, 2.29269032])\n",
      "        >>> sc.bei(x)\n",
      "        array([0.24956604, 0.97229163, 1.93758679, 2.29269032])\n",
      "    \n",
      "    beip = <ufunc 'beip'>\n",
      "        beip(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        beip(x, out=None)\n",
      "        \n",
      "        Derivative of the Kelvin function bei.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            The values of the derivative of bei.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bei\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10#PT5\n",
      "    \n",
      "    ber = <ufunc 'ber'>\n",
      "        ber(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ber(x, out=None)\n",
      "        \n",
      "        Kelvin function ber.\n",
      "        \n",
      "        Defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\mathrm{ber}(x) = \\Re[J_0(x e^{3 \\pi i / 4})]\n",
      "        \n",
      "        where :math:`J_0` is the Bessel function of the first kind of\n",
      "        order zero (see `jv`). See [dlmf]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bei : the corresponding real part\n",
      "        berp : the derivative of bei\n",
      "        jv : Bessel function of the first kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10.61\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        It can be expressed using Bessel functions.\n",
      "        \n",
      "        >>> import scipy.special as sc\n",
      "        >>> x = np.array([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> sc.jv(0, x * np.exp(3 * np.pi * 1j / 4)).real\n",
      "        array([ 0.98438178,  0.75173418, -0.22138025, -2.56341656])\n",
      "        >>> sc.ber(x)\n",
      "        array([ 0.98438178,  0.75173418, -0.22138025, -2.56341656])\n",
      "    \n",
      "    berp = <ufunc 'berp'>\n",
      "        berp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        berp(x, out=None)\n",
      "        \n",
      "        Derivative of the Kelvin function ber.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            The values of the derivative of ber.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ber\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10#PT5\n",
      "    \n",
      "    besselpoly = <ufunc 'besselpoly'>\n",
      "        besselpoly(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        besselpoly(a, lmb, nu, out=None)\n",
      "        \n",
      "        Weighted integral of the Bessel function of the first kind.\n",
      "        \n",
      "        Computes\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\int_0^1 x^\\lambda J_\\nu(2 a x) \\, dx\n",
      "        \n",
      "        where :math:`J_\\nu` is a Bessel function and :math:`\\lambda=lmb`,\n",
      "        :math:`\\nu=nu`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Scale factor inside the Bessel function.\n",
      "        lmb : array_like\n",
      "            Power of `x`\n",
      "        nu : array_like\n",
      "            Order of the Bessel function.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Value of the integral.\n",
      "    \n",
      "    beta = <ufunc 'beta'>\n",
      "        beta(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        beta(a, b, out=None)\n",
      "        \n",
      "        Beta function.\n",
      "        \n",
      "        This function is defined in [1]_ as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            B(a, b) = \\int_0^1 t^{a-1}(1-t)^{b-1}dt\n",
      "                    = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array-like\n",
      "            Real-valued arguments\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function result\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Value of the beta function\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gamma : the gamma function\n",
      "        betainc :  the incomplete beta function\n",
      "        betaln : the natural logarithm of the absolute\n",
      "                 value of the beta function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions,\n",
      "               Eq. 5.12.1. https://dlmf.nist.gov/5.12\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        The beta function relates to the gamma function by the\n",
      "        definition given above:\n",
      "        \n",
      "        >>> sc.beta(2, 3)\n",
      "        0.08333333333333333\n",
      "        >>> sc.gamma(2)*sc.gamma(3)/sc.gamma(2 + 3)\n",
      "        0.08333333333333333\n",
      "        \n",
      "        As this relationship demonstrates, the beta function\n",
      "        is symmetric:\n",
      "        \n",
      "        >>> sc.beta(1.7, 2.4)\n",
      "        0.16567527689031739\n",
      "        >>> sc.beta(2.4, 1.7)\n",
      "        0.16567527689031739\n",
      "        \n",
      "        This function satisfies :math:`B(1, b) = 1/b`:\n",
      "        \n",
      "        >>> sc.beta(1, 4)\n",
      "        0.25\n",
      "    \n",
      "    betainc = <ufunc 'betainc'>\n",
      "        betainc(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        betainc(a, b, x, out=None)\n",
      "        \n",
      "        Incomplete beta function.\n",
      "        \n",
      "        Computes the incomplete beta function, defined as [1]_:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            I_x(a, b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)} \\int_0^x\n",
      "            t^{a-1}(1-t)^{b-1}dt,\n",
      "        \n",
      "        for :math:`0 \\leq x \\leq 1`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array-like\n",
      "               Positive, real-valued parameters\n",
      "        x : array-like\n",
      "            Real-valued such that :math:`0 \\leq x \\leq 1`,\n",
      "            the upper limit of integration\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        array-like\n",
      "            Value of the incomplete beta function\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        beta : beta function\n",
      "        betaincinv : inverse of the incomplete beta function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The incomplete beta function is also sometimes defined\n",
      "        without the `gamma` terms, in which case the above\n",
      "        definition is the so-called regularized incomplete beta\n",
      "        function. Under this definition, you can get the incomplete\n",
      "        beta function by multiplying the result of the SciPy\n",
      "        function by `beta`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/8.17\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Let :math:`B(a, b)` be the `beta` function.\n",
      "        \n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        The coefficient in terms of `gamma` is equal to\n",
      "        :math:`1/B(a, b)`. Also, when :math:`x=1`\n",
      "        the integral is equal to :math:`B(a, b)`.\n",
      "        Therefore, :math:`I_{x=1}(a, b) = 1` for any :math:`a, b`.\n",
      "        \n",
      "        >>> sc.betainc(0.2, 3.5, 1.0)\n",
      "        1.0\n",
      "        \n",
      "        It satisfies\n",
      "        :math:`I_x(a, b) = x^a F(a, 1-b, a+1, x)/ (aB(a, b))`,\n",
      "        where :math:`F` is the hypergeometric function `hyp2f1`:\n",
      "        \n",
      "        >>> a, b, x = 1.4, 3.1, 0.5\n",
      "        >>> x**a * sc.hyp2f1(a, 1 - b, a + 1, x)/(a * sc.beta(a, b))\n",
      "        0.8148904036225295\n",
      "        >>> sc.betainc(a, b, x)\n",
      "        0.8148904036225296\n",
      "        \n",
      "        This functions satisfies the relationship\n",
      "        :math:`I_x(a, b) = 1 - I_{1-x}(b, a)`:\n",
      "        \n",
      "        >>> sc.betainc(2.2, 3.1, 0.4)\n",
      "        0.49339638807619446\n",
      "        >>> 1 - sc.betainc(3.1, 2.2, 1 - 0.4)\n",
      "        0.49339638807619446\n",
      "    \n",
      "    betaincinv = <ufunc 'betaincinv'>\n",
      "        betaincinv(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        betaincinv(a, b, y, out=None)\n",
      "        \n",
      "        Inverse of the incomplete beta function.\n",
      "        \n",
      "        Computes :math:`x` such that:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            y = I_x(a, b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n",
      "            \\int_0^x t^{a-1}(1-t)^{b-1}dt,\n",
      "        \n",
      "        where :math:`I_x` is the normalized incomplete beta\n",
      "        function `betainc` and\n",
      "        :math:`\\Gamma` is the `gamma` function [1]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array-like\n",
      "            Positive, real-valued parameters\n",
      "        y : array-like\n",
      "            Real-valued input\n",
      "        out : ndarray, optional\n",
      "            Optional output array for function values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        array-like\n",
      "            Value of the inverse of the incomplete beta function\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        betainc : incomplete beta function\n",
      "        gamma : gamma function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/8.17\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        This function is the inverse of `betainc` for fixed\n",
      "        values of :math:`a` and :math:`b`.\n",
      "        \n",
      "        >>> a, b = 1.2, 3.1\n",
      "        >>> y = sc.betainc(a, b, 0.2)\n",
      "        >>> sc.betaincinv(a, b, y)\n",
      "        0.2\n",
      "        >>>\n",
      "        >>> a, b = 7.5, 0.4\n",
      "        >>> x = sc.betaincinv(a, b, 0.5)\n",
      "        >>> sc.betainc(a, b, x)\n",
      "        0.5\n",
      "    \n",
      "    betaln = <ufunc 'betaln'>\n",
      "        betaln(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        betaln(a, b)\n",
      "        \n",
      "        Natural logarithm of absolute value of beta function.\n",
      "        \n",
      "        Computes ``ln(abs(beta(a, b)))``.\n",
      "    \n",
      "    binom = <ufunc 'binom'>\n",
      "        binom(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        binom(n, k)\n",
      "        \n",
      "        Binomial coefficient\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        comb : The number of combinations of N things taken k at a time.\n",
      "    \n",
      "    boxcox = <ufunc 'boxcox'>\n",
      "        boxcox(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        boxcox(x, lmbda)\n",
      "        \n",
      "        Compute the Box-Cox transformation.\n",
      "        \n",
      "        The Box-Cox transformation is::\n",
      "        \n",
      "            y = (x**lmbda - 1) / lmbda  if lmbda != 0\n",
      "                log(x)                  if lmbda == 0\n",
      "        \n",
      "        Returns `nan` if ``x < 0``.\n",
      "        Returns `-inf` if ``x == 0`` and ``lmbda < 0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Data to be transformed.\n",
      "        lmbda : array_like\n",
      "            Power parameter of the Box-Cox transform.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : array\n",
      "            Transformed data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.14.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import boxcox\n",
      "        >>> boxcox([1, 4, 10], 2.5)\n",
      "        array([   0.        ,   12.4       ,  126.09110641])\n",
      "        >>> boxcox(2, [0, 1, 2])\n",
      "        array([ 0.69314718,  1.        ,  1.5       ])\n",
      "    \n",
      "    boxcox1p = <ufunc 'boxcox1p'>\n",
      "        boxcox1p(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        boxcox1p(x, lmbda)\n",
      "        \n",
      "        Compute the Box-Cox transformation of 1 + `x`.\n",
      "        \n",
      "        The Box-Cox transformation computed by `boxcox1p` is::\n",
      "        \n",
      "            y = ((1+x)**lmbda - 1) / lmbda  if lmbda != 0\n",
      "                log(1+x)                    if lmbda == 0\n",
      "        \n",
      "        Returns `nan` if ``x < -1``.\n",
      "        Returns `-inf` if ``x == -1`` and ``lmbda < 0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Data to be transformed.\n",
      "        lmbda : array_like\n",
      "            Power parameter of the Box-Cox transform.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : array\n",
      "            Transformed data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.14.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import boxcox1p\n",
      "        >>> boxcox1p(1e-4, [0, 0.5, 1])\n",
      "        array([  9.99950003e-05,   9.99975001e-05,   1.00000000e-04])\n",
      "        >>> boxcox1p([0.01, 0.1], 0.25)\n",
      "        array([ 0.00996272,  0.09645476])\n",
      "    \n",
      "    btdtr = <ufunc 'btdtr'>\n",
      "        btdtr(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        btdtr(a, b, x)\n",
      "        \n",
      "        Cumulative distribution function of the beta distribution.\n",
      "        \n",
      "        Returns the integral from zero to `x` of the beta probability density\n",
      "        function,\n",
      "        \n",
      "        .. math::\n",
      "            I = \\int_0^x \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} t^{a-1} (1-t)^{b-1}\\,dt\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Shape parameter (a > 0).\n",
      "        b : array_like\n",
      "            Shape parameter (b > 0).\n",
      "        x : array_like\n",
      "            Upper limit of integration, in [0, 1].\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            Cumulative distribution function of the beta distribution with\n",
      "            parameters `a` and `b` at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        betainc\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is identical to the incomplete beta integral function\n",
      "        `betainc`.\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `btdtr`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    btdtri = <ufunc 'btdtri'>\n",
      "        btdtri(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        btdtri(a, b, p)\n",
      "        \n",
      "        The `p`-th quantile of the beta distribution.\n",
      "        \n",
      "        This function is the inverse of the beta cumulative distribution function,\n",
      "        `btdtr`, returning the value of `x` for which `btdtr(a, b, x) = p`, or\n",
      "        \n",
      "        .. math::\n",
      "            p = \\int_0^x \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} t^{a-1} (1-t)^{b-1}\\,dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Shape parameter (`a` > 0).\n",
      "        b : array_like\n",
      "            Shape parameter (`b` > 0).\n",
      "        p : array_like\n",
      "            Cumulative probability, in [0, 1].\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The quantile corresponding to `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        betaincinv\n",
      "        btdtr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The value of `x` is found by interval halving or Newton iterations.\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `incbi`, which solves the equivalent\n",
      "        problem of finding the inverse of the incomplete beta integral.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    btdtria = <ufunc 'btdtria'>\n",
      "        btdtria(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        btdtria(p, b, x)\n",
      "        \n",
      "        Inverse of `btdtr` with respect to `a`.\n",
      "        \n",
      "        This is the inverse of the beta cumulative distribution function, `btdtr`,\n",
      "        considered as a function of `a`, returning the value of `a` for which\n",
      "        `btdtr(a, b, x) = p`, or\n",
      "        \n",
      "        .. math::\n",
      "            p = \\int_0^x \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} t^{a-1} (1-t)^{b-1}\\,dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            Cumulative probability, in [0, 1].\n",
      "        b : array_like\n",
      "            Shape parameter (`b` > 0).\n",
      "        x : array_like\n",
      "            The quantile, in [0, 1].\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        a : ndarray\n",
      "            The value of the shape parameter `a` such that `btdtr(a, b, x) = p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        btdtr : Cumulative distribution function of the beta distribution.\n",
      "        btdtri : Inverse with respect to `x`.\n",
      "        btdtrib : Inverse with respect to `b`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdfbet`.\n",
      "        \n",
      "        The cumulative distribution function `p` is computed using a routine by\n",
      "        DiDinato and Morris [2]_. Computation of `a` involves a search for a value\n",
      "        that produces the desired value of `p`. The search relies on the\n",
      "        monotonicity of `p` with `a`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] DiDinato, A. R. and Morris, A. H.,\n",
      "               Algorithm 708: Significant Digit Computation of the Incomplete Beta\n",
      "               Function Ratios. ACM Trans. Math. Softw. 18 (1993), 360-373.\n",
      "    \n",
      "    btdtrib = <ufunc 'btdtrib'>\n",
      "        btdtrib(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        btdtria(a, p, x)\n",
      "        \n",
      "        Inverse of `btdtr` with respect to `b`.\n",
      "        \n",
      "        This is the inverse of the beta cumulative distribution function, `btdtr`,\n",
      "        considered as a function of `b`, returning the value of `b` for which\n",
      "        `btdtr(a, b, x) = p`, or\n",
      "        \n",
      "        .. math::\n",
      "            p = \\int_0^x \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} t^{a-1} (1-t)^{b-1}\\,dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Shape parameter (`a` > 0).\n",
      "        p : array_like\n",
      "            Cumulative probability, in [0, 1].\n",
      "        x : array_like\n",
      "            The quantile, in [0, 1].\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        b : ndarray\n",
      "            The value of the shape parameter `b` such that `btdtr(a, b, x) = p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        btdtr : Cumulative distribution function of the beta distribution.\n",
      "        btdtri : Inverse with respect to `x`.\n",
      "        btdtria : Inverse with respect to `a`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdfbet`.\n",
      "        \n",
      "        The cumulative distribution function `p` is computed using a routine by\n",
      "        DiDinato and Morris [2]_. Computation of `b` involves a search for a value\n",
      "        that produces the desired value of `p`. The search relies on the\n",
      "        monotonicity of `p` with `b`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] DiDinato, A. R. and Morris, A. H.,\n",
      "               Algorithm 708: Significant Digit Computation of the Incomplete Beta\n",
      "               Function Ratios. ACM Trans. Math. Softw. 18 (1993), 360-373.\n",
      "    \n",
      "    cbrt = <ufunc 'cbrt'>\n",
      "        cbrt(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        cbrt(x)\n",
      "        \n",
      "        Element-wise cube root of `x`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            `x` must contain real numbers.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            The cube root of each value in `x`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import cbrt\n",
      "        \n",
      "        >>> cbrt(8)\n",
      "        2.0\n",
      "        >>> cbrt([-8, -3, 0.125, 1.331])\n",
      "        array([-2.        , -1.44224957,  0.5       ,  1.1       ])\n",
      "    \n",
      "    chdtr = <ufunc 'chdtr'>\n",
      "        chdtr(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chdtr(v, x, out=None)\n",
      "        \n",
      "        Chi square cumulative distribution function.\n",
      "        \n",
      "        Returns the area under the left tail (from 0 to `x`) of the Chi\n",
      "        square probability density function with `v` degrees of freedom:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\frac{1}{2^{v/2} \\Gamma(v/2)} \\int_0^x t^{v/2 - 1} e^{-t/2} dt\n",
      "        \n",
      "        Here :math:`\\Gamma` is the Gamma function; see `gamma`. This\n",
      "        integral can be expressed in terms of the regularized lower\n",
      "        incomplete gamma function `gammainc` as\n",
      "        ``gammainc(v / 2, x / 2)``. [1]_\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Degrees of freedom.\n",
      "        x : array_like\n",
      "            Upper bound of the integral.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the cumulative distribution function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chdtrc, chdtri, chdtriv, gammainc\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Chi-Square distribution,\n",
      "            https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It can be expressed in terms of the regularized lower incomplete\n",
      "        gamma function.\n",
      "        \n",
      "        >>> v = 1\n",
      "        >>> x = np.arange(4)\n",
      "        >>> sc.chdtr(v, x)\n",
      "        array([0.        , 0.68268949, 0.84270079, 0.91673548])\n",
      "        >>> sc.gammainc(v / 2, x / 2)\n",
      "        array([0.        , 0.68268949, 0.84270079, 0.91673548])\n",
      "    \n",
      "    chdtrc = <ufunc 'chdtrc'>\n",
      "        chdtrc(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chdtrc(v, x, out=None)\n",
      "        \n",
      "        Chi square survival function.\n",
      "        \n",
      "        Returns the area under the right hand tail (from `x` to infinity)\n",
      "        of the Chi square probability density function with `v` degrees of\n",
      "        freedom:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\frac{1}{2^{v/2} \\Gamma(v/2)} \\int_x^\\infty t^{v/2 - 1} e^{-t/2} dt\n",
      "        \n",
      "        Here :math:`\\Gamma` is the Gamma function; see `gamma`. This\n",
      "        integral can be expressed in terms of the regularized upper\n",
      "        incomplete gamma function `gammaincc` as\n",
      "        ``gammaincc(v / 2, x / 2)``. [1]_\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Degrees of freedom.\n",
      "        x : array_like\n",
      "            Lower bound of the integral.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the survival function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chdtr, chdtri, chdtriv, gammaincc\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Chi-Square distribution,\n",
      "            https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It can be expressed in terms of the regularized upper incomplete\n",
      "        gamma function.\n",
      "        \n",
      "        >>> v = 1\n",
      "        >>> x = np.arange(4)\n",
      "        >>> sc.chdtrc(v, x)\n",
      "        array([1.        , 0.31731051, 0.15729921, 0.08326452])\n",
      "        >>> sc.gammaincc(v / 2, x / 2)\n",
      "        array([1.        , 0.31731051, 0.15729921, 0.08326452])\n",
      "    \n",
      "    chdtri = <ufunc 'chdtri'>\n",
      "        chdtri(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chdtri(v, p, out=None)\n",
      "        \n",
      "        Inverse to `chdtrc` with respect to `x`.\n",
      "        \n",
      "        Returns `x` such that ``chdtrc(v, x) == p``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Degrees of freedom.\n",
      "        p : array_like\n",
      "            Probability.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : scalar or ndarray\n",
      "            Value so that the probability a Chi square random variable\n",
      "            with `v` degrees of freedom is greater than `x` equals `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chdtrc, chdtr, chdtriv\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Chi-Square distribution,\n",
      "            https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It inverts `chdtrc`.\n",
      "        \n",
      "        >>> v, p = 1, 0.3\n",
      "        >>> sc.chdtrc(v, sc.chdtri(v, p))\n",
      "        0.3\n",
      "        >>> x = 1\n",
      "        >>> sc.chdtri(v, sc.chdtrc(v, x))\n",
      "        1.0\n",
      "    \n",
      "    chdtriv = <ufunc 'chdtriv'>\n",
      "        chdtriv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chdtriv(p, x, out=None)\n",
      "        \n",
      "        Inverse to `chdtr` with respect to `v`.\n",
      "        \n",
      "        Returns `v` such that ``chdtr(v, x) == p``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            Probability that the Chi square random variable is less than\n",
      "            or equal to `x`.\n",
      "        x : array_like\n",
      "            Nonnegative input.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Degrees of freedom.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chdtr, chdtrc, chdtri\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Chi-Square distribution,\n",
      "            https://www.itl.nist.gov/div898/handbook/eda/section3/eda3666.htm\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It inverts `chdtr`.\n",
      "        \n",
      "        >>> p, x = 0.5, 1\n",
      "        >>> sc.chdtr(sc.chdtriv(p, x), x)\n",
      "        0.5000000000202172\n",
      "        >>> v = 1\n",
      "        >>> sc.chdtriv(sc.chdtr(v, x), v)\n",
      "        1.0000000000000013\n",
      "    \n",
      "    chndtr = <ufunc 'chndtr'>\n",
      "        chndtr(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chndtr(x, df, nc)\n",
      "        \n",
      "        Non-central chi square cumulative distribution function\n",
      "    \n",
      "    chndtridf = <ufunc 'chndtridf'>\n",
      "        chndtridf(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chndtridf(x, p, nc)\n",
      "        \n",
      "        Inverse to `chndtr` vs `df`\n",
      "    \n",
      "    chndtrinc = <ufunc 'chndtrinc'>\n",
      "        chndtrinc(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chndtrinc(x, df, p)\n",
      "        \n",
      "        Inverse to `chndtr` vs `nc`\n",
      "    \n",
      "    chndtrix = <ufunc 'chndtrix'>\n",
      "        chndtrix(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        chndtrix(p, df, nc)\n",
      "        \n",
      "        Inverse to `chndtr` vs `x`\n",
      "    \n",
      "    cosdg = <ufunc 'cosdg'>\n",
      "        cosdg(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        cosdg(x, out=None)\n",
      "        \n",
      "        Cosine of the angle `x` given in degrees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Angle, given in degrees.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Cosine of the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sindg, tandg, cotdg\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is more accurate than using cosine directly.\n",
      "        \n",
      "        >>> x = 90 + 180 * np.arange(3)\n",
      "        >>> sc.cosdg(x)\n",
      "        array([-0.,  0., -0.])\n",
      "        >>> np.cos(x * np.pi / 180)\n",
      "        array([ 6.1232340e-17, -1.8369702e-16,  3.0616170e-16])\n",
      "    \n",
      "    cosm1 = <ufunc 'cosm1'>\n",
      "        cosm1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        cosm1(x, out=None)\n",
      "        \n",
      "        cos(x) - 1 for use when `x` is near zero.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real valued argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of ``cos(x) - 1``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        expm1, log1p\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is more accurate than computing ``cos(x) - 1`` directly for\n",
      "        ``x`` around 0.\n",
      "        \n",
      "        >>> x = 1e-30\n",
      "        >>> np.cos(x) - 1\n",
      "        0.0\n",
      "        >>> sc.cosm1(x)\n",
      "        -5.0000000000000005e-61\n",
      "    \n",
      "    cotdg = <ufunc 'cotdg'>\n",
      "        cotdg(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        cotdg(x, out=None)\n",
      "        \n",
      "        Cotangent of the angle `x` given in degrees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Angle, given in degrees.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Cotangent at the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sindg, cosdg, tandg\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is more accurate than using cotangent directly.\n",
      "        \n",
      "        >>> x = 90 + 180 * np.arange(3)\n",
      "        >>> sc.cotdg(x)\n",
      "        array([0., 0., 0.])\n",
      "        >>> 1 / np.tan(x * np.pi / 180)\n",
      "        array([6.1232340e-17, 1.8369702e-16, 3.0616170e-16])\n",
      "    \n",
      "    dawsn = <ufunc 'dawsn'>\n",
      "        dawsn(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        dawsn(x)\n",
      "        \n",
      "        Dawson's integral.\n",
      "        \n",
      "        Computes::\n",
      "        \n",
      "            exp(-x**2) * integral(exp(t**2), t=0..x).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        wofz, erf, erfc, erfcx, erfi\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Steven G. Johnson, Faddeeva W function implementation.\n",
      "           http://ab-initio.mit.edu/Faddeeva\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(-15, 15, num=1000)\n",
      "        >>> plt.plot(x, special.dawsn(x))\n",
      "        >>> plt.xlabel('$x$')\n",
      "        >>> plt.ylabel('$dawsn(x)$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    digamma = <ufunc 'psi'>\n",
      "        psi(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        psi(z, out=None)\n",
      "        \n",
      "        The digamma function.\n",
      "        \n",
      "        The logarithmic derivative of the gamma function evaluated at ``z``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex argument.\n",
      "        out : ndarray, optional\n",
      "            Array for the computed values of ``psi``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        digamma : ndarray\n",
      "            Computed values of ``psi``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For large values not close to the negative real axis, ``psi`` is\n",
      "        computed using the asymptotic series (5.11.2) from [1]_. For small\n",
      "        arguments not close to the negative real axis, the recurrence\n",
      "        relation (5.5.2) from [1]_ is used until the argument is large\n",
      "        enough to use the asymptotic series. For values close to the\n",
      "        negative real axis, the reflection formula (5.5.4) from [1]_ is\n",
      "        used first. Note that ``psi`` has a family of zeros on the\n",
      "        negative real axis which occur between the poles at nonpositive\n",
      "        integers. Around the zeros the reflection formula suffers from\n",
      "        cancellation and the implementation loses precision. The sole\n",
      "        positive zero and the first negative zero, however, are handled\n",
      "        separately by precomputing series expansions using [2]_, so the\n",
      "        function should maintain full accuracy around the origin.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/5\n",
      "        .. [2] Fredrik Johansson and others.\n",
      "               \"mpmath: a Python library for arbitrary-precision floating-point arithmetic\"\n",
      "               (Version 0.19) http://mpmath.org/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import psi\n",
      "        >>> z = 3 + 4j\n",
      "        >>> psi(z)\n",
      "        (1.55035981733341+1.0105022091860445j)\n",
      "        \n",
      "        Verify psi(z) = psi(z + 1) - 1/z:\n",
      "        \n",
      "        >>> psi(z + 1) - 1/z\n",
      "        (1.55035981733341+1.0105022091860445j)\n",
      "    \n",
      "    ellipe = <ufunc 'ellipe'>\n",
      "        ellipe(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ellipe(m)\n",
      "        \n",
      "        Complete elliptic integral of the second kind\n",
      "        \n",
      "        This function is defined as\n",
      "        \n",
      "        .. math:: E(m) = \\int_0^{\\pi/2} [1 - m \\sin(t)^2]^{1/2} dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : array_like\n",
      "            Defines the parameter of the elliptic integral.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        E : ndarray\n",
      "            Value of the elliptic integral.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the Cephes [1]_ routine `ellpe`.\n",
      "        \n",
      "        For `m > 0` the computation uses the approximation,\n",
      "        \n",
      "        .. math:: E(m) \\approx P(1-m) - (1-m) \\log(1-m) Q(1-m),\n",
      "        \n",
      "        where :math:`P` and :math:`Q` are tenth-order polynomials.  For\n",
      "        `m < 0`, the relation\n",
      "        \n",
      "        .. math:: E(m) = E(m/(m - 1)) \\sqrt(1-m)\n",
      "        \n",
      "        is used.\n",
      "        \n",
      "        The parameterization in terms of :math:`m` follows that of section\n",
      "        17.2 in [2]_. Other parameterizations in terms of the\n",
      "        complementary parameter :math:`1 - m`, modular angle\n",
      "        :math:`\\sin^2(\\alpha) = m`, or modulus :math:`k^2 = m` are also\n",
      "        used, so be careful that you choose the correct parameter.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellipkm1 : Complete elliptic integral of the first kind, near `m` = 1\n",
      "        ellipk : Complete elliptic integral of the first kind\n",
      "        ellipkinc : Incomplete elliptic integral of the first kind\n",
      "        ellipeinc : Incomplete elliptic integral of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "        .. [2] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        This function is used in finding the circumference of an\n",
      "        ellipse with semi-major axis `a` and semi-minor axis `b`.\n",
      "        \n",
      "        >>> from scipy import special\n",
      "        \n",
      "        >>> a = 3.5\n",
      "        >>> b = 2.1\n",
      "        >>> e_sq = 1.0 - b**2/a**2  # eccentricity squared\n",
      "        \n",
      "        Then the circumference is found using the following:\n",
      "        \n",
      "        >>> C = 4*a*special.ellipe(e_sq)  # circumference formula\n",
      "        >>> C\n",
      "        17.868899204378693\n",
      "        \n",
      "        When `a` and `b` are the same (meaning eccentricity is 0),\n",
      "        this reduces to the circumference of a circle.\n",
      "        \n",
      "        >>> 4*a*special.ellipe(0.0)  # formula for ellipse with a = b\n",
      "        21.991148575128552\n",
      "        >>> 2*np.pi*a  # formula for circle of radius a\n",
      "        21.991148575128552\n",
      "    \n",
      "    ellipeinc = <ufunc 'ellipeinc'>\n",
      "        ellipeinc(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ellipeinc(phi, m)\n",
      "        \n",
      "        Incomplete elliptic integral of the second kind\n",
      "        \n",
      "        This function is defined as\n",
      "        \n",
      "        .. math:: E(\\phi, m) = \\int_0^{\\phi} [1 - m \\sin(t)^2]^{1/2} dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        phi : array_like\n",
      "            amplitude of the elliptic integral.\n",
      "        \n",
      "        m : array_like\n",
      "            parameter of the elliptic integral.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        E : ndarray\n",
      "            Value of the elliptic integral.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the Cephes [1]_ routine `ellie`.\n",
      "        \n",
      "        Computation uses arithmetic-geometric means algorithm.\n",
      "        \n",
      "        The parameterization in terms of :math:`m` follows that of section\n",
      "        17.2 in [2]_. Other parameterizations in terms of the\n",
      "        complementary parameter :math:`1 - m`, modular angle\n",
      "        :math:`\\sin^2(\\alpha) = m`, or modulus :math:`k^2 = m` are also\n",
      "        used, so be careful that you choose the correct parameter.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellipkm1 : Complete elliptic integral of the first kind, near `m` = 1\n",
      "        ellipk : Complete elliptic integral of the first kind\n",
      "        ellipkinc : Incomplete elliptic integral of the first kind\n",
      "        ellipe : Complete elliptic integral of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "        .. [2] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    ellipj = <ufunc 'ellipj'>\n",
      "        ellipj(x1, x2[, out1, out2, out3, out4], / [, out=(None, None, None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ellipj(u, m)\n",
      "        \n",
      "        Jacobian elliptic functions\n",
      "        \n",
      "        Calculates the Jacobian elliptic functions of parameter `m` between\n",
      "        0 and 1, and real argument `u`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : array_like\n",
      "            Parameter.\n",
      "        u : array_like\n",
      "            Argument.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sn, cn, dn, ph : ndarrays\n",
      "            The returned functions::\n",
      "        \n",
      "                sn(u|m), cn(u|m), dn(u|m)\n",
      "        \n",
      "            The value `ph` is such that if `u = ellipkinc(ph, m)`,\n",
      "            then `sn(u|m) = sin(ph)` and `cn(u|m) = cos(ph)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the Cephes [1]_ routine `ellpj`.\n",
      "        \n",
      "        These functions are periodic, with quarter-period on the real axis\n",
      "        equal to the complete elliptic integral `ellipk(m)`.\n",
      "        \n",
      "        Relation to incomplete elliptic integral: If `u = ellipkinc(phi,m)`, then\n",
      "        `sn(u|m) = sin(phi)`, and `cn(u|m) = cos(phi)`. The `phi` is called\n",
      "        the amplitude of `u`.\n",
      "        \n",
      "        Computation is by means of the arithmetic-geometric mean algorithm,\n",
      "        except when `m` is within 1e-9 of 0 or 1. In the latter case with `m`\n",
      "        close to 1, the approximation applies only for `phi < pi/2`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        ellipk : Complete elliptic integral of the first kind\n",
      "        ellipkinc : Incomplete elliptic integral of the first kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    ellipk = <ufunc 'ellipk'>\n",
      "        ellipk(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ellipk(m)\n",
      "        \n",
      "        Complete elliptic integral of the first kind.\n",
      "        \n",
      "        This function is defined as\n",
      "        \n",
      "        .. math:: K(m) = \\int_0^{\\pi/2} [1 - m \\sin(t)^2]^{-1/2} dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : array_like\n",
      "            The parameter of the elliptic integral.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : array_like\n",
      "            Value of the elliptic integral.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For more precision around point m = 1, use `ellipkm1`, which this\n",
      "        function calls.\n",
      "        \n",
      "        The parameterization in terms of :math:`m` follows that of section\n",
      "        17.2 in [1]_. Other parameterizations in terms of the\n",
      "        complementary parameter :math:`1 - m`, modular angle\n",
      "        :math:`\\sin^2(\\alpha) = m`, or modulus :math:`k^2 = m` are also\n",
      "        used, so be careful that you choose the correct parameter.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellipkm1 : Complete elliptic integral of the first kind around m = 1\n",
      "        ellipkinc : Incomplete elliptic integral of the first kind\n",
      "        ellipe : Complete elliptic integral of the second kind\n",
      "        ellipeinc : Incomplete elliptic integral of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    ellipkinc = <ufunc 'ellipkinc'>\n",
      "        ellipkinc(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ellipkinc(phi, m)\n",
      "        \n",
      "        Incomplete elliptic integral of the first kind\n",
      "        \n",
      "        This function is defined as\n",
      "        \n",
      "        .. math:: K(\\phi, m) = \\int_0^{\\phi} [1 - m \\sin(t)^2]^{-1/2} dt\n",
      "        \n",
      "        This function is also called `F(phi, m)`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        phi : array_like\n",
      "            amplitude of the elliptic integral\n",
      "        \n",
      "        m : array_like\n",
      "            parameter of the elliptic integral\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray\n",
      "            Value of the elliptic integral\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the Cephes [1]_ routine `ellik`.  The computation is\n",
      "        carried out using the arithmetic-geometric mean algorithm.\n",
      "        \n",
      "        The parameterization in terms of :math:`m` follows that of section\n",
      "        17.2 in [2]_. Other parameterizations in terms of the\n",
      "        complementary parameter :math:`1 - m`, modular angle\n",
      "        :math:`\\sin^2(\\alpha) = m`, or modulus :math:`k^2 = m` are also\n",
      "        used, so be careful that you choose the correct parameter.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellipkm1 : Complete elliptic integral of the first kind, near `m` = 1\n",
      "        ellipk : Complete elliptic integral of the first kind\n",
      "        ellipe : Complete elliptic integral of the second kind\n",
      "        ellipeinc : Incomplete elliptic integral of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "        .. [2] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    ellipkm1 = <ufunc 'ellipkm1'>\n",
      "        ellipkm1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ellipkm1(p)\n",
      "        \n",
      "        Complete elliptic integral of the first kind around `m` = 1\n",
      "        \n",
      "        This function is defined as\n",
      "        \n",
      "        .. math:: K(p) = \\int_0^{\\pi/2} [1 - m \\sin(t)^2]^{-1/2} dt\n",
      "        \n",
      "        where `m = 1 - p`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            Defines the parameter of the elliptic integral as `m = 1 - p`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray\n",
      "            Value of the elliptic integral.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the Cephes [1]_ routine `ellpk`.\n",
      "        \n",
      "        For `p <= 1`, computation uses the approximation,\n",
      "        \n",
      "        .. math:: K(p) \\approx P(p) - \\log(p) Q(p),\n",
      "        \n",
      "        where :math:`P` and :math:`Q` are tenth-order polynomials.  The\n",
      "        argument `p` is used internally rather than `m` so that the logarithmic\n",
      "        singularity at `m = 1` will be shifted to the origin; this preserves\n",
      "        maximum accuracy.  For `p > 1`, the identity\n",
      "        \n",
      "        .. math:: K(p) = K(1/p)/\\sqrt(p)\n",
      "        \n",
      "        is used.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ellipk : Complete elliptic integral of the first kind\n",
      "        ellipkinc : Incomplete elliptic integral of the first kind\n",
      "        ellipe : Complete elliptic integral of the second kind\n",
      "        ellipeinc : Incomplete elliptic integral of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    entr = <ufunc 'entr'>\n",
      "        entr(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        entr(x)\n",
      "        \n",
      "        Elementwise function for computing entropy.\n",
      "        \n",
      "        .. math:: \\text{entr}(x) = \\begin{cases} - x \\log(x) & x > 0  \\\\ 0 & x = 0 \\\\ -\\infty & \\text{otherwise} \\end{cases}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : ndarray\n",
      "            The value of the elementwise entropy function at the given points `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kl_div, rel_entr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is concave.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "    \n",
      "    erf = <ufunc 'erf'>\n",
      "        erf(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        erf(z)\n",
      "        \n",
      "        Returns the error function of complex argument.\n",
      "        \n",
      "        It is defined as ``2/sqrt(pi)*integral(exp(-t**2), t=0..z)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Input array.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : ndarray\n",
      "            The values of the error function at the given points `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        erfc, erfinv, erfcinv, wofz, erfcx, erfi\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The cumulative of the unit normal distribution is given by\n",
      "        ``Phi(z) = 1/2[1 + erf(z/sqrt(2))]``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Error_function\n",
      "        .. [2] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover,\n",
      "            1972. http://www.math.sfu.ca/~cbm/aands/page_297.htm\n",
      "        .. [3] Steven G. Johnson, Faddeeva W function implementation.\n",
      "           http://ab-initio.mit.edu/Faddeeva\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(-3, 3)\n",
      "        >>> plt.plot(x, special.erf(x))\n",
      "        >>> plt.xlabel('$x$')\n",
      "        >>> plt.ylabel('$erf(x)$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    erfc = <ufunc 'erfc'>\n",
      "        erfc(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        erfc(x, out=None)\n",
      "        \n",
      "        Complementary error function, ``1 - erf(x)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real or complex valued argument\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the complementary error function\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        erf, erfi, erfcx, dawsn, wofz\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Steven G. Johnson, Faddeeva W function implementation.\n",
      "           http://ab-initio.mit.edu/Faddeeva\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(-3, 3)\n",
      "        >>> plt.plot(x, special.erfc(x))\n",
      "        >>> plt.xlabel('$x$')\n",
      "        >>> plt.ylabel('$erfc(x)$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    erfcinv = <ufunc 'erfcinv'>\n",
      "        erfcinv(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        Inverse of the complementary error function.\n",
      "        \n",
      "            Computes the inverse of the complementary error function.\n",
      "        \n",
      "            In the complex domain, there is no unique complex number w satisfying\n",
      "            erfc(w)=z. This indicates a true inverse function would have multi-value.\n",
      "            When the domain restricts to the real, 0 < x < 2, there is a unique real\n",
      "            number satisfying erfc(erfcinv(x)) = erfcinv(erfc(x)).\n",
      "        \n",
      "            It is related to inverse of the error function by erfcinv(1-x) = erfinv(x)\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            y : ndarray\n",
      "                Argument at which to evaluate. Domain: [0, 2]\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            erfcinv : ndarray\n",
      "                The inverse of erfc of y, element-wise\n",
      "        \n",
      "            See Also\n",
      "            --------\n",
      "            erf : Error function of a complex argument\n",
      "            erfc : Complementary error function, ``1 - erf(x)``\n",
      "            erfinv : Inverse of the error function\n",
      "        \n",
      "            Examples\n",
      "            --------\n",
      "            1) evaluating a float number\n",
      "        \n",
      "            >>> from scipy import special\n",
      "            >>> special.erfcinv(0.5)\n",
      "            0.4769362762044698\n",
      "        \n",
      "            2) evaluating an ndarray\n",
      "        \n",
      "            >>> from scipy import special\n",
      "            >>> y = np.linspace(0.0, 2.0, num=11)\n",
      "            >>> special.erfcinv(y)\n",
      "            array([        inf,  0.9061938 ,  0.59511608,  0.37080716,  0.17914345,\n",
      "                   -0.        , -0.17914345, -0.37080716, -0.59511608, -0.9061938 ,\n",
      "                          -inf])\n",
      "    \n",
      "    erfcx = <ufunc 'erfcx'>\n",
      "        erfcx(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        erfcx(x, out=None)\n",
      "        \n",
      "        Scaled complementary error function, ``exp(x**2) * erfc(x)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real or complex valued argument\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the scaled complementary error function\n",
      "        \n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        erf, erfc, erfi, dawsn, wofz\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.12.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Steven G. Johnson, Faddeeva W function implementation.\n",
      "           http://ab-initio.mit.edu/Faddeeva\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(-3, 3)\n",
      "        >>> plt.plot(x, special.erfcx(x))\n",
      "        >>> plt.xlabel('$x$')\n",
      "        >>> plt.ylabel('$erfcx(x)$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    erfi = <ufunc 'erfi'>\n",
      "        erfi(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        erfi(z, out=None)\n",
      "        \n",
      "        Imaginary error function, ``-i erf(i z)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex valued argument\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the imaginary error function\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        erf, erfc, erfcx, dawsn, wofz\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.12.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Steven G. Johnson, Faddeeva W function implementation.\n",
      "           http://ab-initio.mit.edu/Faddeeva\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(-3, 3)\n",
      "        >>> plt.plot(x, special.erfi(x))\n",
      "        >>> plt.xlabel('$x$')\n",
      "        >>> plt.ylabel('$erfi(x)$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    erfinv = <ufunc 'erfinv'>\n",
      "        erfinv(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        Inverse of the error function.\n",
      "        \n",
      "            Computes the inverse of the error function.\n",
      "        \n",
      "            In the complex domain, there is no unique complex number w satisfying\n",
      "            erf(w)=z. This indicates a true inverse function would have multi-value.\n",
      "            When the domain restricts to the real, -1 < x < 1, there is a unique real\n",
      "            number satisfying erf(erfinv(x)) = x.\n",
      "        \n",
      "            Parameters\n",
      "            ----------\n",
      "            y : ndarray\n",
      "                Argument at which to evaluate. Domain: [-1, 1]\n",
      "        \n",
      "            Returns\n",
      "            -------\n",
      "            erfinv : ndarray\n",
      "                The inverse of erf of y, element-wise)\n",
      "        \n",
      "            See Also\n",
      "            --------\n",
      "            erf : Error function of a complex argument\n",
      "            erfc : Complementary error function, ``1 - erf(x)``\n",
      "            erfcinv : Inverse of the complementary error function\n",
      "        \n",
      "            Examples\n",
      "            --------\n",
      "            1) evaluating a float number\n",
      "        \n",
      "            >>> from scipy import special\n",
      "            >>> special.erfinv(0.5)\n",
      "            0.4769362762044698\n",
      "        \n",
      "            2) evaluating an ndarray\n",
      "        \n",
      "            >>> from scipy import special\n",
      "            >>> y = np.linspace(-1.0, 1.0, num=10)\n",
      "            >>> special.erfinv(y)\n",
      "            array([       -inf, -0.86312307, -0.5407314 , -0.30457019, -0.0987901 ,\n",
      "                    0.0987901 ,  0.30457019,  0.5407314 ,  0.86312307,         inf])\n",
      "    \n",
      "    eval_chebyc = <ufunc 'eval_chebyc'>\n",
      "        eval_chebyc(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_chebyc(n, x, out=None)\n",
      "        \n",
      "        Evaluate Chebyshev polynomial of the first kind on [-2, 2] at a\n",
      "        point.\n",
      "        \n",
      "        These polynomials are defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            C_n(x) = 2 T_n(x/2)\n",
      "        \n",
      "        where :math:`T_n` is a Chebyshev polynomial of the first kind. See\n",
      "        22.5.11 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to `eval_chebyt`.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Chebyshev polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray\n",
      "            Values of the Chebyshev polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_chebyc : roots and quadrature weights of Chebyshev\n",
      "                       polynomials of the first kind on [-2, 2]\n",
      "        chebyc : Chebyshev polynomial object\n",
      "        numpy.polynomial.chebyshev.Chebyshev : Chebyshev series\n",
      "        eval_chebyt : evaluate Chebycshev polynomials of the first kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        They are a scaled version of the Chebyshev polynomials of the\n",
      "        first kind.\n",
      "        \n",
      "        >>> x = np.linspace(-2, 2, 6)\n",
      "        >>> sc.eval_chebyc(3, x)\n",
      "        array([-2.   ,  1.872,  1.136, -1.136, -1.872,  2.   ])\n",
      "        >>> 2 * sc.eval_chebyt(3, x / 2)\n",
      "        array([-2.   ,  1.872,  1.136, -1.136, -1.872,  2.   ])\n",
      "    \n",
      "    eval_chebys = <ufunc 'eval_chebys'>\n",
      "        eval_chebys(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_chebys(n, x, out=None)\n",
      "        \n",
      "        Evaluate Chebyshev polynomial of the second kind on [-2, 2] at a\n",
      "        point.\n",
      "        \n",
      "        These polynomials are defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            S_n(x) = U_n(x/2)\n",
      "        \n",
      "        where :math:`U_n` is a Chebyshev polynomial of the second\n",
      "        kind. See 22.5.13 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to `eval_chebyu`.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Chebyshev polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        S : ndarray\n",
      "            Values of the Chebyshev polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_chebys : roots and quadrature weights of Chebyshev\n",
      "                       polynomials of the second kind on [-2, 2]\n",
      "        chebys : Chebyshev polynomial object\n",
      "        eval_chebyu : evaluate Chebyshev polynomials of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        They are a scaled version of the Chebyshev polynomials of the\n",
      "        second kind.\n",
      "        \n",
      "        >>> x = np.linspace(-2, 2, 6)\n",
      "        >>> sc.eval_chebys(3, x)\n",
      "        array([-4.   ,  0.672,  0.736, -0.736, -0.672,  4.   ])\n",
      "        >>> sc.eval_chebyu(3, x / 2)\n",
      "        array([-4.   ,  0.672,  0.736, -0.736, -0.672,  4.   ])\n",
      "    \n",
      "    eval_chebyt = <ufunc 'eval_chebyt'>\n",
      "        eval_chebyt(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_chebyt(n, x, out=None)\n",
      "        \n",
      "        Evaluate Chebyshev polynomial of the first kind at a point.\n",
      "        \n",
      "        The Chebyshev polynomials of the first kind can be defined via the\n",
      "        Gauss hypergeometric function :math:`{}_2F_1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T_n(x) = {}_2F_1(n, -n; 1/2; (1 - x)/2).\n",
      "        \n",
      "        When :math:`n` is an integer the result is a polynomial of degree\n",
      "        :math:`n`. See 22.5.47 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to the Gauss hypergeometric\n",
      "            function.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Chebyshev polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        T : ndarray\n",
      "            Values of the Chebyshev polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_chebyt : roots and quadrature weights of Chebyshev\n",
      "                       polynomials of the first kind\n",
      "        chebyu : Chebychev polynomial object\n",
      "        eval_chebyu : evaluate Chebyshev polynomials of the second kind\n",
      "        hyp2f1 : Gauss hypergeometric function\n",
      "        numpy.polynomial.chebyshev.Chebyshev : Chebyshev series\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This routine is numerically stable for `x` in ``[-1, 1]`` at least\n",
      "        up to order ``10000``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_chebyu = <ufunc 'eval_chebyu'>\n",
      "        eval_chebyu(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_chebyu(n, x, out=None)\n",
      "        \n",
      "        Evaluate Chebyshev polynomial of the second kind at a point.\n",
      "        \n",
      "        The Chebyshev polynomials of the second kind can be defined via\n",
      "        the Gauss hypergeometric function :math:`{}_2F_1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            U_n(x) = (n + 1) {}_2F_1(-n, n + 2; 3/2; (1 - x)/2).\n",
      "        \n",
      "        When :math:`n` is an integer the result is a polynomial of degree\n",
      "        :math:`n`. See 22.5.48 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to the Gauss hypergeometric\n",
      "            function.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Chebyshev polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        U : ndarray\n",
      "            Values of the Chebyshev polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_chebyu : roots and quadrature weights of Chebyshev\n",
      "                       polynomials of the second kind\n",
      "        chebyu : Chebyshev polynomial object\n",
      "        eval_chebyt : evaluate Chebyshev polynomials of the first kind\n",
      "        hyp2f1 : Gauss hypergeometric function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_gegenbauer = <ufunc 'eval_gegenbauer'>\n",
      "        eval_gegenbauer(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_gegenbauer(n, alpha, x, out=None)\n",
      "        \n",
      "        Evaluate Gegenbauer polynomial at a point.\n",
      "        \n",
      "        The Gegenbauer polynomials can be defined via the Gauss\n",
      "        hypergeometric function :math:`{}_2F_1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            C_n^{(\\alpha)} = \\frac{(2\\alpha)_n}{\\Gamma(n + 1)}\n",
      "              {}_2F_1(-n, 2\\alpha + n; \\alpha + 1/2; (1 - z)/2).\n",
      "        \n",
      "        When :math:`n` is an integer the result is a polynomial of degree\n",
      "        :math:`n`. See 22.5.46 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to the Gauss hypergeometric\n",
      "            function.\n",
      "        alpha : array_like\n",
      "            Parameter\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Gegenbauer polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray\n",
      "            Values of the Gegenbauer polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_gegenbauer : roots and quadrature weights of Gegenbauer\n",
      "                           polynomials\n",
      "        gegenbauer : Gegenbauer polynomial object\n",
      "        hyp2f1 : Gauss hypergeometric function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_genlaguerre = <ufunc 'eval_genlaguerre'>\n",
      "        eval_genlaguerre(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_genlaguerre(n, alpha, x, out=None)\n",
      "        \n",
      "        Evaluate generalized Laguerre polynomial at a point.\n",
      "        \n",
      "        The generalized Laguerre polynomials can be defined via the\n",
      "        confluent hypergeometric function :math:`{}_1F_1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            L_n^{(\\alpha)}(x) = \\binom{n + \\alpha}{n}\n",
      "              {}_1F_1(-n, \\alpha + 1, x).\n",
      "        \n",
      "        When :math:`n` is an integer the result is a polynomial of degree\n",
      "        :math:`n`. See 22.5.54 in [AS]_ for details. The Laguerre\n",
      "        polynomials are the special case where :math:`\\alpha = 0`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to the confluent hypergeometric\n",
      "            function.\n",
      "        alpha : array_like\n",
      "            Parameter; must have ``alpha > -1``\n",
      "        x : array_like\n",
      "            Points at which to evaluate the generalized Laguerre\n",
      "            polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        L : ndarray\n",
      "            Values of the generalized Laguerre polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_genlaguerre : roots and quadrature weights of generalized\n",
      "                            Laguerre polynomials\n",
      "        genlaguerre : generalized Laguerre polynomial object\n",
      "        hyp1f1 : confluent hypergeometric function\n",
      "        eval_laguerre : evaluate Laguerre polynomials\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_hermite = <ufunc 'eval_hermite'>\n",
      "        eval_hermite(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_hermite(n, x, out=None)\n",
      "        \n",
      "        Evaluate physicist's Hermite polynomial at a point.\n",
      "        \n",
      "        Defined by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            H_n(x) = (-1)^n e^{x^2} \\frac{d^n}{dx^n} e^{-x^2};\n",
      "        \n",
      "        :math:`H_n` is a polynomial of degree :math:`n`. See 22.11.7 in\n",
      "        [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Hermite polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        H : ndarray\n",
      "            Values of the Hermite polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_hermite : roots and quadrature weights of physicist's\n",
      "                        Hermite polynomials\n",
      "        hermite : physicist's Hermite polynomial object\n",
      "        numpy.polynomial.hermite.Hermite : Physicist's Hermite series\n",
      "        eval_hermitenorm : evaluate Probabilist's Hermite polynomials\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_hermitenorm = <ufunc 'eval_hermitenorm'>\n",
      "        eval_hermitenorm(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_hermitenorm(n, x, out=None)\n",
      "        \n",
      "        Evaluate probabilist's (normalized) Hermite polynomial at a\n",
      "        point.\n",
      "        \n",
      "        Defined by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            He_n(x) = (-1)^n e^{x^2/2} \\frac{d^n}{dx^n} e^{-x^2/2};\n",
      "        \n",
      "        :math:`He_n` is a polynomial of degree :math:`n`. See 22.11.8 in\n",
      "        [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Hermite polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        He : ndarray\n",
      "            Values of the Hermite polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_hermitenorm : roots and quadrature weights of probabilist's\n",
      "                            Hermite polynomials\n",
      "        hermitenorm : probabilist's Hermite polynomial object\n",
      "        numpy.polynomial.hermite_e.HermiteE : Probabilist's Hermite series\n",
      "        eval_hermite : evaluate physicist's Hermite polynomials\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_jacobi = <ufunc 'eval_jacobi'>\n",
      "        eval_jacobi(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_jacobi(n, alpha, beta, x, out=None)\n",
      "        \n",
      "        Evaluate Jacobi polynomial at a point.\n",
      "        \n",
      "        The Jacobi polynomials can be defined via the Gauss hypergeometric\n",
      "        function :math:`{}_2F_1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P_n^{(\\alpha, \\beta)}(x) = \\frac{(\\alpha + 1)_n}{\\Gamma(n + 1)}\n",
      "              {}_2F_1(-n, 1 + \\alpha + \\beta + n; \\alpha + 1; (1 - z)/2)\n",
      "        \n",
      "        where :math:`(\\cdot)_n` is the Pochhammer symbol; see `poch`. When\n",
      "        :math:`n` is an integer the result is a polynomial of degree\n",
      "        :math:`n`. See 22.5.42 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer the result is\n",
      "            determined via the relation to the Gauss hypergeometric\n",
      "            function.\n",
      "        alpha : array_like\n",
      "            Parameter\n",
      "        beta : array_like\n",
      "            Parameter\n",
      "        x : array_like\n",
      "            Points at which to evaluate the polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        P : ndarray\n",
      "            Values of the Jacobi polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_jacobi : roots and quadrature weights of Jacobi polynomials\n",
      "        jacobi : Jacobi polynomial object\n",
      "        hyp2f1 : Gauss hypergeometric function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_laguerre = <ufunc 'eval_laguerre'>\n",
      "        eval_laguerre(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_laguerre(n, x, out=None)\n",
      "        \n",
      "        Evaluate Laguerre polynomial at a point.\n",
      "        \n",
      "        The Laguerre polynomials can be defined via the confluent\n",
      "        hypergeometric function :math:`{}_1F_1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            L_n(x) = {}_1F_1(-n, 1, x).\n",
      "        \n",
      "        See 22.5.16 and 22.5.54 in [AS]_ for details. When :math:`n` is an\n",
      "        integer the result is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer the result is\n",
      "            determined via the relation to the confluent hypergeometric\n",
      "            function.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Laguerre polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        L : ndarray\n",
      "            Values of the Laguerre polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_laguerre : roots and quadrature weights of Laguerre\n",
      "                         polynomials\n",
      "        laguerre : Laguerre polynomial object\n",
      "        numpy.polynomial.laguerre.Laguerre : Laguerre series\n",
      "        eval_genlaguerre : evaluate generalized Laguerre polynomials\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_legendre = <ufunc 'eval_legendre'>\n",
      "        eval_legendre(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_legendre(n, x, out=None)\n",
      "        \n",
      "        Evaluate Legendre polynomial at a point.\n",
      "        \n",
      "        The Legendre polynomials can be defined via the Gauss\n",
      "        hypergeometric function :math:`{}_2F_1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P_n(x) = {}_2F_1(-n, n + 1; 1; (1 - x)/2).\n",
      "        \n",
      "        When :math:`n` is an integer the result is a polynomial of degree\n",
      "        :math:`n`. See 22.5.49 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to the Gauss hypergeometric\n",
      "            function.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the Legendre polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        P : ndarray\n",
      "            Values of the Legendre polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_legendre : roots and quadrature weights of Legendre\n",
      "                         polynomials\n",
      "        legendre : Legendre polynomial object\n",
      "        hyp2f1 : Gauss hypergeometric function\n",
      "        numpy.polynomial.legendre.Legendre : Legendre series\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import eval_legendre\n",
      "        \n",
      "        Evaluate the zero-order Legendre polynomial at x = 0\n",
      "        \n",
      "        >>> eval_legendre(0, 0)\n",
      "        1.0\n",
      "        \n",
      "        Evaluate the first-order Legendre polynomial between -1 and 1\n",
      "        \n",
      "        >>> X = np.linspace(-1, 1, 5)  # Domain of Legendre polynomials\n",
      "        >>> eval_legendre(1, X)\n",
      "        array([-1. , -0.5,  0. ,  0.5,  1. ])\n",
      "        \n",
      "        Evaluate Legendre polynomials of order 0 through 4 at x = 0\n",
      "        \n",
      "        >>> N = range(0, 5)\n",
      "        >>> eval_legendre(N, 0)\n",
      "        array([ 1.   ,  0.   , -0.5  ,  0.   ,  0.375])\n",
      "        \n",
      "        Plot Legendre polynomials of order 0 through 4\n",
      "        \n",
      "        >>> X = np.linspace(-1, 1)\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> for n in range(0, 5):\n",
      "        ...     y = eval_legendre(n, X)\n",
      "        ...     plt.plot(X, y, label=r'$P_{}(x)$'.format(n))\n",
      "        \n",
      "        >>> plt.title(\"Legendre Polynomials\")\n",
      "        >>> plt.xlabel(\"x\")\n",
      "        >>> plt.ylabel(r'$P_n(x)$')\n",
      "        >>> plt.legend(loc='lower right')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    eval_sh_chebyt = <ufunc 'eval_sh_chebyt'>\n",
      "        eval_sh_chebyt(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_sh_chebyt(n, x, out=None)\n",
      "        \n",
      "        Evaluate shifted Chebyshev polynomial of the first kind at a\n",
      "        point.\n",
      "        \n",
      "        These polynomials are defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T_n^*(x) = T_n(2x - 1)\n",
      "        \n",
      "        where :math:`T_n` is a Chebyshev polynomial of the first kind. See\n",
      "        22.5.14 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to `eval_chebyt`.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the shifted Chebyshev polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        T : ndarray\n",
      "            Values of the shifted Chebyshev polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_sh_chebyt : roots and quadrature weights of shifted\n",
      "                          Chebyshev polynomials of the first kind\n",
      "        sh_chebyt : shifted Chebyshev polynomial object\n",
      "        eval_chebyt : evaluate Chebyshev polynomials of the first kind\n",
      "        numpy.polynomial.chebyshev.Chebyshev : Chebyshev series\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_sh_chebyu = <ufunc 'eval_sh_chebyu'>\n",
      "        eval_sh_chebyu(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_sh_chebyu(n, x, out=None)\n",
      "        \n",
      "        Evaluate shifted Chebyshev polynomial of the second kind at a\n",
      "        point.\n",
      "        \n",
      "        These polynomials are defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            U_n^*(x) = U_n(2x - 1)\n",
      "        \n",
      "        where :math:`U_n` is a Chebyshev polynomial of the first kind. See\n",
      "        22.5.15 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to `eval_chebyu`.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the shifted Chebyshev polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        U : ndarray\n",
      "            Values of the shifted Chebyshev polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_sh_chebyu : roots and quadrature weights of shifted\n",
      "                          Chebychev polynomials of the second kind\n",
      "        sh_chebyu : shifted Chebyshev polynomial object\n",
      "        eval_chebyu : evaluate Chebyshev polynomials of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_sh_jacobi = <ufunc 'eval_sh_jacobi'>\n",
      "        eval_sh_jacobi(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_sh_jacobi(n, p, q, x, out=None)\n",
      "        \n",
      "        Evaluate shifted Jacobi polynomial at a point.\n",
      "        \n",
      "        Defined by\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            G_n^{(p, q)}(x)\n",
      "              = \\binom{2n + p - 1}{n}^{-1} P_n^{(p - q, q - 1)}(2x - 1),\n",
      "        \n",
      "        where :math:`P_n^{(\\cdot, \\cdot)}` is the n-th Jacobi\n",
      "        polynomial. See 22.5.2 in [AS]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            Degree of the polynomial. If not an integer, the result is\n",
      "            determined via the relation to `binom` and `eval_jacobi`.\n",
      "        p : float\n",
      "            Parameter\n",
      "        q : float\n",
      "            Parameter\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        G : ndarray\n",
      "            Values of the shifted Jacobi polynomial.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_sh_jacobi : roots and quadrature weights of shifted Jacobi\n",
      "                          polynomials\n",
      "        sh_jacobi : shifted Jacobi polynomial object\n",
      "        eval_jacobi : evaluate Jacobi polynomials\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    eval_sh_legendre = <ufunc 'eval_sh_legendre'>\n",
      "        eval_sh_legendre(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        eval_sh_legendre(n, x, out=None)\n",
      "        \n",
      "        Evaluate shifted Legendre polynomial at a point.\n",
      "        \n",
      "        These polynomials are defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P_n^*(x) = P_n(2x - 1)\n",
      "        \n",
      "        where :math:`P_n` is a Legendre polynomial. See 2.2.11 in [AS]_\n",
      "        for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Degree of the polynomial. If not an integer, the value is\n",
      "            determined via the relation to `eval_legendre`.\n",
      "        x : array_like\n",
      "            Points at which to evaluate the shifted Legendre polynomial\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        P : ndarray\n",
      "            Values of the shifted Legendre polynomial\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roots_sh_legendre : roots and quadrature weights of shifted\n",
      "                            Legendre polynomials\n",
      "        sh_legendre : shifted Legendre polynomial object\n",
      "        eval_legendre : evaluate Legendre polynomials\n",
      "        numpy.polynomial.legendre.Legendre : Legendre series\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [AS] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "            Handbook of Mathematical Functions with Formulas,\n",
      "            Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    exp1 = <ufunc 'exp1'>\n",
      "        exp1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        exp1(z, out=None)\n",
      "        \n",
      "        Exponential integral E1.\n",
      "        \n",
      "        For complex :math:`z \\ne 0` the exponential integral can be defined as\n",
      "        [1]_\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           E_1(z) = \\int_z^\\infty \\frac{e^{-t}}{t} dt,\n",
      "        \n",
      "        where the path of the integral does not cross the negative real\n",
      "        axis or pass through the origin.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z: array_like\n",
      "            Real or complex argument.\n",
      "        out: ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the exponential integral E1\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        expi : exponential integral :math:`Ei`\n",
      "        expn : generalization of :math:`E_1`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For :math:`x > 0` it is related to the exponential integral\n",
      "        :math:`Ei` (see `expi`) via the relation\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           E_1(x) = -Ei(-x).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Digital Library of Mathematical Functions, 6.2.1\n",
      "               https://dlmf.nist.gov/6.2#E1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It has a pole at 0.\n",
      "        \n",
      "        >>> sc.exp1(0)\n",
      "        inf\n",
      "        \n",
      "        It has a branch cut on the negative real axis.\n",
      "        \n",
      "        >>> sc.exp1(-1)\n",
      "        nan\n",
      "        >>> sc.exp1(complex(-1, 0))\n",
      "        (-1.8951178163559368-3.141592653589793j)\n",
      "        >>> sc.exp1(complex(-1, -0.0))\n",
      "        (-1.8951178163559368+3.141592653589793j)\n",
      "        \n",
      "        It approaches 0 along the positive real axis.\n",
      "        \n",
      "        >>> sc.exp1([1, 10, 100, 1000])\n",
      "        array([2.19383934e-01, 4.15696893e-06, 3.68359776e-46, 0.00000000e+00])\n",
      "        \n",
      "        It is related to `expi`.\n",
      "        \n",
      "        >>> x = np.array([1, 2, 3, 4])\n",
      "        >>> sc.exp1(x)\n",
      "        array([0.21938393, 0.04890051, 0.01304838, 0.00377935])\n",
      "        >>> -sc.expi(-x)\n",
      "        array([0.21938393, 0.04890051, 0.01304838, 0.00377935])\n",
      "    \n",
      "    exp10 = <ufunc 'exp10'>\n",
      "        exp10(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        exp10(x)\n",
      "        \n",
      "        Compute ``10**x`` element-wise.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            `x` must contain real numbers.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            ``10**x``, computed element-wise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import exp10\n",
      "        \n",
      "        >>> exp10(3)\n",
      "        1000.0\n",
      "        >>> x = np.array([[-1, -0.5, 0], [0.5, 1, 1.5]])\n",
      "        >>> exp10(x)\n",
      "        array([[  0.1       ,   0.31622777,   1.        ],\n",
      "               [  3.16227766,  10.        ,  31.6227766 ]])\n",
      "    \n",
      "    exp2 = <ufunc 'exp2'>\n",
      "        exp2(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        exp2(x)\n",
      "        \n",
      "        Compute ``2**x`` element-wise.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            `x` must contain real numbers.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            ``2**x``, computed element-wise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import exp2\n",
      "        \n",
      "        >>> exp2(3)\n",
      "        8.0\n",
      "        >>> x = np.array([[-1, -0.5, 0], [0.5, 1, 1.5]])\n",
      "        >>> exp2(x)\n",
      "        array([[ 0.5       ,  0.70710678,  1.        ],\n",
      "               [ 1.41421356,  2.        ,  2.82842712]])\n",
      "    \n",
      "    expi = <ufunc 'expi'>\n",
      "        expi(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        expi(x, out=None)\n",
      "        \n",
      "        Exponential integral Ei.\n",
      "        \n",
      "        For real :math:`x`, the exponential integral is defined as [1]_\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            Ei(x) = \\int_{-\\infty}^x \\frac{e^t}{t} dt.\n",
      "        \n",
      "        For :math:`x > 0` the integral is understood as a Cauchy principle\n",
      "        value.\n",
      "        \n",
      "        It is extended to the complex plane by analytic continuation of\n",
      "        the function on the interval :math:`(0, \\infty)`. The complex\n",
      "        variant has a branch cut on the negative real axis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x: array_like\n",
      "            Real or complex valued argument\n",
      "        out: ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the exponential integral\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The exponential integrals :math:`E_1` and :math:`Ei` satisfy the\n",
      "        relation\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            E_1(x) = -Ei(-x)\n",
      "        \n",
      "        for :math:`x > 0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        exp1 : Exponential integral :math:`E_1`\n",
      "        expn : Generalized exponential integral :math:`E_n`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Digital Library of Mathematical Functions, 6.2.5\n",
      "               https://dlmf.nist.gov/6.2#E5\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is related to `exp1`.\n",
      "        \n",
      "        >>> x = np.array([1, 2, 3, 4])\n",
      "        >>> -sc.expi(-x)\n",
      "        array([0.21938393, 0.04890051, 0.01304838, 0.00377935])\n",
      "        >>> sc.exp1(x)\n",
      "        array([0.21938393, 0.04890051, 0.01304838, 0.00377935])\n",
      "        \n",
      "        The complex variant has a branch cut on the negative real axis.\n",
      "        \n",
      "        >>> import scipy.special as sc\n",
      "        >>> sc.expi(-1 + 1e-12j)\n",
      "        (-0.21938393439552062+3.1415926535894254j)\n",
      "        >>> sc.expi(-1 - 1e-12j)\n",
      "        (-0.21938393439552062-3.1415926535894254j)\n",
      "        \n",
      "        As the complex variant approaches the branch cut, the real parts\n",
      "        approach the value of the real variant.\n",
      "        \n",
      "        >>> sc.expi(-1)\n",
      "        -0.21938393439552062\n",
      "        \n",
      "        The SciPy implementation returns the real variant for complex\n",
      "        values on the branch cut.\n",
      "        \n",
      "        >>> sc.expi(complex(-1, 0.0))\n",
      "        (-0.21938393439552062-0j)\n",
      "        >>> sc.expi(complex(-1, -0.0))\n",
      "        (-0.21938393439552062-0j)\n",
      "    \n",
      "    expit = <ufunc 'expit'>\n",
      "        expit(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        expit(x)\n",
      "        \n",
      "        Expit (a.k.a. logistic sigmoid) ufunc for ndarrays.\n",
      "        \n",
      "        The expit function, also known as the logistic sigmoid function, is\n",
      "        defined as ``expit(x) = 1/(1+exp(-x))``.  It is the inverse of the\n",
      "        logit function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            The ndarray to apply expit to element-wise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            An ndarray of the same shape as x. Its entries\n",
      "            are `expit` of the corresponding entry of x.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        logit\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As a ufunc expit takes a number of optional\n",
      "        keyword arguments. For more information\n",
      "        see `ufuncs <https://docs.scipy.org/doc/numpy/reference/ufuncs.html>`_\n",
      "        \n",
      "        .. versionadded:: 0.10.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import expit, logit\n",
      "        \n",
      "        >>> expit([-np.inf, -1.5, 0, 1.5, np.inf])\n",
      "        array([ 0.        ,  0.18242552,  0.5       ,  0.81757448,  1.        ])\n",
      "        \n",
      "        `logit` is the inverse of `expit`:\n",
      "        \n",
      "        >>> logit(expit([-2.5, 0, 3.1, 5.0]))\n",
      "        array([-2.5,  0. ,  3.1,  5. ])\n",
      "        \n",
      "        Plot expit(x) for x in [-6, 6]:\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(-6, 6, 121)\n",
      "        >>> y = expit(x)\n",
      "        >>> plt.plot(x, y)\n",
      "        >>> plt.grid()\n",
      "        >>> plt.xlim(-6, 6)\n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.title('expit(x)')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    expm1 = <ufunc 'expm1'>\n",
      "        expm1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        expm1(x)\n",
      "        \n",
      "        Compute ``exp(x) - 1``.\n",
      "        \n",
      "        When `x` is near zero, ``exp(x)`` is near 1, so the numerical calculation\n",
      "        of ``exp(x) - 1`` can suffer from catastrophic loss of precision.\n",
      "        ``expm1(x)`` is implemented to avoid the loss of precision that occurs when\n",
      "        `x` is near zero.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            `x` must contain real numbers.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            ``exp(x) - 1`` computed element-wise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import expm1\n",
      "        \n",
      "        >>> expm1(1.0)\n",
      "        1.7182818284590451\n",
      "        >>> expm1([-0.2, -0.1, 0, 0.1, 0.2])\n",
      "        array([-0.18126925, -0.09516258,  0.        ,  0.10517092,  0.22140276])\n",
      "        \n",
      "        The exact value of ``exp(7.5e-13) - 1`` is::\n",
      "        \n",
      "            7.5000000000028125000000007031250000001318...*10**-13.\n",
      "        \n",
      "        Here is what ``expm1(7.5e-13)`` gives:\n",
      "        \n",
      "        >>> expm1(7.5e-13)\n",
      "        7.5000000000028135e-13\n",
      "        \n",
      "        Compare that to ``exp(7.5e-13) - 1``, where the subtraction results in\n",
      "        a \"catastrophic\" loss of precision:\n",
      "        \n",
      "        >>> np.exp(7.5e-13) - 1\n",
      "        7.5006667543675576e-13\n",
      "    \n",
      "    expn = <ufunc 'expn'>\n",
      "        expn(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        expn(n, x, out=None)\n",
      "        \n",
      "        Generalized exponential integral En.\n",
      "        \n",
      "        For integer :math:`n \\geq 0` and real :math:`x \\geq 0` the\n",
      "        generalized exponential integral is defined as [dlmf]_\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            E_n(x) = x^{n - 1} \\int_x^\\infty \\frac{e^{-t}}{t^n} dt.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n: array_like\n",
      "            Non-negative integers\n",
      "        x: array_like\n",
      "            Real argument\n",
      "        out: ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the generalized exponential integral\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        exp1 : special case of :math:`E_n` for :math:`n = 1`\n",
      "        expi : related to :math:`E_n` when :math:`n = 1`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] Digital Library of Mathematical Functions, 8.19.2\n",
      "                  https://dlmf.nist.gov/8.19#E2\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        Its domain is nonnegative n and x.\n",
      "        \n",
      "        >>> sc.expn(-1, 1.0), sc.expn(1, -1.0)\n",
      "        (nan, nan)\n",
      "        \n",
      "        It has a pole at ``x = 0`` for ``n = 1, 2``; for larger ``n`` it\n",
      "        is equal to ``1 / (n - 1)``.\n",
      "        \n",
      "        >>> sc.expn([0, 1, 2, 3, 4], 0)\n",
      "        array([       inf,        inf, 1.        , 0.5       , 0.33333333])\n",
      "        \n",
      "        For n equal to 0 it reduces to ``exp(-x) / x``.\n",
      "        \n",
      "        >>> x = np.array([1, 2, 3, 4])\n",
      "        >>> sc.expn(0, x)\n",
      "        array([0.36787944, 0.06766764, 0.01659569, 0.00457891])\n",
      "        >>> np.exp(-x) / x\n",
      "        array([0.36787944, 0.06766764, 0.01659569, 0.00457891])\n",
      "        \n",
      "        For n equal to 1 it reduces to `exp1`.\n",
      "        \n",
      "        >>> sc.expn(1, x)\n",
      "        array([0.21938393, 0.04890051, 0.01304838, 0.00377935])\n",
      "        >>> sc.exp1(x)\n",
      "        array([0.21938393, 0.04890051, 0.01304838, 0.00377935])\n",
      "    \n",
      "    exprel = <ufunc 'exprel'>\n",
      "        exprel(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        exprel(x)\n",
      "        \n",
      "        Relative error exponential, ``(exp(x) - 1)/x``.\n",
      "        \n",
      "        When `x` is near zero, ``exp(x)`` is near 1, so the numerical calculation\n",
      "        of ``exp(x) - 1`` can suffer from catastrophic loss of precision.\n",
      "        ``exprel(x)`` is implemented to avoid the loss of precision that occurs when\n",
      "        `x` is near zero.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            Input array.  `x` must contain real numbers.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            ``(exp(x) - 1)/x``, computed element-wise.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        expm1\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import exprel\n",
      "        \n",
      "        >>> exprel(0.01)\n",
      "        1.0050167084168056\n",
      "        >>> exprel([-0.25, -0.1, 0, 0.1, 0.25])\n",
      "        array([ 0.88479687,  0.95162582,  1.        ,  1.05170918,  1.13610167])\n",
      "        \n",
      "        Compare ``exprel(5e-9)`` to the naive calculation.  The exact value\n",
      "        is ``1.00000000250000000416...``.\n",
      "        \n",
      "        >>> exprel(5e-9)\n",
      "        1.0000000025\n",
      "        \n",
      "        >>> (np.exp(5e-9) - 1)/5e-9\n",
      "        0.99999999392252903\n",
      "    \n",
      "    fdtr = <ufunc 'fdtr'>\n",
      "        fdtr(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        fdtr(dfn, dfd, x)\n",
      "        \n",
      "        F cumulative distribution function.\n",
      "        \n",
      "        Returns the value of the cumulative distribution function of the\n",
      "        F-distribution, also known as Snedecor's F-distribution or the\n",
      "        Fisher-Snedecor distribution.\n",
      "        \n",
      "        The F-distribution with parameters :math:`d_n` and :math:`d_d` is the\n",
      "        distribution of the random variable,\n",
      "        \n",
      "        .. math::\n",
      "            X = \\frac{U_n/d_n}{U_d/d_d},\n",
      "        \n",
      "        where :math:`U_n` and :math:`U_d` are random variables distributed\n",
      "        :math:`\\chi^2`, with :math:`d_n` and :math:`d_d` degrees of freedom,\n",
      "        respectively.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dfn : array_like\n",
      "            First parameter (positive float).\n",
      "        dfd : array_like\n",
      "            Second parameter (positive float).\n",
      "        x : array_like\n",
      "            Argument (nonnegative float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : ndarray\n",
      "            The CDF of the F-distribution with parameters `dfn` and `dfd` at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The regularized incomplete beta function is used, according to the\n",
      "        formula,\n",
      "        \n",
      "        .. math::\n",
      "            F(d_n, d_d; x) = I_{xd_n/(d_d + xd_n)}(d_n/2, d_d/2).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `fdtr`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    fdtrc = <ufunc 'fdtrc'>\n",
      "        fdtrc(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        fdtrc(dfn, dfd, x)\n",
      "        \n",
      "        F survival function.\n",
      "        \n",
      "        Returns the complemented F-distribution function (the integral of the\n",
      "        density from `x` to infinity).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dfn : array_like\n",
      "            First parameter (positive float).\n",
      "        dfd : array_like\n",
      "            Second parameter (positive float).\n",
      "        x : array_like\n",
      "            Argument (nonnegative float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y : ndarray\n",
      "            The complemented F-distribution function with parameters `dfn` and\n",
      "            `dfd` at `x`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        fdtr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The regularized incomplete beta function is used, according to the\n",
      "        formula,\n",
      "        \n",
      "        .. math::\n",
      "            F(d_n, d_d; x) = I_{d_d/(d_d + xd_n)}(d_d/2, d_n/2).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `fdtrc`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    fdtri = <ufunc 'fdtri'>\n",
      "        fdtri(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        fdtri(dfn, dfd, p)\n",
      "        \n",
      "        The `p`-th quantile of the F-distribution.\n",
      "        \n",
      "        This function is the inverse of the F-distribution CDF, `fdtr`, returning\n",
      "        the `x` such that `fdtr(dfn, dfd, x) = p`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dfn : array_like\n",
      "            First parameter (positive float).\n",
      "        dfd : array_like\n",
      "            Second parameter (positive float).\n",
      "        p : array_like\n",
      "            Cumulative probability, in [0, 1].\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The quantile corresponding to `p`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The computation is carried out using the relation to the inverse\n",
      "        regularized beta function, :math:`I^{-1}_x(a, b)`.  Let\n",
      "        :math:`z = I^{-1}_p(d_d/2, d_n/2).`  Then,\n",
      "        \n",
      "        .. math::\n",
      "            x = \\frac{d_d (1 - z)}{d_n z}.\n",
      "        \n",
      "        If `p` is such that :math:`x < 0.5`, the following relation is used\n",
      "        instead for improved stability: let\n",
      "        :math:`z' = I^{-1}_{1 - p}(d_n/2, d_d/2).` Then,\n",
      "        \n",
      "        .. math::\n",
      "            x = \\frac{d_d z'}{d_n (1 - z')}.\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `fdtri`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    fdtridfd = <ufunc 'fdtridfd'>\n",
      "        fdtridfd(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        fdtridfd(dfn, p, x)\n",
      "        \n",
      "        Inverse to `fdtr` vs dfd\n",
      "        \n",
      "        Finds the F density argument dfd such that ``fdtr(dfn, dfd, x) == p``.\n",
      "    \n",
      "    fresnel = <ufunc 'fresnel'>\n",
      "        fresnel(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        fresnel(z, out=None)\n",
      "        \n",
      "        Fresnel integrals.\n",
      "        \n",
      "        The Fresnel integrals are defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           S(z) &= \\int_0^z \\sin(\\pi t^2 /2) dt \\\\\n",
      "           C(z) &= \\int_0^z \\cos(\\pi t^2 /2) dt.\n",
      "        \n",
      "        See [dlmf]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex valued argument\n",
      "        out : 2-tuple of ndarrays, optional\n",
      "            Optional output arrays for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        S, C : 2-tuple of scalar or ndarray\n",
      "            Values of the Fresnel integrals\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fresnel_zeros : zeros of the Fresnel integrals\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/7.2#iii\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        As z goes to infinity along the real axis, S and C converge to 0.5.\n",
      "        \n",
      "        >>> S, C = sc.fresnel([0.1, 1, 10, 100, np.inf])\n",
      "        >>> S\n",
      "        array([0.00052359, 0.43825915, 0.46816998, 0.4968169 , 0.5       ])\n",
      "        >>> C\n",
      "        array([0.09999753, 0.7798934 , 0.49989869, 0.4999999 , 0.5       ])\n",
      "        \n",
      "        They are related to the error function `erf`.\n",
      "        \n",
      "        >>> z = np.array([1, 2, 3, 4])\n",
      "        >>> zeta = 0.5 * np.sqrt(np.pi) * (1 - 1j) * z\n",
      "        >>> S, C = sc.fresnel(z)\n",
      "        >>> C + 1j*S\n",
      "        array([0.7798934 +0.43825915j, 0.48825341+0.34341568j,\n",
      "               0.60572079+0.496313j  , 0.49842603+0.42051575j])\n",
      "        >>> 0.5 * (1 + 1j) * sc.erf(zeta)\n",
      "        array([0.7798934 +0.43825915j, 0.48825341+0.34341568j,\n",
      "               0.60572079+0.496313j  , 0.49842603+0.42051575j])\n",
      "    \n",
      "    gamma = <ufunc 'gamma'>\n",
      "        gamma(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gamma(z)\n",
      "        \n",
      "        gamma function.\n",
      "        \n",
      "        The gamma function is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} dt\n",
      "        \n",
      "        for :math:`\\Re(z) > 0` and is extended to the rest of the complex\n",
      "        plane by analytic continuation. See [dlmf]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex valued argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the gamma function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The gamma function is often referred to as the generalized\n",
      "        factorial since :math:`\\Gamma(n + 1) = n!` for natural numbers\n",
      "        :math:`n`. More generally it satisfies the recurrence relation\n",
      "        :math:`\\Gamma(z + 1) = z \\cdot \\Gamma(z)` for complex :math:`z`,\n",
      "        which, combined with the fact that :math:`\\Gamma(1) = 1`, implies\n",
      "        the above identity for :math:`z = n`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/5.2#E1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import gamma, factorial\n",
      "        \n",
      "        >>> gamma([0, 0.5, 1, 5])\n",
      "        array([         inf,   1.77245385,   1.        ,  24.        ])\n",
      "        \n",
      "        >>> z = 2.5 + 1j\n",
      "        >>> gamma(z)\n",
      "        (0.77476210455108352+0.70763120437959293j)\n",
      "        >>> gamma(z+1), z*gamma(z)  # Recurrence property\n",
      "        ((1.2292740569981171+2.5438401155000685j),\n",
      "         (1.2292740569981158+2.5438401155000658j))\n",
      "        \n",
      "        >>> gamma(0.5)**2  # gamma(0.5) = sqrt(pi)\n",
      "        3.1415926535897927\n",
      "        \n",
      "        Plot gamma(x) for real x\n",
      "        \n",
      "        >>> x = np.linspace(-3.5, 5.5, 2251)\n",
      "        >>> y = gamma(x)\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(x, y, 'b', alpha=0.6, label='gamma(x)')\n",
      "        >>> k = np.arange(1, 7)\n",
      "        >>> plt.plot(k, factorial(k-1), 'k*', alpha=0.6,\n",
      "        ...          label='(x-1)!, x = 1, 2, ...')\n",
      "        >>> plt.xlim(-3.5, 5.5)\n",
      "        >>> plt.ylim(-10, 25)\n",
      "        >>> plt.grid()\n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.legend(loc='lower right')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    gammainc = <ufunc 'gammainc'>\n",
      "        gammainc(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gammainc(a, x)\n",
      "        \n",
      "        Regularized lower incomplete gamma function.\n",
      "        \n",
      "        It is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P(a, x) = \\frac{1}{\\Gamma(a)} \\int_0^x t^{a - 1}e^{-t} dt\n",
      "        \n",
      "        for :math:`a > 0` and :math:`x \\geq 0`. See [dlmf]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Positive parameter\n",
      "        x : array_like\n",
      "            Nonnegative argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the lower incomplete gamma function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function satisfies the relation ``gammainc(a, x) +\n",
      "        gammaincc(a, x) = 1`` where `gammaincc` is the regularized upper\n",
      "        incomplete gamma function.\n",
      "        \n",
      "        The implementation largely follows that of [boost]_.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        gammaincc : regularized upper incomplete gamma function\n",
      "        gammaincinv : inverse of the regularized lower incomplete gamma\n",
      "            function with respect to `x`\n",
      "        gammainccinv : inverse of the regularized upper incomplete gamma\n",
      "            function with respect to `x`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical functions\n",
      "                  https://dlmf.nist.gov/8.2#E4\n",
      "        .. [boost] Maddock et. al., \"Incomplete Gamma Functions\",\n",
      "           https://www.boost.org/doc/libs/1_61_0/libs/math/doc/html/math_toolkit/sf_gamma/igamma.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is the CDF of the gamma distribution, so it starts at 0 and\n",
      "        monotonically increases to 1.\n",
      "        \n",
      "        >>> sc.gammainc(0.5, [0, 1, 10, 100])\n",
      "        array([0.        , 0.84270079, 0.99999226, 1.        ])\n",
      "        \n",
      "        It is equal to one minus the upper incomplete gamma function.\n",
      "        \n",
      "        >>> a, x = 0.5, 0.4\n",
      "        >>> sc.gammainc(a, x)\n",
      "        0.6289066304773024\n",
      "        >>> 1 - sc.gammaincc(a, x)\n",
      "        0.6289066304773024\n",
      "    \n",
      "    gammaincc = <ufunc 'gammaincc'>\n",
      "        gammaincc(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gammaincc(a, x)\n",
      "        \n",
      "        Regularized upper incomplete gamma function.\n",
      "        \n",
      "        It is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            Q(a, x) = \\frac{1}{\\Gamma(a)} \\int_x^\\infty t^{a - 1}e^{-t} dt\n",
      "        \n",
      "        for :math:`a > 0` and :math:`x \\geq 0`. See [dlmf]_ for details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Positive parameter\n",
      "        x : array_like\n",
      "            Nonnegative argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the upper incomplete gamma function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function satisfies the relation ``gammainc(a, x) +\n",
      "        gammaincc(a, x) = 1`` where `gammainc` is the regularized lower\n",
      "        incomplete gamma function.\n",
      "        \n",
      "        The implementation largely follows that of [boost]_.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        gammainc : regularized lower incomplete gamma function\n",
      "        gammaincinv : inverse of the regularized lower incomplete gamma\n",
      "            function with respect to `x`\n",
      "        gammainccinv : inverse to of the regularized upper incomplete\n",
      "            gamma function with respect to `x`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical functions\n",
      "                  https://dlmf.nist.gov/8.2#E4\n",
      "        .. [boost] Maddock et. al., \"Incomplete Gamma Functions\",\n",
      "           https://www.boost.org/doc/libs/1_61_0/libs/math/doc/html/math_toolkit/sf_gamma/igamma.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is the survival function of the gamma distribution, so it\n",
      "        starts at 1 and monotonically decreases to 0.\n",
      "        \n",
      "        >>> sc.gammaincc(0.5, [0, 1, 10, 100, 1000])\n",
      "        array([1.00000000e+00, 1.57299207e-01, 7.74421643e-06, 2.08848758e-45,\n",
      "               0.00000000e+00])\n",
      "        \n",
      "        It is equal to one minus the lower incomplete gamma function.\n",
      "        \n",
      "        >>> a, x = 0.5, 0.4\n",
      "        >>> sc.gammaincc(a, x)\n",
      "        0.37109336952269756\n",
      "        >>> 1 - sc.gammainc(a, x)\n",
      "        0.37109336952269756\n",
      "    \n",
      "    gammainccinv = <ufunc 'gammainccinv'>\n",
      "        gammainccinv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gammainccinv(a, y)\n",
      "        \n",
      "        Inverse of the upper incomplete gamma function with respect to `x`\n",
      "        \n",
      "        Given an input :math:`y` between 0 and 1, returns :math:`x` such\n",
      "        that :math:`y = Q(a, x)`. Here :math:`Q` is the upper incomplete\n",
      "        gamma function; see `gammaincc`. This is well-defined because the\n",
      "        upper incomplete gamma function is monotonic as can be seen from\n",
      "        its definition in [dlmf]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Positive parameter\n",
      "        y : array_like\n",
      "            Argument between 0 and 1, inclusive\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the inverse of the upper incomplete gamma function\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gammaincc : regularized upper incomplete gamma function\n",
      "        gammainc : regularized lower incomplete gamma function\n",
      "        gammaincinv : inverse of the regularized lower incomplete gamma\n",
      "            function with respect to `x`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/8.2#E4\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It starts at infinity and monotonically decreases to 0.\n",
      "        \n",
      "        >>> sc.gammainccinv(0.5, [0, 0.1, 0.5, 1])\n",
      "        array([       inf, 1.35277173, 0.22746821, 0.        ])\n",
      "        \n",
      "        It inverts the upper incomplete gamma function.\n",
      "        \n",
      "        >>> a, x = 0.5, [0, 0.1, 0.5, 1]\n",
      "        >>> sc.gammaincc(a, sc.gammainccinv(a, x))\n",
      "        array([0. , 0.1, 0.5, 1. ])\n",
      "        \n",
      "        >>> a, x = 0.5, [0, 10, 50]\n",
      "        >>> sc.gammainccinv(a, sc.gammaincc(a, x))\n",
      "        array([ 0., 10., 50.])\n",
      "    \n",
      "    gammaincinv = <ufunc 'gammaincinv'>\n",
      "        gammaincinv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gammaincinv(a, y)\n",
      "        \n",
      "        Inverse to the lower incomplete gamma function with respect to `x`.\n",
      "        \n",
      "        Given an input :math:`y` between 0 and 1, returns :math:`x` such\n",
      "        that :math:`y = P(a, x)`. Here :math:`P` is the regularized lower\n",
      "        incomplete gamma function; see `gammainc`. This is well-defined\n",
      "        because the lower incomplete gamma function is monotonic as can be\n",
      "        seen from its definition in [dlmf]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Positive parameter\n",
      "        y : array_like\n",
      "            Parameter between 0 and 1, inclusive\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the inverse of the lower incomplete gamma function\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gammainc : regularized lower incomplete gamma function\n",
      "        gammaincc : regularized upper incomplete gamma function\n",
      "        gammainccinv : inverse of the regualizred upper incomplete gamma\n",
      "            function with respect to `x`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/8.2#E4\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It starts at 0 and monotonically increases to infinity.\n",
      "        \n",
      "        >>> sc.gammaincinv(0.5, [0, 0.1 ,0.5, 1])\n",
      "        array([0.        , 0.00789539, 0.22746821,        inf])\n",
      "        \n",
      "        It inverts the lower incomplete gamma function.\n",
      "        \n",
      "        >>> a, x = 0.5, [0, 0.1, 0.5, 1]\n",
      "        >>> sc.gammainc(a, sc.gammaincinv(a, x))\n",
      "        array([0. , 0.1, 0.5, 1. ])\n",
      "        \n",
      "        >>> a, x = 0.5, [0, 10, 25]\n",
      "        >>> sc.gammaincinv(a, sc.gammainc(a, x))\n",
      "        array([ 0.        , 10.        , 25.00001465])\n",
      "    \n",
      "    gammaln = <ufunc 'gammaln'>\n",
      "        gammaln(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gammaln(x, out=None)\n",
      "        \n",
      "        Logarithm of the absolute value of the gamma function.\n",
      "        \n",
      "        Defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\ln(\\lvert\\Gamma(x)\\rvert)\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function. For more details on\n",
      "        the gamma function, see [dlmf]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the log of the absolute value of gamma\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gammasgn : sign of the gamma function\n",
      "        loggamma : principal branch of the logarithm of the gamma function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        It is the same function as the Python standard library function\n",
      "        :func:`math.lgamma`.\n",
      "        \n",
      "        When used in conjunction with `gammasgn`, this function is useful\n",
      "        for working in logspace on the real axis without having to deal\n",
      "        with complex numbers via the relation ``exp(gammaln(x)) =\n",
      "        gammasgn(x) * gamma(x)``.\n",
      "        \n",
      "        For complex-valued log-gamma, use `loggamma` instead of `gammaln`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/5\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It has two positive zeros.\n",
      "        \n",
      "        >>> sc.gammaln([1, 2])\n",
      "        array([0., 0.])\n",
      "        \n",
      "        It has poles at nonpositive integers.\n",
      "        \n",
      "        >>> sc.gammaln([0, -1, -2, -3, -4])\n",
      "        array([inf, inf, inf, inf, inf])\n",
      "        \n",
      "        It asymptotically approaches ``x * log(x)`` (Stirling's formula).\n",
      "        \n",
      "        >>> x = np.array([1e10, 1e20, 1e40, 1e80])\n",
      "        >>> sc.gammaln(x)\n",
      "        array([2.20258509e+11, 4.50517019e+21, 9.11034037e+41, 1.83206807e+82])\n",
      "        >>> x * np.log(x)\n",
      "        array([2.30258509e+11, 4.60517019e+21, 9.21034037e+41, 1.84206807e+82])\n",
      "    \n",
      "    gammasgn = <ufunc 'gammasgn'>\n",
      "        gammasgn(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gammasgn(x)\n",
      "        \n",
      "        Sign of the gamma function.\n",
      "        \n",
      "        It is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\text{gammasgn}(x) =\n",
      "           \\begin{cases}\n",
      "             +1 & \\Gamma(x) > 0 \\\\\n",
      "             -1 & \\Gamma(x) < 0\n",
      "           \\end{cases}\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function; see `gamma`. This\n",
      "        definition is complete since the gamma function is never zero;\n",
      "        see the discussion after [dlmf]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Sign of the gamma function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The gamma function can be computed as ``gammasgn(x) *\n",
      "        np.exp(gammaln(x))``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gamma : the gamma function\n",
      "        gammaln : log of the absolute value of the gamma function\n",
      "        loggamma : analytic continuation of the log of the gamma function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/5.2#E1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is 1 for `x > 0`.\n",
      "        \n",
      "        >>> sc.gammasgn([1, 2, 3, 4])\n",
      "        array([1., 1., 1., 1.])\n",
      "        \n",
      "        It alternates between -1 and 1 for negative integers.\n",
      "        \n",
      "        >>> sc.gammasgn([-0.5, -1.5, -2.5, -3.5])\n",
      "        array([-1.,  1., -1.,  1.])\n",
      "        \n",
      "        It can be used to compute the gamma function.\n",
      "        \n",
      "        >>> x = [1.5, 0.5, -0.5, -1.5]\n",
      "        >>> sc.gammasgn(x) * np.exp(sc.gammaln(x))\n",
      "        array([ 0.88622693,  1.77245385, -3.5449077 ,  2.3632718 ])\n",
      "        >>> sc.gamma(x)\n",
      "        array([ 0.88622693,  1.77245385, -3.5449077 ,  2.3632718 ])\n",
      "    \n",
      "    gdtr = <ufunc 'gdtr'>\n",
      "        gdtr(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gdtr(a, b, x)\n",
      "        \n",
      "        Gamma distribution cumulative distribution function.\n",
      "        \n",
      "        Returns the integral from zero to `x` of the gamma probability density\n",
      "        function,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            F = \\int_0^x \\frac{a^b}{\\Gamma(b)} t^{b-1} e^{-at}\\,dt,\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            The rate parameter of the gamma distribution, sometimes denoted\n",
      "            :math:`\\beta` (float).  It is also the reciprocal of the scale\n",
      "            parameter :math:`\\theta`.\n",
      "        b : array_like\n",
      "            The shape parameter of the gamma distribution, sometimes denoted\n",
      "            :math:`\\alpha` (float).\n",
      "        x : array_like\n",
      "            The quantile (upper limit of integration; float).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        gdtrc : 1 - CDF of the gamma distribution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        F : ndarray\n",
      "            The CDF of the gamma distribution with parameters `a` and `b`\n",
      "            evaluated at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The evaluation is carried out using the relation to the incomplete gamma\n",
      "        integral (regularized gamma function).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `gdtr`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    gdtrc = <ufunc 'gdtrc'>\n",
      "        gdtrc(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gdtrc(a, b, x)\n",
      "        \n",
      "        Gamma distribution survival function.\n",
      "        \n",
      "        Integral from `x` to infinity of the gamma probability density function,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            F = \\int_x^\\infty \\frac{a^b}{\\Gamma(b)} t^{b-1} e^{-at}\\,dt,\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            The rate parameter of the gamma distribution, sometimes denoted\n",
      "            :math:`\\beta` (float). It is also the reciprocal of the scale\n",
      "            parameter :math:`\\theta`.\n",
      "        b : array_like\n",
      "            The shape parameter of the gamma distribution, sometimes denoted\n",
      "            :math:`\\alpha` (float).\n",
      "        x : array_like\n",
      "            The quantile (lower limit of integration; float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        F : ndarray\n",
      "            The survival function of the gamma distribution with parameters `a`\n",
      "            and `b` evaluated at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gdtr, gdtrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The evaluation is carried out using the relation to the incomplete gamma\n",
      "        integral (regularized gamma function).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `gdtrc`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    gdtria = <ufunc 'gdtria'>\n",
      "        gdtria(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gdtria(p, b, x, out=None)\n",
      "        \n",
      "        Inverse of `gdtr` vs a.\n",
      "        \n",
      "        Returns the inverse with respect to the parameter `a` of ``p =\n",
      "        gdtr(a, b, x)``, the cumulative distribution function of the gamma\n",
      "        distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            Probability values.\n",
      "        b : array_like\n",
      "            `b` parameter values of `gdtr(a, b, x)`. `b` is the \"shape\" parameter\n",
      "            of the gamma distribution.\n",
      "        x : array_like\n",
      "            Nonnegative real values, from the domain of the gamma distribution.\n",
      "        out : ndarray, optional\n",
      "            If a fourth argument is given, it must be a numpy.ndarray whose size\n",
      "            matches the broadcast result of `a`, `b` and `x`.  `out` is then the\n",
      "            array returned by the function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        a : ndarray\n",
      "            Values of the `a` parameter such that `p = gdtr(a, b, x)`.  `1/a`\n",
      "            is the \"scale\" parameter of the gamma distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gdtr : CDF of the gamma distribution.\n",
      "        gdtrib : Inverse with respect to `b` of `gdtr(a, b, x)`.\n",
      "        gdtrix : Inverse with respect to `x` of `gdtr(a, b, x)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdfgam`.\n",
      "        \n",
      "        The cumulative distribution function `p` is computed using a routine by\n",
      "        DiDinato and Morris [2]_. Computation of `a` involves a search for a value\n",
      "        that produces the desired value of `p`. The search relies on the\n",
      "        monotonicity of `p` with `a`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] DiDinato, A. R. and Morris, A. H.,\n",
      "               Computation of the incomplete gamma function ratios and their\n",
      "               inverse.  ACM Trans. Math. Softw. 12 (1986), 377-393.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First evaluate `gdtr`.\n",
      "        \n",
      "        >>> from scipy.special import gdtr, gdtria\n",
      "        >>> p = gdtr(1.2, 3.4, 5.6)\n",
      "        >>> print(p)\n",
      "        0.94378087442\n",
      "        \n",
      "        Verify the inverse.\n",
      "        \n",
      "        >>> gdtria(p, 3.4, 5.6)\n",
      "        1.2\n",
      "    \n",
      "    gdtrib = <ufunc 'gdtrib'>\n",
      "        gdtrib(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gdtrib(a, p, x, out=None)\n",
      "        \n",
      "        Inverse of `gdtr` vs b.\n",
      "        \n",
      "        Returns the inverse with respect to the parameter `b` of ``p =\n",
      "        gdtr(a, b, x)``, the cumulative distribution function of the gamma\n",
      "        distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            `a` parameter values of `gdtr(a, b, x)`. `1/a` is the \"scale\"\n",
      "            parameter of the gamma distribution.\n",
      "        p : array_like\n",
      "            Probability values.\n",
      "        x : array_like\n",
      "            Nonnegative real values, from the domain of the gamma distribution.\n",
      "        out : ndarray, optional\n",
      "            If a fourth argument is given, it must be a numpy.ndarray whose size\n",
      "            matches the broadcast result of `a`, `b` and `x`.  `out` is then the\n",
      "            array returned by the function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        b : ndarray\n",
      "            Values of the `b` parameter such that `p = gdtr(a, b, x)`.  `b` is\n",
      "            the \"shape\" parameter of the gamma distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gdtr : CDF of the gamma distribution.\n",
      "        gdtria : Inverse with respect to `a` of `gdtr(a, b, x)`.\n",
      "        gdtrix : Inverse with respect to `x` of `gdtr(a, b, x)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdfgam`.\n",
      "        \n",
      "        The cumulative distribution function `p` is computed using a routine by\n",
      "        DiDinato and Morris [2]_. Computation of `b` involves a search for a value\n",
      "        that produces the desired value of `p`. The search relies on the\n",
      "        monotonicity of `p` with `b`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] DiDinato, A. R. and Morris, A. H.,\n",
      "               Computation of the incomplete gamma function ratios and their\n",
      "               inverse.  ACM Trans. Math. Softw. 12 (1986), 377-393.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First evaluate `gdtr`.\n",
      "        \n",
      "        >>> from scipy.special import gdtr, gdtrib\n",
      "        >>> p = gdtr(1.2, 3.4, 5.6)\n",
      "        >>> print(p)\n",
      "        0.94378087442\n",
      "        \n",
      "        Verify the inverse.\n",
      "        \n",
      "        >>> gdtrib(1.2, p, 5.6)\n",
      "        3.3999999999723882\n",
      "    \n",
      "    gdtrix = <ufunc 'gdtrix'>\n",
      "        gdtrix(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        gdtrix(a, b, p, out=None)\n",
      "        \n",
      "        Inverse of `gdtr` vs x.\n",
      "        \n",
      "        Returns the inverse with respect to the parameter `x` of ``p =\n",
      "        gdtr(a, b, x)``, the cumulative distribution function of the gamma\n",
      "        distribution. This is also known as the pth quantile of the\n",
      "        distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            `a` parameter values of `gdtr(a, b, x)`. `1/a` is the \"scale\"\n",
      "            parameter of the gamma distribution.\n",
      "        b : array_like\n",
      "            `b` parameter values of `gdtr(a, b, x)`. `b` is the \"shape\" parameter\n",
      "            of the gamma distribution.\n",
      "        p : array_like\n",
      "            Probability values.\n",
      "        out : ndarray, optional\n",
      "            If a fourth argument is given, it must be a numpy.ndarray whose size\n",
      "            matches the broadcast result of `a`, `b` and `x`. `out` is then the\n",
      "            array returned by the function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Values of the `x` parameter such that `p = gdtr(a, b, x)`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gdtr : CDF of the gamma distribution.\n",
      "        gdtria : Inverse with respect to `a` of `gdtr(a, b, x)`.\n",
      "        gdtrib : Inverse with respect to `b` of `gdtr(a, b, x)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdfgam`.\n",
      "        \n",
      "        The cumulative distribution function `p` is computed using a routine by\n",
      "        DiDinato and Morris [2]_. Computation of `x` involves a search for a value\n",
      "        that produces the desired value of `p`. The search relies on the\n",
      "        monotonicity of `p` with `x`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] DiDinato, A. R. and Morris, A. H.,\n",
      "               Computation of the incomplete gamma function ratios and their\n",
      "               inverse.  ACM Trans. Math. Softw. 12 (1986), 377-393.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First evaluate `gdtr`.\n",
      "        \n",
      "        >>> from scipy.special import gdtr, gdtrix\n",
      "        >>> p = gdtr(1.2, 3.4, 5.6)\n",
      "        >>> print(p)\n",
      "        0.94378087442\n",
      "        \n",
      "        Verify the inverse.\n",
      "        \n",
      "        >>> gdtrix(1.2, 3.4, p)\n",
      "        5.5999999999999996\n",
      "    \n",
      "    hankel1 = <ufunc 'hankel1'>\n",
      "        hankel1(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hankel1(v, z)\n",
      "        \n",
      "        Hankel function of the first kind\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : Values of the Hankel function of the first kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A wrapper for the AMOS [1]_ routine `zbesh`, which carries out the\n",
      "        computation using the relation,\n",
      "        \n",
      "        .. math:: H^{(1)}_v(z) = \\frac{2}{\\imath\\pi} \\exp(-\\imath \\pi v/2) K_v(z \\exp(-\\imath\\pi/2))\n",
      "        \n",
      "        where :math:`K_v` is the modified Bessel function of the second kind.\n",
      "        For negative orders, the relation\n",
      "        \n",
      "        .. math:: H^{(1)}_{-v}(z) = H^{(1)}_v(z) \\exp(\\imath\\pi v)\n",
      "        \n",
      "        is used.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        hankel1e : this function with leading exponential behavior stripped off.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    hankel1e = <ufunc 'hankel1e'>\n",
      "        hankel1e(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hankel1e(v, z)\n",
      "        \n",
      "        Exponentially scaled Hankel function of the first kind\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            hankel1e(v, z) = hankel1(v, z) * exp(-1j * z)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : Values of the exponentially scaled Hankel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A wrapper for the AMOS [1]_ routine `zbesh`, which carries out the\n",
      "        computation using the relation,\n",
      "        \n",
      "        .. math:: H^{(1)}_v(z) = \\frac{2}{\\imath\\pi} \\exp(-\\imath \\pi v/2) K_v(z \\exp(-\\imath\\pi/2))\n",
      "        \n",
      "        where :math:`K_v` is the modified Bessel function of the second kind.\n",
      "        For negative orders, the relation\n",
      "        \n",
      "        .. math:: H^{(1)}_{-v}(z) = H^{(1)}_v(z) \\exp(\\imath\\pi v)\n",
      "        \n",
      "        is used.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    hankel2 = <ufunc 'hankel2'>\n",
      "        hankel2(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hankel2(v, z)\n",
      "        \n",
      "        Hankel function of the second kind\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : Values of the Hankel function of the second kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A wrapper for the AMOS [1]_ routine `zbesh`, which carries out the\n",
      "        computation using the relation,\n",
      "        \n",
      "        .. math:: H^{(2)}_v(z) = -\\frac{2}{\\imath\\pi} \\exp(\\imath \\pi v/2) K_v(z \\exp(\\imath\\pi/2))\n",
      "        \n",
      "        where :math:`K_v` is the modified Bessel function of the second kind.\n",
      "        For negative orders, the relation\n",
      "        \n",
      "        .. math:: H^{(2)}_{-v}(z) = H^{(2)}_v(z) \\exp(-\\imath\\pi v)\n",
      "        \n",
      "        is used.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        hankel2e : this function with leading exponential behavior stripped off.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    hankel2e = <ufunc 'hankel2e'>\n",
      "        hankel2e(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hankel2e(v, z)\n",
      "        \n",
      "        Exponentially scaled Hankel function of the second kind\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            hankel2e(v, z) = hankel2(v, z) * exp(1j * z)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : Values of the exponentially scaled Hankel function of the second kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        A wrapper for the AMOS [1]_ routine `zbesh`, which carries out the\n",
      "        computation using the relation,\n",
      "        \n",
      "        .. math:: H^{(2)}_v(z) = -\\frac{2}{\\imath\\pi} \\exp(\\frac{\\imath \\pi v}{2}) K_v(z exp(\\frac{\\imath\\pi}{2}))\n",
      "        \n",
      "        where :math:`K_v` is the modified Bessel function of the second kind.\n",
      "        For negative orders, the relation\n",
      "        \n",
      "        .. math:: H^{(2)}_{-v}(z) = H^{(2)}_v(z) \\exp(-\\imath\\pi v)\n",
      "        \n",
      "        is used.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    huber = <ufunc 'huber'>\n",
      "        huber(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        huber(delta, r)\n",
      "        \n",
      "        Huber loss function.\n",
      "        \n",
      "        .. math:: \\text{huber}(\\delta, r) = \\begin{cases} \\infty & \\delta < 0  \\\\ \\frac{1}{2}r^2 & 0 \\le \\delta, | r | \\le \\delta \\\\ \\delta ( |r| - \\frac{1}{2}\\delta ) & \\text{otherwise} \\end{cases}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        delta : ndarray\n",
      "            Input array, indicating the quadratic vs. linear loss changepoint.\n",
      "        r : ndarray\n",
      "            Input array, possibly representing residuals.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : ndarray\n",
      "            The computed Huber loss function values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is convex in r.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "    \n",
      "    hyp0f1 = <ufunc 'hyp0f1'>\n",
      "        hyp0f1(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hyp0f1(v, z, out=None)\n",
      "        \n",
      "        Confluent hypergeometric limit function 0F1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Real-valued parameter\n",
      "        z : array_like\n",
      "            Real- or complex-valued argument\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            The confluent hypergeometric limit function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is defined as:\n",
      "        \n",
      "        .. math:: _0F_1(v, z) = \\sum_{k=0}^{\\infty}\\frac{z^k}{(v)_k k!}.\n",
      "        \n",
      "        It's also the limit as :math:`q \\to \\infty` of :math:`_1F_1(q; v; z/q)`,\n",
      "        and satisfies the differential equation :math:`f''(z) + vf'(z) =\n",
      "        f(z)`. See [1]_ for more information.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wolfram MathWorld, \"Confluent Hypergeometric Limit Function\",\n",
      "               http://mathworld.wolfram.com/ConfluentHypergeometricLimitFunction.html\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is one when `z` is zero.\n",
      "        \n",
      "        >>> sc.hyp0f1(1, 0)\n",
      "        1.0\n",
      "        \n",
      "        It is the limit of the confluent hypergeometric function as `q`\n",
      "        goes to infinity.\n",
      "        \n",
      "        >>> q = np.array([1, 10, 100, 1000])\n",
      "        >>> v = 1\n",
      "        >>> z = 1\n",
      "        >>> sc.hyp1f1(q, v, z / q)\n",
      "        array([2.71828183, 2.31481985, 2.28303778, 2.27992985])\n",
      "        >>> sc.hyp0f1(v, z)\n",
      "        2.2795853023360673\n",
      "        \n",
      "        It is related to Bessel functions.\n",
      "        \n",
      "        >>> n = 1\n",
      "        >>> x = np.linspace(0, 1, 5)\n",
      "        >>> sc.jv(n, x)\n",
      "        array([0.        , 0.12402598, 0.24226846, 0.3492436 , 0.44005059])\n",
      "        >>> (0.5 * x)**n / sc.factorial(n) * sc.hyp0f1(n + 1, -0.25 * x**2)\n",
      "        array([0.        , 0.12402598, 0.24226846, 0.3492436 , 0.44005059])\n",
      "    \n",
      "    hyp1f1 = <ufunc 'hyp1f1'>\n",
      "        hyp1f1(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hyp1f1(a, b, x, out=None)\n",
      "        \n",
      "        Confluent hypergeometric function 1F1.\n",
      "        \n",
      "        The confluent hypergeometric function is defined by the series\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           {}_1F_1(a; b; x) = \\sum_{k = 0}^\\infty \\frac{(a)_k}{(b)_k k!} x^k.\n",
      "        \n",
      "        See [dlmf]_ for more details. Here :math:`(\\cdot)_k` is the\n",
      "        Pochhammer symbol; see `poch`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array_like\n",
      "            Real parameters\n",
      "        x : array_like\n",
      "            Real or complex argument\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the confluent hypergeometric function\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        hyperu : another confluent hypergeometric function\n",
      "        hyp0f1 : confluent hypergeometric limit function\n",
      "        hyp2f1 : Gaussian hypergeometric function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/13.2#E2\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is one when `x` is zero:\n",
      "        \n",
      "        >>> sc.hyp1f1(0.5, 0.5, 0)\n",
      "        1.0\n",
      "        \n",
      "        It is singular when `b` is a nonpositive integer.\n",
      "        \n",
      "        >>> sc.hyp1f1(0.5, -1, 0)\n",
      "        inf\n",
      "        \n",
      "        It is a polynomial when `a` is a nonpositive integer.\n",
      "        \n",
      "        >>> a, b, x = -1, 0.5, np.array([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> sc.hyp1f1(a, b, x)\n",
      "        array([-1., -3., -5., -7.])\n",
      "        >>> 1 + (a / b) * x\n",
      "        array([-1., -3., -5., -7.])\n",
      "        \n",
      "        It reduces to the exponential function when `a = b`.\n",
      "        \n",
      "        >>> sc.hyp1f1(2, 2, [1, 2, 3, 4])\n",
      "        array([ 2.71828183,  7.3890561 , 20.08553692, 54.59815003])\n",
      "        >>> np.exp([1, 2, 3, 4])\n",
      "        array([ 2.71828183,  7.3890561 , 20.08553692, 54.59815003])\n",
      "    \n",
      "    hyp2f1 = <ufunc 'hyp2f1'>\n",
      "        hyp2f1(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hyp2f1(a, b, c, z)\n",
      "        \n",
      "        Gauss hypergeometric function 2F1(a, b; c; z)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b, c : array_like\n",
      "            Arguments, should be real-valued.\n",
      "        z : array_like\n",
      "            Argument, real or complex.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        hyp2f1 : scalar or ndarray\n",
      "            The values of the gaussian hypergeometric function.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        hyp0f1 : confluent hypergeometric limit function.\n",
      "        hyp1f1 : Kummer's (confluent hypergeometric) function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is defined for :math:`|z| < 1` as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\mathrm{hyp2f1}(a, b, c, z) = \\sum_{n=0}^\\infty\n",
      "           \\frac{(a)_n (b)_n}{(c)_n}\\frac{z^n}{n!},\n",
      "        \n",
      "        and defined on the rest of the complex z-plane by analytic\n",
      "        continuation [1]_.\n",
      "        Here :math:`(\\cdot)_n` is the Pochhammer symbol; see `poch`. When\n",
      "        :math:`n` is an integer the result is a polynomial of degree :math:`n`.\n",
      "        \n",
      "        The implementation for complex values of ``z`` is described in [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/15.2\n",
      "        .. [2] S. Zhang and J.M. Jin, \"Computation of Special Functions\", Wiley 1996\n",
      "        .. [3] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It has poles when `c` is a negative integer.\n",
      "        \n",
      "        >>> sc.hyp2f1(1, 1, -2, 1)\n",
      "        inf\n",
      "        \n",
      "        It is a polynomial when `a` or `b` is a negative integer.\n",
      "        \n",
      "        >>> a, b, c = -1, 1, 1.5\n",
      "        >>> z = np.linspace(0, 1, 5)\n",
      "        >>> sc.hyp2f1(a, b, c, z)\n",
      "        array([1.        , 0.83333333, 0.66666667, 0.5       , 0.33333333])\n",
      "        >>> 1 + a * b * z / c\n",
      "        array([1.        , 0.83333333, 0.66666667, 0.5       , 0.33333333])\n",
      "        \n",
      "        It is symmetric in `a` and `b`.\n",
      "        \n",
      "        >>> a = np.linspace(0, 1, 5)\n",
      "        >>> b = np.linspace(0, 1, 5)\n",
      "        >>> sc.hyp2f1(a, b, 1, 0.5)\n",
      "        array([1.        , 1.03997334, 1.1803406 , 1.47074441, 2.        ])\n",
      "        >>> sc.hyp2f1(b, a, 1, 0.5)\n",
      "        array([1.        , 1.03997334, 1.1803406 , 1.47074441, 2.        ])\n",
      "        \n",
      "        It contains many other functions as special cases.\n",
      "        \n",
      "        >>> z = 0.5\n",
      "        >>> sc.hyp2f1(1, 1, 2, z)\n",
      "        1.3862943611198901\n",
      "        >>> -np.log(1 - z) / z\n",
      "        1.3862943611198906\n",
      "        \n",
      "        >>> sc.hyp2f1(0.5, 1, 1.5, z**2)\n",
      "        1.098612288668109\n",
      "        >>> np.log((1 + z) / (1 - z)) / (2 * z)\n",
      "        1.0986122886681098\n",
      "        \n",
      "        >>> sc.hyp2f1(0.5, 1, 1.5, -z**2)\n",
      "        0.9272952180016117\n",
      "        >>> np.arctan(z) / z\n",
      "        0.9272952180016123\n",
      "    \n",
      "    hyperu = <ufunc 'hyperu'>\n",
      "        hyperu(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        hyperu(a, b, x, out=None)\n",
      "        \n",
      "        Confluent hypergeometric function U\n",
      "        \n",
      "        It is defined as the solution to the equation\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           x \\frac{d^2w}{dx^2} + (b - x) \\frac{dw}{dx} - aw = 0\n",
      "        \n",
      "        which satisfies the property\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           U(a, b, x) \\sim x^{-a}\n",
      "        \n",
      "        as :math:`x \\to \\infty`. See [dlmf]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a, b : array_like\n",
      "            Real-valued parameters\n",
      "        x : array_like\n",
      "            Real-valued argument\n",
      "        out : ndarray\n",
      "            Optional output array for the function values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of `U`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematics Functions\n",
      "                  https://dlmf.nist.gov/13.2#E6\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It has a branch cut along the negative `x` axis.\n",
      "        \n",
      "        >>> x = np.linspace(-0.1, -10, 5)\n",
      "        >>> sc.hyperu(1, 1, x)\n",
      "        array([nan, nan, nan, nan, nan])\n",
      "        \n",
      "        It approaches zero as `x` goes to infinity.\n",
      "        \n",
      "        >>> x = np.array([1, 10, 100])\n",
      "        >>> sc.hyperu(1, 1, x)\n",
      "        array([0.59634736, 0.09156333, 0.00990194])\n",
      "        \n",
      "        It satisfies Kummer's transformation.\n",
      "        \n",
      "        >>> a, b, x = 2, 1, 1\n",
      "        >>> sc.hyperu(a, b, x)\n",
      "        0.1926947246463881\n",
      "        >>> x**(1 - b) * sc.hyperu(a - b + 1, 2 - b, x)\n",
      "        0.1926947246463881\n",
      "    \n",
      "    i0 = <ufunc 'i0'>\n",
      "        i0(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        i0(x)\n",
      "        \n",
      "        Modified Bessel function of order 0.\n",
      "        \n",
      "        Defined as,\n",
      "        \n",
      "        .. math::\n",
      "            I_0(x) = \\sum_{k=0}^\\infty \\frac{(x^2/4)^k}{(k!)^2} = J_0(\\imath x),\n",
      "        \n",
      "        where :math:`J_0` is the Bessel function of the first kind of order 0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            Value of the modified Bessel function of order 0 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 8] and (8, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `i0`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        iv\n",
      "        i0e\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    i0e = <ufunc 'i0e'>\n",
      "        i0e(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        i0e(x)\n",
      "        \n",
      "        Exponentially scaled modified Bessel function of order 0.\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            i0e(x) = exp(-abs(x)) * i0(x).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            Value of the exponentially scaled modified Bessel function of order 0\n",
      "            at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 8] and (8, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval. The\n",
      "        polynomial expansions used are the same as those in `i0`, but\n",
      "        they are not multiplied by the dominant exponential factor.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `i0e`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        iv\n",
      "        i0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    i1 = <ufunc 'i1'>\n",
      "        i1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        i1(x)\n",
      "        \n",
      "        Modified Bessel function of order 1.\n",
      "        \n",
      "        Defined as,\n",
      "        \n",
      "        .. math::\n",
      "            I_1(x) = \\frac{1}{2}x \\sum_{k=0}^\\infty \\frac{(x^2/4)^k}{k! (k + 1)!}\n",
      "                   = -\\imath J_1(\\imath x),\n",
      "        \n",
      "        where :math:`J_1` is the Bessel function of the first kind of order 1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            Value of the modified Bessel function of order 1 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 8] and (8, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `i1`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        iv\n",
      "        i1e\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    i1e = <ufunc 'i1e'>\n",
      "        i1e(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        i1e(x)\n",
      "        \n",
      "        Exponentially scaled modified Bessel function of order 1.\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            i1e(x) = exp(-abs(x)) * i1(x)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            Value of the exponentially scaled modified Bessel function of order 1\n",
      "            at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 8] and (8, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval. The\n",
      "        polynomial expansions used are the same as those in `i1`, but\n",
      "        they are not multiplied by the dominant exponential factor.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `i1e`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        iv\n",
      "        i1\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    inv_boxcox = <ufunc 'inv_boxcox'>\n",
      "        inv_boxcox(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        inv_boxcox(y, lmbda)\n",
      "        \n",
      "        Compute the inverse of the Box-Cox transformation.\n",
      "        \n",
      "        Find ``x`` such that::\n",
      "        \n",
      "            y = (x**lmbda - 1) / lmbda  if lmbda != 0\n",
      "                log(x)                  if lmbda == 0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Data to be transformed.\n",
      "        lmbda : array_like\n",
      "            Power parameter of the Box-Cox transform.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : array\n",
      "            Transformed data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.16.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import boxcox, inv_boxcox\n",
      "        >>> y = boxcox([1, 4, 10], 2.5)\n",
      "        >>> inv_boxcox(y, 2.5)\n",
      "        array([1., 4., 10.])\n",
      "    \n",
      "    inv_boxcox1p = <ufunc 'inv_boxcox1p'>\n",
      "        inv_boxcox1p(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        inv_boxcox1p(y, lmbda)\n",
      "        \n",
      "        Compute the inverse of the Box-Cox transformation.\n",
      "        \n",
      "        Find ``x`` such that::\n",
      "        \n",
      "            y = ((1+x)**lmbda - 1) / lmbda  if lmbda != 0\n",
      "                log(1+x)                    if lmbda == 0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            Data to be transformed.\n",
      "        lmbda : array_like\n",
      "            Power parameter of the Box-Cox transform.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : array\n",
      "            Transformed data.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.16.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import boxcox1p, inv_boxcox1p\n",
      "        >>> y = boxcox1p([1, 4, 10], 2.5)\n",
      "        >>> inv_boxcox1p(y, 2.5)\n",
      "        array([1., 4., 10.])\n",
      "    \n",
      "    it2i0k0 = <ufunc 'it2i0k0'>\n",
      "        it2i0k0(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        it2i0k0(x, out=None)\n",
      "        \n",
      "        Integrals related to modified Bessel functions of order 0.\n",
      "        \n",
      "        Computes the integrals\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\int_0^x \\frac{I_0(t) - 1}{t} dt \\\\\n",
      "            \\int_x^\\infty \\frac{K_0(t)}{t} dt.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Values at which to evaluate the integrals.\n",
      "        out : tuple of ndarrays, optional\n",
      "            Optional output arrays for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ii0 : scalar or ndarray\n",
      "            The integral for `i0`\n",
      "        ik0 : scalar or ndarray\n",
      "            The integral for `k0`\n",
      "    \n",
      "    it2j0y0 = <ufunc 'it2j0y0'>\n",
      "        it2j0y0(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        it2j0y0(x, out=None)\n",
      "        \n",
      "        Integrals related to Bessel functions of the first kind of order 0.\n",
      "        \n",
      "        Computes the integrals\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\int_0^x \\frac{1 - J_0(t)}{t} dt \\\\\n",
      "            \\int_x^\\infty \\frac{Y_0(t)}{t} dt.\n",
      "        \n",
      "        For more on :math:`J_0` and :math:`Y_0` see `j0` and `y0`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Values at which to evaluate the integrals.\n",
      "        out : tuple of ndarrays, optional\n",
      "            Optional output arrays for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ij0 : scalar or ndarray\n",
      "            The integral for `j0`\n",
      "        iy0 : scalar or ndarray\n",
      "            The integral for `y0`\n",
      "    \n",
      "    it2struve0 = <ufunc 'it2struve0'>\n",
      "        it2struve0(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        it2struve0(x)\n",
      "        \n",
      "        Integral related to the Struve function of order 0.\n",
      "        \n",
      "        Returns the integral,\n",
      "        \n",
      "        .. math::\n",
      "            \\int_x^\\infty \\frac{H_0(t)}{t}\\,dt\n",
      "        \n",
      "        where :math:`H_0` is the Struve function of order 0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Lower limit of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            The value of the integral.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        struve\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for a Fortran routine created by Shanjie Zhang and Jianming\n",
      "        Jin [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    itairy = <ufunc 'itairy'>\n",
      "        itairy(x[, out1, out2, out3, out4], / [, out=(None, None, None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        itairy(x)\n",
      "        \n",
      "        Integrals of Airy functions\n",
      "        \n",
      "        Calculates the integrals of Airy functions from 0 to `x`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        \n",
      "        x: array_like\n",
      "            Upper limit of integration (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Apt\n",
      "            Integral of Ai(t) from 0 to x.\n",
      "        Bpt\n",
      "            Integral of Bi(t) from 0 to x.\n",
      "        Ant\n",
      "            Integral of Ai(-t) from 0 to x.\n",
      "        Bnt\n",
      "            Integral of Bi(-t) from 0 to x.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        Wrapper for a Fortran routine created by Shanjie Zhang and Jianming\n",
      "        Jin [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    iti0k0 = <ufunc 'iti0k0'>\n",
      "        iti0k0(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        iti0k0(x, out=None)\n",
      "        \n",
      "        Integrals of modified Bessel functions of order 0.\n",
      "        \n",
      "        Computes the integrals\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\int_0^x I_0(t) dt \\\\\n",
      "            \\int_0^x K_0(t) dt.\n",
      "        \n",
      "        For more on :math:`I_0` and :math:`K_0` see `i0` and `k0`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Values at which to evaluate the integrals.\n",
      "        out : tuple of ndarrays, optional\n",
      "            Optional output arrays for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ii0 : scalar or ndarray\n",
      "            The integral for `i0`\n",
      "        ik0 : scalar or ndarray\n",
      "            The integral for `k0`\n",
      "    \n",
      "    itj0y0 = <ufunc 'itj0y0'>\n",
      "        itj0y0(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        itj0y0(x, out=None)\n",
      "        \n",
      "        Integrals of Bessel functions of the first kind of order 0.\n",
      "        \n",
      "        Computes the integrals\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\int_0^x J_0(t) dt \\\\\n",
      "            \\int_0^x Y_0(t) dt.\n",
      "        \n",
      "        For more on :math:`J_0` and :math:`Y_0` see `j0` and `y0`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Values at which to evaluate the integrals.\n",
      "        out : tuple of ndarrays, optional\n",
      "            Optional output arrays for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ij0 : scalar or ndarray\n",
      "            The integral of `j0`\n",
      "        iy0 : scalar or ndarray\n",
      "            The integral of `y0`\n",
      "    \n",
      "    itmodstruve0 = <ufunc 'itmodstruve0'>\n",
      "        itmodstruve0(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        itmodstruve0(x)\n",
      "        \n",
      "        Integral of the modified Struve function of order 0.\n",
      "        \n",
      "        .. math::\n",
      "            I = \\int_0^x L_0(t)\\,dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Upper limit of integration (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            The integral of :math:`L_0` from 0 to `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for a Fortran routine created by Shanjie Zhang and Jianming\n",
      "        Jin [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    itstruve0 = <ufunc 'itstruve0'>\n",
      "        itstruve0(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        itstruve0(x)\n",
      "        \n",
      "        Integral of the Struve function of order 0.\n",
      "        \n",
      "        .. math::\n",
      "            I = \\int_0^x H_0(t)\\,dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Upper limit of integration (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        I : ndarray\n",
      "            The integral of :math:`H_0` from 0 to `x`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        struve\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for a Fortran routine created by Shanjie Zhang and Jianming\n",
      "        Jin [1]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    iv = <ufunc 'iv'>\n",
      "        iv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        iv(v, z)\n",
      "        \n",
      "        Modified Bessel function of the first kind of real order.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order. If `z` is of real type and negative, `v` must be integer\n",
      "            valued.\n",
      "        z : array_like of float or complex\n",
      "            Argument.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Values of the modified Bessel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For real `z` and :math:`v \\in [-50, 50]`, the evaluation is carried out\n",
      "        using Temme's method [1]_.  For larger orders, uniform asymptotic\n",
      "        expansions are applied.\n",
      "        \n",
      "        For complex `z` and positive `v`, the AMOS [2]_ `zbesi` routine is\n",
      "        called. It uses a power series for small `z`, the asymptotic expansion\n",
      "        for large `abs(z)`, the Miller algorithm normalized by the Wronskian\n",
      "        and a Neumann series for intermediate magnitudes, and the uniform\n",
      "        asymptotic expansions for :math:`I_v(z)` and :math:`J_v(z)` for large\n",
      "        orders. Backward recurrence is used to generate sequences or reduce\n",
      "        orders when necessary.\n",
      "        \n",
      "        The calculations above are done in the right half plane and continued\n",
      "        into the left half plane by the formula,\n",
      "        \n",
      "        .. math:: I_v(z \\exp(\\pm\\imath\\pi)) = \\exp(\\pm\\pi v) I_v(z)\n",
      "        \n",
      "        (valid when the real part of `z` is positive).  For negative `v`, the\n",
      "        formula\n",
      "        \n",
      "        .. math:: I_{-v}(z) = I_v(z) + \\frac{2}{\\pi} \\sin(\\pi v) K_v(z)\n",
      "        \n",
      "        is used, where :math:`K_v(z)` is the modified Bessel function of the\n",
      "        second kind, evaluated using the AMOS routine `zbesk`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        kve : This function with leading exponential behavior stripped off.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Temme, Journal of Computational Physics, vol 21, 343 (1976)\n",
      "        .. [2] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    ive = <ufunc 'ive'>\n",
      "        ive(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ive(v, z)\n",
      "        \n",
      "        Exponentially scaled modified Bessel function of the first kind\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            ive(v, z) = iv(v, z) * exp(-abs(z.real))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like of float\n",
      "            Order.\n",
      "        z : array_like of float or complex\n",
      "            Argument.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            Values of the exponentially scaled modified Bessel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For positive `v`, the AMOS [1]_ `zbesi` routine is called. It uses a\n",
      "        power series for small `z`, the asymptotic expansion for large\n",
      "        `abs(z)`, the Miller algorithm normalized by the Wronskian and a\n",
      "        Neumann series for intermediate magnitudes, and the uniform asymptotic\n",
      "        expansions for :math:`I_v(z)` and :math:`J_v(z)` for large orders.\n",
      "        Backward recurrence is used to generate sequences or reduce orders when\n",
      "        necessary.\n",
      "        \n",
      "        The calculations above are done in the right half plane and continued\n",
      "        into the left half plane by the formula,\n",
      "        \n",
      "        .. math:: I_v(z \\exp(\\pm\\imath\\pi)) = \\exp(\\pm\\pi v) I_v(z)\n",
      "        \n",
      "        (valid when the real part of `z` is positive).  For negative `v`, the\n",
      "        formula\n",
      "        \n",
      "        .. math:: I_{-v}(z) = I_v(z) + \\frac{2}{\\pi} \\sin(\\pi v) K_v(z)\n",
      "        \n",
      "        is used, where :math:`K_v(z)` is the modified Bessel function of the\n",
      "        second kind, evaluated using the AMOS routine `zbesk`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    j0 = <ufunc 'j0'>\n",
      "        j0(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        j0(x)\n",
      "        \n",
      "        Bessel function of the first kind of order 0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        J : ndarray\n",
      "            Value of the Bessel function of the first kind of order 0 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The domain is divided into the intervals [0, 5] and (5, infinity). In the\n",
      "        first interval the following rational approximation is used:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            J_0(x) \\approx (w - r_1^2)(w - r_2^2) \\frac{P_3(w)}{Q_8(w)},\n",
      "        \n",
      "        where :math:`w = x^2` and :math:`r_1`, :math:`r_2` are the zeros of\n",
      "        :math:`J_0`, and :math:`P_3` and :math:`Q_8` are polynomials of degrees 3\n",
      "        and 8, respectively.\n",
      "        \n",
      "        In the second interval, the Hankel asymptotic expansion is employed with\n",
      "        two rational functions of degree 6/6 and 7/7.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `j0`.\n",
      "        It should not be confused with the spherical Bessel functions (see\n",
      "        `spherical_jn`).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jv : Bessel function of real order and complex argument.\n",
      "        spherical_jn : spherical Bessel functions.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    j1 = <ufunc 'j1'>\n",
      "        j1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        j1(x)\n",
      "        \n",
      "        Bessel function of the first kind of order 1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        J : ndarray\n",
      "            Value of the Bessel function of the first kind of order 1 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The domain is divided into the intervals [0, 8] and (8, infinity). In the\n",
      "        first interval a 24 term Chebyshev expansion is used. In the second, the\n",
      "        asymptotic trigonometric representation is employed using two rational\n",
      "        functions of degree 5/5.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `j1`.\n",
      "        It should not be confused with the spherical Bessel functions (see\n",
      "        `spherical_jn`).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jv\n",
      "        spherical_jn : spherical Bessel functions.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    jn = <ufunc 'jv'>\n",
      "        jv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        jv(v, z)\n",
      "        \n",
      "        Bessel function of the first kind of real order and complex argument.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        J : ndarray\n",
      "            Value of the Bessel function, :math:`J_v(z)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For positive `v` values, the computation is carried out using the AMOS\n",
      "        [1]_ `zbesj` routine, which exploits the connection to the modified\n",
      "        Bessel function :math:`I_v`,\n",
      "        \n",
      "        .. math::\n",
      "            J_v(z) = \\exp(v\\pi\\imath/2) I_v(-\\imath z)\\qquad (\\Im z > 0)\n",
      "        \n",
      "            J_v(z) = \\exp(-v\\pi\\imath/2) I_v(\\imath z)\\qquad (\\Im z < 0)\n",
      "        \n",
      "        For negative `v` values the formula,\n",
      "        \n",
      "        .. math:: J_{-v}(z) = J_v(z) \\cos(\\pi v) - Y_v(z) \\sin(\\pi v)\n",
      "        \n",
      "        is used, where :math:`Y_v(z)` is the Bessel function of the second\n",
      "        kind, computed using the AMOS routine `zbesy`.  Note that the second\n",
      "        term is exactly zero for integer `v`; to improve accuracy the second\n",
      "        term is explicitly omitted for `v` values such that `v = floor(v)`.\n",
      "        \n",
      "        Not to be confused with the spherical Bessel functions (see `spherical_jn`).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jve : :math:`J_v` with leading exponential behavior stripped off.\n",
      "        spherical_jn : spherical Bessel functions.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    jv = <ufunc 'jv'>\n",
      "        jv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        jv(v, z)\n",
      "        \n",
      "        Bessel function of the first kind of real order and complex argument.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        J : ndarray\n",
      "            Value of the Bessel function, :math:`J_v(z)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For positive `v` values, the computation is carried out using the AMOS\n",
      "        [1]_ `zbesj` routine, which exploits the connection to the modified\n",
      "        Bessel function :math:`I_v`,\n",
      "        \n",
      "        .. math::\n",
      "            J_v(z) = \\exp(v\\pi\\imath/2) I_v(-\\imath z)\\qquad (\\Im z > 0)\n",
      "        \n",
      "            J_v(z) = \\exp(-v\\pi\\imath/2) I_v(\\imath z)\\qquad (\\Im z < 0)\n",
      "        \n",
      "        For negative `v` values the formula,\n",
      "        \n",
      "        .. math:: J_{-v}(z) = J_v(z) \\cos(\\pi v) - Y_v(z) \\sin(\\pi v)\n",
      "        \n",
      "        is used, where :math:`Y_v(z)` is the Bessel function of the second\n",
      "        kind, computed using the AMOS routine `zbesy`.  Note that the second\n",
      "        term is exactly zero for integer `v`; to improve accuracy the second\n",
      "        term is explicitly omitted for `v` values such that `v = floor(v)`.\n",
      "        \n",
      "        Not to be confused with the spherical Bessel functions (see `spherical_jn`).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        jve : :math:`J_v` with leading exponential behavior stripped off.\n",
      "        spherical_jn : spherical Bessel functions.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    jve = <ufunc 'jve'>\n",
      "        jve(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        jve(v, z)\n",
      "        \n",
      "        Exponentially scaled Bessel function of order `v`.\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            jve(v, z) = jv(v, z) * exp(-abs(z.imag))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        J : ndarray\n",
      "            Value of the exponentially scaled Bessel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For positive `v` values, the computation is carried out using the AMOS\n",
      "        [1]_ `zbesj` routine, which exploits the connection to the modified\n",
      "        Bessel function :math:`I_v`,\n",
      "        \n",
      "        .. math::\n",
      "            J_v(z) = \\exp(v\\pi\\imath/2) I_v(-\\imath z)\\qquad (\\Im z > 0)\n",
      "        \n",
      "            J_v(z) = \\exp(-v\\pi\\imath/2) I_v(\\imath z)\\qquad (\\Im z < 0)\n",
      "        \n",
      "        For negative `v` values the formula,\n",
      "        \n",
      "        .. math:: J_{-v}(z) = J_v(z) \\cos(\\pi v) - Y_v(z) \\sin(\\pi v)\n",
      "        \n",
      "        is used, where :math:`Y_v(z)` is the Bessel function of the second\n",
      "        kind, computed using the AMOS routine `zbesy`.  Note that the second\n",
      "        term is exactly zero for integer `v`; to improve accuracy the second\n",
      "        term is explicitly omitted for `v` values such that `v = floor(v)`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    k0 = <ufunc 'k0'>\n",
      "        k0(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        k0(x)\n",
      "        \n",
      "        Modified Bessel function of the second kind of order 0, :math:`K_0`.\n",
      "        \n",
      "        This function is also sometimes referred to as the modified Bessel\n",
      "        function of the third kind of order 0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray\n",
      "            Value of the modified Bessel function :math:`K_0` at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 2] and (2, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `k0`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        kv\n",
      "        k0e\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    k0e = <ufunc 'k0e'>\n",
      "        k0e(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        k0e(x)\n",
      "        \n",
      "        Exponentially scaled modified Bessel function K of order 0\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            k0e(x) = exp(x) * k0(x).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray\n",
      "            Value of the exponentially scaled modified Bessel function K of order\n",
      "            0 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 2] and (2, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `k0e`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        kv\n",
      "        k0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    k1 = <ufunc 'k1'>\n",
      "        k1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        k1(x)\n",
      "        \n",
      "        Modified Bessel function of the second kind of order 1, :math:`K_1(x)`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray\n",
      "            Value of the modified Bessel function K of order 1 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 2] and (2, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `k1`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        kv\n",
      "        k1e\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    k1e = <ufunc 'k1e'>\n",
      "        k1e(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        k1e(x)\n",
      "        \n",
      "        Exponentially scaled modified Bessel function K of order 1\n",
      "        \n",
      "        Defined as::\n",
      "        \n",
      "            k1e(x) = exp(x) * k1(x)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray\n",
      "            Value of the exponentially scaled modified Bessel function K of order\n",
      "            1 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The range is partitioned into the two intervals [0, 2] and (2, infinity).\n",
      "        Chebyshev polynomial expansions are employed in each interval.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `k1e`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        kv\n",
      "        k1\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    kei = <ufunc 'kei'>\n",
      "        kei(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kei(x, out=None)\n",
      "        \n",
      "        Kelvin function kei.\n",
      "        \n",
      "        Defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\mathrm{kei}(x) = \\Im[K_0(x e^{\\pi i / 4})]\n",
      "        \n",
      "        where :math:`K_0` is the modified Bessel function of the second\n",
      "        kind (see `kv`). See [dlmf]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the Kelvin function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ker : the corresponding real part\n",
      "        keip : the derivative of kei\n",
      "        kv : modified Bessel function of the second kind\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10.61\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        It can be expressed using the modified Bessel function of the\n",
      "        second kind.\n",
      "        \n",
      "        >>> import scipy.special as sc\n",
      "        >>> x = np.array([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> sc.kv(0, x * np.exp(np.pi * 1j / 4)).imag\n",
      "        array([-0.49499464, -0.20240007, -0.05112188,  0.0021984 ])\n",
      "        >>> sc.kei(x)\n",
      "        array([-0.49499464, -0.20240007, -0.05112188,  0.0021984 ])\n",
      "    \n",
      "    keip = <ufunc 'keip'>\n",
      "        keip(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        keip(x, out=None)\n",
      "        \n",
      "        Derivative of the Kelvin function kei.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            The values of the derivative of kei.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kei\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10#PT5\n",
      "    \n",
      "    kelvin = <ufunc 'kelvin'>\n",
      "        kelvin(x[, out1, out2, out3, out4], / [, out=(None, None, None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kelvin(x)\n",
      "        \n",
      "        Kelvin functions as complex numbers\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Be, Ke, Bep, Kep\n",
      "            The tuple (Be, Ke, Bep, Kep) contains complex numbers\n",
      "            representing the real and imaginary Kelvin functions and their\n",
      "            derivatives evaluated at `x`.  For example, kelvin(x)[0].real =\n",
      "            ber x and kelvin(x)[0].imag = bei x with similar relationships\n",
      "            for ker and kei.\n",
      "    \n",
      "    ker = <ufunc 'ker'>\n",
      "        ker(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ker(x, out=None)\n",
      "        \n",
      "        Kelvin function ker.\n",
      "        \n",
      "        Defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\mathrm{ker}(x) = \\Re[K_0(x e^{\\pi i / 4})]\n",
      "        \n",
      "        Where :math:`K_0` is the modified Bessel function of the second\n",
      "        kind (see `kv`). See [dlmf]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kei : the corresponding imaginary part\n",
      "        kerp : the derivative of ker\n",
      "        kv : modified Bessel function of the second kind\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the Kelvin function.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10.61\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        It can be expressed using the modified Bessel function of the\n",
      "        second kind.\n",
      "        \n",
      "        >>> import scipy.special as sc\n",
      "        >>> x = np.array([1.0, 2.0, 3.0, 4.0])\n",
      "        >>> sc.kv(0, x * np.exp(np.pi * 1j / 4)).real\n",
      "        array([ 0.28670621, -0.04166451, -0.06702923, -0.03617885])\n",
      "        >>> sc.ker(x)\n",
      "        array([ 0.28670621, -0.04166451, -0.06702923, -0.03617885])\n",
      "    \n",
      "    kerp = <ufunc 'kerp'>\n",
      "        kerp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kerp(x, out=None)\n",
      "        \n",
      "        Derivative of the Kelvin function ker.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the derivative of ker.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ker\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST, Digital Library of Mathematical Functions,\n",
      "            https://dlmf.nist.gov/10#PT5\n",
      "    \n",
      "    kl_div = <ufunc 'kl_div'>\n",
      "        kl_div(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kl_div(x, y, out=None)\n",
      "        \n",
      "        Elementwise function for computing Kullback-Leibler divergence.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\mathrm{kl\\_div}(x, y) =\n",
      "              \\begin{cases}\n",
      "                x \\log(x / y) - x + y & x > 0, y > 0 \\\\\n",
      "                y & x = 0, y \\ge 0 \\\\\n",
      "                \\infty & \\text{otherwise}\n",
      "              \\end{cases}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Real arguments\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the Kullback-Liebler divergence.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        entr, rel_entr\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        This function is non-negative and is jointly convex in `x` and `y`.\n",
      "        \n",
      "        The origin of this function is in convex programming; see [1]_ for\n",
      "        details. This is why the the function contains the extra :math:`-x\n",
      "        + y` terms over what might be expected from the Kullback-Leibler\n",
      "        divergence. For a version of the function without the extra terms,\n",
      "        see `rel_entr`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grant, Boyd, and Ye, \"CVX: Matlab Software for Disciplined Convex\n",
      "            Programming\", http://cvxr.com/cvx/\n",
      "    \n",
      "    kn = <ufunc 'kn'>\n",
      "        kn(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kn(n, x)\n",
      "        \n",
      "        Modified Bessel function of the second kind of integer order `n`\n",
      "        \n",
      "        Returns the modified Bessel function of the second kind for integer order\n",
      "        `n` at real `z`.\n",
      "        \n",
      "        These are also sometimes called functions of the third kind, Basset\n",
      "        functions, or Macdonald functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like of int\n",
      "            Order of Bessel functions (floats will truncate with a warning)\n",
      "        z : array_like of float\n",
      "            Argument at which to evaluate the Bessel functions\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            The results\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for AMOS [1]_ routine `zbesk`.  For a discussion of the\n",
      "        algorithm used, see [2]_ and the references therein.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kv : Same function, but accepts real order and complex argument\n",
      "        kvp : Derivative of this function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "        .. [2] Donald E. Amos, \"Algorithm 644: A portable package for Bessel\n",
      "               functions of a complex argument and nonnegative order\", ACM\n",
      "               TOMS Vol. 12 Issue 3, Sept. 1986, p. 265\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Plot the function of several orders for real input:\n",
      "        \n",
      "        >>> from scipy.special import kn\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(0, 5, 1000)\n",
      "        >>> for N in range(6):\n",
      "        ...     plt.plot(x, kn(N, x), label='$K_{}(x)$'.format(N))\n",
      "        >>> plt.ylim(0, 10)\n",
      "        >>> plt.legend()\n",
      "        >>> plt.title(r'Modified Bessel function of the second kind $K_n(x)$')\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Calculate for a single value at multiple orders:\n",
      "        \n",
      "        >>> kn([4, 5, 6], 1)\n",
      "        array([   44.23241585,   360.9605896 ,  3653.83831186])\n",
      "    \n",
      "    kolmogi = <ufunc 'kolmogi'>\n",
      "        kolmogi(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kolmogi(p)\n",
      "        \n",
      "        Inverse Survival Function of Kolmogorov distribution\n",
      "        \n",
      "        It is the inverse function to `kolmogorov`.\n",
      "        Returns y such that ``kolmogorov(y) == p``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : float array_like\n",
      "            Probability\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            The value(s) of kolmogi(p)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `kolmogorov` is used by `stats.kstest` in the application of the\n",
      "        Kolmogorov-Smirnov Goodness of Fit test. For historial reasons this\n",
      "        function is exposed in `scpy.special`, but the recommended way to achieve\n",
      "        the most accurate CDF/SF/PDF/PPF/ISF computations is to use the\n",
      "        `stats.kstwobign` distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kolmogorov : The Survival Function for the distribution\n",
      "        scipy.stats.kstwobign : Provides the functionality as a continuous distribution\n",
      "        smirnov, smirnovi : Functions for the one-sided distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import kolmogi\n",
      "        >>> kolmogi([0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0])\n",
      "        array([        inf,  1.22384787,  1.01918472,  0.82757356,  0.67644769,\n",
      "                0.57117327,  0.        ])\n",
      "    \n",
      "    kolmogorov = <ufunc 'kolmogorov'>\n",
      "        kolmogorov(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kolmogorov(y)\n",
      "        \n",
      "        Complementary cumulative distribution (Survival Function) function of\n",
      "        Kolmogorov distribution.\n",
      "        \n",
      "        Returns the complementary cumulative distribution function of\n",
      "        Kolmogorov's limiting distribution (``D_n*\\sqrt(n)`` as n goes to infinity)\n",
      "        of a two-sided test for equality between an empirical and a theoretical\n",
      "        distribution. It is equal to the (limit as n->infinity of the)\n",
      "        probability that ``sqrt(n) * max absolute deviation > y``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : float array_like\n",
      "          Absolute deviation between the Empirical CDF (ECDF) and the target CDF,\n",
      "          multiplied by sqrt(n).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            The value(s) of kolmogorov(y)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `kolmogorov` is used by `stats.kstest` in the application of the\n",
      "        Kolmogorov-Smirnov Goodness of Fit test. For historial reasons this\n",
      "        function is exposed in `scpy.special`, but the recommended way to achieve\n",
      "        the most accurate CDF/SF/PDF/PPF/ISF computations is to use the\n",
      "        `stats.kstwobign` distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kolmogi : The Inverse Survival Function for the distribution\n",
      "        scipy.stats.kstwobign : Provides the functionality as a continuous distribution\n",
      "        smirnov, smirnovi : Functions for the one-sided distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Show the probability of a gap at least as big as 0, 0.5 and 1.0.\n",
      "        \n",
      "        >>> from scipy.special import kolmogorov\n",
      "        >>> from scipy.stats import kstwobign\n",
      "        >>> kolmogorov([0, 0.5, 1.0])\n",
      "        array([ 1.        ,  0.96394524,  0.26999967])\n",
      "        \n",
      "        Compare a sample of size 1000 drawn from a Laplace(0, 1) distribution against\n",
      "        the target distribution, a Normal(0, 1) distribution.\n",
      "        \n",
      "        >>> from scipy.stats import norm, laplace\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> n = 1000\n",
      "        >>> lap01 = laplace(0, 1)\n",
      "        >>> x = np.sort(lap01.rvs(n, random_state=rng))\n",
      "        >>> np.mean(x), np.std(x)\n",
      "        (-0.05841730131499543, 1.3968109101997568)\n",
      "        \n",
      "        Construct the Empirical CDF and the K-S statistic Dn.\n",
      "        \n",
      "        >>> target = norm(0,1)  # Normal mean 0, stddev 1\n",
      "        >>> cdfs = target.cdf(x)\n",
      "        >>> ecdfs = np.arange(n+1, dtype=float)/n\n",
      "        >>> gaps = np.column_stack([cdfs - ecdfs[:n], ecdfs[1:] - cdfs])\n",
      "        >>> Dn = np.max(gaps)\n",
      "        >>> Kn = np.sqrt(n) * Dn\n",
      "        >>> print('Dn=%f, sqrt(n)*Dn=%f' % (Dn, Kn))\n",
      "        Dn=0.043363, sqrt(n)*Dn=1.371265\n",
      "        >>> print(chr(10).join(['For a sample of size n drawn from a N(0, 1) distribution:',\n",
      "        ...   ' the approximate Kolmogorov probability that sqrt(n)*Dn>=%f is %f' %  (Kn, kolmogorov(Kn)),\n",
      "        ...   ' the approximate Kolmogorov probability that sqrt(n)*Dn<=%f is %f' %  (Kn, kstwobign.cdf(Kn))]))\n",
      "        For a sample of size n drawn from a N(0, 1) distribution:\n",
      "         the approximate Kolmogorov probability that sqrt(n)*Dn>=1.371265 is 0.046533\n",
      "         the approximate Kolmogorov probability that sqrt(n)*Dn<=1.371265 is 0.953467\n",
      "        \n",
      "        Plot the Empirical CDF against the target N(0, 1) CDF.\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.step(np.concatenate([[-3], x]), ecdfs, where='post', label='Empirical CDF')\n",
      "        >>> x3 = np.linspace(-3, 3, 100)\n",
      "        >>> plt.plot(x3, target.cdf(x3), label='CDF for N(0, 1)')\n",
      "        >>> plt.ylim([0, 1]); plt.grid(True); plt.legend();\n",
      "        >>> # Add vertical lines marking Dn+ and Dn-\n",
      "        >>> iminus, iplus = np.argmax(gaps, axis=0)\n",
      "        >>> plt.vlines([x[iminus]], ecdfs[iminus], cdfs[iminus], color='r', linestyle='dashed', lw=4)\n",
      "        >>> plt.vlines([x[iplus]], cdfs[iplus], ecdfs[iplus+1], color='r', linestyle='dashed', lw=4)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    kv = <ufunc 'kv'>\n",
      "        kv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kv(v, z)\n",
      "        \n",
      "        Modified Bessel function of the second kind of real order `v`\n",
      "        \n",
      "        Returns the modified Bessel function of the second kind for real order\n",
      "        `v` at complex `z`.\n",
      "        \n",
      "        These are also sometimes called functions of the third kind, Basset\n",
      "        functions, or Macdonald functions.  They are defined as those solutions\n",
      "        of the modified Bessel equation for which,\n",
      "        \n",
      "        .. math::\n",
      "            K_v(x) \\sim \\sqrt{\\pi/(2x)} \\exp(-x)\n",
      "        \n",
      "        as :math:`x \\to \\infty` [3]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like of float\n",
      "            Order of Bessel functions\n",
      "        z : array_like of complex\n",
      "            Argument at which to evaluate the Bessel functions\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            The results. Note that input must be of complex type to get complex\n",
      "            output, e.g. ``kv(3, -2+0j)`` instead of ``kv(3, -2)``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for AMOS [1]_ routine `zbesk`.  For a discussion of the\n",
      "        algorithm used, see [2]_ and the references therein.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        kve : This function with leading exponential behavior stripped off.\n",
      "        kvp : Derivative of this function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "        .. [2] Donald E. Amos, \"Algorithm 644: A portable package for Bessel\n",
      "               functions of a complex argument and nonnegative order\", ACM\n",
      "               TOMS Vol. 12 Issue 3, Sept. 1986, p. 265\n",
      "        .. [3] NIST Digital Library of Mathematical Functions,\n",
      "               Eq. 10.25.E3. https://dlmf.nist.gov/10.25.E3\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Plot the function of several orders for real input:\n",
      "        \n",
      "        >>> from scipy.special import kv\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(0, 5, 1000)\n",
      "        >>> for N in np.linspace(0, 6, 5):\n",
      "        ...     plt.plot(x, kv(N, x), label='$K_{{{}}}(x)$'.format(N))\n",
      "        >>> plt.ylim(0, 10)\n",
      "        >>> plt.legend()\n",
      "        >>> plt.title(r'Modified Bessel function of the second kind $K_\\nu(x)$')\n",
      "        >>> plt.show()\n",
      "        \n",
      "        Calculate for a single value at multiple orders:\n",
      "        \n",
      "        >>> kv([4, 4.5, 5], 1+2j)\n",
      "        array([ 0.1992+2.3892j,  2.3493+3.6j   ,  7.2827+3.8104j])\n",
      "    \n",
      "    kve = <ufunc 'kve'>\n",
      "        kve(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        kve(v, z)\n",
      "        \n",
      "        Exponentially scaled modified Bessel function of the second kind.\n",
      "        \n",
      "        Returns the exponentially scaled, modified Bessel function of the\n",
      "        second kind (sometimes called the third kind) for real order `v` at\n",
      "        complex `z`::\n",
      "        \n",
      "            kve(v, z) = kv(v, z) * exp(z)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like of float\n",
      "            Order of Bessel functions\n",
      "        z : array_like of complex\n",
      "            Argument at which to evaluate the Bessel functions\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            The exponentially scaled modified Bessel function of the second kind.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for AMOS [1]_ routine `zbesk`.  For a discussion of the\n",
      "        algorithm used, see [2]_ and the references therein.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "        .. [2] Donald E. Amos, \"Algorithm 644: A portable package for Bessel\n",
      "               functions of a complex argument and nonnegative order\", ACM\n",
      "               TOMS Vol. 12 Issue 3, Sept. 1986, p. 265\n",
      "    \n",
      "    log1p = <ufunc 'log1p'>\n",
      "        log1p(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        log1p(x, out=None)\n",
      "        \n",
      "        Calculates log(1 + x) for use when `x` is near zero.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real or complex valued input.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of ``log(1 + x)``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        expm1, cosm1\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is more accurate than using ``log(1 + x)`` directly for ``x``\n",
      "        near 0. Note that in the below example ``1 + 1e-17 == 1`` to\n",
      "        double precision.\n",
      "        \n",
      "        >>> sc.log1p(1e-17)\n",
      "        1e-17\n",
      "        >>> np.log(1 + 1e-17)\n",
      "        0.0\n",
      "    \n",
      "    log_ndtr = <ufunc 'log_ndtr'>\n",
      "        log_ndtr(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        log_ndtr(x)\n",
      "        \n",
      "        Logarithm of Gaussian cumulative distribution function.\n",
      "        \n",
      "        Returns the log of the area under the standard Gaussian probability\n",
      "        density function, integrated from minus infinity to `x`::\n",
      "        \n",
      "            log(1/sqrt(2*pi) * integral(exp(-t**2 / 2), t=-inf..x))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like, real or complex\n",
      "            Argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            The value of the log of the normal CDF evaluated at `x`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        erf\n",
      "        erfc\n",
      "        scipy.stats.norm\n",
      "        ndtr\n",
      "    \n",
      "    loggamma = <ufunc 'loggamma'>\n",
      "        loggamma(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        loggamma(z, out=None)\n",
      "        \n",
      "        Principal branch of the logarithm of the gamma function.\n",
      "        \n",
      "        Defined to be :math:`\\log(\\Gamma(x))` for :math:`x > 0` and\n",
      "        extended to the complex plane by analytic continuation. The\n",
      "        function has a single branch cut on the negative real axis.\n",
      "        \n",
      "        .. versionadded:: 0.18.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array-like\n",
      "            Values in the complex plain at which to compute ``loggamma``\n",
      "        out : ndarray, optional\n",
      "            Output array for computed values of ``loggamma``\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loggamma : ndarray\n",
      "            Values of ``loggamma`` at z.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        It is not generally true that :math:`\\log\\Gamma(z) =\n",
      "        \\log(\\Gamma(z))`, though the real parts of the functions do\n",
      "        agree. The benefit of not defining `loggamma` as\n",
      "        :math:`\\log(\\Gamma(z))` is that the latter function has a\n",
      "        complicated branch cut structure whereas `loggamma` is analytic\n",
      "        except for on the negative real axis.\n",
      "        \n",
      "        The identities\n",
      "        \n",
      "        .. math::\n",
      "          \\exp(\\log\\Gamma(z)) &= \\Gamma(z) \\\\\n",
      "          \\log\\Gamma(z + 1) &= \\log(z) + \\log\\Gamma(z)\n",
      "        \n",
      "        make `loggamma` useful for working in complex logspace.\n",
      "        \n",
      "        On the real line `loggamma` is related to `gammaln` via\n",
      "        ``exp(loggamma(x + 0j)) = gammasgn(x)*exp(gammaln(x))``, up to\n",
      "        rounding error.\n",
      "        \n",
      "        The implementation here is based on [hare1997]_.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        gammaln : logarithm of the absolute value of the gamma function\n",
      "        gammasgn : sign of the gamma function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [hare1997] D.E.G. Hare,\n",
      "          *Computing the Principal Branch of log-Gamma*,\n",
      "          Journal of Algorithms, Volume 25, Issue 2, November 1997, pages 221-236.\n",
      "    \n",
      "    logit = <ufunc 'logit'>\n",
      "        logit(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        logit(x)\n",
      "        \n",
      "        Logit ufunc for ndarrays.\n",
      "        \n",
      "        The logit function is defined as logit(p) = log(p/(1-p)).\n",
      "        Note that logit(0) = -inf, logit(1) = inf, and logit(p)\n",
      "        for p<0 or p>1 yields nan.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray\n",
      "            The ndarray to apply logit to element-wise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray\n",
      "            An ndarray of the same shape as x. Its entries\n",
      "            are logit of the corresponding entry of x.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        expit\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        As a ufunc logit takes a number of optional\n",
      "        keyword arguments. For more information\n",
      "        see `ufuncs <https://docs.scipy.org/doc/numpy/reference/ufuncs.html>`_\n",
      "        \n",
      "        .. versionadded:: 0.10.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import logit, expit\n",
      "        \n",
      "        >>> logit([0, 0.25, 0.5, 0.75, 1])\n",
      "        array([       -inf, -1.09861229,  0.        ,  1.09861229,         inf])\n",
      "        \n",
      "        `expit` is the inverse of `logit`:\n",
      "        \n",
      "        >>> expit(logit([0.1, 0.75, 0.999]))\n",
      "        array([ 0.1  ,  0.75 ,  0.999])\n",
      "        \n",
      "        Plot logit(x) for x in [0, 1]:\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> x = np.linspace(0, 1, 501)\n",
      "        >>> y = logit(x)\n",
      "        >>> plt.plot(x, y)\n",
      "        >>> plt.grid()\n",
      "        >>> plt.ylim(-6, 6)\n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.title('logit(x)')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    lpmv = <ufunc 'lpmv'>\n",
      "        lpmv(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        lpmv(m, v, x)\n",
      "        \n",
      "        Associated Legendre function of integer order and real degree.\n",
      "        \n",
      "        Defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P_v^m = (-1)^m (1 - x^2)^{m/2} \\frac{d^m}{dx^m} P_v(x)\n",
      "        \n",
      "        where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            P_v = \\sum_{k = 0}^\\infty \\frac{(-v)_k (v + 1)_k}{(k!)^2}\n",
      "                    \\left(\\frac{1 - x}{2}\\right)^k\n",
      "        \n",
      "        is the Legendre function of the first kind. Here :math:`(\\cdot)_k`\n",
      "        is the Pochhammer symbol; see `poch`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : array_like\n",
      "            Order (int or float). If passed a float not equal to an\n",
      "            integer the function returns NaN.\n",
      "        v : array_like\n",
      "            Degree (float).\n",
      "        x : array_like\n",
      "            Argument (float). Must have ``|x| <= 1``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        pmv : ndarray\n",
      "            Value of the associated Legendre function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lpmn : Compute the associated Legendre function for all orders\n",
      "               ``0, ..., m`` and degrees ``0, ..., n``.\n",
      "        clpmn : Compute the associated Legendre function at complex\n",
      "                arguments.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Note that this implementation includes the Condon-Shortley phase.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Zhang, Jin, \"Computation of Special Functions\", John Wiley\n",
      "               and Sons, Inc, 1996.\n",
      "    \n",
      "    mathieu_a = <ufunc 'mathieu_a'>\n",
      "        mathieu_a(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_a(m, q)\n",
      "        \n",
      "        Characteristic value of even Mathieu functions\n",
      "        \n",
      "        Returns the characteristic value for the even solution,\n",
      "        ``ce_m(z, q)``, of Mathieu's equation.\n",
      "    \n",
      "    mathieu_b = <ufunc 'mathieu_b'>\n",
      "        mathieu_b(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_b(m, q)\n",
      "        \n",
      "        Characteristic value of odd Mathieu functions\n",
      "        \n",
      "        Returns the characteristic value for the odd solution,\n",
      "        ``se_m(z, q)``, of Mathieu's equation.\n",
      "    \n",
      "    mathieu_cem = <ufunc 'mathieu_cem'>\n",
      "        mathieu_cem(x1, x2, x3[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_cem(m, q, x)\n",
      "        \n",
      "        Even Mathieu function and its derivative\n",
      "        \n",
      "        Returns the even Mathieu function, ``ce_m(x, q)``, of order `m` and\n",
      "        parameter `q` evaluated at `x` (given in degrees).  Also returns the\n",
      "        derivative with respect to `x` of ce_m(x, q)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m\n",
      "            Order of the function\n",
      "        q\n",
      "            Parameter of the function\n",
      "        x\n",
      "            Argument of the function, *given in degrees, not radians*\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y\n",
      "            Value of the function\n",
      "        yp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    mathieu_modcem1 = <ufunc 'mathieu_modcem1'>\n",
      "        mathieu_modcem1(x1, x2, x3[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_modcem1(m, q, x)\n",
      "        \n",
      "        Even modified Mathieu function of the first kind and its derivative\n",
      "        \n",
      "        Evaluates the even modified Mathieu function of the first kind,\n",
      "        ``Mc1m(x, q)``, and its derivative at `x` for order `m` and parameter\n",
      "        `q`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y\n",
      "            Value of the function\n",
      "        yp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    mathieu_modcem2 = <ufunc 'mathieu_modcem2'>\n",
      "        mathieu_modcem2(x1, x2, x3[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_modcem2(m, q, x)\n",
      "        \n",
      "        Even modified Mathieu function of the second kind and its derivative\n",
      "        \n",
      "        Evaluates the even modified Mathieu function of the second kind,\n",
      "        Mc2m(x, q), and its derivative at `x` (given in degrees) for order `m`\n",
      "        and parameter `q`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y\n",
      "            Value of the function\n",
      "        yp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    mathieu_modsem1 = <ufunc 'mathieu_modsem1'>\n",
      "        mathieu_modsem1(x1, x2, x3[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_modsem1(m, q, x)\n",
      "        \n",
      "        Odd modified Mathieu function of the first kind and its derivative\n",
      "        \n",
      "        Evaluates the odd modified Mathieu function of the first kind,\n",
      "        Ms1m(x, q), and its derivative at `x` (given in degrees) for order `m`\n",
      "        and parameter `q`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y\n",
      "            Value of the function\n",
      "        yp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    mathieu_modsem2 = <ufunc 'mathieu_modsem2'>\n",
      "        mathieu_modsem2(x1, x2, x3[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_modsem2(m, q, x)\n",
      "        \n",
      "        Odd modified Mathieu function of the second kind and its derivative\n",
      "        \n",
      "        Evaluates the odd modified Mathieu function of the second kind,\n",
      "        Ms2m(x, q), and its derivative at `x` (given in degrees) for order `m`\n",
      "        and parameter q.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y\n",
      "            Value of the function\n",
      "        yp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    mathieu_sem = <ufunc 'mathieu_sem'>\n",
      "        mathieu_sem(x1, x2, x3[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        mathieu_sem(m, q, x)\n",
      "        \n",
      "        Odd Mathieu function and its derivative\n",
      "        \n",
      "        Returns the odd Mathieu function, se_m(x, q), of order `m` and\n",
      "        parameter `q` evaluated at `x` (given in degrees).  Also returns the\n",
      "        derivative with respect to `x` of se_m(x, q).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m\n",
      "            Order of the function\n",
      "        q\n",
      "            Parameter of the function\n",
      "        x\n",
      "            Argument of the function, *given in degrees, not radians*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y\n",
      "            Value of the function\n",
      "        yp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    modfresnelm = <ufunc 'modfresnelm'>\n",
      "        modfresnelm(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        modfresnelm(x)\n",
      "        \n",
      "        Modified Fresnel negative integrals\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fm\n",
      "            Integral ``F_-(x)``: ``integral(exp(-1j*t*t), t=x..inf)``\n",
      "        km\n",
      "            Integral ``K_-(x)``: ``1/sqrt(pi)*exp(1j*(x*x+pi/4))*fp``\n",
      "    \n",
      "    modfresnelp = <ufunc 'modfresnelp'>\n",
      "        modfresnelp(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        modfresnelp(x)\n",
      "        \n",
      "        Modified Fresnel positive integrals\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fp\n",
      "            Integral ``F_+(x)``: ``integral(exp(1j*t*t), t=x..inf)``\n",
      "        kp\n",
      "            Integral ``K_+(x)``: ``1/sqrt(pi)*exp(-1j*(x*x+pi/4))*fp``\n",
      "    \n",
      "    modstruve = <ufunc 'modstruve'>\n",
      "        modstruve(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        modstruve(v, x)\n",
      "        \n",
      "        Modified Struve function.\n",
      "        \n",
      "        Return the value of the modified Struve function of order `v` at `x`.  The\n",
      "        modified Struve function is defined as,\n",
      "        \n",
      "        .. math::\n",
      "            L_v(x) = -\\imath \\exp(-\\pi\\imath v/2) H_v(\\imath x),\n",
      "        \n",
      "        where :math:`H_v` is the Struve function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order of the modified Struve function (float).\n",
      "        x : array_like\n",
      "            Argument of the Struve function (float; must be positive unless `v` is\n",
      "            an integer).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        L : ndarray\n",
      "            Value of the modified Struve function of order `v` at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Three methods discussed in [1]_ are used to evaluate the function:\n",
      "        \n",
      "        - power series\n",
      "        - expansion in Bessel functions (if :math:`|x| < |v| + 20`)\n",
      "        - asymptotic large-x expansion (if :math:`x \\geq 0.7v + 12`)\n",
      "        \n",
      "        Rounding errors are estimated based on the largest terms in the sums, and\n",
      "        the result associated with the smallest error is returned.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        struve\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/11\n",
      "    \n",
      "    nbdtr = <ufunc 'nbdtr'>\n",
      "        nbdtr(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nbdtr(k, n, p)\n",
      "        \n",
      "        Negative binomial cumulative distribution function.\n",
      "        \n",
      "        Returns the sum of the terms 0 through `k` of the negative binomial\n",
      "        distribution probability mass function,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            F = \\sum_{j=0}^k {{n + j - 1}\\choose{j}} p^n (1 - p)^j.\n",
      "        \n",
      "        In a sequence of Bernoulli trials with individual success probabilities\n",
      "        `p`, this is the probability that `k` or fewer failures precede the nth\n",
      "        success.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            The maximum number of allowed failures (nonnegative int).\n",
      "        n : array_like\n",
      "            The target number of successes (positive int).\n",
      "        p : array_like\n",
      "            Probability of success in a single event (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        F : ndarray\n",
      "            The probability of `k` or fewer failures before `n` successes in a\n",
      "            sequence of events with individual success probability `p`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        nbdtrc\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If floating point values are passed for `k` or `n`, they will be truncated\n",
      "        to integers.\n",
      "        \n",
      "        The terms are not summed directly; instead the regularized incomplete beta\n",
      "        function is employed, according to the formula,\n",
      "        \n",
      "        .. math::\n",
      "            \\mathrm{nbdtr}(k, n, p) = I_{p}(n, k + 1).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `nbdtr`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    nbdtrc = <ufunc 'nbdtrc'>\n",
      "        nbdtrc(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nbdtrc(k, n, p)\n",
      "        \n",
      "        Negative binomial survival function.\n",
      "        \n",
      "        Returns the sum of the terms `k + 1` to infinity of the negative binomial\n",
      "        distribution probability mass function,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            F = \\sum_{j=k + 1}^\\infty {{n + j - 1}\\choose{j}} p^n (1 - p)^j.\n",
      "        \n",
      "        In a sequence of Bernoulli trials with individual success probabilities\n",
      "        `p`, this is the probability that more than `k` failures precede the nth\n",
      "        success.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            The maximum number of allowed failures (nonnegative int).\n",
      "        n : array_like\n",
      "            The target number of successes (positive int).\n",
      "        p : array_like\n",
      "            Probability of success in a single event (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        F : ndarray\n",
      "            The probability of `k + 1` or more failures before `n` successes in a\n",
      "            sequence of events with individual success probability `p`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If floating point values are passed for `k` or `n`, they will be truncated\n",
      "        to integers.\n",
      "        \n",
      "        The terms are not summed directly; instead the regularized incomplete beta\n",
      "        function is employed, according to the formula,\n",
      "        \n",
      "        .. math::\n",
      "            \\mathrm{nbdtrc}(k, n, p) = I_{1 - p}(k + 1, n).\n",
      "        \n",
      "        Wrapper for the Cephes [1]_ routine `nbdtrc`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    nbdtri = <ufunc 'nbdtri'>\n",
      "        nbdtri(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nbdtri(k, n, y)\n",
      "        \n",
      "        Inverse of `nbdtr` vs `p`.\n",
      "        \n",
      "        Returns the inverse with respect to the parameter `p` of\n",
      "        `y = nbdtr(k, n, p)`, the negative binomial cumulative distribution\n",
      "        function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            The maximum number of allowed failures (nonnegative int).\n",
      "        n : array_like\n",
      "            The target number of successes (positive int).\n",
      "        y : array_like\n",
      "            The probability of `k` or fewer failures before `n` successes (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        p : ndarray\n",
      "            Probability of success in a single event (float) such that\n",
      "            `nbdtr(k, n, p) = y`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        nbdtr : Cumulative distribution function of the negative binomial.\n",
      "        nbdtrik : Inverse with respect to `k` of `nbdtr(k, n, p)`.\n",
      "        nbdtrin : Inverse with respect to `n` of `nbdtr(k, n, p)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the Cephes [1]_ routine `nbdtri`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    nbdtrik = <ufunc 'nbdtrik'>\n",
      "        nbdtrik(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nbdtrik(y, n, p)\n",
      "        \n",
      "        Inverse of `nbdtr` vs `k`.\n",
      "        \n",
      "        Returns the inverse with respect to the parameter `k` of\n",
      "        `y = nbdtr(k, n, p)`, the negative binomial cumulative distribution\n",
      "        function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like\n",
      "            The probability of `k` or fewer failures before `n` successes (float).\n",
      "        n : array_like\n",
      "            The target number of successes (positive int).\n",
      "        p : array_like\n",
      "            Probability of success in a single event (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        k : ndarray\n",
      "            The maximum number of allowed failures such that `nbdtr(k, n, p) = y`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        nbdtr : Cumulative distribution function of the negative binomial.\n",
      "        nbdtri : Inverse with respect to `p` of `nbdtr(k, n, p)`.\n",
      "        nbdtrin : Inverse with respect to `n` of `nbdtr(k, n, p)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdfnbn`.\n",
      "        \n",
      "        Formula 26.5.26 of [2]_,\n",
      "        \n",
      "        .. math::\n",
      "            \\sum_{j=k + 1}^\\infty {{n + j - 1}\\choose{j}} p^n (1 - p)^j = I_{1 - p}(k + 1, n),\n",
      "        \n",
      "        is used to reduce calculation of the cumulative distribution function to\n",
      "        that of a regularized incomplete beta :math:`I`.\n",
      "        \n",
      "        Computation of `k` involves a search for a value that produces the desired\n",
      "        value of `y`.  The search relies on the monotonicity of `y` with `k`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    nbdtrin = <ufunc 'nbdtrin'>\n",
      "        nbdtrin(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nbdtrin(k, y, p)\n",
      "        \n",
      "        Inverse of `nbdtr` vs `n`.\n",
      "        \n",
      "        Returns the inverse with respect to the parameter `n` of\n",
      "        `y = nbdtr(k, n, p)`, the negative binomial cumulative distribution\n",
      "        function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            The maximum number of allowed failures (nonnegative int).\n",
      "        y : array_like\n",
      "            The probability of `k` or fewer failures before `n` successes (float).\n",
      "        p : array_like\n",
      "            Probability of success in a single event (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        n : ndarray\n",
      "            The number of successes `n` such that `nbdtr(k, n, p) = y`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        nbdtr : Cumulative distribution function of the negative binomial.\n",
      "        nbdtri : Inverse with respect to `p` of `nbdtr(k, n, p)`.\n",
      "        nbdtrik : Inverse with respect to `k` of `nbdtr(k, n, p)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdfnbn`.\n",
      "        \n",
      "        Formula 26.5.26 of [2]_,\n",
      "        \n",
      "        .. math::\n",
      "            \\sum_{j=k + 1}^\\infty {{n + j - 1}\\choose{j}} p^n (1 - p)^j = I_{1 - p}(k + 1, n),\n",
      "        \n",
      "        is used to reduce calculation of the cumulative distribution function to\n",
      "        that of a regularized incomplete beta :math:`I`.\n",
      "        \n",
      "        Computation of `n` involves a search for a value that produces the desired\n",
      "        value of `y`.  The search relies on the monotonicity of `y` with `n`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "    \n",
      "    ncfdtr = <ufunc 'ncfdtr'>\n",
      "        ncfdtr(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ncfdtr(dfn, dfd, nc, f)\n",
      "        \n",
      "        Cumulative distribution function of the non-central F distribution.\n",
      "        \n",
      "        The non-central F describes the distribution of,\n",
      "        \n",
      "        .. math::\n",
      "            Z = \\frac{X/d_n}{Y/d_d}\n",
      "        \n",
      "        where :math:`X` and :math:`Y` are independently distributed, with\n",
      "        :math:`X` distributed non-central :math:`\\chi^2` with noncentrality\n",
      "        parameter `nc` and :math:`d_n` degrees of freedom, and :math:`Y`\n",
      "        distributed :math:`\\chi^2` with :math:`d_d` degrees of freedom.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dfn : array_like\n",
      "            Degrees of freedom of the numerator sum of squares.  Range (0, inf).\n",
      "        dfd : array_like\n",
      "            Degrees of freedom of the denominator sum of squares.  Range (0, inf).\n",
      "        nc : array_like\n",
      "            Noncentrality parameter.  Should be in range (0, 1e4).\n",
      "        f : array_like\n",
      "            Quantiles, i.e. the upper limit of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        cdf : float or ndarray\n",
      "            The calculated CDF.  If all inputs are scalar, the return will be a\n",
      "            float.  Otherwise it will be an array.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ncfdtri : Quantile function; inverse of `ncfdtr` with respect to `f`.\n",
      "        ncfdtridfd : Inverse of `ncfdtr` with respect to `dfd`.\n",
      "        ncfdtridfn : Inverse of `ncfdtr` with respect to `dfn`.\n",
      "        ncfdtrinc : Inverse of `ncfdtr` with respect to `nc`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the CDFLIB [1]_ Fortran routine `cdffnc`.\n",
      "        \n",
      "        The cumulative distribution function is computed using Formula 26.6.20 of\n",
      "        [2]_:\n",
      "        \n",
      "        .. math::\n",
      "            F(d_n, d_d, n_c, f) = \\sum_{j=0}^\\infty e^{-n_c/2} \\frac{(n_c/2)^j}{j!} I_{x}(\\frac{d_n}{2} + j, \\frac{d_d}{2}),\n",
      "        \n",
      "        where :math:`I` is the regularized incomplete beta function, and\n",
      "        :math:`x = f d_n/(f d_n + d_d)`.\n",
      "        \n",
      "        The computation time required for this routine is proportional to the\n",
      "        noncentrality parameter `nc`.  Very large values of this parameter can\n",
      "        consume immense computer resources.  This is why the search range is\n",
      "        bounded by 10,000.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Barry Brown, James Lovato, and Kathy Russell,\n",
      "               CDFLIB: Library of Fortran Routines for Cumulative Distribution\n",
      "               Functions, Inverses, and Other Parameters.\n",
      "        .. [2] Milton Abramowitz and Irene A. Stegun, eds.\n",
      "               Handbook of Mathematical Functions with Formulas,\n",
      "               Graphs, and Mathematical Tables. New York: Dover, 1972.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Plot the CDF of the non-central F distribution, for nc=0.  Compare with the\n",
      "        F-distribution from scipy.stats:\n",
      "        \n",
      "        >>> x = np.linspace(-1, 8, num=500)\n",
      "        >>> dfn = 3\n",
      "        >>> dfd = 2\n",
      "        >>> ncf_stats = stats.f.cdf(x, dfn, dfd)\n",
      "        >>> ncf_special = special.ncfdtr(dfn, dfd, 0, x)\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, ncf_stats, 'b-', lw=3)\n",
      "        >>> ax.plot(x, ncf_special, 'r-')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    ncfdtri = <ufunc 'ncfdtri'>\n",
      "        ncfdtri(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ncfdtri(dfn, dfd, nc, p)\n",
      "        \n",
      "        Inverse with respect to `f` of the CDF of the non-central F distribution.\n",
      "        \n",
      "        See `ncfdtr` for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dfn : array_like\n",
      "            Degrees of freedom of the numerator sum of squares.  Range (0, inf).\n",
      "        dfd : array_like\n",
      "            Degrees of freedom of the denominator sum of squares.  Range (0, inf).\n",
      "        nc : array_like\n",
      "            Noncentrality parameter.  Should be in range (0, 1e4).\n",
      "        p : array_like\n",
      "            Value of the cumulative distribution function.  Must be in the\n",
      "            range [0, 1].\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            Quantiles, i.e., the upper limit of integration.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ncfdtr : CDF of the non-central F distribution.\n",
      "        ncfdtridfd : Inverse of `ncfdtr` with respect to `dfd`.\n",
      "        ncfdtridfn : Inverse of `ncfdtr` with respect to `dfn`.\n",
      "        ncfdtrinc : Inverse of `ncfdtr` with respect to `nc`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import ncfdtr, ncfdtri\n",
      "        \n",
      "        Compute the CDF for several values of `f`:\n",
      "        \n",
      "        >>> f = [0.5, 1, 1.5]\n",
      "        >>> p = ncfdtr(2, 3, 1.5, f)\n",
      "        >>> p\n",
      "        array([ 0.20782291,  0.36107392,  0.47345752])\n",
      "        \n",
      "        Compute the inverse.  We recover the values of `f`, as expected:\n",
      "        \n",
      "        >>> ncfdtri(2, 3, 1.5, p)\n",
      "        array([ 0.5,  1. ,  1.5])\n",
      "    \n",
      "    ncfdtridfd = <ufunc 'ncfdtridfd'>\n",
      "        ncfdtridfd(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ncfdtridfd(dfn, p, nc, f)\n",
      "        \n",
      "        Calculate degrees of freedom (denominator) for the noncentral F-distribution.\n",
      "        \n",
      "        This is the inverse with respect to `dfd` of `ncfdtr`.\n",
      "        See `ncfdtr` for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dfn : array_like\n",
      "            Degrees of freedom of the numerator sum of squares.  Range (0, inf).\n",
      "        p : array_like\n",
      "            Value of the cumulative distribution function.  Must be in the\n",
      "            range [0, 1].\n",
      "        nc : array_like\n",
      "            Noncentrality parameter.  Should be in range (0, 1e4).\n",
      "        f : array_like\n",
      "            Quantiles, i.e., the upper limit of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dfd : float\n",
      "            Degrees of freedom of the denominator sum of squares.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ncfdtr : CDF of the non-central F distribution.\n",
      "        ncfdtri : Quantile function; inverse of `ncfdtr` with respect to `f`.\n",
      "        ncfdtridfn : Inverse of `ncfdtr` with respect to `dfn`.\n",
      "        ncfdtrinc : Inverse of `ncfdtr` with respect to `nc`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The value of the cumulative noncentral F distribution is not necessarily\n",
      "        monotone in either degrees of freedom. There thus may be two values that\n",
      "        provide a given CDF value. This routine assumes monotonicity and will\n",
      "        find an arbitrary one of the two values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import ncfdtr, ncfdtridfd\n",
      "        \n",
      "        Compute the CDF for several values of `dfd`:\n",
      "        \n",
      "        >>> dfd = [1, 2, 3]\n",
      "        >>> p = ncfdtr(2, dfd, 0.25, 15)\n",
      "        >>> p\n",
      "        array([ 0.8097138 ,  0.93020416,  0.96787852])\n",
      "        \n",
      "        Compute the inverse.  We recover the values of `dfd`, as expected:\n",
      "        \n",
      "        >>> ncfdtridfd(2, p, 0.25, 15)\n",
      "        array([ 1.,  2.,  3.])\n",
      "    \n",
      "    ncfdtridfn = <ufunc 'ncfdtridfn'>\n",
      "        ncfdtridfn(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ncfdtridfn(p, dfd, nc, f)\n",
      "        \n",
      "        Calculate degrees of freedom (numerator) for the noncentral F-distribution.\n",
      "        \n",
      "        This is the inverse with respect to `dfn` of `ncfdtr`.\n",
      "        See `ncfdtr` for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            Value of the cumulative distribution function. Must be in the\n",
      "            range [0, 1].\n",
      "        dfd : array_like\n",
      "            Degrees of freedom of the denominator sum of squares. Range (0, inf).\n",
      "        nc : array_like\n",
      "            Noncentrality parameter.  Should be in range (0, 1e4).\n",
      "        f : float\n",
      "            Quantiles, i.e., the upper limit of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dfn : float\n",
      "            Degrees of freedom of the numerator sum of squares.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ncfdtr : CDF of the non-central F distribution.\n",
      "        ncfdtri : Quantile function; inverse of `ncfdtr` with respect to `f`.\n",
      "        ncfdtridfd : Inverse of `ncfdtr` with respect to `dfd`.\n",
      "        ncfdtrinc : Inverse of `ncfdtr` with respect to `nc`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The value of the cumulative noncentral F distribution is not necessarily\n",
      "        monotone in either degrees of freedom. There thus may be two values that\n",
      "        provide a given CDF value. This routine assumes monotonicity and will\n",
      "        find an arbitrary one of the two values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import ncfdtr, ncfdtridfn\n",
      "        \n",
      "        Compute the CDF for several values of `dfn`:\n",
      "        \n",
      "        >>> dfn = [1, 2, 3]\n",
      "        >>> p = ncfdtr(dfn, 2, 0.25, 15)\n",
      "        >>> p\n",
      "        array([ 0.92562363,  0.93020416,  0.93188394])\n",
      "        \n",
      "        Compute the inverse. We recover the values of `dfn`, as expected:\n",
      "        \n",
      "        >>> ncfdtridfn(p, 2, 0.25, 15)\n",
      "        array([ 1.,  2.,  3.])\n",
      "    \n",
      "    ncfdtrinc = <ufunc 'ncfdtrinc'>\n",
      "        ncfdtrinc(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ncfdtrinc(dfn, dfd, p, f)\n",
      "        \n",
      "        Calculate non-centrality parameter for non-central F distribution.\n",
      "        \n",
      "        This is the inverse with respect to `nc` of `ncfdtr`.\n",
      "        See `ncfdtr` for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dfn : array_like\n",
      "            Degrees of freedom of the numerator sum of squares. Range (0, inf).\n",
      "        dfd : array_like\n",
      "            Degrees of freedom of the denominator sum of squares. Range (0, inf).\n",
      "        p : array_like\n",
      "            Value of the cumulative distribution function. Must be in the\n",
      "            range [0, 1].\n",
      "        f : array_like\n",
      "            Quantiles, i.e., the upper limit of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nc : float\n",
      "            Noncentrality parameter.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ncfdtr : CDF of the non-central F distribution.\n",
      "        ncfdtri : Quantile function; inverse of `ncfdtr` with respect to `f`.\n",
      "        ncfdtridfd : Inverse of `ncfdtr` with respect to `dfd`.\n",
      "        ncfdtridfn : Inverse of `ncfdtr` with respect to `dfn`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import ncfdtr, ncfdtrinc\n",
      "        \n",
      "        Compute the CDF for several values of `nc`:\n",
      "        \n",
      "        >>> nc = [0.5, 1.5, 2.0]\n",
      "        >>> p = ncfdtr(2, 3, nc, 15)\n",
      "        >>> p\n",
      "        array([ 0.96309246,  0.94327955,  0.93304098])\n",
      "        \n",
      "        Compute the inverse. We recover the values of `nc`, as expected:\n",
      "        \n",
      "        >>> ncfdtrinc(2, 3, p, 15)\n",
      "        array([ 0.5,  1.5,  2. ])\n",
      "    \n",
      "    nctdtr = <ufunc 'nctdtr'>\n",
      "        nctdtr(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nctdtr(df, nc, t)\n",
      "        \n",
      "        Cumulative distribution function of the non-central `t` distribution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        df : array_like\n",
      "            Degrees of freedom of the distribution. Should be in range (0, inf).\n",
      "        nc : array_like\n",
      "            Noncentrality parameter. Should be in range (-1e6, 1e6).\n",
      "        t : array_like\n",
      "            Quantiles, i.e., the upper limit of integration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        cdf : float or ndarray\n",
      "            The calculated CDF. If all inputs are scalar, the return will be a\n",
      "            float. Otherwise, it will be an array.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nctdtrit : Inverse CDF (iCDF) of the non-central t distribution.\n",
      "        nctdtridf : Calculate degrees of freedom, given CDF and iCDF values.\n",
      "        nctdtrinc : Calculate non-centrality parameter, given CDF iCDF values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> from scipy import stats\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        Plot the CDF of the non-central t distribution, for nc=0. Compare with the\n",
      "        t-distribution from scipy.stats:\n",
      "        \n",
      "        >>> x = np.linspace(-5, 5, num=500)\n",
      "        >>> df = 3\n",
      "        >>> nct_stats = stats.t.cdf(x, df)\n",
      "        >>> nct_special = special.nctdtr(df, 0, x)\n",
      "        \n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(x, nct_stats, 'b-', lw=3)\n",
      "        >>> ax.plot(x, nct_special, 'r-')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    nctdtridf = <ufunc 'nctdtridf'>\n",
      "        nctdtridf(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nctdtridf(p, nc, t)\n",
      "        \n",
      "        Calculate degrees of freedom for non-central t distribution.\n",
      "        \n",
      "        See `nctdtr` for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            CDF values, in range (0, 1].\n",
      "        nc : array_like\n",
      "            Noncentrality parameter. Should be in range (-1e6, 1e6).\n",
      "        t : array_like\n",
      "            Quantiles, i.e., the upper limit of integration.\n",
      "    \n",
      "    nctdtrinc = <ufunc 'nctdtrinc'>\n",
      "        nctdtrinc(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nctdtrinc(df, p, t)\n",
      "        \n",
      "        Calculate non-centrality parameter for non-central t distribution.\n",
      "        \n",
      "        See `nctdtr` for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        df : array_like\n",
      "            Degrees of freedom of the distribution. Should be in range (0, inf).\n",
      "        p : array_like\n",
      "            CDF values, in range (0, 1].\n",
      "        t : array_like\n",
      "            Quantiles, i.e., the upper limit of integration.\n",
      "    \n",
      "    nctdtrit = <ufunc 'nctdtrit'>\n",
      "        nctdtrit(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nctdtrit(df, nc, p)\n",
      "        \n",
      "        Inverse cumulative distribution function of the non-central t distribution.\n",
      "        \n",
      "        See `nctdtr` for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        df : array_like\n",
      "            Degrees of freedom of the distribution. Should be in range (0, inf).\n",
      "        nc : array_like\n",
      "            Noncentrality parameter. Should be in range (-1e6, 1e6).\n",
      "        p : array_like\n",
      "            CDF values, in range (0, 1].\n",
      "    \n",
      "    ndtr = <ufunc 'ndtr'>\n",
      "        ndtr(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ndtr(x)\n",
      "        \n",
      "        Gaussian cumulative distribution function.\n",
      "        \n",
      "        Returns the area under the standard Gaussian probability\n",
      "        density function, integrated from minus infinity to `x`\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^x \\exp(-t^2/2) dt\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like, real or complex\n",
      "            Argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ndarray\n",
      "            The value of the normal CDF evaluated at `x`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        erf\n",
      "        erfc\n",
      "        scipy.stats.norm\n",
      "        log_ndtr\n",
      "    \n",
      "    ndtri = <ufunc 'ndtri'>\n",
      "        ndtri(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ndtri(y)\n",
      "        \n",
      "        Inverse of `ndtr` vs x\n",
      "        \n",
      "        Returns the argument x for which the area under the Gaussian\n",
      "        probability density function (integrated from minus infinity to `x`)\n",
      "        is equal to y.\n",
      "    \n",
      "    ndtri_exp = <ufunc 'ndtri_exp'>\n",
      "        ndtri_exp(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        ndtri_exp(y)\n",
      "        \n",
      "        Inverse of `log_ndtr` vs x. Allows for greater precision than\n",
      "        `ndtri` composed with `numpy.exp` for very small values of y and for\n",
      "        y close to 0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array_like of float\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Inverse of the log CDF of the standard normal distribution, evaluated\n",
      "            at y.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        `ndtri_exp` agrees with the naive implementation when the latter does\n",
      "        not suffer from underflow.\n",
      "        \n",
      "        >>> sc.ndtri_exp(-1)\n",
      "        -0.33747496376420244\n",
      "        >>> sc.ndtri(np.exp(-1))\n",
      "        -0.33747496376420244\n",
      "        \n",
      "        For extreme values of y, the naive approach fails\n",
      "        \n",
      "        >>> sc.ndtri(np.exp(-800))\n",
      "        -inf\n",
      "        >>> sc.ndtri(np.exp(-1e-20))\n",
      "        inf\n",
      "        \n",
      "        whereas `ndtri_exp` is still able to compute the result to high precision.\n",
      "        \n",
      "        >>> sc.ndtri_exp(-800)\n",
      "        -39.88469483825668\n",
      "        >>> sc.ndtri_exp(-1e-20)\n",
      "        9.262340089798409\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        log_ndtr, ndtri, ndtr\n",
      "    \n",
      "    nrdtrimn = <ufunc 'nrdtrimn'>\n",
      "        nrdtrimn(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nrdtrimn(p, x, std)\n",
      "        \n",
      "        Calculate mean of normal distribution given other params.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            CDF values, in range (0, 1].\n",
      "        x : array_like\n",
      "            Quantiles, i.e. the upper limit of integration.\n",
      "        std : array_like\n",
      "            Standard deviation.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mn : float or ndarray\n",
      "            The mean of the normal distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nrdtrimn, ndtr\n",
      "    \n",
      "    nrdtrisd = <ufunc 'nrdtrisd'>\n",
      "        nrdtrisd(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        nrdtrisd(p, x, mn)\n",
      "        \n",
      "        Calculate standard deviation of normal distribution given other params.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        p : array_like\n",
      "            CDF values, in range (0, 1].\n",
      "        x : array_like\n",
      "            Quantiles, i.e. the upper limit of integration.\n",
      "        mn : float or ndarray\n",
      "            The mean of the normal distribution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        std : array_like\n",
      "            Standard deviation.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ndtr\n",
      "    \n",
      "    obl_ang1 = <ufunc 'obl_ang1'>\n",
      "        obl_ang1(x1, x2, x3, x4[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        obl_ang1(m, n, c, x)\n",
      "        \n",
      "        Oblate spheroidal angular function of the first kind and its derivative\n",
      "        \n",
      "        Computes the oblate spheroidal angular function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    obl_ang1_cv = <ufunc 'obl_ang1_cv'>\n",
      "        obl_ang1_cv(x1, x2, x3, x4, x5[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        obl_ang1_cv(m, n, c, cv, x)\n",
      "        \n",
      "        Oblate spheroidal angular function obl_ang1 for precomputed characteristic value\n",
      "        \n",
      "        Computes the oblate spheroidal angular function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``. Requires\n",
      "        pre-computed characteristic value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    obl_cv = <ufunc 'obl_cv'>\n",
      "        obl_cv(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        obl_cv(m, n, c)\n",
      "        \n",
      "        Characteristic value of oblate spheroidal function\n",
      "        \n",
      "        Computes the characteristic value of oblate spheroidal wave\n",
      "        functions of order `m`, `n` (n>=m) and spheroidal parameter `c`.\n",
      "    \n",
      "    obl_rad1 = <ufunc 'obl_rad1'>\n",
      "        obl_rad1(x1, x2, x3, x4[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        obl_rad1(m, n, c, x)\n",
      "        \n",
      "        Oblate spheroidal radial function of the first kind and its derivative\n",
      "        \n",
      "        Computes the oblate spheroidal radial function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    obl_rad1_cv = <ufunc 'obl_rad1_cv'>\n",
      "        obl_rad1_cv(x1, x2, x3, x4, x5[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        obl_rad1_cv(m, n, c, cv, x)\n",
      "        \n",
      "        Oblate spheroidal radial function obl_rad1 for precomputed characteristic value\n",
      "        \n",
      "        Computes the oblate spheroidal radial function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``. Requires\n",
      "        pre-computed characteristic value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    obl_rad2 = <ufunc 'obl_rad2'>\n",
      "        obl_rad2(x1, x2, x3, x4[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        obl_rad2(m, n, c, x)\n",
      "        \n",
      "        Oblate spheroidal radial function of the second kind and its derivative.\n",
      "        \n",
      "        Computes the oblate spheroidal radial function of the second kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    obl_rad2_cv = <ufunc 'obl_rad2_cv'>\n",
      "        obl_rad2_cv(x1, x2, x3, x4, x5[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        obl_rad2_cv(m, n, c, cv, x)\n",
      "        \n",
      "        Oblate spheroidal radial function obl_rad2 for precomputed characteristic value\n",
      "        \n",
      "        Computes the oblate spheroidal radial function of the second kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``. Requires\n",
      "        pre-computed characteristic value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    owens_t = <ufunc 'owens_t'>\n",
      "        owens_t(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        owens_t(h, a)\n",
      "        \n",
      "        Owen's T Function.\n",
      "        \n",
      "        The function T(h, a) gives the probability of the event\n",
      "        (X > h and 0 < Y < a * X) where X and Y are independent\n",
      "        standard normal random variables.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        h: array_like\n",
      "            Input value.\n",
      "        a: array_like\n",
      "            Input value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        t: scalar or ndarray\n",
      "            Probability of the event (X > h and 0 < Y < a * X),\n",
      "            where X and Y are independent standard normal random variables.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> a = 3.5\n",
      "        >>> h = 0.78\n",
      "        >>> special.owens_t(h, a)\n",
      "        0.10877216734852274\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] M. Patefield and D. Tandy, \"Fast and accurate calculation of\n",
      "               Owen's T Function\", Statistical Software vol. 5, pp. 1-25, 2000.\n",
      "    \n",
      "    pbdv = <ufunc 'pbdv'>\n",
      "        pbdv(x1, x2[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pbdv(v, x)\n",
      "        \n",
      "        Parabolic cylinder function D\n",
      "        \n",
      "        Returns (d, dp) the parabolic cylinder function Dv(x) in d and the\n",
      "        derivative, Dv'(x) in dp.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        d\n",
      "            Value of the function\n",
      "        dp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pbvv = <ufunc 'pbvv'>\n",
      "        pbvv(x1, x2[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pbvv(v, x)\n",
      "        \n",
      "        Parabolic cylinder function V\n",
      "        \n",
      "        Returns the parabolic cylinder function Vv(x) in v and the\n",
      "        derivative, Vv'(x) in vp.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v\n",
      "            Value of the function\n",
      "        vp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pbwa = <ufunc 'pbwa'>\n",
      "        pbwa(x1, x2[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pbwa(a, x)\n",
      "        \n",
      "        Parabolic cylinder function W.\n",
      "        \n",
      "        The function is a particular solution to the differential equation\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            y'' + \\left(\\frac{1}{4}x^2 - a\\right)y = 0,\n",
      "        \n",
      "        for a full definition see section 12.14 in [1]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like\n",
      "            Real parameter\n",
      "        x : array_like\n",
      "            Real argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        w : scalar or ndarray\n",
      "            Value of the function\n",
      "        wp : scalar or ndarray\n",
      "            Value of the derivative in x\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is a wrapper for a Fortran routine by Zhang and Jin\n",
      "        [2]_. The implementation is accurate only for ``|a|, |x| < 5`` and\n",
      "        returns NaN outside that range.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Digital Library of Mathematical Functions, 14.30.\n",
      "               https://dlmf.nist.gov/14.30\n",
      "        .. [2] Zhang, Shanjie and Jin, Jianming. \"Computation of Special\n",
      "               Functions\", John Wiley and Sons, 1996.\n",
      "               https://people.sc.fsu.edu/~jburkardt/f_src/special_functions/special_functions.html\n",
      "    \n",
      "    pdtr = <ufunc 'pdtr'>\n",
      "        pdtr(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pdtr(k, m, out=None)\n",
      "        \n",
      "        Poisson cumulative distribution function.\n",
      "        \n",
      "        Defined as the probability that a Poisson-distributed random\n",
      "        variable with event rate :math:`m` is less than or equal to\n",
      "        :math:`k`. More concretely, this works out to be [1]_\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "           \\exp(-m) \\sum_{j = 0}^{\\lfloor{k}\\rfloor} \\frac{m^j}{m!}.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        k : array_like\n",
      "            Nonnegative real argument\n",
      "        m : array_like\n",
      "            Nonnegative real shape parameter\n",
      "        out : ndarray\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pdtrc : Poisson survival function\n",
      "        pdtrik : inverse of `pdtr` with respect to `k`\n",
      "        pdtri : inverse of `pdtr` with respect to `m`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the Poisson cumulative distribution function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Poisson_distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is a cumulative distribution function, so it converges to 1\n",
      "        monotonically as `k` goes to infinity.\n",
      "        \n",
      "        >>> sc.pdtr([1, 10, 100, np.inf], 1)\n",
      "        array([0.73575888, 0.99999999, 1.        , 1.        ])\n",
      "        \n",
      "        It is discontinuous at integers and constant between integers.\n",
      "        \n",
      "        >>> sc.pdtr([1, 1.5, 1.9, 2], 1)\n",
      "        array([0.73575888, 0.73575888, 0.73575888, 0.9196986 ])\n",
      "    \n",
      "    pdtrc = <ufunc 'pdtrc'>\n",
      "        pdtrc(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pdtrc(k, m)\n",
      "        \n",
      "        Poisson survival function\n",
      "        \n",
      "        Returns the sum of the terms from k+1 to infinity of the Poisson\n",
      "        distribution: sum(exp(-m) * m**j / j!, j=k+1..inf) = gammainc(\n",
      "        k+1, m). Arguments must both be non-negative doubles.\n",
      "    \n",
      "    pdtri = <ufunc 'pdtri'>\n",
      "        pdtri(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pdtri(k, y)\n",
      "        \n",
      "        Inverse to `pdtr` vs m\n",
      "        \n",
      "        Returns the Poisson variable `m` such that the sum from 0 to `k` of\n",
      "        the Poisson density is equal to the given probability `y`:\n",
      "        calculated by gammaincinv(k+1, y). `k` must be a nonnegative\n",
      "        integer and `y` between 0 and 1.\n",
      "    \n",
      "    pdtrik = <ufunc 'pdtrik'>\n",
      "        pdtrik(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pdtrik(p, m)\n",
      "        \n",
      "        Inverse to `pdtr` vs k\n",
      "        \n",
      "        Returns the quantile k such that ``pdtr(k, m) = p``\n",
      "    \n",
      "    poch = <ufunc 'poch'>\n",
      "        poch(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        poch(z, m)\n",
      "        \n",
      "        Pochhammer symbol.\n",
      "        \n",
      "        The Pochhammer symbol (rising factorial) is defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            (z)_m = \\frac{\\Gamma(z + m)}{\\Gamma(z)}\n",
      "        \n",
      "        For positive integer `m` it reads\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            (z)_m = z (z + 1) ... (z + m - 1)\n",
      "        \n",
      "        See [dlmf]_ for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z, m : array_like\n",
      "            Real-valued arguments.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            The value of the function.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] Nist, Digital Library of Mathematical Functions\n",
      "            https://dlmf.nist.gov/5.2#iii\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is 1 when m is 0.\n",
      "        \n",
      "        >>> sc.poch([1, 2, 3, 4], 0)\n",
      "        array([1., 1., 1., 1.])\n",
      "        \n",
      "        For z equal to 1 it reduces to the factorial function.\n",
      "        \n",
      "        >>> sc.poch(1, 5)\n",
      "        120.0\n",
      "        >>> 1 * 2 * 3 * 4 * 5\n",
      "        120\n",
      "        \n",
      "        It can be expressed in terms of the gamma function.\n",
      "        \n",
      "        >>> z, m = 3.7, 2.1\n",
      "        >>> sc.poch(z, m)\n",
      "        20.529581933776953\n",
      "        >>> sc.gamma(z + m) / sc.gamma(z)\n",
      "        20.52958193377696\n",
      "    \n",
      "    pro_ang1 = <ufunc 'pro_ang1'>\n",
      "        pro_ang1(x1, x2, x3, x4[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pro_ang1(m, n, c, x)\n",
      "        \n",
      "        Prolate spheroidal angular function of the first kind and its derivative\n",
      "        \n",
      "        Computes the prolate spheroidal angular function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pro_ang1_cv = <ufunc 'pro_ang1_cv'>\n",
      "        pro_ang1_cv(x1, x2, x3, x4, x5[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pro_ang1_cv(m, n, c, cv, x)\n",
      "        \n",
      "        Prolate spheroidal angular function pro_ang1 for precomputed characteristic value\n",
      "        \n",
      "        Computes the prolate spheroidal angular function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``. Requires\n",
      "        pre-computed characteristic value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pro_cv = <ufunc 'pro_cv'>\n",
      "        pro_cv(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pro_cv(m, n, c)\n",
      "        \n",
      "        Characteristic value of prolate spheroidal function\n",
      "        \n",
      "        Computes the characteristic value of prolate spheroidal wave\n",
      "        functions of order `m`, `n` (n>=m) and spheroidal parameter `c`.\n",
      "    \n",
      "    pro_rad1 = <ufunc 'pro_rad1'>\n",
      "        pro_rad1(x1, x2, x3, x4[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pro_rad1(m, n, c, x)\n",
      "        \n",
      "        Prolate spheroidal radial function of the first kind and its derivative\n",
      "        \n",
      "        Computes the prolate spheroidal radial function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pro_rad1_cv = <ufunc 'pro_rad1_cv'>\n",
      "        pro_rad1_cv(x1, x2, x3, x4, x5[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pro_rad1_cv(m, n, c, cv, x)\n",
      "        \n",
      "        Prolate spheroidal radial function pro_rad1 for precomputed characteristic value\n",
      "        \n",
      "        Computes the prolate spheroidal radial function of the first kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``. Requires\n",
      "        pre-computed characteristic value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pro_rad2 = <ufunc 'pro_rad2'>\n",
      "        pro_rad2(x1, x2, x3, x4[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pro_rad2(m, n, c, x)\n",
      "        \n",
      "        Prolate spheroidal radial function of the second kind and its derivative\n",
      "        \n",
      "        Computes the prolate spheroidal radial function of the second kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pro_rad2_cv = <ufunc 'pro_rad2_cv'>\n",
      "        pro_rad2_cv(x1, x2, x3, x4, x5[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pro_rad2_cv(m, n, c, cv, x)\n",
      "        \n",
      "        Prolate spheroidal radial function pro_rad2 for precomputed characteristic value\n",
      "        \n",
      "        Computes the prolate spheroidal radial function of the second kind\n",
      "        and its derivative (with respect to `x`) for mode parameters m>=0\n",
      "        and n>=m, spheroidal parameter `c` and ``|x| < 1.0``. Requires\n",
      "        pre-computed characteristic value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s\n",
      "            Value of the function\n",
      "        sp\n",
      "            Value of the derivative vs x\n",
      "    \n",
      "    pseudo_huber = <ufunc 'pseudo_huber'>\n",
      "        pseudo_huber(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        pseudo_huber(delta, r)\n",
      "        \n",
      "        Pseudo-Huber loss function.\n",
      "        \n",
      "        .. math:: \\mathrm{pseudo\\_huber}(\\delta, r) = \\delta^2 \\left( \\sqrt{ 1 + \\left( \\frac{r}{\\delta} \\right)^2 } - 1 \\right)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        delta : ndarray\n",
      "            Input array, indicating the soft quadratic vs. linear loss changepoint.\n",
      "        r : ndarray\n",
      "            Input array, possibly representing residuals.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : ndarray\n",
      "            The computed Pseudo-Huber loss function values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function is convex in :math:`r`.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "    \n",
      "    psi = <ufunc 'psi'>\n",
      "        psi(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        psi(z, out=None)\n",
      "        \n",
      "        The digamma function.\n",
      "        \n",
      "        The logarithmic derivative of the gamma function evaluated at ``z``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex argument.\n",
      "        out : ndarray, optional\n",
      "            Array for the computed values of ``psi``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        digamma : ndarray\n",
      "            Computed values of ``psi``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For large values not close to the negative real axis, ``psi`` is\n",
      "        computed using the asymptotic series (5.11.2) from [1]_. For small\n",
      "        arguments not close to the negative real axis, the recurrence\n",
      "        relation (5.5.2) from [1]_ is used until the argument is large\n",
      "        enough to use the asymptotic series. For values close to the\n",
      "        negative real axis, the reflection formula (5.5.4) from [1]_ is\n",
      "        used first. Note that ``psi`` has a family of zeros on the\n",
      "        negative real axis which occur between the poles at nonpositive\n",
      "        integers. Around the zeros the reflection formula suffers from\n",
      "        cancellation and the implementation loses precision. The sole\n",
      "        positive zero and the first negative zero, however, are handled\n",
      "        separately by precomputing series expansions using [2]_, so the\n",
      "        function should maintain full accuracy around the origin.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/5\n",
      "        .. [2] Fredrik Johansson and others.\n",
      "               \"mpmath: a Python library for arbitrary-precision floating-point arithmetic\"\n",
      "               (Version 0.19) http://mpmath.org/\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import psi\n",
      "        >>> z = 3 + 4j\n",
      "        >>> psi(z)\n",
      "        (1.55035981733341+1.0105022091860445j)\n",
      "        \n",
      "        Verify psi(z) = psi(z + 1) - 1/z:\n",
      "        \n",
      "        >>> psi(z + 1) - 1/z\n",
      "        (1.55035981733341+1.0105022091860445j)\n",
      "    \n",
      "    radian = <ufunc 'radian'>\n",
      "        radian(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        radian(d, m, s, out=None)\n",
      "        \n",
      "        Convert from degrees to radians.\n",
      "        \n",
      "        Returns the angle given in (d)egrees, (m)inutes, and (s)econds in\n",
      "        radians.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        d : array_like\n",
      "            Degrees, can be real-valued.\n",
      "        m : array_like\n",
      "            Minutes, can be real-valued.\n",
      "        s : array_like\n",
      "            Seconds, can be real-valued.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Values of the inputs in radians.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        There are many ways to specify an angle.\n",
      "        \n",
      "        >>> sc.radian(90, 0, 0)\n",
      "        1.5707963267948966\n",
      "        >>> sc.radian(0, 60 * 90, 0)\n",
      "        1.5707963267948966\n",
      "        >>> sc.radian(0, 0, 60**2 * 90)\n",
      "        1.5707963267948966\n",
      "        \n",
      "        The inputs can be real-valued.\n",
      "        \n",
      "        >>> sc.radian(1.5, 0, 0)\n",
      "        0.02617993877991494\n",
      "        >>> sc.radian(1, 30, 0)\n",
      "        0.02617993877991494\n",
      "    \n",
      "    rel_entr = <ufunc 'rel_entr'>\n",
      "        rel_entr(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        rel_entr(x, y, out=None)\n",
      "        \n",
      "        Elementwise function for computing relative entropy.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\mathrm{rel\\_entr}(x, y) =\n",
      "                \\begin{cases}\n",
      "                    x \\log(x / y) & x > 0, y > 0 \\\\\n",
      "                    0 & x = 0, y \\ge 0 \\\\\n",
      "                    \\infty & \\text{otherwise}\n",
      "                \\end{cases}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array_like\n",
      "            Input arrays\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Relative entropy of the inputs\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        entr, kl_div\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        This function is jointly convex in x and y.\n",
      "        \n",
      "        The origin of this function is in convex programming; see\n",
      "        [1]_. Given two discrete probability distributions :math:`p_1,\n",
      "        \\ldots, p_n` and :math:`q_1, \\ldots, q_n`, to get the relative\n",
      "        entropy of statistics compute the sum\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\sum_{i = 1}^n \\mathrm{rel\\_entr}(p_i, q_i).\n",
      "        \n",
      "        See [2]_ for details.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grant, Boyd, and Ye, \"CVX: Matlab Software for Disciplined Convex\n",
      "            Programming\", http://cvxr.com/cvx/\n",
      "        .. [2] Kullback-Leibler divergence,\n",
      "            https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
      "    \n",
      "    rgamma = <ufunc 'rgamma'>\n",
      "        rgamma(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        rgamma(z, out=None)\n",
      "        \n",
      "        Reciprocal of the gamma function.\n",
      "        \n",
      "        Defined as :math:`1 / \\Gamma(z)`, where :math:`\\Gamma` is the\n",
      "        gamma function. For more on the gamma function see `gamma`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Real or complex valued input\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Function results\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The gamma function has no zeros and has simple poles at\n",
      "        nonpositive integers, so `rgamma` is an entire function with zeros\n",
      "        at the nonpositive integers. See the discussion in [dlmf]_ for\n",
      "        more details.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        gamma, gammaln, loggamma\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] Nist, Digital Library of Mathematical functions,\n",
      "            https://dlmf.nist.gov/5.2#i\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is the reciprocal of the gamma function.\n",
      "        \n",
      "        >>> sc.rgamma([1, 2, 3, 4])\n",
      "        array([1.        , 1.        , 0.5       , 0.16666667])\n",
      "        >>> 1 / sc.gamma([1, 2, 3, 4])\n",
      "        array([1.        , 1.        , 0.5       , 0.16666667])\n",
      "        \n",
      "        It is zero at nonpositive integers.\n",
      "        \n",
      "        >>> sc.rgamma([0, -1, -2, -3])\n",
      "        array([0., 0., 0., 0.])\n",
      "        \n",
      "        It rapidly underflows to zero along the positive real axis.\n",
      "        \n",
      "        >>> sc.rgamma([10, 100, 179])\n",
      "        array([2.75573192e-006, 1.07151029e-156, 0.00000000e+000])\n",
      "    \n",
      "    round = <ufunc 'round'>\n",
      "        round(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        round(x, out=None)\n",
      "        \n",
      "        Round to the nearest integer.\n",
      "        \n",
      "        Returns the nearest integer to `x`.  If `x` ends in 0.5 exactly,\n",
      "        the nearest even integer is chosen.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real valued input.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            The nearest integers to the elements of `x`. The result is of\n",
      "            floating type, not integer type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It rounds to even.\n",
      "        \n",
      "        >>> sc.round([0.5, 1.5])\n",
      "        array([0., 2.])\n",
      "    \n",
      "    shichi = <ufunc 'shichi'>\n",
      "        shichi(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        shichi(x, out=None)\n",
      "        \n",
      "        Hyperbolic sine and cosine integrals.\n",
      "        \n",
      "        The hyperbolic sine integral is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "          \\int_0^x \\frac{\\sinh{t}}{t}dt\n",
      "        \n",
      "        and the hyperbolic cosine integral is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "          \\gamma + \\log(x) + \\int_0^x \\frac{\\cosh{t} - 1}{t} dt\n",
      "        \n",
      "        where :math:`\\gamma` is Euler's constant and :math:`\\log` is the\n",
      "        principle branch of the logarithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real or complex points at which to compute the hyperbolic sine\n",
      "            and cosine integrals.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        si : ndarray\n",
      "            Hyperbolic sine integral at ``x``\n",
      "        ci : ndarray\n",
      "            Hyperbolic cosine integral at ``x``\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For real arguments with ``x < 0``, ``chi`` is the real part of the\n",
      "        hyperbolic cosine integral. For such points ``chi(x)`` and ``chi(x\n",
      "        + 0j)`` differ by a factor of ``1j*pi``.\n",
      "        \n",
      "        For real arguments the function is computed by calling Cephes'\n",
      "        [1]_ *shichi* routine. For complex arguments the algorithm is based\n",
      "        on Mpmath's [2]_ *shi* and *chi* routines.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "        .. [2] Fredrik Johansson and others.\n",
      "               \"mpmath: a Python library for arbitrary-precision floating-point arithmetic\"\n",
      "               (Version 0.19) http://mpmath.org/\n",
      "    \n",
      "    sici = <ufunc 'sici'>\n",
      "        sici(x[, out1, out2], / [, out=(None, None)], *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        sici(x, out=None)\n",
      "        \n",
      "        Sine and cosine integrals.\n",
      "        \n",
      "        The sine integral is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "          \\int_0^x \\frac{\\sin{t}}{t}dt\n",
      "        \n",
      "        and the cosine integral is\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "          \\gamma + \\log(x) + \\int_0^x \\frac{\\cos{t} - 1}{t}dt\n",
      "        \n",
      "        where :math:`\\gamma` is Euler's constant and :math:`\\log` is the\n",
      "        principle branch of the logarithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real or complex points at which to compute the sine and cosine\n",
      "            integrals.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        si : ndarray\n",
      "            Sine integral at ``x``\n",
      "        ci : ndarray\n",
      "            Cosine integral at ``x``\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For real arguments with ``x < 0``, ``ci`` is the real part of the\n",
      "        cosine integral. For such points ``ci(x)`` and ``ci(x + 0j)``\n",
      "        differ by a factor of ``1j*pi``.\n",
      "        \n",
      "        For real arguments the function is computed by calling Cephes'\n",
      "        [1]_ *sici* routine. For complex arguments the algorithm is based\n",
      "        on Mpmath's [2]_ *si* and *ci* routines.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "        .. [2] Fredrik Johansson and others.\n",
      "               \"mpmath: a Python library for arbitrary-precision floating-point arithmetic\"\n",
      "               (Version 0.19) http://mpmath.org/\n",
      "    \n",
      "    sindg = <ufunc 'sindg'>\n",
      "        sindg(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        sindg(x, out=None)\n",
      "        \n",
      "        Sine of the angle `x` given in degrees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Angle, given in degrees.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Sine at the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cosdg, tandg, cotdg\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is more accurate than using sine directly.\n",
      "        \n",
      "        >>> x = 180 * np.arange(3)\n",
      "        >>> sc.sindg(x)\n",
      "        array([ 0., -0.,  0.])\n",
      "        >>> np.sin(x * np.pi / 180)\n",
      "        array([ 0.0000000e+00,  1.2246468e-16, -2.4492936e-16])\n",
      "    \n",
      "    smirnov = <ufunc 'smirnov'>\n",
      "        smirnov(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        smirnov(n, d)\n",
      "        \n",
      "        Kolmogorov-Smirnov complementary cumulative distribution function\n",
      "        \n",
      "        Returns the exact Kolmogorov-Smirnov complementary cumulative\n",
      "        distribution function,(aka the Survival Function) of Dn+ (or Dn-)\n",
      "        for a one-sided test of equality between an empirical and a\n",
      "        theoretical distribution. It is equal to the probability that the\n",
      "        maximum difference between a theoretical distribution and an empirical\n",
      "        one based on `n` samples is greater than d.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "          Number of samples\n",
      "        d : float array_like\n",
      "          Deviation between the Empirical CDF (ECDF) and the target CDF.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            The value(s) of smirnov(n, d), Prob(Dn+ >= d) (Also Prob(Dn- >= d))\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `smirnov` is used by `stats.kstest` in the application of the\n",
      "        Kolmogorov-Smirnov Goodness of Fit test. For historial reasons this\n",
      "        function is exposed in `scpy.special`, but the recommended way to achieve\n",
      "        the most accurate CDF/SF/PDF/PPF/ISF computations is to use the\n",
      "        `stats.ksone` distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        smirnovi : The Inverse Survival Function for the distribution\n",
      "        scipy.stats.ksone : Provides the functionality as a continuous distribution\n",
      "        kolmogorov, kolmogi : Functions for the two-sided distribution\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import smirnov\n",
      "        \n",
      "        Show the probability of a gap at least as big as 0, 0.5 and 1.0 for a sample of size 5\n",
      "        \n",
      "        >>> smirnov(5, [0, 0.5, 1.0])\n",
      "        array([ 1.   ,  0.056,  0.   ])\n",
      "        \n",
      "        Compare a sample of size 5 drawn from a source N(0.5, 1) distribution against\n",
      "        a target N(0, 1) CDF.\n",
      "        \n",
      "        >>> from scipy.stats import norm\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> n = 5\n",
      "        >>> gendist = norm(0.5, 1)       # Normal distribution, mean 0.5, stddev 1\n",
      "        >>> x = np.sort(gendist.rvs(size=n, random_state=rng))\n",
      "        >>> x\n",
      "        array([-1.3922078 , -0.13526532,  0.1371477 ,  0.18981686,  1.81948167])\n",
      "        >>> target = norm(0, 1)\n",
      "        >>> cdfs = target.cdf(x)\n",
      "        >>> cdfs\n",
      "        array([0.08192974, 0.44620105, 0.55454297, 0.57527368, 0.96558101])\n",
      "        # Construct the Empirical CDF and the K-S statistics (Dn+, Dn-, Dn)\n",
      "        >>> ecdfs = np.arange(n+1, dtype=float)/n\n",
      "        >>> cols = np.column_stack([x, ecdfs[1:], cdfs, cdfs - ecdfs[:n], ecdfs[1:] - cdfs])\n",
      "        >>> np.set_printoptions(precision=3)\n",
      "        >>> cols\n",
      "        array([[-1.392,  0.2  ,  0.082,  0.082,  0.118],\n",
      "               [-0.135,  0.4  ,  0.446,  0.246, -0.046],\n",
      "               [ 0.137,  0.6  ,  0.555,  0.155,  0.045],\n",
      "               [ 0.19 ,  0.8  ,  0.575, -0.025,  0.225],\n",
      "               [ 1.819,  1.   ,  0.966,  0.166,  0.034]])\n",
      "        >>> gaps = cols[:, -2:]\n",
      "        >>> Dnpm = np.max(gaps, axis=0)\n",
      "        >>> print('Dn-=%f, Dn+=%f' % (Dnpm[0], Dnpm[1]))\n",
      "        Dn-=0.246201, Dn+=0.224726\n",
      "        >>> probs = smirnov(n, Dnpm)\n",
      "        >>> print(chr(10).join(['For a sample of size %d drawn from a N(0, 1) distribution:' % n,\n",
      "        ...      ' Smirnov n=%d: Prob(Dn- >= %f) = %.4f' % (n, Dnpm[0], probs[0]),\n",
      "        ...      ' Smirnov n=%d: Prob(Dn+ >= %f) = %.4f' % (n, Dnpm[1], probs[1])]))\n",
      "        For a sample of size 5 drawn from a N(0, 1) distribution:\n",
      "         Smirnov n=5: Prob(Dn- >= 0.246201) = 0.4713\n",
      "         Smirnov n=5: Prob(Dn+ >= 0.224726) = 0.5243\n",
      "        \n",
      "        Plot the Empirical CDF against the target N(0, 1) CDF\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.step(np.concatenate([[-3], x]), ecdfs, where='post', label='Empirical CDF')\n",
      "        >>> x3 = np.linspace(-3, 3, 100)\n",
      "        >>> plt.plot(x3, target.cdf(x3), label='CDF for N(0, 1)')\n",
      "        >>> plt.ylim([0, 1]); plt.grid(True); plt.legend();\n",
      "        # Add vertical lines marking Dn+ and Dn-\n",
      "        >>> iminus, iplus = np.argmax(gaps, axis=0)\n",
      "        >>> plt.vlines([x[iminus]], ecdfs[iminus], cdfs[iminus], color='r', linestyle='dashed', lw=4)\n",
      "        >>> plt.vlines([x[iplus]], cdfs[iplus], ecdfs[iplus+1], color='m', linestyle='dashed', lw=4)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    smirnovi = <ufunc 'smirnovi'>\n",
      "        smirnovi(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        smirnovi(n, p)\n",
      "        \n",
      "        Inverse to `smirnov`\n",
      "        \n",
      "        Returns `d` such that ``smirnov(n, d) == p``, the critical value\n",
      "        corresponding to `p`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "          Number of samples\n",
      "        p : float array_like\n",
      "            Probability\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        float\n",
      "            The value(s) of smirnovi(n, p), the critical values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `smirnov` is used by `stats.kstest` in the application of the\n",
      "        Kolmogorov-Smirnov Goodness of Fit test. For historial reasons this\n",
      "        function is exposed in `scpy.special`, but the recommended way to achieve\n",
      "        the most accurate CDF/SF/PDF/PPF/ISF computations is to use the\n",
      "        `stats.ksone` distribution.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        smirnov  : The Survival Function (SF) for the distribution\n",
      "        scipy.stats.ksone : Provides the functionality as a continuous distribution\n",
      "        kolmogorov, kolmogi, scipy.stats.kstwobign : Functions for the two-sided distribution\n",
      "    \n",
      "    spence = <ufunc 'spence'>\n",
      "        spence(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        spence(z, out=None)\n",
      "        \n",
      "        Spence's function, also known as the dilogarithm.\n",
      "        \n",
      "        It is defined to be\n",
      "        \n",
      "        .. math::\n",
      "          \\int_1^z \\frac{\\log(t)}{1 - t}dt\n",
      "        \n",
      "        for complex :math:`z`, where the contour of integration is taken\n",
      "        to avoid the branch cut of the logarithm. Spence's function is\n",
      "        analytic everywhere except the negative real axis where it has a\n",
      "        branch cut.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Points at which to evaluate Spence's function\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        s : ndarray\n",
      "            Computed values of Spence's function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        There is a different convention which defines Spence's function by\n",
      "        the integral\n",
      "        \n",
      "        .. math::\n",
      "          -\\int_0^z \\frac{\\log(1 - t)}{t}dt;\n",
      "        \n",
      "        this is our ``spence(1 - z)``.\n",
      "    \n",
      "    sph_harm = <ufunc 'sph_harm'>\n",
      "        sph_harm(x1, x2, x3, x4, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        sph_harm(m, n, theta, phi)\n",
      "        \n",
      "        Compute spherical harmonics.\n",
      "        \n",
      "        The spherical harmonics are defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            Y^m_n(\\theta,\\phi) = \\sqrt{\\frac{2n+1}{4\\pi} \\frac{(n-m)!}{(n+m)!}}\n",
      "              e^{i m \\theta} P^m_n(\\cos(\\phi))\n",
      "        \n",
      "        where :math:`P_n^m` are the associated Legendre functions; see `lpmv`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        m : array_like\n",
      "            Order of the harmonic (int); must have ``|m| <= n``.\n",
      "        n : array_like\n",
      "           Degree of the harmonic (int); must have ``n >= 0``. This is\n",
      "           often denoted by ``l`` (lower case L) in descriptions of\n",
      "           spherical harmonics.\n",
      "        theta : array_like\n",
      "           Azimuthal (longitudinal) coordinate; must be in ``[0, 2*pi]``.\n",
      "        phi : array_like\n",
      "           Polar (colatitudinal) coordinate; must be in ``[0, pi]``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        y_mn : complex float\n",
      "           The harmonic :math:`Y^m_n` sampled at ``theta`` and ``phi``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        There are different conventions for the meanings of the input\n",
      "        arguments ``theta`` and ``phi``. In SciPy ``theta`` is the\n",
      "        azimuthal angle and ``phi`` is the polar angle. It is common to\n",
      "        see the opposite convention, that is, ``theta`` as the polar angle\n",
      "        and ``phi`` as the azimuthal angle.\n",
      "        \n",
      "        Note that SciPy's spherical harmonics include the Condon-Shortley\n",
      "        phase [2]_ because it is part of `lpmv`.\n",
      "        \n",
      "        With SciPy's conventions, the first several spherical harmonics\n",
      "        are\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            Y_0^0(\\theta, \\phi) &= \\frac{1}{2} \\sqrt{\\frac{1}{\\pi}} \\\\\n",
      "            Y_1^{-1}(\\theta, \\phi) &= \\frac{1}{2} \\sqrt{\\frac{3}{2\\pi}}\n",
      "                                        e^{-i\\theta} \\sin(\\phi) \\\\\n",
      "            Y_1^0(\\theta, \\phi) &= \\frac{1}{2} \\sqrt{\\frac{3}{\\pi}}\n",
      "                                     \\cos(\\phi) \\\\\n",
      "            Y_1^1(\\theta, \\phi) &= -\\frac{1}{2} \\sqrt{\\frac{3}{2\\pi}}\n",
      "                                     e^{i\\theta} \\sin(\\phi).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Digital Library of Mathematical Functions, 14.30.\n",
      "               https://dlmf.nist.gov/14.30\n",
      "        .. [2] https://en.wikipedia.org/wiki/Spherical_harmonics#Condon.E2.80.93Shortley_phase\n",
      "    \n",
      "    stdtr = <ufunc 'stdtr'>\n",
      "        stdtr(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        stdtr(df, t)\n",
      "        \n",
      "        Student t distribution cumulative distribution function\n",
      "        \n",
      "        Returns the integral from minus infinity to t of the Student t\n",
      "        distribution with df > 0 degrees of freedom::\n",
      "        \n",
      "           gamma((df+1)/2)/(sqrt(df*pi)*gamma(df/2)) *\n",
      "           integral((1+x**2/df)**(-df/2-1/2), x=-inf..t)\n",
      "    \n",
      "    stdtridf = <ufunc 'stdtridf'>\n",
      "        stdtridf(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        stdtridf(p, t)\n",
      "        \n",
      "        Inverse of `stdtr` vs df\n",
      "        \n",
      "        Returns the argument df such that stdtr(df, t) is equal to `p`.\n",
      "    \n",
      "    stdtrit = <ufunc 'stdtrit'>\n",
      "        stdtrit(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        stdtrit(df, p)\n",
      "        \n",
      "        Inverse of `stdtr` vs `t`\n",
      "        \n",
      "        Returns the argument `t` such that stdtr(df, t) is equal to `p`.\n",
      "    \n",
      "    struve = <ufunc 'struve'>\n",
      "        struve(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        struve(v, x)\n",
      "        \n",
      "        Struve function.\n",
      "        \n",
      "        Return the value of the Struve function of order `v` at `x`.  The Struve\n",
      "        function is defined as,\n",
      "        \n",
      "        .. math::\n",
      "            H_v(x) = (z/2)^{v + 1} \\sum_{n=0}^\\infty \\frac{(-1)^n (z/2)^{2n}}{\\Gamma(n + \\frac{3}{2}) \\Gamma(n + v + \\frac{3}{2})},\n",
      "        \n",
      "        where :math:`\\Gamma` is the gamma function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order of the Struve function (float).\n",
      "        x : array_like\n",
      "            Argument of the Struve function (float; must be positive unless `v` is\n",
      "            an integer).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        H : ndarray\n",
      "            Value of the Struve function of order `v` at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Three methods discussed in [1]_ are used to evaluate the Struve function:\n",
      "        \n",
      "        - power series\n",
      "        - expansion in Bessel functions (if :math:`|z| < |v| + 20`)\n",
      "        - asymptotic large-z expansion (if :math:`z \\geq 0.7v + 12`)\n",
      "        \n",
      "        Rounding errors are estimated based on the largest terms in the sums, and\n",
      "        the result associated with the smallest error is returned.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        modstruve\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] NIST Digital Library of Mathematical Functions\n",
      "               https://dlmf.nist.gov/11\n",
      "    \n",
      "    tandg = <ufunc 'tandg'>\n",
      "        tandg(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        tandg(x, out=None)\n",
      "        \n",
      "        Tangent of angle `x` given in degrees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Angle, given in degrees.\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function results.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            Tangent at the input.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sindg, cosdg, cotdg\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import scipy.special as sc\n",
      "        \n",
      "        It is more accurate than using tangent directly.\n",
      "        \n",
      "        >>> x = 180 * np.arange(3)\n",
      "        >>> sc.tandg(x)\n",
      "        array([0., 0., 0.])\n",
      "        >>> np.tan(x * np.pi / 180)\n",
      "        array([ 0.0000000e+00, -1.2246468e-16, -2.4492936e-16])\n",
      "    \n",
      "    tklmbda = <ufunc 'tklmbda'>\n",
      "        tklmbda(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        tklmbda(x, lmbda)\n",
      "        \n",
      "        Tukey-Lambda cumulative distribution function\n",
      "    \n",
      "    voigt_profile = <ufunc 'voigt_profile'>\n",
      "        voigt_profile(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        voigt_profile(x, sigma, gamma, out=None)\n",
      "        \n",
      "        Voigt profile.\n",
      "        \n",
      "        The Voigt profile is a convolution of a 1-D Normal distribution with\n",
      "        standard deviation ``sigma`` and a 1-D Cauchy distribution with half-width at\n",
      "        half-maximum ``gamma``.\n",
      "        \n",
      "        If ``sigma = 0``, PDF of Cauchy distribution is returned.\n",
      "        Conversely, if ``gamma = 0``, PDF of Normal distribution is returned.\n",
      "        If ``sigma = gamma = 0``, the return value is ``Inf`` for ``x = 0``, and ``0`` for all other ``x``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Real argument\n",
      "        sigma : array_like\n",
      "            The standard deviation of the Normal distribution part\n",
      "        gamma : array_like\n",
      "            The half-width at half-maximum of the Cauchy distribution part\n",
      "        out : ndarray, optional\n",
      "            Optional output array for the function values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scalar or ndarray\n",
      "            The Voigt profile at the given arguments\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        It can be expressed in terms of Faddeeva function\n",
      "        \n",
      "        .. math:: V(x; \\sigma, \\gamma) = \\frac{Re[w(z)]}{\\sigma\\sqrt{2\\pi}},\n",
      "        .. math:: z = \\frac{x + i\\gamma}{\\sqrt{2}\\sigma}\n",
      "        \n",
      "        where :math:`w(z)` is the Faddeeva function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        wofz : Faddeeva function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] https://en.wikipedia.org/wiki/Voigt_profile\n",
      "    \n",
      "    wofz = <ufunc 'wofz'>\n",
      "        wofz(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        wofz(z)\n",
      "        \n",
      "        Faddeeva function\n",
      "        \n",
      "        Returns the value of the Faddeeva function for complex argument::\n",
      "        \n",
      "            exp(-z**2) * erfc(-i*z)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dawsn, erf, erfc, erfcx, erfi\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Steven G. Johnson, Faddeeva W function implementation.\n",
      "           http://ab-initio.mit.edu/Faddeeva\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import special\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> x = np.linspace(-3, 3)\n",
      "        >>> z = special.wofz(x)\n",
      "        \n",
      "        >>> plt.plot(x, z.real, label='wofz(x).real')\n",
      "        >>> plt.plot(x, z.imag, label='wofz(x).imag')\n",
      "        >>> plt.xlabel('$x$')\n",
      "        >>> plt.legend(framealpha=1, shadow=True)\n",
      "        >>> plt.grid(alpha=0.25)\n",
      "        >>> plt.show()\n",
      "    \n",
      "    wright_bessel = <ufunc 'wright_bessel'>\n",
      "        wright_bessel(x1, x2, x3, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        wright_bessel(a, b, x)\n",
      "        \n",
      "        Wright's generalized Bessel function.\n",
      "        \n",
      "        Wright's generalized Bessel function is an entire function and defined as\n",
      "        \n",
      "        .. math:: \\Phi(a, b; x) = \\sum_{k=0}^\\infty \\frac{x^k}{k! \\Gamma(a k + b)}\n",
      "        \n",
      "        See also [1].\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : array_like of float\n",
      "            a >= 0\n",
      "        b : array_like of float\n",
      "            b >= 0\n",
      "        x : array_like of float\n",
      "            x >= 0\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Due to the compexity of the function with its three parameters, only\n",
      "        non-negative arguments are implemented.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import wright_bessel\n",
      "        >>> a, b, x = 1.5, 1.1, 2.5\n",
      "        >>> wright_bessel(a, b-1, x)\n",
      "        4.5314465939443025\n",
      "        \n",
      "        Now, let us verify the relation\n",
      "        \n",
      "        .. math:: \\Phi(a, b-1; x) = a x \\Phi(a, b+a; x) + (b-1) \\Phi(a, b; x)\n",
      "        \n",
      "        >>> a * x * wright_bessel(a, b+a, x) + (b-1) * wright_bessel(a, b, x)\n",
      "        4.5314465939443025\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Digital Library of Mathematical Functions, 10.46.\n",
      "               https://dlmf.nist.gov/10.46.E1\n",
      "    \n",
      "    wrightomega = <ufunc 'wrightomega'>\n",
      "        wrightomega(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        wrightomega(z, out=None)\n",
      "        \n",
      "        Wright Omega function.\n",
      "        \n",
      "        Defined as the solution to\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\omega + \\log(\\omega) = z\n",
      "        \n",
      "        where :math:`\\log` is the principal branch of the complex logarithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        z : array_like\n",
      "            Points at which to evaluate the Wright Omega function\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        omega : ndarray\n",
      "            Values of the Wright Omega function\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.19.0\n",
      "        \n",
      "        The function can also be defined as\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\omega(z) = W_{K(z)}(e^z)\n",
      "        \n",
      "        where :math:`K(z) = \\lceil (\\Im(z) - \\pi)/(2\\pi) \\rceil` is the\n",
      "        unwinding number and :math:`W` is the Lambert W function.\n",
      "        \n",
      "        The implementation here is taken from [1]_.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lambertw : The Lambert W function\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Lawrence, Corless, and Jeffrey, \"Algorithm 917: Complex\n",
      "               Double-Precision Evaluation of the Wright :math:`\\omega`\n",
      "               Function.\" ACM Transactions on Mathematical Software,\n",
      "               2012. :doi:`10.1145/2168773.2168779`.\n",
      "    \n",
      "    xlog1py = <ufunc 'xlog1py'>\n",
      "        xlog1py(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        xlog1py(x, y)\n",
      "        \n",
      "        Compute ``x*log1p(y)`` so that the result is 0 if ``x = 0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Multiplier\n",
      "        y : array_like\n",
      "            Argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : array_like\n",
      "            Computed x*log1p(y)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.13.0\n",
      "    \n",
      "    xlogy = <ufunc 'xlogy'>\n",
      "        xlogy(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        xlogy(x, y)\n",
      "        \n",
      "        Compute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Multiplier\n",
      "        y : array_like\n",
      "            Argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : array_like\n",
      "            Computed x*log(y)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        .. versionadded:: 0.13.0\n",
      "    \n",
      "    y0 = <ufunc 'y0'>\n",
      "        y0(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        y0(x)\n",
      "        \n",
      "        Bessel function of the second kind of order 0.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            Value of the Bessel function of the second kind of order 0 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        The domain is divided into the intervals [0, 5] and (5, infinity). In the\n",
      "        first interval a rational approximation :math:`R(x)` is employed to\n",
      "        compute,\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            Y_0(x) = R(x) + \\frac{2 \\log(x) J_0(x)}{\\pi},\n",
      "        \n",
      "        where :math:`J_0` is the Bessel function of the first kind of order 0.\n",
      "        \n",
      "        In the second interval, the Hankel asymptotic expansion is employed with\n",
      "        two rational functions of degree 6/6 and 7/7.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `y0`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        j0\n",
      "        yv\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    y1 = <ufunc 'y1'>\n",
      "        y1(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        y1(x)\n",
      "        \n",
      "        Bessel function of the second kind of order 1.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            Argument (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            Value of the Bessel function of the second kind of order 1 at `x`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        The domain is divided into the intervals [0, 8] and (8, infinity). In the\n",
      "        first interval a 25 term Chebyshev expansion is used, and computing\n",
      "        :math:`J_1` (the Bessel function of the first kind) is required. In the\n",
      "        second, the asymptotic trigonometric representation is employed using two\n",
      "        rational functions of degree 5/5.\n",
      "        \n",
      "        This function is a wrapper for the Cephes [1]_ routine `y1`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        j1\n",
      "        yn\n",
      "        yv\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    yn = <ufunc 'yn'>\n",
      "        yn(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        yn(n, x)\n",
      "        \n",
      "        Bessel function of the second kind of integer order and real argument.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : array_like\n",
      "            Order (integer).\n",
      "        z : array_like\n",
      "            Argument (float).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            Value of the Bessel function, :math:`Y_n(x)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Wrapper for the Cephes [1]_ routine `yn`.\n",
      "        \n",
      "        The function is evaluated by forward recurrence on `n`, starting with\n",
      "        values computed by the Cephes routines `y0` and `y1`. If `n = 0` or 1,\n",
      "        the routine for `y0` or `y1` is called directly.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        yv : For real order and real or complex argument.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Cephes Mathematical Functions Library,\n",
      "               http://www.netlib.org/cephes/\n",
      "    \n",
      "    yv = <ufunc 'yv'>\n",
      "        yv(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        yv(v, z)\n",
      "        \n",
      "        Bessel function of the second kind of real order and complex argument.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            Value of the Bessel function of the second kind, :math:`Y_v(x)`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For positive `v` values, the computation is carried out using the\n",
      "        AMOS [1]_ `zbesy` routine, which exploits the connection to the Hankel\n",
      "        Bessel functions :math:`H_v^{(1)}` and :math:`H_v^{(2)}`,\n",
      "        \n",
      "        .. math:: Y_v(z) = \\frac{1}{2\\imath} (H_v^{(1)} - H_v^{(2)}).\n",
      "        \n",
      "        For negative `v` values the formula,\n",
      "        \n",
      "        .. math:: Y_{-v}(z) = Y_v(z) \\cos(\\pi v) + J_v(z) \\sin(\\pi v)\n",
      "        \n",
      "        is used, where :math:`J_v(z)` is the Bessel function of the first kind,\n",
      "        computed using the AMOS routine `zbesj`.  Note that the second term is\n",
      "        exactly zero for integer `v`; to improve accuracy the second term is\n",
      "        explicitly omitted for `v` values such that `v = floor(v)`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        yve : :math:`Y_v` with leading exponential behavior stripped off.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    yve = <ufunc 'yve'>\n",
      "        yve(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        yve(v, z)\n",
      "        \n",
      "        Exponentially scaled Bessel function of the second kind of real order.\n",
      "        \n",
      "        Returns the exponentially scaled Bessel function of the second\n",
      "        kind of real order `v` at complex `z`::\n",
      "        \n",
      "            yve(v, z) = yv(v, z) * exp(-abs(z.imag))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        v : array_like\n",
      "            Order (float).\n",
      "        z : array_like\n",
      "            Argument (float or complex).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Y : ndarray\n",
      "            Value of the exponentially scaled Bessel function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For positive `v` values, the computation is carried out using the\n",
      "        AMOS [1]_ `zbesy` routine, which exploits the connection to the Hankel\n",
      "        Bessel functions :math:`H_v^{(1)}` and :math:`H_v^{(2)}`,\n",
      "        \n",
      "        .. math:: Y_v(z) = \\frac{1}{2\\imath} (H_v^{(1)} - H_v^{(2)}).\n",
      "        \n",
      "        For negative `v` values the formula,\n",
      "        \n",
      "        .. math:: Y_{-v}(z) = Y_v(z) \\cos(\\pi v) + J_v(z) \\sin(\\pi v)\n",
      "        \n",
      "        is used, where :math:`J_v(z)` is the Bessel function of the first kind,\n",
      "        computed using the AMOS routine `zbesj`.  Note that the second term is\n",
      "        exactly zero for integer `v`; to improve accuracy the second term is\n",
      "        explicitly omitted for `v` values such that `v = floor(v)`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Donald E. Amos, \"AMOS, A Portable Package for Bessel Functions\n",
      "               of a Complex Argument and Nonnegative Order\",\n",
      "               http://netlib.org/amos/\n",
      "    \n",
      "    zetac = <ufunc 'zetac'>\n",
      "        zetac(x, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n",
      "        \n",
      "        zetac(x)\n",
      "        \n",
      "        Riemann zeta function minus 1.\n",
      "        \n",
      "        This function is defined as\n",
      "        \n",
      "        .. math:: \\zeta(x) = \\sum_{k=2}^{\\infty} 1 / k^x,\n",
      "        \n",
      "        where ``x > 1``.  For ``x < 1`` the analytic continuation is\n",
      "        computed. For more information on the Riemann zeta function, see\n",
      "        [dlmf]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like of float\n",
      "            Values at which to compute zeta(x) - 1 (must be real).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : array_like\n",
      "            Values of zeta(x) - 1.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        zeta\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.special import zetac, zeta\n",
      "        \n",
      "        Some special values:\n",
      "        \n",
      "        >>> zetac(2), np.pi**2/6 - 1\n",
      "        (0.64493406684822641, 0.6449340668482264)\n",
      "        \n",
      "        >>> zetac(-1), -1.0/12 - 1\n",
      "        (-1.0833333333333333, -1.0833333333333333)\n",
      "        \n",
      "        Compare ``zetac(x)`` to ``zeta(x) - 1`` for large `x`:\n",
      "        \n",
      "        >>> zetac(60), zeta(60) - 1\n",
      "        (8.673617380119933e-19, 0.0)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [dlmf] NIST Digital Library of Mathematical Functions\n",
      "                  https://dlmf.nist.gov/25\n",
      "\n",
      "FILE\n",
      "    c:\\users\\amant\\anaconda3\\lib\\site-packages\\scipy\\special\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968c5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42183076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BFGS',\n",
       " 'Bounds',\n",
       " 'HessianUpdateStrategy',\n",
       " 'LbfgsInvHessProduct',\n",
       " 'LinearConstraint',\n",
       " 'NonlinearConstraint',\n",
       " 'OptimizeResult',\n",
       " 'OptimizeWarning',\n",
       " 'RootResults',\n",
       " 'SR1',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__nnls',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_basinhopping',\n",
       " '_bglu_dense',\n",
       " '_cobyla',\n",
       " '_constraints',\n",
       " '_differentiable_functions',\n",
       " '_differentialevolution',\n",
       " '_dual_annealing',\n",
       " '_group_columns',\n",
       " '_hessian_update_strategy',\n",
       " '_highs',\n",
       " '_lbfgsb',\n",
       " '_linprog',\n",
       " '_linprog_doc',\n",
       " '_linprog_highs',\n",
       " '_linprog_ip',\n",
       " '_linprog_rs',\n",
       " '_linprog_simplex',\n",
       " '_linprog_util',\n",
       " '_lsap',\n",
       " '_lsap_module',\n",
       " '_lsq',\n",
       " '_minimize',\n",
       " '_minpack',\n",
       " '_nnls',\n",
       " '_numdiff',\n",
       " '_qap',\n",
       " '_remove_redundancy',\n",
       " '_root',\n",
       " '_root_scalar',\n",
       " '_shgo',\n",
       " '_shgo_lib',\n",
       " '_slsqp',\n",
       " '_spectral',\n",
       " '_trlib',\n",
       " '_trustregion',\n",
       " '_trustregion_constr',\n",
       " '_trustregion_dogleg',\n",
       " '_trustregion_exact',\n",
       " '_trustregion_krylov',\n",
       " '_trustregion_ncg',\n",
       " '_zeros',\n",
       " 'anderson',\n",
       " 'approx_fprime',\n",
       " 'basinhopping',\n",
       " 'bisect',\n",
       " 'bracket',\n",
       " 'brent',\n",
       " 'brenth',\n",
       " 'brentq',\n",
       " 'broyden1',\n",
       " 'broyden2',\n",
       " 'brute',\n",
       " 'check_grad',\n",
       " 'cobyla',\n",
       " 'curve_fit',\n",
       " 'diagbroyden',\n",
       " 'differential_evolution',\n",
       " 'dual_annealing',\n",
       " 'excitingmixing',\n",
       " 'fixed_point',\n",
       " 'fmin',\n",
       " 'fmin_bfgs',\n",
       " 'fmin_cg',\n",
       " 'fmin_cobyla',\n",
       " 'fmin_l_bfgs_b',\n",
       " 'fmin_ncg',\n",
       " 'fmin_powell',\n",
       " 'fmin_slsqp',\n",
       " 'fmin_tnc',\n",
       " 'fminbound',\n",
       " 'fsolve',\n",
       " 'golden',\n",
       " 'lbfgsb',\n",
       " 'least_squares',\n",
       " 'leastsq',\n",
       " 'line_search',\n",
       " 'linear_sum_assignment',\n",
       " 'linearmixing',\n",
       " 'linesearch',\n",
       " 'linprog',\n",
       " 'linprog_verbose_callback',\n",
       " 'lsq_linear',\n",
       " 'minimize',\n",
       " 'minimize_scalar',\n",
       " 'minpack',\n",
       " 'minpack2',\n",
       " 'moduleTNC',\n",
       " 'newton',\n",
       " 'newton_krylov',\n",
       " 'nnls',\n",
       " 'nonlin',\n",
       " 'optimize',\n",
       " 'quadratic_assignment',\n",
       " 'ridder',\n",
       " 'root',\n",
       " 'root_scalar',\n",
       " 'rosen',\n",
       " 'rosen_der',\n",
       " 'rosen_hess',\n",
       " 'rosen_hess_prod',\n",
       " 'shgo',\n",
       " 'show_options',\n",
       " 'slsqp',\n",
       " 'test',\n",
       " 'tnc',\n",
       " 'toms748',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "156189f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.optimize in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.optimize\n",
      "\n",
      "DESCRIPTION\n",
      "    =====================================================\n",
      "    Optimization and root finding (:mod:`scipy.optimize`)\n",
      "    =====================================================\n",
      "    \n",
      "    .. currentmodule:: scipy.optimize\n",
      "    \n",
      "    SciPy ``optimize`` provides functions for minimizing (or maximizing)\n",
      "    objective functions, possibly subject to constraints. It includes\n",
      "    solvers for nonlinear problems (with support for both local and global\n",
      "    optimization algorithms), linear programing, constrained\n",
      "    and nonlinear least-squares, root finding, and curve fitting.\n",
      "    \n",
      "    Common functions and objects, shared across different solvers, are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       show_options - Show specific options optimization solvers.\n",
      "       OptimizeResult - The optimization result returned by some optimizers.\n",
      "       OptimizeWarning - The optimization encountered problems.\n",
      "    \n",
      "    \n",
      "    Optimization\n",
      "    ============\n",
      "    \n",
      "    Scalar functions optimization\n",
      "    -----------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize_scalar - Interface for minimizers of univariate functions\n",
      "    \n",
      "    The `minimize_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize_scalar-brent\n",
      "       optimize.minimize_scalar-bounded\n",
      "       optimize.minimize_scalar-golden\n",
      "    \n",
      "    Local (multivariate) optimization\n",
      "    ---------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize - Interface for minimizers of multivariate functions.\n",
      "    \n",
      "    The `minimize` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize-neldermead\n",
      "       optimize.minimize-powell\n",
      "       optimize.minimize-cg\n",
      "       optimize.minimize-bfgs\n",
      "       optimize.minimize-newtoncg\n",
      "       optimize.minimize-lbfgsb\n",
      "       optimize.minimize-tnc\n",
      "       optimize.minimize-cobyla\n",
      "       optimize.minimize-slsqp\n",
      "       optimize.minimize-trustconstr\n",
      "       optimize.minimize-dogleg\n",
      "       optimize.minimize-trustncg\n",
      "       optimize.minimize-trustkrylov\n",
      "       optimize.minimize-trustexact\n",
      "    \n",
      "    Constraints are passed to `minimize` function as a single object or\n",
      "    as a list of objects from the following classes:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       NonlinearConstraint - Class defining general nonlinear constraints.\n",
      "       LinearConstraint - Class defining general linear constraints.\n",
      "    \n",
      "    Simple bound constraints are handled separately and there is a special class\n",
      "    for them:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       Bounds - Bound constraints.\n",
      "    \n",
      "    Quasi-Newton strategies implementing `HessianUpdateStrategy`\n",
      "    interface can be used to approximate the Hessian in `minimize`\n",
      "    function (available only for the 'trust-constr' method). Available\n",
      "    quasi-Newton methods implementing this interface are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       BFGS - Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "       SR1 - Symmetric-rank-1 Hessian update strategy.\n",
      "    \n",
      "    Global optimization\n",
      "    -------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       basinhopping - Basinhopping stochastic optimizer.\n",
      "       brute - Brute force searching optimizer.\n",
      "       differential_evolution - stochastic minimization using differential evolution.\n",
      "    \n",
      "       shgo - simplicial homology global optimisation\n",
      "       dual_annealing - Dual annealing stochastic optimizer.\n",
      "    \n",
      "    \n",
      "    Least-squares and curve fitting\n",
      "    ===============================\n",
      "    \n",
      "    Nonlinear least-squares\n",
      "    -----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       least_squares - Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "    \n",
      "    Linear least-squares\n",
      "    --------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       nnls - Linear least-squares problem with non-negativity constraint.\n",
      "       lsq_linear - Linear least-squares problem with bound constraints.\n",
      "    \n",
      "    Curve fitting\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       curve_fit -- Fit curve to a set of points.\n",
      "    \n",
      "    Root finding\n",
      "    ============\n",
      "    \n",
      "    Scalar functions\n",
      "    ----------------\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root_scalar - Unified interface for nonlinear solvers of scalar functions.\n",
      "       brentq - quadratic interpolation Brent method.\n",
      "       brenth - Brent method, modified by Harris with hyperbolic extrapolation.\n",
      "       ridder - Ridder's method.\n",
      "       bisect - Bisection method.\n",
      "       newton - Newton's method (also Secant and Halley's methods).\n",
      "       toms748 - Alefeld, Potra & Shi Algorithm 748.\n",
      "       RootResults - The root finding result returned by some root finders.\n",
      "    \n",
      "    The `root_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root_scalar-brentq\n",
      "       optimize.root_scalar-brenth\n",
      "       optimize.root_scalar-bisect\n",
      "       optimize.root_scalar-ridder\n",
      "       optimize.root_scalar-newton\n",
      "       optimize.root_scalar-toms748\n",
      "       optimize.root_scalar-secant\n",
      "       optimize.root_scalar-halley\n",
      "    \n",
      "    \n",
      "    \n",
      "    The table below lists situations and appropriate methods, along with\n",
      "    *asymptotic* convergence rates per iteration (and per function evaluation)\n",
      "    for successful convergence to a simple root(*).\n",
      "    Bisection is the slowest of them all, adding one bit of accuracy for each\n",
      "    function evaluation, but is guaranteed to converge.\n",
      "    The other bracketing methods all (eventually) increase the number of accurate\n",
      "    bits by about 50% for every function evaluation.\n",
      "    The derivative-based methods, all built on `newton`, can converge quite quickly\n",
      "    if the initial value is close to the root.  They can also be applied to\n",
      "    functions defined on (a subset of) the complex plane.\n",
      "    \n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | Domain of f | Bracket? |    Derivatives?      | Solvers     |        Convergence           |\n",
      "    +             +          +----------+-----------+             +-------------+----------------+\n",
      "    |             |          | `fprime` | `fprime2` |             | Guaranteed? |  Rate(s)(*)    |\n",
      "    +=============+==========+==========+===========+=============+=============+================+\n",
      "    | `R`         | Yes      | N/A      | N/A       | - bisection | - Yes       | - 1 \"Linear\"   |\n",
      "    |             |          |          |           | - brentq    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - brenth    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - ridder    | - Yes       | - 2.0 (1.41)   |\n",
      "    |             |          |          |           | - toms748   | - Yes       | - 2.7 (1.65)   |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | No       | No        | secant      | No          | 1.62 (1.62)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | No        | newton      | No          | 2.00 (1.41)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | Yes       | halley      | No          | 3.00 (1.44)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "       `scipy.optimize.cython_optimize` -- Typed Cython versions of zeros functions\n",
      "    \n",
      "    Fixed point finding:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fixed_point - Single-variable fixed-point solver.\n",
      "    \n",
      "    Multidimensional\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root - Unified interface for nonlinear solvers of multivariate functions.\n",
      "    \n",
      "    The `root` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root-hybr\n",
      "       optimize.root-lm\n",
      "       optimize.root-broyden1\n",
      "       optimize.root-broyden2\n",
      "       optimize.root-anderson\n",
      "       optimize.root-linearmixing\n",
      "       optimize.root-diagbroyden\n",
      "       optimize.root-excitingmixing\n",
      "       optimize.root-krylov\n",
      "       optimize.root-dfsane\n",
      "    \n",
      "    Linear programming\n",
      "    ==================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog -- Unified interface for minimizers of linear programming problems.\n",
      "    \n",
      "    The `linprog` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.linprog-simplex\n",
      "       optimize.linprog-interior-point\n",
      "       optimize.linprog-revised_simplex\n",
      "       optimize.linprog-highs-ipm\n",
      "       optimize.linprog-highs-ds\n",
      "       optimize.linprog-highs\n",
      "    \n",
      "    The simplex, interior-point, and revised simplex methods support callback\n",
      "    functions, such as:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog_verbose_callback -- Sample callback function for linprog (simplex).\n",
      "    \n",
      "    Assignment problems\n",
      "    ===================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linear_sum_assignment -- Solves the linear-sum assignment problem.\n",
      "       quadratic_assignment -- Solves the quadratic assignment problem.\n",
      "    \n",
      "    The `quadratic_assignment` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.qap-faq\n",
      "       optimize.qap-2opt\n",
      "    \n",
      "    Utilities\n",
      "    =========\n",
      "    \n",
      "    Finite-difference approximation\n",
      "    -------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       approx_fprime - Approximate the gradient of a scalar function.\n",
      "       check_grad - Check the supplied derivative using finite differences.\n",
      "    \n",
      "    \n",
      "    Line search\n",
      "    -----------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       bracket - Bracket a minimum, given two starting points.\n",
      "       line_search - Return a step that satisfies the strong Wolfe conditions.\n",
      "    \n",
      "    Hessian approximation\n",
      "    ---------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       LbfgsInvHessProduct - Linear operator for L-BFGS approximate inverse Hessian.\n",
      "       HessianUpdateStrategy - Interface for implementing Hessian update strategies\n",
      "    \n",
      "    Benchmark problems\n",
      "    ------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rosen - The Rosenbrock function.\n",
      "       rosen_der - The derivative of the Rosenbrock function.\n",
      "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
      "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
      "    \n",
      "    Legacy functions\n",
      "    ================\n",
      "    \n",
      "    The functions below are not recommended for use in new scripts;\n",
      "    all of these methods are accessible via a newer, more consistent\n",
      "    interfaces, provided by the interfaces above.\n",
      "    \n",
      "    Optimization\n",
      "    ------------\n",
      "    \n",
      "    General-purpose multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin - Nelder-Mead Simplex algorithm.\n",
      "       fmin_powell - Powell's (modified) level set method.\n",
      "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm.\n",
      "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno).\n",
      "       fmin_ncg - Line-search Newton Conjugate Gradient.\n",
      "    \n",
      "    Constrained multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer.\n",
      "       fmin_tnc - Truncated Newton code.\n",
      "       fmin_cobyla - Constrained optimization by linear approximation.\n",
      "       fmin_slsqp - Minimization using sequential least-squares programming.\n",
      "    \n",
      "    Univariate (scalar) minimization methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fminbound - Bounded minimization of a scalar function.\n",
      "       brent - 1-D function minimization using Brent method.\n",
      "       golden - 1-D function minimization using Golden Section method.\n",
      "    \n",
      "    Least-squares\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       leastsq - Minimize the sum of squares of M equations in N unknowns.\n",
      "    \n",
      "    Root finding\n",
      "    ------------\n",
      "    \n",
      "    General nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fsolve - Non-linear multivariable equation solver.\n",
      "       broyden1 - Broyden's first method.\n",
      "       broyden2 - Broyden's second method.\n",
      "    \n",
      "    Large-scale nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       newton_krylov\n",
      "       anderson\n",
      "    \n",
      "    Simple iteration solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       excitingmixing\n",
      "       linearmixing\n",
      "       diagbroyden\n",
      "    \n",
      "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __nnls\n",
      "    _basinhopping\n",
      "    _bglu_dense\n",
      "    _cobyla\n",
      "    _constraints\n",
      "    _differentiable_functions\n",
      "    _differentialevolution\n",
      "    _dual_annealing\n",
      "    _group_columns\n",
      "    _hessian_update_strategy\n",
      "    _highs (package)\n",
      "    _lbfgsb\n",
      "    _linprog\n",
      "    _linprog_doc\n",
      "    _linprog_highs\n",
      "    _linprog_ip\n",
      "    _linprog_rs\n",
      "    _linprog_simplex\n",
      "    _linprog_util\n",
      "    _lsap\n",
      "    _lsap_module\n",
      "    _lsq (package)\n",
      "    _minimize\n",
      "    _minpack\n",
      "    _nnls\n",
      "    _numdiff\n",
      "    _qap\n",
      "    _remove_redundancy\n",
      "    _root\n",
      "    _root_scalar\n",
      "    _shgo\n",
      "    _shgo_lib (package)\n",
      "    _slsqp\n",
      "    _spectral\n",
      "    _trlib (package)\n",
      "    _trustregion\n",
      "    _trustregion_constr (package)\n",
      "    _trustregion_dogleg\n",
      "    _trustregion_exact\n",
      "    _trustregion_krylov\n",
      "    _trustregion_ncg\n",
      "    _tstutils\n",
      "    _zeros\n",
      "    cobyla\n",
      "    cython_optimize (package)\n",
      "    lbfgsb\n",
      "    linesearch\n",
      "    minpack\n",
      "    minpack2\n",
      "    moduleTNC\n",
      "    nonlin\n",
      "    optimize\n",
      "    setup\n",
      "    slsqp\n",
      "    tests (package)\n",
      "    tnc\n",
      "    zeros\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        scipy.optimize.optimize.OptimizeWarning\n",
      "    builtins.dict(builtins.object)\n",
      "        scipy.optimize.optimize.OptimizeResult\n",
      "    builtins.object\n",
      "        scipy.optimize._constraints.Bounds\n",
      "        scipy.optimize._constraints.LinearConstraint\n",
      "        scipy.optimize._constraints.NonlinearConstraint\n",
      "        scipy.optimize._hessian_update_strategy.HessianUpdateStrategy\n",
      "        scipy.optimize.zeros.RootResults\n",
      "    scipy.optimize._hessian_update_strategy.FullHessianUpdateStrategy(scipy.optimize._hessian_update_strategy.HessianUpdateStrategy)\n",
      "        scipy.optimize._hessian_update_strategy.BFGS\n",
      "        scipy.optimize._hessian_update_strategy.SR1\n",
      "    scipy.sparse.linalg.interface.LinearOperator(builtins.object)\n",
      "        scipy.optimize.lbfgsb.LbfgsInvHessProduct\n",
      "    \n",
      "    class BFGS(FullHessianUpdateStrategy)\n",
      "     |  BFGS(exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |  \n",
      "     |  Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  exception_strategy : {'skip_update', 'damp_update'}, optional\n",
      "     |      Define how to proceed when the curvature condition is violated.\n",
      "     |      Set it to 'skip_update' to just skip the update. Or, alternatively,\n",
      "     |      set it to 'damp_update' to interpolate between the actual BFGS\n",
      "     |      result and the unmodified matrix. Both exceptions strategies\n",
      "     |      are explained  in [1]_, p.536-537.\n",
      "     |  min_curvature : float\n",
      "     |      This number, scaled by a normalization factor, defines the\n",
      "     |      minimum curvature ``dot(delta_grad, delta_x)`` allowed to go\n",
      "     |      unaffected by the exception strategy. By default is equal to\n",
      "     |      1e-8 when ``exception_strategy = 'skip_update'`` and equal\n",
      "     |      to 0.2 when ``exception_strategy = 'damp_update'``.\n",
      "     |  init_scale : {float, 'auto'}\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.140.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BFGS\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Bounds(builtins.object)\n",
      "     |  Bounds(lb, ub, keep_feasible=False)\n",
      "     |  \n",
      "     |  Bounds constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= x <= ub\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on independent variables. Each array must\n",
      "     |      have the same size as x or be a scalar, in which case a bound will be\n",
      "     |      the same for all the variables. Set components of `lb` and `ub` equal\n",
      "     |      to fix a variable. Use ``np.inf`` with an appropriate sign to disable\n",
      "     |      bounds on all or some variables. Note that you can mix constraints of\n",
      "     |      different types: interval, one-sided or equality, by setting different\n",
      "     |      components of `lb` and `ub` as necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HessianUpdateStrategy(builtins.object)\n",
      "     |  Interface for implementing Hessian update strategies.\n",
      "     |  \n",
      "     |  Many optimization methods make use of Hessian (or inverse Hessian)\n",
      "     |  approximations, such as the quasi-Newton methods BFGS, SR1, L-BFGS.\n",
      "     |  Some of these  approximations, however, do not actually need to store\n",
      "     |  the entire matrix or can compute the internal matrix product with a\n",
      "     |  given vector in a very efficiently manner. This class serves as an\n",
      "     |  abstract interface between the optimization algorithm and the\n",
      "     |  quasi-Newton update strategies, giving freedom of implementation\n",
      "     |  to store and update the internal matrix as efficiently as possible.\n",
      "     |  Different choices of initialization and update procedure will result\n",
      "     |  in different quasi-Newton strategies.\n",
      "     |  \n",
      "     |  Four methods should be implemented in derived classes: ``initialize``,\n",
      "     |  ``update``, ``dot`` and ``get_matrix``.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Any instance of a class that implements this interface,\n",
      "     |  can be accepted by the method ``minimize`` and used by\n",
      "     |  the compatible solvers to approximate the Hessian (or\n",
      "     |  inverse Hessian) used by the optimization algorithms.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      H : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian\n",
      "     |          or its inverse (depending on how 'approx_type'\n",
      "     |          is defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LbfgsInvHessProduct(scipy.sparse.linalg.interface.LinearOperator)\n",
      "     |  LbfgsInvHessProduct(sk, yk)\n",
      "     |  \n",
      "     |  Linear operator for the L-BFGS approximate inverse Hessian.\n",
      "     |  \n",
      "     |  This operator computes the product of a vector with the approximate inverse\n",
      "     |  of the Hessian of the objective function, using the L-BFGS limited\n",
      "     |  memory approximation to the inverse Hessian, accumulated during the\n",
      "     |  optimization.\n",
      "     |  \n",
      "     |  Objects of this class implement the ``scipy.sparse.linalg.LinearOperator``\n",
      "     |  interface.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the solution vector.\n",
      "     |      (See [1]).\n",
      "     |  yk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the gradient. (See [1]).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge. \"Updating quasi-Newton matrices with limited\n",
      "     |     storage.\" Mathematics of computation 35.151 (1980): 773-782.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LbfgsInvHessProduct\n",
      "     |      scipy.sparse.linalg.interface.LinearOperator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sk, yk)\n",
      "     |      Construct the operator.\n",
      "     |  \n",
      "     |  todense(self)\n",
      "     |      Return a dense array representation of this operator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arr : ndarray, shape=(n, n)\n",
      "     |          An array with the same shape and containing\n",
      "     |          the same data represented by this `LinearOperator`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __add__(self, x)\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __matmul__(self, other)\n",
      "     |  \n",
      "     |  __mul__(self, x)\n",
      "     |  \n",
      "     |  __neg__(self)\n",
      "     |  \n",
      "     |  __pow__(self, p)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmatmul__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, x)\n",
      "     |  \n",
      "     |  __sub__(self, x)\n",
      "     |  \n",
      "     |  adjoint(self)\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  dot(self, x)\n",
      "     |      Matrix-matrix or matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          1-d or 2-d array, representing a vector or matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Ax : array\n",
      "     |          1-d or 2-d array (depending on the shape of x) that represents\n",
      "     |          the result of applying this linear operator on x.\n",
      "     |  \n",
      "     |  matmat(self, X)\n",
      "     |      Matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*X where A is an MxN linear\n",
      "     |      operator and X dense N*K matrix or ndarray.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          An array with shape (N,K).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,K) depending on\n",
      "     |          the type of the X argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matmat wraps any user-specified matmat routine or overridden\n",
      "     |      _matmat method to ensure that y has the correct type.\n",
      "     |  \n",
      "     |  matvec(self, x)\n",
      "     |      Matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (N,) or (N,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,) or (M,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matvec wraps the user-specified matvec routine or overridden\n",
      "     |      _matvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  rmatmat(self, X)\n",
      "     |      Adjoint matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array, or 2-d array.\n",
      "     |      The default implementation defers to the adjoint.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          A matrix or 2D array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or 2D array depending on the type of the input.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatmat wraps the user-specified rmatmat routine.\n",
      "     |  \n",
      "     |  rmatvec(self, x)\n",
      "     |      Adjoint matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (M,) or (M,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (N,) or (N,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatvec wraps the user-specified rmatvec routine or overridden\n",
      "     |      _rmatvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  transpose(self)\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  H\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  T\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  ndim = 2\n",
      "    \n",
      "    class LinearConstraint(builtins.object)\n",
      "     |  LinearConstraint(A, lb, ub, keep_feasible=False)\n",
      "     |  \n",
      "     |  Linear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= A.dot(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and the matrix A has shape (m, n).\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  A : {array_like, sparse matrix}, shape (m, n)\n",
      "     |      Matrix defining the constraint.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, A, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NonlinearConstraint(builtins.object)\n",
      "     |  NonlinearConstraint(fun, lb, ub, jac='2-point', hess=<scipy.optimize._hessian_update_strategy.BFGS object at 0x00000181AB596B20>, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |  \n",
      "     |  Nonlinear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= fun(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and ``fun`` returns a vector with m components.\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fun : callable\n",
      "     |      The function defining the constraint.\n",
      "     |      The signature is ``fun(x) -> array_like, shape (m,)``.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  jac : {callable,  '2-point', '3-point', 'cs'}, optional\n",
      "     |      Method of computing the Jacobian matrix (an m-by-n matrix,\n",
      "     |      where element (i, j) is the partial derivative of f[i] with\n",
      "     |      respect to x[j]).  The keywords {'2-point', '3-point',\n",
      "     |      'cs'} select a finite difference scheme for the numerical estimation.\n",
      "     |      A callable must have the following signature:\n",
      "     |      ``jac(x) -> {ndarray, sparse matrix}, shape (m, n)``.\n",
      "     |      Default is '2-point'.\n",
      "     |  hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy, None}, optional\n",
      "     |      Method for computing the Hessian matrix. The keywords\n",
      "     |      {'2-point', '3-point', 'cs'} select a finite difference scheme for\n",
      "     |      numerical  estimation.  Alternatively, objects implementing\n",
      "     |      `HessianUpdateStrategy` interface can be used to approximate the\n",
      "     |      Hessian. Currently available implementations are:\n",
      "     |  \n",
      "     |          - `BFGS` (default option)\n",
      "     |          - `SR1`\n",
      "     |  \n",
      "     |      A callable must return the Hessian matrix of ``dot(fun, v)`` and\n",
      "     |      must have the following signature:\n",
      "     |      ``hess(x, v) -> {LinearOperator, sparse matrix, array_like}, shape (n, n)``.\n",
      "     |      Here ``v`` is ndarray with shape (m,) containing Lagrange multipliers.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  finite_diff_rel_step: None or array_like, optional\n",
      "     |      Relative step size for the finite difference approximation. Default is\n",
      "     |      None, which will select a reasonable value automatically depending\n",
      "     |      on a finite difference scheme.\n",
      "     |  finite_diff_jac_sparsity: {None, array_like, sparse matrix}, optional\n",
      "     |      Defines the sparsity structure of the Jacobian matrix for finite\n",
      "     |      difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "     |      only few non-zero elements in *each* row, providing the sparsity\n",
      "     |      structure will greatly speed up the computations. A zero entry means\n",
      "     |      that a corresponding element in the Jacobian is identically zero.\n",
      "     |      If provided, forces the use of 'lsmr' trust-region solver.\n",
      "     |      If None (default) then dense differencing will be used.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Finite difference schemes {'2-point', '3-point', 'cs'} may be used for\n",
      "     |  approximating either the Jacobian or the Hessian. We, however, do not allow\n",
      "     |  its use for approximating both simultaneously. Hence whenever the Jacobian\n",
      "     |  is estimated via finite-differences, we require the Hessian to be estimated\n",
      "     |  using one of the quasi-Newton strategies.\n",
      "     |  \n",
      "     |  The scheme 'cs' is potentially the most accurate, but requires the function\n",
      "     |  to correctly handles complex inputs and be analytically continuable to the\n",
      "     |  complex plane. The scheme '3-point' is more accurate than '2-point' but\n",
      "     |  requires twice as many operations.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  Constrain ``x[0] < sin(x[1]) + 1.9``\n",
      "     |  \n",
      "     |  >>> from scipy.optimize import NonlinearConstraint\n",
      "     |  >>> con = lambda x: x[0] - np.sin(x[1])\n",
      "     |  >>> nlc = NonlinearConstraint(con, -np.inf, 1.9)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fun, lb, ub, jac='2-point', hess=<scipy.optimize._hessian_update_strategy.BFGS object at 0x00000181AB596B20>, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OptimizeResult(builtins.dict)\n",
      "     |  Represents the optimization result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  x : ndarray\n",
      "     |      The solution of the optimization.\n",
      "     |  success : bool\n",
      "     |      Whether or not the optimizer exited successfully.\n",
      "     |  status : int\n",
      "     |      Termination status of the optimizer. Its value depends on the\n",
      "     |      underlying solver. Refer to `message` for details.\n",
      "     |  message : str\n",
      "     |      Description of the cause of the termination.\n",
      "     |  fun, jac, hess: ndarray\n",
      "     |      Values of objective function, its Jacobian and its Hessian (if\n",
      "     |      available). The Hessians may be approximations, see the documentation\n",
      "     |      of the function in question.\n",
      "     |  hess_inv : object\n",
      "     |      Inverse of the objective function's Hessian; may be an approximation.\n",
      "     |      Not available for all solvers. The type of this attribute may be\n",
      "     |      either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
      "     |  nfev, njev, nhev : int\n",
      "     |      Number of evaluations of the objective functions and of its\n",
      "     |      Jacobian and Hessian.\n",
      "     |  nit : int\n",
      "     |      Number of iterations performed by the optimizer.\n",
      "     |  maxcv : float\n",
      "     |      The maximum constraint violation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There may be additional attributes not listed above depending of the\n",
      "     |  specific solver. Since this class is essentially a subclass of dict\n",
      "     |  with attribute accessors, one can see which attributes are available\n",
      "     |  using the `keys()` method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the dict keys.\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If key is not found, default is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(self, /)\n",
      "     |      Remove and return a (key, value) pair as a 2-tuple.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      "     |      Raises KeyError if the dict is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None, /)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None, /) from builtins.type\n",
      "     |      Create a new dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class OptimizeWarning(builtins.UserWarning)\n",
      "     |  Method resolution order:\n",
      "     |      OptimizeWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class RootResults(builtins.object)\n",
      "     |  RootResults(root, iterations, function_calls, flag)\n",
      "     |  \n",
      "     |  Represents the root finding result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  root : float\n",
      "     |      Estimated root location.\n",
      "     |  iterations : int\n",
      "     |      Number of iterations needed to find the root.\n",
      "     |  function_calls : int\n",
      "     |      Number of times the function was called.\n",
      "     |  converged : bool\n",
      "     |      True if the routine converged.\n",
      "     |  flag : str\n",
      "     |      Description of the cause of termination.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, iterations, function_calls, flag)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SR1(FullHessianUpdateStrategy)\n",
      "     |  SR1(min_denominator=1e-08, init_scale='auto')\n",
      "     |  \n",
      "     |  Symmetric-rank-1 Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  min_denominator : float\n",
      "     |      This number, scaled by a normalization factor,\n",
      "     |      defines the minimum denominator magnitude allowed\n",
      "     |      in the update. When the condition is violated we skip\n",
      "     |      the update. By default uses ``1e-8``.\n",
      "     |  init_scale : {float, 'auto'}, optional\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.144-146.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SR1\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, min_denominator=1e-08, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-D array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-D represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using (extended) Anderson mixing.\n",
      "        \n",
      "        The Jacobian is formed by for a 'best' solution in the space\n",
      "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
      "        inversions and MxN multiplications are required. [Ey]_\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        M : float, optional\n",
      "            Number of previous vectors to retain. Defaults to 5.\n",
      "        w0 : float, optional\n",
      "            Regularization parameter for numerical stability.\n",
      "            Compared to unity, good values of the order of 0.01.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='anderson'`` in particular.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.anderson(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116588, 0.15883789])\n",
      "    \n",
      "    approx_fprime(xk, f, epsilon, *args)\n",
      "        Finite-difference approximation of the gradient of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The coordinate vector at which to determine the gradient of `f`.\n",
      "        f : callable\n",
      "            The function of which to determine the gradient (partial derivatives).\n",
      "            Should take `xk` as first argument, other arguments to `f` can be\n",
      "            supplied in ``*args``. Should return a scalar, the value of the\n",
      "            function at `xk`.\n",
      "        epsilon : array_like\n",
      "            Increment to `xk` to use for determining the function gradient.\n",
      "            If a scalar, uses the same finite difference delta for all partial\n",
      "            derivatives. If an array, should contain one value per element of\n",
      "            `xk`.\n",
      "        \\*args : args, optional\n",
      "            Any other arguments that are to be passed to `f`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        grad : ndarray\n",
      "            The partial derivatives of `f` to `xk`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        check_grad : Check correctness of gradient function against approx_fprime.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function gradient is determined by the forward finite difference\n",
      "        formula::\n",
      "        \n",
      "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
      "            f'[i] = ---------------------------------\n",
      "                                epsilon[i]\n",
      "        \n",
      "        The main use of `approx_fprime` is in scalar function optimizers like\n",
      "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c0, c1):\n",
      "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
      "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
      "        \n",
      "        >>> x = np.ones(2)\n",
      "        >>> c0, c1 = (1, 200)\n",
      "        >>> eps = np.sqrt(np.finfo(float).eps)\n",
      "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
      "        array([   2.        ,  400.00004198])\n",
      "    \n",
      "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, seed=None)\n",
      "        Find the global minimum of a function using the basin-hopping algorithm.\n",
      "        \n",
      "        Basin-hopping is a two-phase method that combines a global stepping\n",
      "        algorithm with local minimization at each step. Designed to mimic\n",
      "        the natural process of energy minimization of clusters of atoms, it works\n",
      "        well for similar problems with \"funnel-like, but rugged\" energy landscapes\n",
      "        [5]_.\n",
      "        \n",
      "        As the step-taking, step acceptance, and minimization methods are all\n",
      "        customizable, this function can also be used to implement other two-phase\n",
      "        methods.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            Function to be optimized.  ``args`` can be passed as an optional item\n",
      "            in the dict ``minimizer_kwargs``\n",
      "        x0 : array_like\n",
      "            Initial guess.\n",
      "        niter : integer, optional\n",
      "            The number of basin-hopping iterations. There will be a total of\n",
      "            ``niter + 1`` runs of the local minimizer.\n",
      "        T : float, optional\n",
      "            The \"temperature\" parameter for the accept or reject criterion. Higher\n",
      "            \"temperatures\" mean that larger jumps in function value will be\n",
      "            accepted.  For best results ``T`` should be comparable to the\n",
      "            separation (in function value) between local minima.\n",
      "        stepsize : float, optional\n",
      "            Maximum step size for use in the random displacement.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            ``scipy.optimize.minimize()`` Some important options could be:\n",
      "        \n",
      "                method : str\n",
      "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
      "                args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "        \n",
      "        take_step : callable ``take_step(x)``, optional\n",
      "            Replace the default step-taking routine with this routine. The default\n",
      "            step-taking routine is a random displacement of the coordinates, but\n",
      "            other step-taking algorithms may be better for some systems.\n",
      "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
      "            If this attribute exists, then ``basinhopping`` will adjust\n",
      "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
      "            search.\n",
      "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
      "            Define a test which will be used to judge whether or not to accept the\n",
      "            step.  This will be used in addition to the Metropolis test based on\n",
      "            \"temperature\" ``T``.  The acceptable return values are True,\n",
      "            False, or ``\"force accept\"``. If any of the tests return False\n",
      "            then the step is rejected. If the latter, then this will override any\n",
      "            other tests in order to accept the step. This can be used, for example,\n",
      "            to forcefully escape from a local minimum that ``basinhopping`` is\n",
      "            trapped in.\n",
      "        callback : callable, ``callback(x, f, accept)``, optional\n",
      "            A callback function which will be called for all minima found. ``x``\n",
      "            and ``f`` are the coordinates and function value of the trial minimum,\n",
      "            and ``accept`` is whether or not that minimum was accepted. This can\n",
      "            be used, for example, to save the lowest N minima found. Also,\n",
      "            ``callback`` can be used to specify a user defined stop criterion by\n",
      "            optionally returning True to stop the ``basinhopping`` routine.\n",
      "        interval : integer, optional\n",
      "            interval for how often to update the ``stepsize``\n",
      "        disp : bool, optional\n",
      "            Set to True to print status messages\n",
      "        niter_success : integer, optional\n",
      "            Stop the run if the global minimum candidate remains the same for this\n",
      "            number of iterations.\n",
      "        seed : {None, int, `numpy.random.Generator`,\n",
      "                `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the default Metropolis\n",
      "            `accept_test` and the default `take_step`. If you supply your own\n",
      "            `take_step` and `accept_test`, and these functions use random\n",
      "            number generation, then those functions are responsible for the state\n",
      "            of their random number generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination. The ``OptimizeResult`` object returned by the\n",
      "            selected minimizer at the lowest minimum is also contained within this\n",
      "            object and can be accessed through the ``lowest_optimization_result``\n",
      "            attribute.  See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize :\n",
      "            The local minimization function called once for each basinhopping step.\n",
      "            ``minimizer_kwargs`` is passed to this routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
      "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
      "        [4]_. The algorithm in its current form was described by David Wales and\n",
      "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
      "        \n",
      "        The algorithm is iterative with each cycle composed of the following\n",
      "        features\n",
      "        \n",
      "        1) random perturbation of the coordinates\n",
      "        \n",
      "        2) local minimization\n",
      "        \n",
      "        3) accept or reject the new coordinates based on the minimized function\n",
      "           value\n",
      "        \n",
      "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
      "        Carlo algorithms, although there are many other possibilities [3]_.\n",
      "        \n",
      "        This global minimization method has been shown to be extremely efficient\n",
      "        for a wide variety of problems in physics and chemistry. It is\n",
      "        particularly useful when the function has many minima separated by large\n",
      "        barriers. See the Cambridge Cluster Database\n",
      "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
      "        that have been optimized primarily using basin-hopping. This database\n",
      "        includes minimization problems exceeding 300 degrees of freedom.\n",
      "        \n",
      "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
      "        a Fortran implementation of basin-hopping. This implementation has many\n",
      "        different variations of the procedure described above, including more\n",
      "        advanced step taking algorithms and alternate acceptance criterion.\n",
      "        \n",
      "        For stochastic global optimization there is no way to determine if the true\n",
      "        global minimum has actually been found. Instead, as a consistency check,\n",
      "        the algorithm can be run from a number of different random starting points\n",
      "        to ensure the lowest minimum found in each example has converged to the\n",
      "        global minimum. For this reason, ``basinhopping`` will by default simply\n",
      "        run for the number of iterations ``niter`` and return the lowest minimum\n",
      "        found. It is left to the user to ensure that this is in fact the global\n",
      "        minimum.\n",
      "        \n",
      "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
      "        depends on the problem being solved. The step is chosen uniformly in the\n",
      "        region from x0-stepsize to x0+stepsize, in each dimension. Ideally, it\n",
      "        should be comparable to the typical separation (in argument values) between\n",
      "        local minima of the function being optimized. ``basinhopping`` will, by\n",
      "        default, adjust ``stepsize`` to find an optimal value, but this may take\n",
      "        many iterations. You will get quicker results if you set a sensible\n",
      "        initial value for ``stepsize``.\n",
      "        \n",
      "        Choosing ``T``: The parameter ``T`` is the \"temperature\" used in the\n",
      "        Metropolis criterion. Basinhopping steps are always accepted if\n",
      "        ``func(xnew) < func(xold)``. Otherwise, they are accepted with\n",
      "        probability::\n",
      "        \n",
      "            exp( -(func(xnew) - func(xold)) / T )\n",
      "        \n",
      "        So, for best results, ``T`` should to be comparable to the typical\n",
      "        difference (in function values) between local minima. (The height of\n",
      "        \"walls\" between local minima is irrelevant.)\n",
      "        \n",
      "        If ``T`` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all\n",
      "        steps that increase energy are rejected.\n",
      "        \n",
      "        .. versionadded:: 0.12.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
      "            Cambridge, UK.\n",
      "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
      "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
      "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
      "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
      "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
      "            1987, 84, 6611.\n",
      "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
      "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
      "        .. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as\n",
      "            a General and Versatile Optimization Framework for the Characterization\n",
      "            of Biological Macromolecules, Advances in Artificial Intelligence,\n",
      "            Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 1-D minimization problem, with many\n",
      "        local minima superimposed on a parabola.\n",
      "        \n",
      "        >>> from scipy.optimize import basinhopping\n",
      "        >>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
      "        >>> x0=[1.]\n",
      "        \n",
      "        Basinhopping, internally, uses a local minimization algorithm. We will use\n",
      "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
      "        use and how to set up that minimizer. This parameter will be passed to\n",
      "        ``scipy.optimize.minimize()``.\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
      "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
      "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
      "        \n",
      "        Next consider a 2-D minimization problem. Also, this time, we\n",
      "        will use gradient information to significantly speed up the search.\n",
      "        \n",
      "        >>> def func2d(x):\n",
      "        ...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
      "        ...                                                            0.2) * x[0]\n",
      "        ...     df = np.zeros(2)\n",
      "        ...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
      "        ...     df[1] = 2. * x[1] + 0.2\n",
      "        ...     return f, df\n",
      "        \n",
      "        We'll also use a different local minimization algorithm. Also, we must tell\n",
      "        the minimizer that our function returns both energy and gradient (Jacobian).\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
      "        >>> x0 = [1.0, 1.0]\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Here is an example using a custom step-taking routine. Imagine you want\n",
      "        the first coordinate to take larger steps than the rest of the coordinates.\n",
      "        This can be implemented like so:\n",
      "        \n",
      "        >>> class MyTakeStep:\n",
      "        ...    def __init__(self, stepsize=0.5):\n",
      "        ...        self.stepsize = stepsize\n",
      "        ...        self.rng = np.random.default_rng()\n",
      "        ...    def __call__(self, x):\n",
      "        ...        s = self.stepsize\n",
      "        ...        x[0] += self.rng.uniform(-2.*s, 2.*s)\n",
      "        ...        x[1:] += self.rng.uniform(-s, s, x[1:].shape)\n",
      "        ...        return x\n",
      "        \n",
      "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
      "        of ``stepsize`` to optimize the search. We'll use the same 2-D function as\n",
      "        before\n",
      "        \n",
      "        >>> mytakestep = MyTakeStep()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200, take_step=mytakestep)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Now, let's do an example using a custom callback function which prints the\n",
      "        value of every minimum found\n",
      "        \n",
      "        >>> def print_fun(x, f, accepted):\n",
      "        ...         print(\"at minimum %.4f accepted %d\" % (f, int(accepted)))\n",
      "        \n",
      "        We'll run it for only 10 basinhopping steps this time.\n",
      "        \n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, callback=print_fun, seed=rng)\n",
      "        at minimum 0.4159 accepted 1\n",
      "        at minimum -0.4317 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.4317 accepted 0\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -0.7425 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.4317 accepted 0\n",
      "        at minimum -0.7425 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        \n",
      "        \n",
      "        The minimum at -1.0109 is actually the global minimum, found already on the\n",
      "        8th iteration.\n",
      "        \n",
      "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
      "        \n",
      "        >>> class MyBounds:\n",
      "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
      "        ...         self.xmax = np.array(xmax)\n",
      "        ...         self.xmin = np.array(xmin)\n",
      "        ...     def __call__(self, **kwargs):\n",
      "        ...         x = kwargs[\"x_new\"]\n",
      "        ...         tmax = bool(np.all(x <= self.xmax))\n",
      "        ...         tmin = bool(np.all(x >= self.xmin))\n",
      "        ...         return tmax and tmin\n",
      "        \n",
      "        >>> mybounds = MyBounds()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, accept_test=mybounds)\n",
      "    \n",
      "    bisect(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find root of a function within an interval using bisection.\n",
      "        \n",
      "        Basic bisection routine to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\n",
      "        Slow but sure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  `f` must be continuous, and\n",
      "            f(a) and f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in a `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.bisect(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.bisect(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        fsolve : n-dimensional root-finding\n",
      "    \n",
      "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
      "        Bracket the minimum of the function.\n",
      "        \n",
      "        Given a function and distinct initial points, search in the\n",
      "        downhill direction (as defined by the initial points) and return\n",
      "        new points xa, xb, xc that bracket the minimum of the function\n",
      "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
      "        solution will satisfy xa<=x<=xb.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to minimize.\n",
      "        xa, xb : float, optional\n",
      "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to `func`.\n",
      "        grow_limit : float, optional\n",
      "            Maximum grow limit.  Defaults to 110.0\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Defaults to 1000.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xa, xb, xc : float\n",
      "            Bracket.\n",
      "        fa, fb, fc : float\n",
      "            Objective function values in bracket.\n",
      "        funcalls : int\n",
      "            Number of function evaluations made.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        This function can find a downward convex region of a function:\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import bracket\n",
      "        >>> def f(x):\n",
      "        ...     return 10*x**2 + 3*x + 5\n",
      "        >>> x = np.linspace(-2, 2)\n",
      "        >>> y = f(x)\n",
      "        >>> init_xa, init_xb = 0, 1\n",
      "        >>> xa, xb, xc, fa, fb, fc, funcalls = bracket(f, xa=init_xa, xb=init_xb)\n",
      "        >>> plt.axvline(x=init_xa, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.axvline(x=init_xb, color=\"k\", linestyle=\"--\")\n",
      "        >>> plt.plot(x, y, \"-k\")\n",
      "        >>> plt.plot(xa, fa, \"bx\")\n",
      "        >>> plt.plot(xb, fb, \"rx\")\n",
      "        >>> plt.plot(xc, fc, \"bx\")\n",
      "        >>> plt.show()\n",
      "    \n",
      "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
      "        Given a function of one variable and a possible bracket, return\n",
      "        the local minimum of the function isolated to a fractional precision\n",
      "        of tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present).\n",
      "        brack : tuple, optional\n",
      "            Either a triple (xa,xb,xc) where xa<xb<xc and func(xb) <\n",
      "            func(xa), func(xc) or a pair (xa,xb) which are used as a\n",
      "            starting interval for a downhill bracket search (see\n",
      "            `bracket`). Providing the pair (xa,xb) does not always mean\n",
      "            the obtained solution will satisfy xa<=x<=xb.\n",
      "        tol : float, optional\n",
      "            Stop if between iteration change is less than `tol`.\n",
      "        full_output : bool, optional\n",
      "            If True, return all output args (xmin, fval, iter,\n",
      "            funcalls).\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations in solution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            Optimum value.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of objective function evaluations made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Brent' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses inverse parabolic interpolation when possible to speed up\n",
      "        convergence of golden section method.\n",
      "        \n",
      "        Does not ensure that the minimum lies in the range specified by\n",
      "        `brack`. See `fminbound`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range (xa,xb).\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.brent(f,brack=(1,2))\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.brent(f,brack=(-1,0.5,2))\n",
      "        >>> minimum\n",
      "        -2.7755575615628914e-17\n",
      "    \n",
      "    brenth(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's\n",
      "        method with hyperbolic extrapolation.\n",
      "        \n",
      "        A variation on the classic Brent routine to find a zero of the function f\n",
      "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
      "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
      "        f(a) and f(b) cannot have the same signs. Generally, on a par with the\n",
      "        brent routine, but not as heavily tested. It is a safe version of the\n",
      "        secant method that uses hyperbolic extrapolation. The version here is by\n",
      "        Chuck Harris.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. As with `brentq`, for nice\n",
      "            functions the method will often satisfy the above condition\n",
      "            with ``xtol/2`` and ``rtol/2``.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. As with `brentq`, for nice functions\n",
      "            the method will often satisfy the above condition with\n",
      "            ``xtol/2`` and ``rtol/2``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brenth(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brenth(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fmin, fmin_powell, fmin_cg,\n",
      "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
      "        \n",
      "        leastsq : nonlinear least squares minimizer\n",
      "        \n",
      "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
      "        \n",
      "        basinhopping, differential_evolution, brute : global optimizers\n",
      "        \n",
      "        fminbound, brent, golden, bracket : local scalar minimizers\n",
      "        \n",
      "        fsolve : N-D root-finding\n",
      "        \n",
      "        brentq, brenth, ridder, bisect, newton : 1-D root-finding\n",
      "        \n",
      "        fixed_point : scalar fixed-point finder\n",
      "    \n",
      "    brentq(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's method.\n",
      "        \n",
      "        Uses the classic Brent's method to find a zero of the function `f` on\n",
      "        the sign changing interval [a , b]. Generally considered the best of the\n",
      "        rootfinding routines here. It is a safe version of the secant method that\n",
      "        uses inverse quadratic extrapolation. Brent's method combines root\n",
      "        bracketing, interval bisection, and inverse quadratic interpolation. It is\n",
      "        sometimes known as the van Wijngaarden-Dekker-Brent method. Brent (1973)\n",
      "        claims convergence is guaranteed for functions computable within [a,b].\n",
      "        \n",
      "        [Brent1973]_ provides the classic description of the algorithm. Another\n",
      "        description can be found in a recent edition of Numerical Recipes, including\n",
      "        [PressEtal1992]_. A third description is at\n",
      "        http://mathworld.wolfram.com/BrentsMethod.html. It should be easy to\n",
      "        understand the algorithm just by reading our code. Our code diverges a bit\n",
      "        from standard presentations: we choose a different formula for the\n",
      "        extrapolation step.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)` must\n",
      "            have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval :math:`[a, b]`.\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval :math:`[a, b]`.\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
      "        \n",
      "        Related functions fall into several classes:\n",
      "        \n",
      "        multivariate local optimizers\n",
      "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
      "        nonlinear least squares minimizer\n",
      "          `leastsq`\n",
      "        constrained multivariate optimizers\n",
      "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
      "        global optimizers\n",
      "          `basinhopping`, `brute`, `differential_evolution`\n",
      "        local scalar minimizers\n",
      "          `fminbound`, `brent`, `golden`, `bracket`\n",
      "        N-D root-finding\n",
      "          `fsolve`\n",
      "        1-D root-finding\n",
      "          `brenth`, `ridder`, `bisect`, `newton`\n",
      "        scalar fixed-point finder\n",
      "          `fixed_point`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Brent1973]\n",
      "           Brent, R. P.,\n",
      "           *Algorithms for Minimization Without Derivatives*.\n",
      "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
      "        \n",
      "        .. [PressEtal1992]\n",
      "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
      "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
      "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
      "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brentq(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brentq(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "    \n",
      "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \\\"Broyden's good method\\\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden1'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
      "        \n",
      "        which corresponds to Broyden's first Jacobian update\n",
      "        \n",
      "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \\\"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.broyden1(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116396, 0.15883641])\n",
      "    \n",
      "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \"Broyden's bad method\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (i.e., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden2'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
      "        \n",
      "        corresponding to Broyden's second method.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.broyden2(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116365, 0.15883529])\n",
      "    \n",
      "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin at 0x00000181AB553040>, disp=False, workers=1)\n",
      "        Minimize a function over a given range by brute force.\n",
      "        \n",
      "        Uses the \"brute force\" method, i.e., computes the function's value\n",
      "        at each point of a multidimensional grid of points, to find the global\n",
      "        minimum of the function.\n",
      "        \n",
      "        The function is evaluated everywhere in the range with the datatype of the\n",
      "        first call to the function, as enforced by the ``vectorize`` NumPy\n",
      "        function. The value and type of the function evaluation returned when\n",
      "        ``full_output=True`` are affected in addition by the ``finish`` argument\n",
      "        (see Notes).\n",
      "        \n",
      "        The brute force approach is inefficient because the number of grid points\n",
      "        increases exponentially - the number of grid points to evaluate is\n",
      "        ``Ns ** len(x)``. Consequently, even with coarse grid spacing, even\n",
      "        moderately sized problems can take a long time to run, and/or run into\n",
      "        memory limitations.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the\n",
      "            form ``f(x, *args)``, where ``x`` is the argument in\n",
      "            the form of a 1-D array and ``args`` is a tuple of any\n",
      "            additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        ranges : tuple\n",
      "            Each component of the `ranges` tuple must be either a\n",
      "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
      "            The program uses these to create the grid of points on which\n",
      "            the objective function will be computed. See `Note 2` for\n",
      "            more detail.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        Ns : int, optional\n",
      "            Number of grid points along the axes, if not otherwise\n",
      "            specified. See `Note2`.\n",
      "        full_output : bool, optional\n",
      "            If True, return the evaluation grid and the objective function's\n",
      "            values on it.\n",
      "        finish : callable, optional\n",
      "            An optimization function that is called with the result of brute force\n",
      "            minimization as initial guess. `finish` should take `func` and\n",
      "            the initial guess as positional arguments, and take `args` as\n",
      "            keyword arguments. It may additionally take `full_output`\n",
      "            and/or `disp` as keyword arguments. Use None if no \"polishing\"\n",
      "            function is to be used. See Notes for more details.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages from the `finish` callable.\n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the grid is subdivided into `workers`\n",
      "            sections and evaluated in parallel (uses\n",
      "            `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply `-1` to use all cores available to the Process.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the grid in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.3.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : ndarray\n",
      "            A 1-D array containing the coordinates of a point at which the\n",
      "            objective function had its minimum value. (See `Note 1` for\n",
      "            which point is returned.)\n",
      "        fval : float\n",
      "            Function value at the point `x0`. (Returned when `full_output` is\n",
      "            True.)\n",
      "        grid : tuple\n",
      "            Representation of the evaluation grid. It has the same\n",
      "            length as `x0`. (Returned when `full_output` is True.)\n",
      "        Jout : ndarray\n",
      "            Function values at each point of the evaluation\n",
      "            grid, i.e., ``Jout = func(*grid)``. (Returned\n",
      "            when `full_output` is True.)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        basinhopping, differential_evolution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
      "        of the objective function occurs. If `finish` is None, that is the\n",
      "        point returned. When the global minimum occurs within (or not very far\n",
      "        outside) the grid's boundaries, and the grid is fine enough, that\n",
      "        point will be in the neighborhood of the global minimum.\n",
      "        \n",
      "        However, users often employ some other optimization program to\n",
      "        \"polish\" the gridpoint values, i.e., to seek a more precise\n",
      "        (local) minimum near `brute's` best gridpoint.\n",
      "        The `brute` function's `finish` option provides a convenient way to do\n",
      "        that. Any polishing program used must take `brute's` output as its\n",
      "        initial guess as a positional argument, and take `brute's` input values\n",
      "        for `args` as keyword arguments, otherwise an error will be raised.\n",
      "        It may additionally take `full_output` and/or `disp` as keyword arguments.\n",
      "        \n",
      "        `brute` assumes that the `finish` function returns either an\n",
      "        `OptimizeResult` object or a tuple in the form:\n",
      "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing\n",
      "        value of the argument, ``Jmin`` is the minimum value of the objective\n",
      "        function, \"...\" may be some other returned values (which are not used\n",
      "        by `brute`), and ``statuscode`` is the status code of the `finish` program.\n",
      "        \n",
      "        Note that when `finish` is not None, the values returned are those\n",
      "        of the `finish` program, *not* the gridpoint ones. Consequently,\n",
      "        while `brute` confines its search to the input grid points,\n",
      "        the `finish` program's results usually will not coincide with any\n",
      "        gridpoint, and may fall outside the grid's boundary. Thus, if a\n",
      "        minimum only needs to be found over the provided grid points, make\n",
      "        sure to pass in `finish=None`.\n",
      "        \n",
      "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
      "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
      "        Each component of the `ranges` tuple can be either a slice object or a\n",
      "        two-tuple giving a range of values, such as (0, 5). If the component is a\n",
      "        slice object, `brute` uses it directly. If the component is a two-tuple\n",
      "        range, `brute` internally converts it to a slice object that interpolates\n",
      "        `Ns` points from its low-value to its high-value, inclusive.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the use of `brute` to seek the global minimum of a function\n",
      "        of two variables that is given as the sum of a positive-definite\n",
      "        quadratic and two deep \"Gaussian-shaped\" craters. Specifically, define\n",
      "        the objective function `f` as the sum of three other functions,\n",
      "        ``f = f1 + f2 + f3``. We suppose each of these has a signature\n",
      "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
      "        are as defined below.\n",
      "        \n",
      "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
      "        >>> def f1(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
      "        \n",
      "        >>> def f2(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
      "        \n",
      "        >>> def f3(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
      "        \n",
      "        >>> def f(z, *params):\n",
      "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
      "        \n",
      "        Thus, the objective function may have local minima near the minimum\n",
      "        of each of the three functions of which it is composed. To\n",
      "        use `fmin` to polish its gridpoint result, we may then continue as\n",
      "        follows:\n",
      "        \n",
      "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
      "        >>> from scipy import optimize\n",
      "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
      "        ...                           finish=optimize.fmin)\n",
      "        >>> resbrute[0]  # global minimum\n",
      "        array([-1.05665192,  1.80834843])\n",
      "        >>> resbrute[1]  # function value at global minimum\n",
      "        -3.4085818767\n",
      "        \n",
      "        Note that if `finish` had been set to None, we would have gotten the\n",
      "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
      "    \n",
      "    check_grad(func, grad, x0, *args, **kwargs)\n",
      "        Check the correctness of a gradient function by comparing it against a\n",
      "        (forward) finite-difference approximation of the gradient.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x0, *args)``\n",
      "            Function whose derivative is to be checked.\n",
      "        grad : callable ``grad(x0, *args)``\n",
      "            Gradient of `func`.\n",
      "        x0 : ndarray\n",
      "            Points to check `grad` against forward difference approximation of grad\n",
      "            using `func`.\n",
      "        args : \\*args, optional\n",
      "            Extra arguments passed to `func` and `grad`.\n",
      "        epsilon : float, optional\n",
      "            Step size used for the finite difference approximation. It defaults to\n",
      "            ``sqrt(np.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        err : float\n",
      "            The square root of the sum of squares (i.e., the 2-norm) of the\n",
      "            difference between ``grad(x0, *args)`` and the finite difference\n",
      "            approximation of `grad` using func at the points `x0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        approx_fprime\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def func(x):\n",
      "        ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "        >>> def grad(x):\n",
      "        ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "        >>> from scipy.optimize import check_grad\n",
      "        >>> check_grad(func, grad, [1.5, -1.5])\n",
      "        2.9802322387695312e-08\n",
      "    \n",
      "    curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)\n",
      "        Use non-linear least squares to fit a function, f, to data.\n",
      "        \n",
      "        Assumes ``ydata = f(xdata, *params) + eps``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            The model function, f(x, ...). It must take the independent\n",
      "            variable as the first argument and the parameters to fit as\n",
      "            separate remaining arguments.\n",
      "        xdata : array_like or object\n",
      "            The independent variable where the data is measured.\n",
      "            Should usually be an M-length sequence or an (k,M)-shaped array for\n",
      "            functions with k predictors, but can actually be any object.\n",
      "        ydata : array_like\n",
      "            The dependent data, a length M array - nominally ``f(xdata, ...)``.\n",
      "        p0 : array_like, optional\n",
      "            Initial guess for the parameters (length N). If None, then the\n",
      "            initial values will all be 1 (if the number of parameters for the\n",
      "            function can be determined using introspection, otherwise a\n",
      "            ValueError is raised).\n",
      "        sigma : None or M-length sequence or MxM array, optional\n",
      "            Determines the uncertainty in `ydata`. If we define residuals as\n",
      "            ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n",
      "            depends on its number of dimensions:\n",
      "        \n",
      "                - A 1-D `sigma` should contain values of standard deviations of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = sum((r / sigma) ** 2)``.\n",
      "        \n",
      "                - A 2-D `sigma` should contain the covariance matrix of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = r.T @ inv(sigma) @ r``.\n",
      "        \n",
      "                  .. versionadded:: 0.19\n",
      "        \n",
      "            None (default) is equivalent of 1-D `sigma` filled with ones.\n",
      "        absolute_sigma : bool, optional\n",
      "            If True, `sigma` is used in an absolute sense and the estimated parameter\n",
      "            covariance `pcov` reflects these absolute values.\n",
      "        \n",
      "            If False (default), only the relative magnitudes of the `sigma` values matter.\n",
      "            The returned parameter covariance matrix `pcov` is based on scaling\n",
      "            `sigma` by a constant factor. This constant is set by demanding that the\n",
      "            reduced `chisq` for the optimal parameters `popt` when using the\n",
      "            *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n",
      "            match the sample variance of the residuals after the fit. Default is False.\n",
      "            Mathematically,\n",
      "            ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n",
      "        check_finite : bool, optional\n",
      "            If True, check that the input arrays do not contain nans of infs,\n",
      "            and raise a ValueError if they do. Setting this parameter to\n",
      "            False may silently produce nonsensical results if the input arrays\n",
      "            do contain nans. Default is True.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on parameters. Defaults to no bounds.\n",
      "            Each element of the tuple must be either an array with the length equal\n",
      "            to the number of parameters, or a scalar (in which case the bound is\n",
      "            taken to be the same for all parameters). Use ``np.inf`` with an\n",
      "            appropriate sign to disable bounds on all or some parameters.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        method : {'lm', 'trf', 'dogbox'}, optional\n",
      "            Method to use for optimization. See `least_squares` for more details.\n",
      "            Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n",
      "            provided. The method 'lm' won't work when the number of observations\n",
      "            is less than the number of variables, use 'trf' or 'dogbox' in this\n",
      "            case.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        jac : callable, string or None, optional\n",
      "            Function with signature ``jac(x, ...)`` which computes the Jacobian\n",
      "            matrix of the model function with respect to parameters as a dense\n",
      "            array_like structure. It will be scaled according to provided `sigma`.\n",
      "            If None (default), the Jacobian will be estimated numerically.\n",
      "            String keywords for 'trf' and 'dogbox' methods can be used to select\n",
      "            a finite difference scheme, see `least_squares`.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        kwargs\n",
      "            Keyword arguments passed to `leastsq` for ``method='lm'`` or\n",
      "            `least_squares` otherwise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        popt : array\n",
      "            Optimal values for the parameters so that the sum of the squared\n",
      "            residuals of ``f(xdata, *popt) - ydata`` is minimized.\n",
      "        pcov : 2-D array\n",
      "            The estimated covariance of popt. The diagonals provide the variance\n",
      "            of the parameter estimate. To compute one standard deviation errors\n",
      "            on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\n",
      "        \n",
      "            How the `sigma` parameter affects the estimated covariance\n",
      "            depends on `absolute_sigma` argument, as described above.\n",
      "        \n",
      "            If the Jacobian matrix at the solution doesn't have a full rank, then\n",
      "            'lm' method returns a matrix filled with ``np.inf``, on the other hand\n",
      "            'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n",
      "            the covariance matrix.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            if either `ydata` or `xdata` contain NaNs, or if incompatible options\n",
      "            are used.\n",
      "        \n",
      "        RuntimeError\n",
      "            if the least-squares minimization fails.\n",
      "        \n",
      "        OptimizeWarning\n",
      "            if covariance of the parameters can not be estimated.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Minimize the sum of squares of nonlinear functions.\n",
      "        scipy.stats.linregress : Calculate a linear least squares regression for\n",
      "                                 two sets of measurements.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n",
      "        through `leastsq`. Note that this algorithm can only deal with\n",
      "        unconstrained problems.\n",
      "        \n",
      "        Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n",
      "        the docstring of `least_squares` for more information.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import curve_fit\n",
      "        \n",
      "        >>> def func(x, a, b, c):\n",
      "        ...     return a * np.exp(-b * x) + c\n",
      "        \n",
      "        Define the data to be fit with some noise:\n",
      "        \n",
      "        >>> xdata = np.linspace(0, 4, 50)\n",
      "        >>> y = func(xdata, 2.5, 1.3, 0.5)\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> y_noise = 0.2 * rng.normal(size=xdata.size)\n",
      "        >>> ydata = y + y_noise\n",
      "        >>> plt.plot(xdata, ydata, 'b-', label='data')\n",
      "        \n",
      "        Fit for the parameters a, b, c of the function `func`:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata)\n",
      "        >>> popt\n",
      "        array([2.56274217, 1.37268521, 0.47427475])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'r-',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        Constrain the optimization to the region of ``0 <= a <= 3``,\n",
      "        ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n",
      "        >>> popt\n",
      "        array([2.43736712, 1.        , 0.34463856])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'g--',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.ylabel('y')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "    \n",
      "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
      "        \n",
      "        The Jacobian approximation is derived from previous iterations, by\n",
      "        retaining only the diagonal of Broyden matrices.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='diagbroyden'`` in particular.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.diagbroyden(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.84116403, 0.15883384])\n",
      "    \n",
      "    differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=(), x0=None)\n",
      "        Finds the global minimum of a multivariate function.\n",
      "        \n",
      "        Differential Evolution is stochastic in nature (does not use gradient\n",
      "        methods) to find the minimum, and can search large areas of candidate\n",
      "        space, but often requires larger numbers of function evaluations than\n",
      "        conventional gradient-based techniques.\n",
      "        \n",
      "        The algorithm is due to Storn and Price [1]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence or `Bounds`\n",
      "            Bounds for variables. There are two ways to specify the bounds:\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. ``(min, max)`` pairs for each element in ``x``, defining the finite\n",
      "            lower and upper bounds for the optimizing argument of `func`. It is\n",
      "            required to have ``len(bounds) == len(x)``. ``len(bounds)`` is used\n",
      "            to determine the number of parameters in ``x``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to\n",
      "            completely specify the objective function.\n",
      "        strategy : str, optional\n",
      "            The differential evolution strategy to use. Should be one of:\n",
      "        \n",
      "                - 'best1bin'\n",
      "                - 'best1exp'\n",
      "                - 'rand1exp'\n",
      "                - 'randtobest1exp'\n",
      "                - 'currenttobest1exp'\n",
      "                - 'best2exp'\n",
      "                - 'rand2exp'\n",
      "                - 'randtobest1bin'\n",
      "                - 'currenttobest1bin'\n",
      "                - 'best2bin'\n",
      "                - 'rand2bin'\n",
      "                - 'rand1bin'\n",
      "        \n",
      "            The default is 'best1bin'.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of generations over which the entire population is\n",
      "            evolved. The maximum number of function evaluations (with no polishing)\n",
      "            is: ``(maxiter + 1) * popsize * len(x)``\n",
      "        popsize : int, optional\n",
      "            A multiplier for setting the total population size. The population has\n",
      "            ``popsize * len(x)`` individuals. This keyword is overridden if an\n",
      "            initial population is supplied via the `init` keyword. When using\n",
      "            ``init='sobol'`` the population size is calculated as the next power\n",
      "            of 2 after ``popsize * len(x)``.\n",
      "        tol : float, optional\n",
      "            Relative tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        mutation : float or tuple(float, float), optional\n",
      "            The mutation constant. In the literature this is also known as\n",
      "            differential weight, being denoted by F.\n",
      "            If specified as a float it should be in the range [0, 2].\n",
      "            If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n",
      "            randomly changes the mutation constant on a generation by generation\n",
      "            basis. The mutation constant for that generation is taken from\n",
      "            ``U[min, max)``. Dithering can help speed convergence significantly.\n",
      "            Increasing the mutation constant increases the search radius, but will\n",
      "            slow down convergence.\n",
      "        recombination : float, optional\n",
      "            The recombination constant, should be in the range [0, 1]. In the\n",
      "            literature this is also known as the crossover probability, being\n",
      "            denoted by CR. Increasing this value allows a larger number of mutants\n",
      "            to progress into the next generation, but at the risk of population\n",
      "            stability.\n",
      "        seed : {None, int, `numpy.random.Generator`,\n",
      "                `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "            Specify `seed` for repeatable minimizations.\n",
      "        disp : bool, optional\n",
      "            Prints the evaluated `func` at every iteration.\n",
      "        callback : callable, `callback(xk, convergence=val)`, optional\n",
      "            A function to follow the progress of the minimization. ``xk`` is\n",
      "            the current value of ``x0``. ``val`` represents the fractional\n",
      "            value of the population convergence.  When ``val`` is greater than one\n",
      "            the function halts. If callback returns `True`, then the minimization\n",
      "            is halted (any polishing is still carried out).\n",
      "        polish : bool, optional\n",
      "            If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n",
      "            method is used to polish the best population member at the end, which\n",
      "            can improve the minimization slightly. If a constrained problem is\n",
      "            being studied then the `trust-constr` method is used instead.\n",
      "        init : str or array-like, optional\n",
      "            Specify which type of population initialization is performed. Should be\n",
      "            one of:\n",
      "        \n",
      "                - 'latinhypercube'\n",
      "                - 'sobol'\n",
      "                - 'halton'\n",
      "                - 'random'\n",
      "                - array specifying the initial population. The array should have\n",
      "                  shape ``(M, len(x))``, where M is the total population size and\n",
      "                  len(x) is the number of parameters.\n",
      "                  `init` is clipped to `bounds` before use.\n",
      "        \n",
      "            The default is 'latinhypercube'. Latin Hypercube sampling tries to\n",
      "            maximize coverage of the available parameter space.\n",
      "        \n",
      "            'sobol' and 'halton' are superior alternatives and maximize even more\n",
      "            the parameter space. 'sobol' will enforce an initial population\n",
      "            size which is calculated as the next power of 2 after\n",
      "            ``popsize * len(x)``. 'halton' has no requirements but is a bit less\n",
      "            efficient. See `scipy.stats.qmc` for more details.\n",
      "        \n",
      "            'random' initializes the population randomly - this has the drawback\n",
      "            that clustering can occur, preventing the whole of parameter space\n",
      "            being covered. Use of an array to specify a population could be used,\n",
      "            for example, to create a tight bunch of initial guesses in an location\n",
      "            where the solution is known to exist, thereby reducing time for\n",
      "            convergence.\n",
      "        atol : float, optional\n",
      "            Absolute tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        updating : {'immediate', 'deferred'}, optional\n",
      "            If ``'immediate'``, the best solution vector is continuously updated\n",
      "            within a single generation [4]_. This can lead to faster convergence as\n",
      "            trial vectors can take advantage of continuous improvements in the best\n",
      "            solution.\n",
      "            With ``'deferred'``, the best solution vector is updated once per\n",
      "            generation. Only ``'deferred'`` is compatible with parallelization, and\n",
      "            the `workers` keyword can over-ride this option.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the population is subdivided into `workers`\n",
      "            sections and evaluated in parallel\n",
      "            (uses `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply -1 to use all available CPU cores.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the population in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            This option will override the `updating` keyword to\n",
      "            ``updating='deferred'`` if ``workers != 1``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        constraints : {NonLinearConstraint, LinearConstraint, Bounds}\n",
      "            Constraints on the solver, over and above those applied by the `bounds`\n",
      "            kwd. Uses the approach by Lampinen [5]_.\n",
      "        \n",
      "            .. versionadded:: 1.4.0\n",
      "        \n",
      "        x0 : None or array-like, optional\n",
      "            Provides an initial guess to the minimization. Once the population has\n",
      "            been initialized this vector replaces the first (best) member. This\n",
      "            replacement is done even if `init` is given an initial population.\n",
      "        \n",
      "            .. versionadded:: 1.7.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes. If `polish`\n",
      "            was employed, and a lower minimum was obtained by the polishing, then\n",
      "            OptimizeResult also contains the ``jac`` attribute.\n",
      "            If the eventual solution does not satisfy the applied constraints\n",
      "            ``success`` will be `False`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Differential evolution is a stochastic population based method that is\n",
      "        useful for global optimization problems. At each pass through the population\n",
      "        the algorithm mutates each candidate solution by mixing with other candidate\n",
      "        solutions to create a trial candidate. There are several strategies [2]_ for\n",
      "        creating trial candidates, which suit some problems more than others. The\n",
      "        'best1bin' strategy is a good starting point for many systems. In this\n",
      "        strategy two members of the population are randomly chosen. Their difference\n",
      "        is used to mutate the best member (the 'best' in 'best1bin'), :math:`b_0`,\n",
      "        so far:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            b' = b_0 + mutation * (population[rand0] - population[rand1])\n",
      "        \n",
      "        A trial vector is then constructed. Starting with a randomly chosen ith\n",
      "        parameter the trial is sequentially filled (in modulo) with parameters from\n",
      "        ``b'`` or the original candidate. The choice of whether to use ``b'`` or the\n",
      "        original candidate is made with a binomial distribution (the 'bin' in\n",
      "        'best1bin') - a random number in [0, 1) is generated. If this number is\n",
      "        less than the `recombination` constant then the parameter is loaded from\n",
      "        ``b'``, otherwise it is loaded from the original candidate. The final\n",
      "        parameter is always loaded from ``b'``. Once the trial candidate is built\n",
      "        its fitness is assessed. If the trial is better than the original candidate\n",
      "        then it takes its place. If it is also better than the best overall\n",
      "        candidate it also replaces that.\n",
      "        To improve your chances of finding a global minimum use higher `popsize`\n",
      "        values, with higher `mutation` and (dithering), but lower `recombination`\n",
      "        values. This has the effect of widening the search radius, but slowing\n",
      "        convergence.\n",
      "        By default the best solution vector is updated continuously within a single\n",
      "        iteration (``updating='immediate'``). This is a modification [4]_ of the\n",
      "        original differential evolution algorithm which can lead to faster\n",
      "        convergence as trial vectors can immediately benefit from improved\n",
      "        solutions. To use the original Storn and Price behaviour, updating the best\n",
      "        solution once per iteration, set ``updating='deferred'``.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function is implemented in `rosen` in `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, differential_evolution\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Now repeat, but with parallelization.\n",
      "        \n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds, updating='deferred',\n",
      "        ...                                 workers=2)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Let's try and do a constrained minimization\n",
      "        \n",
      "        >>> from scipy.optimize import NonlinearConstraint, Bounds\n",
      "        >>> def constr_f(x):\n",
      "        ...     return np.array(x[0] + x[1])\n",
      "        >>>\n",
      "        >>> # the sum of x[0] and x[1] must be less than 1.9\n",
      "        >>> nlc = NonlinearConstraint(constr_f, -np.inf, 1.9)\n",
      "        >>> # specify limits using a `Bounds` object.\n",
      "        >>> bounds = Bounds([0., 0.], [2., 2.])\n",
      "        >>> result = differential_evolution(rosen, bounds, constraints=(nlc),\n",
      "        ...                                 seed=1)\n",
      "        >>> result.x, result.fun\n",
      "        (array([0.96633867, 0.93363577]), 0.0011361355854792312)\n",
      "        \n",
      "        Next find the minimum of the Ackley function\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "        \n",
      "        >>> from scipy.optimize import differential_evolution\n",
      "        >>> import numpy as np\n",
      "        >>> def ackley(x):\n",
      "        ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
      "        ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
      "        ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
      "        >>> bounds = [(-5, 5), (-5, 5)]\n",
      "        >>> result = differential_evolution(ackley, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 0.,  0.]), 4.4408920985006262e-16)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Storn, R and Price, K, Differential Evolution - a Simple and\n",
      "               Efficient Heuristic for Global Optimization over Continuous Spaces,\n",
      "               Journal of Global Optimization, 1997, 11, 341 - 359.\n",
      "        .. [2] http://www1.icsi.berkeley.edu/~storn/code.html\n",
      "        .. [3] http://en.wikipedia.org/wiki/Differential_evolution\n",
      "        .. [4] Wormington, M., Panaccione, C., Matney, K. M., Bowen, D. K., -\n",
      "               Characterization of structures from X-ray scattering data using\n",
      "               genetic algorithms, Phil. Trans. R. Soc. Lond. A, 1999, 357,\n",
      "               2827-2848\n",
      "        .. [5] Lampinen, J., A constraint handling approach for the differential\n",
      "               evolution algorithm. Proceedings of the 2002 Congress on\n",
      "               Evolutionary Computation. CEC'02 (Cat. No. 02TH8600). Vol. 2. IEEE,\n",
      "               2002.\n",
      "    \n",
      "    dual_annealing(func, bounds, args=(), maxiter=1000, local_search_options={}, initial_temp=5230.0, restart_temp_ratio=2e-05, visit=2.62, accept=-5.0, maxfun=10000000.0, seed=None, no_local_search=False, callback=None, x0=None)\n",
      "        Find the global minimum of a function using Dual Annealing.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence, shape (n, 2)\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining bounds for the objective function parameter.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of global search iterations. Default value is 1000.\n",
      "        local_search_options : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            (`minimize`). Some important options could be:\n",
      "            ``method`` for the minimizer method to use and ``args`` for\n",
      "            objective function additional arguments.\n",
      "        initial_temp : float, optional\n",
      "            The initial temperature, use higher values to facilitates a wider\n",
      "            search of the energy landscape, allowing dual_annealing to escape\n",
      "            local minima that it is trapped in. Default value is 5230. Range is\n",
      "            (0.01, 5.e4].\n",
      "        restart_temp_ratio : float, optional\n",
      "            During the annealing process, temperature is decreasing, when it\n",
      "            reaches ``initial_temp * restart_temp_ratio``, the reannealing process\n",
      "            is triggered. Default value of the ratio is 2e-5. Range is (0, 1).\n",
      "        visit : float, optional\n",
      "            Parameter for visiting distribution. Default value is 2.62. Higher\n",
      "            values give the visiting distribution a heavier tail, this makes\n",
      "            the algorithm jump to a more distant region. The value range is (1, 3].\n",
      "        accept : float, optional\n",
      "            Parameter for acceptance distribution. It is used to control the\n",
      "            probability of acceptance. The lower the acceptance parameter, the\n",
      "            smaller the probability of acceptance. Default value is -5.0 with\n",
      "            a range (-1e4, -5].\n",
      "        maxfun : int, optional\n",
      "            Soft limit for the number of objective function calls. If the\n",
      "            algorithm is in the middle of a local search, this number will be\n",
      "            exceeded, the algorithm will stop just after the local search is\n",
      "            done. Default value is 1e7.\n",
      "        seed : {None, int, `numpy.random.Generator`,\n",
      "                `numpy.random.RandomState`}, optional\n",
      "        \n",
      "            If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "            that instance is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the visiting distribution function\n",
      "            and new coordinates generation.\n",
      "        no_local_search : bool, optional\n",
      "            If `no_local_search` is set to True, a traditional Generalized\n",
      "            Simulated Annealing will be performed with no local search\n",
      "            strategy applied.\n",
      "        callback : callable, optional\n",
      "            A callback function with signature ``callback(x, f, context)``,\n",
      "            which will be called for all minima found.\n",
      "            ``x`` and ``f`` are the coordinates and function value of the\n",
      "            latest minimum found, and ``context`` has value in [0, 1, 2], with the\n",
      "            following meaning:\n",
      "        \n",
      "                - 0: minimum detected in the annealing process.\n",
      "                - 1: detection occurred in the local search process.\n",
      "                - 2: detection done in the dual annealing process.\n",
      "        \n",
      "            If the callback implementation returns True, the algorithm will stop.\n",
      "        x0 : ndarray, shape(n,), optional\n",
      "            Coordinates of a single N-D starting point.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination.\n",
      "            See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements the Dual Annealing optimization. This stochastic\n",
      "        approach derived from [3]_ combines the generalization of CSA (Classical\n",
      "        Simulated Annealing) and FSA (Fast Simulated Annealing) [1]_ [2]_ coupled\n",
      "        to a strategy for applying a local search on accepted locations [4]_.\n",
      "        An alternative implementation of this same algorithm is described in [5]_\n",
      "        and benchmarks are presented in [6]_. This approach introduces an advanced\n",
      "        method to refine the solution found by the generalized annealing\n",
      "        process. This algorithm uses a distorted Cauchy-Lorentz visiting\n",
      "        distribution, with its shape controlled by the parameter :math:`q_{v}`\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            g_{q_{v}}(\\Delta x(t)) \\propto \\frac{ \\\n",
      "            \\left[T_{q_{v}}(t) \\right]^{-\\frac{D}{3-q_{v}}}}{ \\\n",
      "            \\left[{1+(q_{v}-1)\\frac{(\\Delta x(t))^{2}} { \\\n",
      "            \\left[T_{q_{v}}(t)\\right]^{\\frac{2}{3-q_{v}}}}}\\right]^{ \\\n",
      "            \\frac{1}{q_{v}-1}+\\frac{D-1}{2}}}\n",
      "        \n",
      "        Where :math:`t` is the artificial time. This visiting distribution is used\n",
      "        to generate a trial jump distance :math:`\\Delta x(t)` of variable\n",
      "        :math:`x(t)` under artificial temperature :math:`T_{q_{v}}(t)`.\n",
      "        \n",
      "        From the starting point, after calling the visiting distribution\n",
      "        function, the acceptance probability is computed as follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            p_{q_{a}} = \\min{\\{1,\\left[1-(1-q_{a}) \\beta \\Delta E \\right]^{ \\\n",
      "            \\frac{1}{1-q_{a}}}\\}}\n",
      "        \n",
      "        Where :math:`q_{a}` is a acceptance parameter. For :math:`q_{a}<1`, zero\n",
      "        acceptance probability is assigned to the cases where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            [1-(1-q_{a}) \\beta \\Delta E] < 0\n",
      "        \n",
      "        The artificial temperature :math:`T_{q_{v}}(t)` is decreased according to\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T_{q_{v}}(t) = T_{q_{v}}(1) \\frac{2^{q_{v}-1}-1}{\\left( \\\n",
      "            1 + t\\right)^{q_{v}-1}-1}\n",
      "        \n",
      "        Where :math:`q_{v}` is the visiting parameter.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsallis C. Possible generalization of Boltzmann-Gibbs\n",
      "            statistics. Journal of Statistical Physics, 52, 479-487 (1998).\n",
      "        .. [2] Tsallis C, Stariolo DA. Generalized Simulated Annealing.\n",
      "            Physica A, 233, 395-406 (1996).\n",
      "        .. [3] Xiang Y, Sun DY, Fan W, Gong XG. Generalized Simulated\n",
      "            Annealing Algorithm and Its Application to the Thomson Model.\n",
      "            Physics Letters A, 233, 216-220 (1997).\n",
      "        .. [4] Xiang Y, Gong XG. Efficiency of Generalized Simulated\n",
      "            Annealing. Physical Review E, 62, 4473 (2000).\n",
      "        .. [5] Xiang Y, Gubian S, Suomela B, Hoeng J. Generalized\n",
      "            Simulated Annealing for Efficient Global Optimization: the GenSA\n",
      "            Package for R. The R Journal, Volume 5/1 (2013).\n",
      "        .. [6] Mullen, K. Continuous Global Optimization in R. Journal of\n",
      "            Statistical Software, 60(6), 1 - 45, (2014).\n",
      "            :doi:`10.18637/jss.v060.i06`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 10-D problem, with many local minima.\n",
      "        The function involved is called Rastrigin\n",
      "        (https://en.wikipedia.org/wiki/Rastrigin_function)\n",
      "        \n",
      "        >>> from scipy.optimize import dual_annealing\n",
      "        >>> func = lambda x: np.sum(x*x - 10*np.cos(2*np.pi*x)) + 10*np.size(x)\n",
      "        >>> lw = [-5.12] * 10\n",
      "        >>> up = [5.12] * 10\n",
      "        >>> ret = dual_annealing(func, bounds=list(zip(lw, up)))\n",
      "        >>> ret.x\n",
      "        array([-4.26437714e-09, -3.91699361e-09, -1.86149218e-09, -3.97165720e-09,\n",
      "               -6.29151648e-09, -6.53145322e-09, -3.93616815e-09, -6.55623025e-09,\n",
      "               -6.05775280e-09, -5.00668935e-09]) # random\n",
      "        >>> ret.fun\n",
      "        0.000000\n",
      "    \n",
      "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
      "        \n",
      "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='excitingmixing'`` in particular.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial Jacobian approximation is (-1/alpha).\n",
      "        alphamax : float, optional\n",
      "            The entries of the diagonal Jacobian are kept in the range\n",
      "            ``[alpha, alphamax]``.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "    \n",
      "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500, method='del2')\n",
      "        Find a fixed point of the function.\n",
      "        \n",
      "        Given a function of one or more variables and a starting point, find a\n",
      "        fixed point of the function: i.e., where ``func(x0) == x0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            Function to evaluate.\n",
      "        x0 : array_like\n",
      "            Fixed point of function.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to `func`.\n",
      "        xtol : float, optional\n",
      "            Convergence tolerance, defaults to 1e-08.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations, defaults to 500.\n",
      "        method : {\"del2\", \"iteration\"}, optional\n",
      "            Method of finding the fixed-point, defaults to \"del2\",\n",
      "            which uses Steffensen's Method with Aitken's ``Del^2``\n",
      "            convergence acceleration [1]_. The \"iteration\" method simply iterates\n",
      "            the function until convergence is detected, without attempting to\n",
      "            accelerate the convergence.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c1, c2):\n",
      "        ...    return np.sqrt(c1/(x+c2))\n",
      "        >>> c1 = np.array([10,12.])\n",
      "        >>> c2 = np.array([3, 5.])\n",
      "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
      "        array([ 1.4920333 ,  1.37228132])\n",
      "    \n",
      "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
      "        Minimize a function using the downhill simplex algorithm.\n",
      "        \n",
      "        This algorithm only uses function values, not derivatives or second\n",
      "        derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            The objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func, i.e., ``f(x,*args)``.\n",
      "        xtol : float, optional\n",
      "            Absolute error in xopt between iterations that is acceptable for\n",
      "            convergence.\n",
      "        ftol : number, optional\n",
      "            Absolute error in func(xopt) between iterations that is acceptable for\n",
      "            convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : number, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            Set to True if fopt and warnflag outputs are desired.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        retall : bool, optional\n",
      "            Set to True to return list of solutions at each iteration.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        initial_simplex : array_like of shape (N + 1, N), optional\n",
      "            Initial simplex. If given, overrides `x0`.\n",
      "            ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "            the jth vertex of the ``N+1`` vertices in the simplex, where\n",
      "            ``N`` is the dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter that minimizes function.\n",
      "        fopt : float\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        iter : int\n",
      "            Number of iterations performed.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            1 : Maximum number of function evaluations made.\n",
      "            2 : Maximum number of iterations reached.\n",
      "        allvecs : list\n",
      "            Solution at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Nelder-Mead' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
      "        one or more variables.\n",
      "        \n",
      "        This algorithm has a long history of successful use in applications.\n",
      "        But it will usually be slower than an algorithm that uses first or\n",
      "        second derivative information. In practice, it can have poor\n",
      "        performance in high-dimensional problems and is not robust to\n",
      "        minimizing complicated functions. Additionally, there currently is no\n",
      "        complete theory describing when the algorithm will successfully\n",
      "        converge to the minimum, or how fast it will if it does. Both the ftol and\n",
      "        xtol criteria must be met for convergence.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin(f, 1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 17\n",
      "                 Function evaluations: 34\n",
      "        >>> minimum[0]\n",
      "        -8.8817841970012523e-16\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
      "               minimization\", The Computer Journal, 7, pp. 308-313\n",
      "        \n",
      "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
      "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
      "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
      "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
      "               Harlow, UK, pp. 191-208.\n",
      "    \n",
      "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using the BFGS algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x,*args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x,*args)``, optional\n",
      "            Gradient of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f and fprime.\n",
      "        gtol : float, optional\n",
      "            Gradient norm must be less than `gtol` before successful termination.\n",
      "        norm : float, optional\n",
      "            Order of norm (Inf is max, -Inf is min)\n",
      "        epsilon : int or ndarray, optional\n",
      "            If `fprime` is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function to call after each\n",
      "            iteration. Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return ``fopt``, ``func_calls``, ``grad_calls``, and\n",
      "            ``warnflag`` in addition to ``xopt``.\n",
      "        disp : bool, optional\n",
      "            Print convergence message if True.\n",
      "        retall : bool, optional\n",
      "            Return a list of results at each iteration if True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Minimum value.\n",
      "        gopt : ndarray\n",
      "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
      "        Bopt : ndarray\n",
      "            Value of 1/f''(xopt), i.e., the inverse Hessian matrix.\n",
      "        func_calls : int\n",
      "            Number of function_calls made.\n",
      "        grad_calls : int\n",
      "            Number of gradient calls made.\n",
      "        warnflag : integer\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Gradient and/or function calls not changing.\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The value of `xopt` at each iteration. Only returned if `retall` is\n",
      "            True.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Optimize the function, `f`, whose gradient is given by `fprime`\n",
      "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
      "        and Shanno (BFGS).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See ``method='BFGS'`` in particular.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright, and Nocedal 'Numerical Optimization', 1999, p. 198.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import fmin_bfgs\n",
      "        >>> def quadratic_cost(x, Q):\n",
      "        ...     return x @ Q @ x\n",
      "        ...\n",
      "        >>> x0 = np.array([-3, -4])\n",
      "        >>> cost_weight =  np.diag([1., 10.])\n",
      "        >>> # Note that a trailing comma is necessary for a tuple with single element\n",
      "        >>> fmin_bfgs(quadratic_cost, x0, args=(cost_weight,))\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 0.000000\n",
      "                Iterations: 7                   # may vary\n",
      "                Function evaluations: 24        # may vary\n",
      "                Gradient evaluations: 8         # may vary\n",
      "        array([ 2.85169950e-06, -4.61820139e-07])\n",
      "        \n",
      "        >>> def quadratic_cost_grad(x, Q):\n",
      "        ...     return 2 * Q @ x\n",
      "        ...\n",
      "        >>> fmin_bfgs(quadratic_cost, x0, quadratic_cost_grad, args=(cost_weight,))\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 0.000000\n",
      "                Iterations: 7\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        array([ 2.85916637e-06, -4.54371951e-07])\n",
      "    \n",
      "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable, ``f(x, *args)``\n",
      "            Objective function to be minimized. Here `x` must be a 1-D array of\n",
      "            the variables that are to be changed in the search for a minimum, and\n",
      "            `args` are the other (fixed) parameters of `f`.\n",
      "        x0 : ndarray\n",
      "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
      "            It must be a 1-D array of values.\n",
      "        fprime : callable, ``fprime(x, *args)``, optional\n",
      "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
      "            are as described above for `f`. The returned value must be a 1-D array.\n",
      "            Defaults to None, in which case the gradient is approximated\n",
      "            numerically (see `epsilon`, below).\n",
      "        args : tuple, optional\n",
      "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
      "            additional fixed parameters are needed to completely specify the\n",
      "            functions `f` and `fprime`.\n",
      "        gtol : float, optional\n",
      "            Stop when the norm of the gradient is less than `gtol`.\n",
      "        norm : float, optional\n",
      "            Order to use for the norm of the gradient\n",
      "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
      "        epsilon : float or ndarray, optional\n",
      "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
      "            scalar or a 1-D array. Defaults to ``sqrt(eps)``, with eps the\n",
      "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
      "            1.5e-8.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
      "        full_output : bool, optional\n",
      "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
      "            addition to `xopt`.  See the Returns section below for additional\n",
      "            information on optional return values.\n",
      "        disp : bool, optional\n",
      "            If True, return a convergence message, followed by `xopt`.\n",
      "        retall : bool, optional\n",
      "            If True, add to the returned values the results of each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each iteration.\n",
      "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float, optional\n",
      "            Minimum value found, f(xopt). Only returned if `full_output` is True.\n",
      "        func_calls : int, optional\n",
      "            The number of function_calls made. Only returned if `full_output`\n",
      "            is True.\n",
      "        grad_calls : int, optional\n",
      "            The number of gradient calls made. Only returned if `full_output` is\n",
      "            True.\n",
      "        warnflag : int, optional\n",
      "            Integer value with warning status, only returned if `full_output` is\n",
      "            True.\n",
      "        \n",
      "            0 : Success.\n",
      "        \n",
      "            1 : The maximum number of iterations was exceeded.\n",
      "        \n",
      "            2 : Gradient and/or function calls were not changing. May indicate\n",
      "                that precision was lost, i.e., the routine did not converge.\n",
      "        \n",
      "            3 : NaN result encountered.\n",
      "        \n",
      "        allvecs : list of ndarray, optional\n",
      "            List of arrays, containing the results at each iteration.\n",
      "            Only returned if `retall` is True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize : common interface to all `scipy.optimize` algorithms for\n",
      "                   unconstrained and constrained minimization of multivariate\n",
      "                   functions. It provides an alternative way to call\n",
      "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
      "        [1]_.\n",
      "        \n",
      "        Conjugate gradient methods tend to work better when:\n",
      "        \n",
      "        1. `f` has a unique global minimizing point, and no local minima or\n",
      "           other stationary points,\n",
      "        2. `f` is, at least locally, reasonably well approximated by a\n",
      "           quadratic function of the variables,\n",
      "        3. `f` is continuous and has a continuous gradient,\n",
      "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
      "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
      "           minimizing point, `xopt`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Example 1: seek the minimum value of the expression\n",
      "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
      "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
      "        \n",
      "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
      "        >>> def f(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
      "        >>> def gradf(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
      "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
      "        ...     return np.asarray((gu, gv))\n",
      "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
      "        >>> from scipy import optimize\n",
      "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 1.617021\n",
      "                 Iterations: 4\n",
      "                 Function evaluations: 8\n",
      "                 Gradient evaluations: 8\n",
      "        >>> res1\n",
      "        array([-1.80851064, -0.25531915])\n",
      "        \n",
      "        Example 2: solve the same problem using the `minimize` function.\n",
      "        (This `myopts` dictionary shows all of the available options,\n",
      "        although in practice only non-default values would be needed.\n",
      "        The returned value will be a dictionary.)\n",
      "        \n",
      "        >>> opts = {'maxiter' : None,    # default value.\n",
      "        ...         'disp' : True,    # non-default value.\n",
      "        ...         'gtol' : 1e-5,    # default value.\n",
      "        ...         'norm' : np.inf,  # default value.\n",
      "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
      "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
      "        ...                          method='CG', options=opts)\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 1.617021\n",
      "                Iterations: 4\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        >>> res2.x  # minimum found\n",
      "        array([-1.80851064, -0.25531915])\n",
      "    \n",
      "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, maxfun=1000, disp=None, catol=0.0002)\n",
      "        Minimize a function using the Constrained Optimization By Linear\n",
      "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
      "        implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Function to minimize. In the form func(x, \\*args).\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        cons : sequence\n",
      "            Constraint functions; must all be ``>=0`` (a single function\n",
      "            if only 1 constraint). Each function takes the parameters `x`\n",
      "            as its first argument, and it can return either a single number or\n",
      "            an array or list of numbers.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        consargs : tuple, optional\n",
      "            Extra arguments to pass to constraint functions (default of None means\n",
      "            use same extra arguments as those passed to func).\n",
      "            Use ``()`` for no extra arguments.\n",
      "        rhobeg : float, optional\n",
      "            Reasonable initial changes to the variables.\n",
      "        rhoend : float, optional\n",
      "            Final accuracy in the optimization (not precisely guaranteed). This\n",
      "            is a lower bound on the size of the trust region.\n",
      "        disp : {0, 1, 2, 3}, optional\n",
      "            Controls the frequency of output; 0 implies no output.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        catol : float, optional\n",
      "            Absolute tolerance for constraint violations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The argument that minimises `f`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'COBYLA' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm is based on linear approximations to the objective\n",
      "        function and each constraint. We briefly describe the algorithm.\n",
      "        \n",
      "        Suppose the function is being minimized over k variables. At the\n",
      "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
      "        an approximate solution x_j, and a radius RHO_j.\n",
      "        (i.e., linear plus a constant) approximations to the objective\n",
      "        function and constraint functions such that their function values\n",
      "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
      "        This gives a linear program to solve (where the linear approximations\n",
      "        of the constraint functions are constrained to be non-negative).\n",
      "        \n",
      "        However, the linear approximations are likely only good\n",
      "        approximations near the current simplex, so the linear program is\n",
      "        given the further requirement that the solution, which\n",
      "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
      "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
      "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
      "        like a trust region algorithm.\n",
      "        \n",
      "        Additionally, the linear program may be inconsistent, or the\n",
      "        approximation may give poor improvement. For details about\n",
      "        how these issues are resolved, as well as how the points v_i are\n",
      "        updated, refer to the source code or the references below.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
      "        the objective and constraint functions by linear interpolation.\", in\n",
      "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
      "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
      "        \n",
      "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
      "        calculations\", Acta Numerica 7, 287-336\n",
      "        \n",
      "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
      "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Minimize the objective function f(x,y) = x*y subject\n",
      "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
      "        \n",
      "            >>> def objective(x):\n",
      "            ...     return x[0]*x[1]\n",
      "            ...\n",
      "            >>> def constr1(x):\n",
      "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
      "            ...\n",
      "            >>> def constr2(x):\n",
      "            ...     return x[1]\n",
      "            ...\n",
      "            >>> from scipy.optimize import fmin_cobyla\n",
      "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
      "            array([-0.70710685,  0.70710671])\n",
      "        \n",
      "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
      "    \n",
      "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None, maxls=20)\n",
      "        Minimize a function func using the L-BFGS-B algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Function to minimize.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable fprime(x,*args), optional\n",
      "            The gradient of `func`. If None, then `func` returns the function\n",
      "            value and the gradient (``f, g = func(x, *args)``), unless\n",
      "            `approx_grad` is True in which case `func` returns only ``f``.\n",
      "        args : sequence, optional\n",
      "            Arguments to pass to `func` and `fprime`.\n",
      "        approx_grad : bool, optional\n",
      "            Whether to approximate the gradient numerically (in which case\n",
      "            `func` returns only the function value).\n",
      "        bounds : list, optional\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None or +-inf for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction.\n",
      "        m : int, optional\n",
      "            The maximum number of variable metric corrections\n",
      "            used to define the limited memory matrix. (The limited memory BFGS\n",
      "            method does not store the full hessian but uses this many terms in an\n",
      "            approximation to it.)\n",
      "        factr : float, optional\n",
      "            The iteration stops when\n",
      "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
      "            where ``eps`` is the machine precision, which is automatically\n",
      "            generated by the code. Typical values for `factr` are: 1e12 for\n",
      "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
      "            high accuracy. See Notes for relationship to `ftol`, which is exposed\n",
      "            (instead of `factr`) by the `scipy.optimize.minimize` interface to\n",
      "            L-BFGS-B.\n",
      "        pgtol : float, optional\n",
      "            The iteration will stop when\n",
      "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
      "            where ``pg_i`` is the i-th component of the projected gradient.\n",
      "        epsilon : float, optional\n",
      "            Step size used when `approx_grad` is True, for numerically\n",
      "            calculating the gradient\n",
      "        iprint : int, optional\n",
      "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "            ``iprint = 0``    print only one line at the last iteration;\n",
      "            ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "            ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "            ``iprint = 100``  print also the changes of active set and final x;\n",
      "            ``iprint > 100``  print details of every iteration including x and g.\n",
      "        disp : int, optional\n",
      "            If zero, then no output. If a positive number, then this over-rides\n",
      "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxls : int, optional\n",
      "            Maximum number of line search steps (per iteration). Default is 20.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : array_like\n",
      "            Estimated position of the minimum.\n",
      "        f : float\n",
      "            Value of `func` at the minimum.\n",
      "        d : dict\n",
      "            Information dictionary.\n",
      "        \n",
      "            * d['warnflag'] is\n",
      "        \n",
      "              - 0 if converged,\n",
      "              - 1 if too many function evaluations or too many iterations,\n",
      "              - 2 if stopped for another reason, given in d['task']\n",
      "        \n",
      "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
      "            * d['funcalls'] is the number of function calls made.\n",
      "            * d['nit'] is the number of iterations.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'L-BFGS-B' `method` in particular. Note that the\n",
      "            `ftol` option is made available via that interface, while `factr` is\n",
      "            provided via this interface, where `factr` is the factor multiplying\n",
      "            the default machine floating-point precision to arrive at `ftol`:\n",
      "            ``ftol = factr * numpy.finfo(float).eps``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        License of L-BFGS-B (FORTRAN code):\n",
      "        \n",
      "        The version included here (in fortran code) is 3.0\n",
      "        (released April 25, 2011). It was written by Ciyou Zhu, Richard Byrd,\n",
      "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
      "        condition for use:\n",
      "        \n",
      "        This software is freely available, but we expect that all publications\n",
      "        describing work using this software, or all commercial products using it,\n",
      "        quote at least one of the references given below. This software is released\n",
      "        under the BSD License.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
      "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
      "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
      "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
      "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
      "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
      "          ACM Transactions on Mathematical Software, 38, 1.\n",
      "    \n",
      "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Unconstrained minimization of a function using the Newton-CG method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x, *args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x, *args)``\n",
      "            Gradient of f.\n",
      "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
      "            Function which computes the Hessian of f times an\n",
      "            arbitrary vector, p.\n",
      "        fhess : callable ``fhess(x, *args)``, optional\n",
      "            Function to compute the Hessian matrix of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
      "            (the same set of extra arguments is supplied to all of\n",
      "            these functions).\n",
      "        epsilon : float or ndarray, optional\n",
      "            If fhess is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function which is called after\n",
      "            each iteration. Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        avextol : float, optional\n",
      "            Convergence is assumed when the average relative error in\n",
      "            the minimizer falls below this amount.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return the optional outputs.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence message.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of results at each iteration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e., ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Value of the function at xopt, i.e., ``fopt = f(xopt)``.\n",
      "        fcalls : int\n",
      "            Number of function calls made.\n",
      "        gcalls : int\n",
      "            Number of gradient calls made.\n",
      "        hcalls : int\n",
      "            Number of Hessian calls made.\n",
      "        warnflag : int\n",
      "            Warnings generated by the algorithm.\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Line search failure (precision loss).\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The result at each iteration, if retall is True (see below).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Newton-CG' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
      "        is provided, then `fhess_p` will be ignored. If neither `fhess`\n",
      "        nor `fhess_p` is provided, then the hessian product will be\n",
      "        approximated using finite differences on `fprime`. `fhess_p`\n",
      "        must compute the hessian times an arbitrary vector. If it is not\n",
      "        given, finite-differences on `fprime` are used to compute\n",
      "        it.\n",
      "        \n",
      "        Newton-CG methods are also called truncated Newton methods. This\n",
      "        function differs from scipy.optimize.fmin_tnc because\n",
      "        \n",
      "        1. scipy.optimize.fmin_ncg is written purely in Python using NumPy\n",
      "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
      "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
      "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
      "            or box constrained minimization. (Box constraints give\n",
      "            lower and upper bounds for each variable separately.)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright & Nocedal, 'Numerical Optimization', 1999, p. 140.\n",
      "    \n",
      "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
      "        Minimize a function using modified Powell's method.\n",
      "        \n",
      "        This method only uses function values, not derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func.\n",
      "        xtol : float, optional\n",
      "            Line-search error tolerance.\n",
      "        ftol : float, optional\n",
      "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            If True, ``fopt``, ``xi``, ``direc``, ``iter``, ``funcalls``, and\n",
      "            ``warnflag`` are returned.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence messages.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of the solution at each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each\n",
      "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        direc : ndarray, optional\n",
      "            Initial fitting step and parameter order set as an (N, N) array, where N\n",
      "            is the number of fitting parameters in `x0`. Defaults to step size 1.0\n",
      "            fitting all parameters simultaneously (``np.eye((N, N))``). To\n",
      "            prevent initial consideration of values in a step or to change initial\n",
      "            step size, set to 0 or desired step size in the Jth position in the Mth\n",
      "            block, where J is the position in `x0` and M is the desired evaluation\n",
      "            step, with steps being evaluated in index order. Step size and ordering\n",
      "            will change freely as minimization proceeds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter which minimizes `func`.\n",
      "        fopt : number\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        direc : ndarray\n",
      "            Current direction set.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            Integer warning flag:\n",
      "                1 : Maximum number of function evaluations.\n",
      "                2 : Maximum number of iterations.\n",
      "                3 : NaN result encountered.\n",
      "                4 : The result is out of the provided bounds.\n",
      "        allvecs : list\n",
      "            List of solutions at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to unconstrained minimization algorithms for\n",
      "            multivariate functions. See the 'Powell' method in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a modification of Powell's method to find the minimum of\n",
      "        a function of N variables. Powell's method is a conjugate\n",
      "        direction method.\n",
      "        \n",
      "        The algorithm has two loops. The outer loop merely iterates over the inner\n",
      "        loop. The inner loop minimizes over each current direction in the direction\n",
      "        set. At the end of the inner loop, if certain conditions are met, the\n",
      "        direction that gave the largest decrease is dropped and replaced with the\n",
      "        difference between the current estimated x and the estimated x from the\n",
      "        beginning of the inner-loop.\n",
      "        \n",
      "        The technical conditions for replacing the direction of greatest\n",
      "        increase amount to checking that\n",
      "        \n",
      "        1. No further gain can be made along the direction of greatest increase\n",
      "           from that iteration.\n",
      "        2. The direction of greatest increase accounted for a large sufficient\n",
      "           fraction of the decrease in the function value from that iteration of\n",
      "           the inner loop.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
      "        function of several variables without calculating derivatives,\n",
      "        Computer Journal, 7 (2):155-162.\n",
      "        \n",
      "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
      "        Numerical Recipes (any edition), Cambridge University Press\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin_powell(f, -1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 2\n",
      "                 Function evaluations: 18\n",
      "        >>> minimum\n",
      "        array(0.0)\n",
      "    \n",
      "    fmin_slsqp(func, x0, eqcons=(), f_eqcons=None, ieqcons=(), f_ieqcons=None, bounds=(), fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08, callback=None)\n",
      "        Minimize a function using Sequential Least Squares Programming\n",
      "        \n",
      "        Python interface function for the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.  Must return a scalar.\n",
      "        x0 : 1-D ndarray of float\n",
      "            Initial guess for the independent variable(s).\n",
      "        eqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_eqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D array in which each element must equal 0.0 in a\n",
      "            successfully optimized problem. If f_eqcons is specified,\n",
      "            eqcons is ignored.\n",
      "        ieqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_ieqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D ndarray in which each element must be greater or\n",
      "            equal to 0.0 in a successfully optimized problem. If\n",
      "            f_ieqcons is specified, ieqcons is ignored.\n",
      "        bounds : list, optional\n",
      "            A list of tuples specifying the lower and upper bound\n",
      "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
      "            Infinite values will be interpreted as large floating values.\n",
      "        fprime : callable `f(x,*args)`, optional\n",
      "            A function that evaluates the partial derivatives of func.\n",
      "        fprime_eqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of equality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
      "        fprime_ieqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of inequality constraint normals. If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
      "        args : sequence, optional\n",
      "            Additional arguments passed to func and fprime.\n",
      "        iter : int, optional\n",
      "            The maximum number of iterations.\n",
      "        acc : float, optional\n",
      "            Requested accuracy.\n",
      "        iprint : int, optional\n",
      "            The verbosity of fmin_slsqp :\n",
      "        \n",
      "            * iprint <= 0 : Silent operation\n",
      "            * iprint == 1 : Print summary upon completion (default)\n",
      "            * iprint >= 2 : Print status of each iterate and summary\n",
      "        disp : int, optional\n",
      "            Overrides the iprint interface (preferred).\n",
      "        full_output : bool, optional\n",
      "            If False, return only the minimizer of func (default).\n",
      "            Otherwise, output final objective function and summary\n",
      "            information.\n",
      "        epsilon : float, optional\n",
      "            The step size for finite-difference derivative estimates.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(x)``, where ``x`` is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray of float\n",
      "            The final minimizer of func.\n",
      "        fx : ndarray of float, if full_output is true\n",
      "            The final value of the objective function.\n",
      "        its : int, if full_output is true\n",
      "            The number of iterations.\n",
      "        imode : int, if full_output is true\n",
      "            The exit mode from the optimizer (see below).\n",
      "        smode : string, if full_output is true\n",
      "            Message describing the exit mode from the optimizer.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'SLSQP' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Exit modes are defined as follows ::\n",
      "        \n",
      "            -1 : Gradient evaluation required (g & a)\n",
      "             0 : Optimization terminated successfully\n",
      "             1 : Function evaluation required (f & c)\n",
      "             2 : More equality constraints than independent variables\n",
      "             3 : More than 3*n iterations in LSQ subproblem\n",
      "             4 : Inequality constraints incompatible\n",
      "             5 : Singular matrix E in LSQ subproblem\n",
      "             6 : Singular matrix C in LSQ subproblem\n",
      "             7 : Rank-deficient equality constraint subproblem HFTI\n",
      "             8 : Positive directional derivative for linesearch\n",
      "             9 : Iteration limit reached\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
      "    \n",
      "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
      "        Minimize a function with variables subject to bounds, using\n",
      "        gradient information in a truncated Newton algorithm. This\n",
      "        method wraps a C implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x, *args)``\n",
      "            Function to minimize.  Must do one of:\n",
      "        \n",
      "            1. Return f and g, where f is the value of the function and g its\n",
      "               gradient (a list of floats).\n",
      "        \n",
      "            2. Return the function value but supply gradient function\n",
      "               separately as `fprime`.\n",
      "        \n",
      "            3. Return the function value and set ``approx_grad=True``.\n",
      "        \n",
      "            If the function returns None, the minimization\n",
      "            is aborted.\n",
      "        x0 : array_like\n",
      "            Initial estimate of minimum.\n",
      "        fprime : callable ``fprime(x, *args)``, optional\n",
      "            Gradient of `func`. If None, then either `func` must return the\n",
      "            function value and the gradient (``f,g = func(x, *args)``)\n",
      "            or `approx_grad` must be True.\n",
      "        args : tuple, optional\n",
      "            Arguments to pass to function.\n",
      "        approx_grad : bool, optional\n",
      "            If true, approximate the gradient numerically.\n",
      "        bounds : list, optional\n",
      "            (min, max) pairs for each element in x0, defining the\n",
      "            bounds on that parameter. Use None or +/-inf for one of\n",
      "            min or max when there is no bound in that direction.\n",
      "        epsilon : float, optional\n",
      "            Used if approx_grad is True. The stepsize in a finite\n",
      "            difference approximation for fprime.\n",
      "        scale : array_like, optional\n",
      "            Scaling factors to apply to each variable. If None, the\n",
      "            factors are up-low for interval bounded variables and\n",
      "            1+|x| for the others. Defaults to None.\n",
      "        offset : array_like, optional\n",
      "            Value to subtract from each variable. If None, the\n",
      "            offsets are (up+low)/2 for interval bounded variables\n",
      "            and x for the others.\n",
      "        messages : int, optional\n",
      "            Bit mask used to select messages display during\n",
      "            minimization values defined in the MSGS dict. Defaults to\n",
      "            MGS_ALL.\n",
      "        disp : int, optional\n",
      "            Integer interface to messages. 0 = no message, 5 = all messages\n",
      "        maxCGit : int, optional\n",
      "            Maximum number of hessian*vector evaluations per main\n",
      "            iteration. If maxCGit == 0, the direction chosen is\n",
      "            -gradient if maxCGit < 0, maxCGit is set to\n",
      "            max(1,min(50,n/2)). Defaults to -1.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluation. If None, maxfun is\n",
      "            set to max(100, 10*len(x0)). Defaults to None.\n",
      "        eta : float, optional\n",
      "            Severity of the line search. If < 0 or > 1, set to 0.25.\n",
      "            Defaults to -1.\n",
      "        stepmx : float, optional\n",
      "            Maximum step for the line search. May be increased during\n",
      "            call. If too small, it will be set to 10.0. Defaults to 0.\n",
      "        accuracy : float, optional\n",
      "            Relative precision for finite difference calculations. If\n",
      "            <= machine_precision, set to sqrt(machine_precision).\n",
      "            Defaults to 0.\n",
      "        fmin : float, optional\n",
      "            Minimum function value estimate. Defaults to 0.\n",
      "        ftol : float, optional\n",
      "            Precision goal for the value of f in the stopping criterion.\n",
      "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "        xtol : float, optional\n",
      "            Precision goal for the value of x in the stopping\n",
      "            criterion (after applying x scaling factors). If xtol <\n",
      "            0.0, xtol is set to sqrt(machine_precision). Defaults to\n",
      "            -1.\n",
      "        pgtol : float, optional\n",
      "            Precision goal for the value of the projected gradient in\n",
      "            the stopping criterion (after applying x scaling factors).\n",
      "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
      "            Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "        rescale : float, optional\n",
      "            Scaling factor (in log10) used to trigger f value\n",
      "            rescaling. If 0, rescale at each iteration. If a large\n",
      "            value, never rescale. If < 0, rescale is set to 1.3.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution.\n",
      "        nfeval : int\n",
      "            The number of function evaluations.\n",
      "        rc : int\n",
      "            Return code, see below\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'TNC' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The underlying algorithm is truncated Newton, also called\n",
      "        Newton Conjugate-Gradient. This method differs from\n",
      "        scipy.optimize.fmin_ncg in that\n",
      "        \n",
      "        1. it wraps a C implementation of the algorithm\n",
      "        2. it allows each variable to be given an upper and lower bound.\n",
      "        \n",
      "        The algorithm incorporates the bound constraints by determining\n",
      "        the descent direction as in an unconstrained truncated Newton,\n",
      "        but never taking a step-size large enough to leave the space\n",
      "        of feasible x's. The algorithm keeps track of a set of\n",
      "        currently active constraints, and ignores them when computing\n",
      "        the minimum allowable step size. (The x's associated with the\n",
      "        active constraint are kept fixed.) If the maximum allowable\n",
      "        step size is zero then a new constraint is added. At the end\n",
      "        of each iteration one of the constraints may be deemed no\n",
      "        longer active and removed. A constraint is considered\n",
      "        no longer active is if it is currently active\n",
      "        but the gradient for that variable points inward from the\n",
      "        constraint. The specific constraint removed is the one\n",
      "        associated with the variable of largest index whose\n",
      "        constraint is no longer active.\n",
      "        \n",
      "        Return codes are defined as follows::\n",
      "        \n",
      "            -1 : Infeasible (lower bound > upper bound)\n",
      "             0 : Local minimum reached (|pg| ~= 0)\n",
      "             1 : Converged (|f_n-f_(n-1)| ~= 0)\n",
      "             2 : Converged (|x_n-x_(n-1)| ~= 0)\n",
      "             3 : Max. number of function evaluations reached\n",
      "             4 : Linear search failed\n",
      "             5 : All lower bounds are equal to the upper bounds\n",
      "             6 : Unable to progress\n",
      "             7 : User requested end of minimization\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
      "        \n",
      "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
      "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
      "    \n",
      "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
      "        Bounded minimization for scalar functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized (must accept and return scalars).\n",
      "        x1, x2 : float or array scalar\n",
      "            The optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to function.\n",
      "        xtol : float, optional\n",
      "            The convergence tolerance.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations allowed.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        disp : int, optional\n",
      "            If non-zero, print messages.\n",
      "                0 : no message printing.\n",
      "                1 : non-convergence notification messages only.\n",
      "                2 : print a message on convergence too.\n",
      "                3 : print iteration results.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters (over given interval) which minimize the\n",
      "            objective function.\n",
      "        fval : number\n",
      "            The function value at the minimum point.\n",
      "        ierr : int\n",
      "            An error flag (0 if converged, 1 if maximum number of\n",
      "            function calls reached).\n",
      "        numfunc : int\n",
      "          The number of function calls made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Bounded' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Finds a local minimizer of the scalar function `func` in the\n",
      "        interval x1 < xopt < x2 using Brent's method. (See `brent`\n",
      "        for auto-bracketing.)\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        `fminbound` finds the minimum of the function in the given range.\n",
      "        The following examples illustrate the same\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fminbound(f, -1, 2)\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.fminbound(f, 1, 2)\n",
      "        >>> minimum\n",
      "        1.0000059608609866\n",
      "    \n",
      "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
      "        Find the roots of a function.\n",
      "        \n",
      "        Return the roots of the (non-linear) equations defined by\n",
      "        ``func(x) = 0`` given a starting estimate.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            A function that takes at least one (possibly vector) argument,\n",
      "            and returns a value of the same length.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the roots of ``func(x) = 0``.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to `func`.\n",
      "        fprime : callable ``f(x, *args)``, optional\n",
      "            A function to compute the Jacobian of `func` with derivatives\n",
      "            across the rows. By default, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            Specify whether the Jacobian function computes derivatives down\n",
      "            the columns (faster, because there is no transpose operation).\n",
      "        xtol : float, optional\n",
      "            The calculation will terminate if the relative error between two\n",
      "            consecutive iterates is at most `xtol`.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If zero, then\n",
      "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "            in `x0`.\n",
      "        band : tuple, optional\n",
      "            If set to a two-sequence containing the number of sub- and\n",
      "            super-diagonals within the band of the Jacobi matrix, the\n",
      "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "        epsfcn : float, optional\n",
      "            A suitable step length for the forward-difference\n",
      "            approximation of the Jacobian (for ``fprime=None``). If\n",
      "            `epsfcn` is less than the machine precision, it is assumed\n",
      "            that the relative errors in the functions are of the order of\n",
      "            the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in the interval\n",
      "            ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the\n",
      "            variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for\n",
      "            an unsuccessful call).\n",
      "        infodict : dict\n",
      "            A dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                number of function calls\n",
      "            ``njev``\n",
      "                number of Jacobian calls\n",
      "            ``fvec``\n",
      "                function evaluated at the output\n",
      "            ``fjac``\n",
      "                the orthogonal matrix, q, produced by the QR\n",
      "                factorization of the final approximate Jacobian\n",
      "                matrix, stored column wise\n",
      "            ``r``\n",
      "                upper triangular matrix produced by QR factorization\n",
      "                of the same matrix\n",
      "            ``qtf``\n",
      "                the vector ``(transpose(q) * fvec)``\n",
      "        \n",
      "        ier : int\n",
      "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
      "            to `mesg` for more information.\n",
      "        mesg : str\n",
      "            If no solution is found, `mesg` details the cause of failure.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See the ``method=='hybr'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Find a solution to the system of equations:\n",
      "        ``x0*cos(x1) = 4,  x1*x0 - x1 = 5``.\n",
      "        \n",
      "        >>> from scipy.optimize import fsolve\n",
      "        >>> def func(x):\n",
      "        ...     return [x[0] * np.cos(x[1]) - 4,\n",
      "        ...             x[1] * x[0] - x[1] - 5]\n",
      "        >>> root = fsolve(func, [1, 1])\n",
      "        >>> root\n",
      "        array([6.50409711, 0.90841421])\n",
      "        >>> np.isclose(func(root), [0.0, 0.0])  # func(root) should be almost 0.0.\n",
      "        array([ True,  True])\n",
      "    \n",
      "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0, maxiter=5000)\n",
      "        Return the minimum of a function of one variable using golden section\n",
      "        method.\n",
      "        \n",
      "        Given a function of one variable and a possible bracketing interval,\n",
      "        return the minimum of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            Objective function to minimize.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to func.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
      "            func(a),func(c). If bracket consists of two numbers (a,\n",
      "            c), then they are assumed to be a starting interval for a\n",
      "            downhill bracket search (see `bracket`); it doesn't always\n",
      "            mean that obtained solution will satisfy a<=x<=c.\n",
      "        tol : float, optional\n",
      "            x tolerance stop criterion\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Golden' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses analog of bisection method to decrease the bracketed\n",
      "        interval.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3, respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range ``(xa, xb)``.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.golden(f, brack=(1, 2))\n",
      "        >>> minimum\n",
      "        1.5717277788484873e-162\n",
      "        >>> minimum = optimize.golden(f, brack=(-1, 0.5, 2))\n",
      "        >>> minimum\n",
      "        -1.5717277788484873e-162\n",
      "    \n",
      "    least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})\n",
      "        Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given the residuals f(x) (an m-D real function of n real\n",
      "        variables) and the loss function rho(s) (a scalar function), `least_squares`\n",
      "        finds a local minimum of the cost function F(x)::\n",
      "        \n",
      "            minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        The purpose of the loss function rho(s) is to reduce the influence of\n",
      "        outliers on the solution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Function which computes the vector of residuals, with the signature\n",
      "            ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\n",
      "            respect to its first argument. The argument ``x`` passed to this\n",
      "            function is an ndarray of shape (n,) (never a scalar, even for n=1).\n",
      "            It must allocate and return a 1-D array_like of shape (m,) or a scalar.\n",
      "            If the argument ``x`` is complex or the function ``fun`` returns\n",
      "            complex residuals, it must be wrapped in a real function of real\n",
      "            arguments, as shown at the end of the Examples section.\n",
      "        x0 : array_like with shape (n,) or float\n",
      "            Initial guess on independent variables. If float, it will be treated\n",
      "            as a 1-D array with one element.\n",
      "        jac : {'2-point', '3-point', 'cs', callable}, optional\n",
      "            Method of computing the Jacobian matrix (an m-by-n matrix, where\n",
      "            element (i, j) is the partial derivative of f[i] with respect to\n",
      "            x[j]). The keywords select a finite difference scheme for numerical\n",
      "            estimation. The scheme '3-point' is more accurate, but requires\n",
      "            twice as many operations as '2-point' (default). The scheme 'cs'\n",
      "            uses complex steps, and while potentially the most accurate, it is\n",
      "            applicable only when `fun` correctly handles complex inputs and\n",
      "            can be analytically continued to the complex plane. Method 'lm'\n",
      "            always uses the '2-point' scheme. If callable, it is used as\n",
      "            ``jac(x, *args, **kwargs)`` and should return a good approximation\n",
      "            (or the exact value) for the Jacobian as an array_like (np.atleast_2d\n",
      "            is applied), a sparse matrix (csr_matrix preferred for performance) or\n",
      "            a `scipy.sparse.linalg.LinearOperator`.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must match the size of `x0` or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : {'trf', 'dogbox', 'lm'}, optional\n",
      "            Algorithm to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm, particularly suitable\n",
      "                  for large sparse problems with bounds. Generally robust method.\n",
      "                * 'dogbox' : dogleg algorithm with rectangular trust regions,\n",
      "                  typical use case is small problems with bounds. Not recommended\n",
      "                  for problems with rank-deficient Jacobian.\n",
      "                * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\n",
      "                  Doesn't handle bounds and sparse Jacobians. Usually the most\n",
      "                  efficient method for small unconstrained problems.\n",
      "        \n",
      "            Default is 'trf'. See Notes for more information.\n",
      "        ftol : float or None, optional\n",
      "            Tolerance for termination by the change of the cost function. Default\n",
      "            is 1e-8. The optimization process is stopped when ``dF < ftol * F``,\n",
      "            and there was an adequate agreement between a local quadratic model and\n",
      "            the true model in the last step.\n",
      "        \n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        xtol : float or None, optional\n",
      "            Tolerance for termination by the change of the independent variables.\n",
      "            Default is 1e-8. The exact condition depends on the `method` used:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``.\n",
      "                * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\n",
      "                  a trust-region radius and ``xs`` is the value of ``x``\n",
      "                  scaled according to `x_scale` parameter (see below).\n",
      "        \n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        gtol : float or None, optional\n",
      "            Tolerance for termination by the norm of the gradient. Default is 1e-8.\n",
      "            The exact condition depends on a `method` used:\n",
      "        \n",
      "                * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\n",
      "                  ``g_scaled`` is the value of the gradient scaled to account for\n",
      "                  the presence of the bounds [STIR]_.\n",
      "                * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\n",
      "                  ``g_free`` is the gradient with respect to the variables which\n",
      "                  are not in the optimal state on the boundary.\n",
      "                * For 'lm' : the maximum absolute value of the cosine of angles\n",
      "                  between columns of the Jacobian and the residual vector is less\n",
      "                  than `gtol`, or the residual vector is zero.\n",
      "        \n",
      "            If None and 'method' is not 'lm', the termination by this condition is\n",
      "            disabled. If 'method' is 'lm', this tolerance must be higher than\n",
      "            machine epsilon.\n",
      "        x_scale : array_like or 'jac', optional\n",
      "            Characteristic scale of each variable. Setting `x_scale` is equivalent\n",
      "            to reformulating the problem in scaled variables ``xs = x / x_scale``.\n",
      "            An alternative view is that the size of a trust region along jth\n",
      "            dimension is proportional to ``x_scale[j]``. Improved convergence may\n",
      "            be achieved by setting `x_scale` such that a step of a given size\n",
      "            along any of the scaled variables has a similar effect on the cost\n",
      "            function. If set to 'jac', the scale is iteratively updated using the\n",
      "            inverse norms of the columns of the Jacobian matrix (as described in\n",
      "            [JJMore]_).\n",
      "        loss : str or callable, optional\n",
      "            Determines the loss function. The following keyword values are allowed:\n",
      "        \n",
      "                * 'linear' (default) : ``rho(z) = z``. Gives a standard\n",
      "                  least-squares problem.\n",
      "                * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\n",
      "                  approximation of l1 (absolute value) loss. Usually a good\n",
      "                  choice for robust least squares.\n",
      "                * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\n",
      "                  similarly to 'soft_l1'.\n",
      "                * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\n",
      "                  influence, but may cause difficulties in optimization process.\n",
      "                * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\n",
      "                  a single residual, has properties similar to 'cauchy'.\n",
      "        \n",
      "            If callable, it must take a 1-D ndarray ``z=f**2`` and return an\n",
      "            array_like with shape (3, m) where row 0 contains function values,\n",
      "            row 1 contains first derivatives and row 2 contains second\n",
      "            derivatives. Method 'lm' supports only 'linear' loss.\n",
      "        f_scale : float, optional\n",
      "            Value of soft margin between inlier and outlier residuals, default\n",
      "            is 1.0. The loss function is evaluated as follows\n",
      "            ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\n",
      "            and ``rho`` is determined by `loss` parameter. This parameter has\n",
      "            no effect with ``loss='linear'``, but for other `loss` values it is\n",
      "            of crucial importance.\n",
      "        max_nfev : None or int, optional\n",
      "            Maximum number of function evaluations before the termination.\n",
      "            If None (default), the value is chosen automatically:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : 100 * n.\n",
      "                * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\n",
      "                  otherwise (because 'lm' counts function calls in Jacobian\n",
      "                  estimation).\n",
      "        \n",
      "        diff_step : None or array_like, optional\n",
      "            Determines the relative step size for the finite difference\n",
      "            approximation of the Jacobian. The actual step is computed as\n",
      "            ``x * diff_step``. If None (default), then `diff_step` is taken to be\n",
      "            a conventional \"optimal\" power of machine epsilon for the finite\n",
      "            difference scheme used [NR]_.\n",
      "        tr_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method for solving trust-region subproblems, relevant only for 'trf'\n",
      "            and 'dogbox' methods.\n",
      "        \n",
      "                * 'exact' is suitable for not very large problems with dense\n",
      "                  Jacobian matrices. The computational complexity per iteration is\n",
      "                  comparable to a singular value decomposition of the Jacobian\n",
      "                  matrix.\n",
      "                * 'lsmr' is suitable for problems with sparse and large Jacobian\n",
      "                  matrices. It uses the iterative procedure\n",
      "                  `scipy.sparse.linalg.lsmr` for finding a solution of a linear\n",
      "                  least-squares problem and only requires matrix-vector product\n",
      "                  evaluations.\n",
      "        \n",
      "            If None (default), the solver is chosen based on the type of Jacobian\n",
      "            returned on the first iteration.\n",
      "        tr_options : dict, optional\n",
      "            Keyword options passed to trust-region solver.\n",
      "        \n",
      "                * ``tr_solver='exact'``: `tr_options` are ignored.\n",
      "                * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\n",
      "                  Additionally,  ``method='trf'`` supports  'regularize' option\n",
      "                  (bool, default is True), which adds a regularization term to the\n",
      "                  normal equation, which improves convergence if the Jacobian is\n",
      "                  rank-deficient [Byrd]_ (eq. 3.4).\n",
      "        \n",
      "        jac_sparsity : {None, array_like, sparse matrix}, optional\n",
      "            Defines the sparsity structure of the Jacobian matrix for finite\n",
      "            difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "            only few non-zero elements in *each* row, providing the sparsity\n",
      "            structure will greatly speed up the computations [Curtis]_. A zero\n",
      "            entry means that a corresponding element in the Jacobian is identically\n",
      "            zero. If provided, forces the use of 'lsmr' trust-region solver.\n",
      "            If None (default), then dense differencing will be used. Has no effect\n",
      "            for 'lm' method.\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 (default) : work silently.\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations (not supported by 'lm'\n",
      "                  method).\n",
      "        \n",
      "        args, kwargs : tuple and dict, optional\n",
      "            Additional arguments passed to `fun` and `jac`. Both empty by default.\n",
      "            The calling signature is ``fun(x, *args, **kwargs)`` and the same for\n",
      "            `jac`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        result : OptimizeResult\n",
      "            `OptimizeResult` with the following fields defined:\n",
      "        \n",
      "                x : ndarray, shape (n,)\n",
      "                    Solution found.\n",
      "                cost : float\n",
      "                    Value of the cost function at the solution.\n",
      "                fun : ndarray, shape (m,)\n",
      "                    Vector of residuals at the solution.\n",
      "                jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\n",
      "                    Modified Jacobian matrix at the solution, in the sense that J^T J\n",
      "                    is a Gauss-Newton approximation of the Hessian of the cost function.\n",
      "                    The type is the same as the one used by the algorithm.\n",
      "                grad : ndarray, shape (m,)\n",
      "                    Gradient of the cost function at the solution.\n",
      "                optimality : float\n",
      "                    First-order optimality measure. In unconstrained problems, it is\n",
      "                    always the uniform norm of the gradient. In constrained problems,\n",
      "                    it is the quantity which was compared with `gtol` during iterations.\n",
      "                active_mask : ndarray of int, shape (n,)\n",
      "                    Each component shows whether a corresponding constraint is active\n",
      "                    (that is, whether a variable is at the bound):\n",
      "        \n",
      "                        *  0 : a constraint is not active.\n",
      "                        * -1 : a lower bound is active.\n",
      "                        *  1 : an upper bound is active.\n",
      "        \n",
      "                    Might be somewhat arbitrary for 'trf' method as it generates a\n",
      "                    sequence of strictly feasible iterates and `active_mask` is\n",
      "                    determined within a tolerance threshold.\n",
      "                nfev : int\n",
      "                    Number of function evaluations done. Methods 'trf' and 'dogbox' do\n",
      "                    not count function calls for numerical Jacobian approximation, as\n",
      "                    opposed to 'lm' method.\n",
      "                njev : int or None\n",
      "                    Number of Jacobian evaluations done. If numerical Jacobian\n",
      "                    approximation is used in 'lm' method, it is set to None.\n",
      "                status : int\n",
      "                    The reason for algorithm termination:\n",
      "        \n",
      "                        * -1 : improper input parameters status returned from MINPACK.\n",
      "                        *  0 : the maximum number of function evaluations is exceeded.\n",
      "                        *  1 : `gtol` termination condition is satisfied.\n",
      "                        *  2 : `ftol` termination condition is satisfied.\n",
      "                        *  3 : `xtol` termination condition is satisfied.\n",
      "                        *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\n",
      "        \n",
      "                message : str\n",
      "                    Verbal description of the termination reason.\n",
      "                success : bool\n",
      "                    True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        leastsq : A legacy wrapper for the MINPACK implementation of the\n",
      "                  Levenberg-Marquadt algorithm.\n",
      "        curve_fit : Least-squares minimization applied to a curve-fitting problem.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\n",
      "        algorithms implemented in MINPACK (lmder, lmdif). It runs the\n",
      "        Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\n",
      "        The implementation is based on paper [JJMore]_, it is very robust and\n",
      "        efficient with a lot of smart tricks. It should be your first choice\n",
      "        for unconstrained problems. Note that it doesn't support bounds. Also,\n",
      "        it doesn't work when m < n.\n",
      "        \n",
      "        Method 'trf' (Trust Region Reflective) is motivated by the process of\n",
      "        solving a system of equations, which constitute the first-order optimality\n",
      "        condition for a bound-constrained minimization problem as formulated in\n",
      "        [STIR]_. The algorithm iteratively solves trust-region subproblems\n",
      "        augmented by a special diagonal quadratic term and with trust-region shape\n",
      "        determined by the distance from the bounds and the direction of the\n",
      "        gradient. This enhancements help to avoid making steps directly into bounds\n",
      "        and efficiently explore the whole space of variables. To further improve\n",
      "        convergence, the algorithm considers search directions reflected from the\n",
      "        bounds. To obey theoretical requirements, the algorithm keeps iterates\n",
      "        strictly feasible. With dense Jacobians trust-region subproblems are\n",
      "        solved by an exact method very similar to the one described in [JJMore]_\n",
      "        (and implemented in MINPACK). The difference from the MINPACK\n",
      "        implementation is that a singular value decomposition of a Jacobian\n",
      "        matrix is done once per iteration, instead of a QR decomposition and series\n",
      "        of Givens rotation eliminations. For large sparse Jacobians a 2-D subspace\n",
      "        approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\n",
      "        The subspace is spanned by a scaled gradient and an approximate\n",
      "        Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\n",
      "        constraints are imposed the algorithm is very similar to MINPACK and has\n",
      "        generally comparable performance. The algorithm works quite robust in\n",
      "        unbounded and bounded problems, thus it is chosen as a default algorithm.\n",
      "        \n",
      "        Method 'dogbox' operates in a trust-region framework, but considers\n",
      "        rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\n",
      "        The intersection of a current trust region and initial bounds is again\n",
      "        rectangular, so on each iteration a quadratic minimization problem subject\n",
      "        to bound constraints is solved approximately by Powell's dogleg method\n",
      "        [NumOpt]_. The required Gauss-Newton step can be computed exactly for\n",
      "        dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\n",
      "        sparse Jacobians. The algorithm is likely to exhibit slow convergence when\n",
      "        the rank of Jacobian is less than the number of variables. The algorithm\n",
      "        often outperforms 'trf' in bounded problems with a small number of\n",
      "        variables.\n",
      "        \n",
      "        Robust loss functions are implemented as described in [BA]_. The idea\n",
      "        is to modify a residual vector and a Jacobian matrix on each iteration\n",
      "        such that computed gradient and Gauss-Newton Hessian approximation match\n",
      "        the true gradient and Hessian approximation of the cost function. Then\n",
      "        the algorithm proceeds in a normal way, i.e., robust loss functions are\n",
      "        implemented as a simple wrapper over standard least-squares algorithms.\n",
      "        \n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\n",
      "                Computing. 3rd edition\", Sec. 5.7.\n",
      "        .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\n",
      "                  solution of the trust region problem by minimization over\n",
      "                  two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\n",
      "                  1988.\n",
      "        .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\n",
      "                    sparse Jacobian matrices\", Journal of the Institute of\n",
      "                    Mathematics and its Applications, 13, pp. 117-120, 1974.\n",
      "        .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\n",
      "                    and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n",
      "                    Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n",
      "        .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\n",
      "                    Dogleg Approach for Unconstrained and Bound Constrained\n",
      "                    Nonlinear Optimization\", WSEAS International Conference on\n",
      "                    Applied Mathematics, Corfu, Greece, 2004.\n",
      "        .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\n",
      "                    2nd edition\", Chapter 4.\n",
      "        .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n",
      "                Proceedings of the International Workshop on Vision Algorithms:\n",
      "                Theory and Practice, pp. 298-372, 1999.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example we find a minimum of the Rosenbrock function without bounds\n",
      "        on independent variables.\n",
      "        \n",
      "        >>> def fun_rosenbrock(x):\n",
      "        ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
      "        \n",
      "        Notice that we only provide the vector of the residuals. The algorithm\n",
      "        constructs the cost function as a sum of squares of the residuals, which\n",
      "        gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> x0_rosenbrock = np.array([2, 2])\n",
      "        >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n",
      "        >>> res_1.x\n",
      "        array([ 1.,  1.])\n",
      "        >>> res_1.cost\n",
      "        9.8669242910846867e-30\n",
      "        >>> res_1.optimality\n",
      "        8.8928864934219529e-14\n",
      "        \n",
      "        We now constrain the variables, in such a way that the previous solution\n",
      "        becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\n",
      "        ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\n",
      "        to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\n",
      "        \n",
      "        We also provide the analytic Jacobian:\n",
      "        \n",
      "        >>> def jac_rosenbrock(x):\n",
      "        ...     return np.array([\n",
      "        ...         [-20 * x[0], 10],\n",
      "        ...         [-1, 0]])\n",
      "        \n",
      "        Putting this all together, we see that the new solution lies on the bound:\n",
      "        \n",
      "        >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n",
      "        ...                       bounds=([-np.inf, 1.5], np.inf))\n",
      "        >>> res_2.x\n",
      "        array([ 1.22437075,  1.5       ])\n",
      "        >>> res_2.cost\n",
      "        0.025213093946805685\n",
      "        >>> res_2.optimality\n",
      "        1.5885401433157753e-07\n",
      "        \n",
      "        Now we solve a system of equations (i.e., the cost function should be zero\n",
      "        at a minimum) for a Broyden tridiagonal vector-valued function of 100000\n",
      "        variables:\n",
      "        \n",
      "        >>> def fun_broyden(x):\n",
      "        ...     f = (3 - x) * x + 1\n",
      "        ...     f[1:] -= x[:-1]\n",
      "        ...     f[:-1] -= 2 * x[1:]\n",
      "        ...     return f\n",
      "        \n",
      "        The corresponding Jacobian matrix is sparse. We tell the algorithm to\n",
      "        estimate it by finite differences and provide the sparsity structure of\n",
      "        Jacobian to significantly speed up this process.\n",
      "        \n",
      "        >>> from scipy.sparse import lil_matrix\n",
      "        >>> def sparsity_broyden(n):\n",
      "        ...     sparsity = lil_matrix((n, n), dtype=int)\n",
      "        ...     i = np.arange(n)\n",
      "        ...     sparsity[i, i] = 1\n",
      "        ...     i = np.arange(1, n)\n",
      "        ...     sparsity[i, i - 1] = 1\n",
      "        ...     i = np.arange(n - 1)\n",
      "        ...     sparsity[i, i + 1] = 1\n",
      "        ...     return sparsity\n",
      "        ...\n",
      "        >>> n = 100000\n",
      "        >>> x0_broyden = -np.ones(n)\n",
      "        ...\n",
      "        >>> res_3 = least_squares(fun_broyden, x0_broyden,\n",
      "        ...                       jac_sparsity=sparsity_broyden(n))\n",
      "        >>> res_3.cost\n",
      "        4.5687069299604613e-23\n",
      "        >>> res_3.optimality\n",
      "        1.1650454296851518e-11\n",
      "        \n",
      "        Let's also solve a curve fitting problem using robust loss function to\n",
      "        take care of outliers in the data. Define the model function as\n",
      "        ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\n",
      "        observation and a, b, c are parameters to estimate.\n",
      "        \n",
      "        First, define the function which generates the data with noise and\n",
      "        outliers, define the model parameters, and generate data:\n",
      "        \n",
      "        >>> from numpy.random import default_rng\n",
      "        >>> rng = default_rng()\n",
      "        >>> def gen_data(t, a, b, c, noise=0., n_outliers=0, seed=None):\n",
      "        ...     rng = default_rng(seed)\n",
      "        ...\n",
      "        ...     y = a + b * np.exp(t * c)\n",
      "        ...\n",
      "        ...     error = noise * rng.standard_normal(t.size)\n",
      "        ...     outliers = rng.integers(0, t.size, n_outliers)\n",
      "        ...     error[outliers] *= 10\n",
      "        ...\n",
      "        ...     return y + error\n",
      "        ...\n",
      "        >>> a = 0.5\n",
      "        >>> b = 2.0\n",
      "        >>> c = -1\n",
      "        >>> t_min = 0\n",
      "        >>> t_max = 10\n",
      "        >>> n_points = 15\n",
      "        ...\n",
      "        >>> t_train = np.linspace(t_min, t_max, n_points)\n",
      "        >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
      "        \n",
      "        Define function for computing residuals and initial estimate of\n",
      "        parameters.\n",
      "        \n",
      "        >>> def fun(x, t, y):\n",
      "        ...     return x[0] + x[1] * np.exp(x[2] * t) - y\n",
      "        ...\n",
      "        >>> x0 = np.array([1.0, 1.0, 0.0])\n",
      "        \n",
      "        Compute a standard least-squares solution:\n",
      "        \n",
      "        >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\n",
      "        \n",
      "        Now compute two solutions with two different robust loss functions. The\n",
      "        parameter `f_scale` is set to 0.1, meaning that inlier residuals should\n",
      "        not significantly exceed 0.1 (the noise level used).\n",
      "        \n",
      "        >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
      "        ...                             args=(t_train, y_train))\n",
      "        >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
      "        ...                         args=(t_train, y_train))\n",
      "        \n",
      "        And, finally, plot all the curves. We see that by selecting an appropriate\n",
      "        `loss`  we can get estimates close to optimal even in the presence of\n",
      "        strong outliers. But keep in mind that generally it is recommended to try\n",
      "        'soft_l1' or 'huber' losses first (if at all necessary) as the other two\n",
      "        options may cause difficulties in optimization process.\n",
      "        \n",
      "        >>> t_test = np.linspace(t_min, t_max, n_points * 10)\n",
      "        >>> y_true = gen_data(t_test, a, b, c)\n",
      "        >>> y_lsq = gen_data(t_test, *res_lsq.x)\n",
      "        >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
      "        >>> y_log = gen_data(t_test, *res_log.x)\n",
      "        ...\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(t_train, y_train, 'o')\n",
      "        >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
      "        >>> plt.plot(t_test, y_lsq, label='linear loss')\n",
      "        >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
      "        >>> plt.plot(t_test, y_log, label='cauchy loss')\n",
      "        >>> plt.xlabel(\"t\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "        \n",
      "        In the next example, we show how complex-valued residual functions of\n",
      "        complex variables can be optimized with ``least_squares()``. Consider the\n",
      "        following function:\n",
      "        \n",
      "        >>> def f(z):\n",
      "        ...     return z - (0.5 + 0.5j)\n",
      "        \n",
      "        We wrap it into a function of real variables that returns real residuals\n",
      "        by simply handling the real and imaginary parts as independent variables:\n",
      "        \n",
      "        >>> def f_wrap(x):\n",
      "        ...     fx = f(x[0] + 1j*x[1])\n",
      "        ...     return np.array([fx.real, fx.imag])\n",
      "        \n",
      "        Thus, instead of the original m-D complex function of n complex\n",
      "        variables we optimize a 2m-D real function of 2n real variables:\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n",
      "        >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n",
      "        >>> z\n",
      "        (0.49999999999925893+0.49999999999925893j)\n",
      "    \n",
      "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
      "        Minimize the sum of squares of a set of equations.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            x = arg min(sum(func(y)**2,axis=0))\n",
      "                     y\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Should take at least one (possibly length ``N`` vector) argument and\n",
      "            returns ``M`` floating point numbers. It must not return NaNs or\n",
      "            fitting might fail. ``M`` must be greater than or equal to ``N``.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the minimization.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to func are placed in this tuple.\n",
      "        Dfun : callable, optional\n",
      "            A function or method to compute the Jacobian of func with derivatives\n",
      "            across the rows. If this is None, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            non-zero to return all optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            non-zero to specify that the Jacobian function computes derivatives\n",
      "            down the columns (faster, because there is no transpose operation).\n",
      "        ftol : float, optional\n",
      "            Relative error desired in the sum of squares.\n",
      "        xtol : float, optional\n",
      "            Relative error desired in the approximate solution.\n",
      "        gtol : float, optional\n",
      "            Orthogonality desired between the function vector and the columns of\n",
      "            the Jacobian.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If `Dfun` is provided,\n",
      "            then the default `maxfev` is 100*(N+1) where N is the number of elements\n",
      "            in x0, otherwise the default `maxfev` is 200*(N+1).\n",
      "        epsfcn : float, optional\n",
      "            A variable used in determining a suitable step length for the forward-\n",
      "            difference approximation of the Jacobian (for Dfun=None).\n",
      "            Normally the actual step length will be sqrt(epsfcn)*x\n",
      "            If epsfcn is less than the machine precision, it is assumed that the\n",
      "            relative errors are of the order of the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for an unsuccessful\n",
      "            call).\n",
      "        cov_x : ndarray\n",
      "            The inverse of the Hessian. `fjac` and `ipvt` are used to construct an\n",
      "            estimate of the Hessian. A value of None indicates a singular matrix,\n",
      "            which means the curvature in parameters `x` is numerically flat. To\n",
      "            obtain the covariance matrix of the parameters `x`, `cov_x` must be\n",
      "            multiplied by the variance of the residuals -- see curve_fit.\n",
      "        infodict : dict\n",
      "            a dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                The number of function calls\n",
      "            ``fvec``\n",
      "                The function evaluated at the output\n",
      "            ``fjac``\n",
      "                A permutation of the R matrix of a QR\n",
      "                factorization of the final approximate\n",
      "                Jacobian matrix, stored column wise.\n",
      "                Together with ipvt, the covariance of the\n",
      "                estimate can be approximated.\n",
      "            ``ipvt``\n",
      "                An integer array of length N which defines\n",
      "                a permutation matrix, p, such that\n",
      "                fjac*p = q*r, where r is upper triangular\n",
      "                with diagonal elements of nonincreasing\n",
      "                magnitude. Column j of p is column ipvt(j)\n",
      "                of the identity matrix.\n",
      "            ``qtf``\n",
      "                The vector (transpose(q) * fvec).\n",
      "        \n",
      "        mesg : str\n",
      "            A string message giving information about the cause of failure.\n",
      "        ier : int\n",
      "            An integer flag. If it is equal to 1, 2, 3 or 4, the solution was\n",
      "            found. Otherwise, the solution was not found. In either case, the\n",
      "            optional output variable 'mesg' gives more information.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Newer interface to solve nonlinear least-squares problems\n",
      "            with bounds on the variables. See ``method=='lm'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
      "        \n",
      "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
      "        objective function.\n",
      "        This approximation assumes that the objective function is based on the\n",
      "        difference between some observed target data (ydata) and a (non-linear)\n",
      "        function of the parameters `f(xdata, params)` ::\n",
      "        \n",
      "               func(params) = ydata - f(xdata, params)\n",
      "        \n",
      "        so that the objective function is ::\n",
      "        \n",
      "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
      "             params\n",
      "        \n",
      "        The solution, `x`, is always a 1-D array, regardless of the shape of `x0`,\n",
      "        or whether `x0` is a scalar.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import leastsq\n",
      "        >>> def func(x):\n",
      "        ...     return 2*(x-3)**2+1\n",
      "        >>> leastsq(func, 0)\n",
      "        (array([2.99999999]), 1)\n",
      "    \n",
      "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=None, extra_condition=None, maxiter=10)\n",
      "        Find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function.\n",
      "        myfprime : callable f'(x,*args)\n",
      "            Objective function gradient.\n",
      "        xk : ndarray\n",
      "            Starting point.\n",
      "        pk : ndarray\n",
      "            Search direction.\n",
      "        gfk : ndarray, optional\n",
      "            Gradient value for x=xk (xk being the current parameter\n",
      "            estimate). Will be recomputed if omitted.\n",
      "        old_fval : float, optional\n",
      "            Function value for x=xk. Will be recomputed if omitted.\n",
      "        old_old_fval : float, optional\n",
      "            Function value for the point preceding x=xk.\n",
      "        args : tuple, optional\n",
      "            Additional arguments passed to objective function.\n",
      "        c1 : float, optional\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, optional\n",
      "            Parameter for curvature condition rule.\n",
      "        amax : float, optional\n",
      "            Maximum step size\n",
      "        extra_condition : callable, optional\n",
      "            A callable of the form ``extra_condition(alpha, x, f, g)``\n",
      "            returning a boolean. Arguments are the proposed step ``alpha``\n",
      "            and the corresponding ``x``, ``f`` and ``g`` values. The line search\n",
      "            accepts the value of ``alpha`` only if this\n",
      "            callable returns ``True``. If the callable returns ``False``\n",
      "            for the step length, the algorithm will continue with\n",
      "            new iterates. The callable is only called for iterates\n",
      "            satisfying the strong Wolfe conditions.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alpha : float or None\n",
      "            Alpha for which ``x_new = x0 + alpha * pk``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        fc : int\n",
      "            Number of function evaluations made.\n",
      "        gc : int\n",
      "            Number of gradient evaluations made.\n",
      "        new_fval : float or None\n",
      "            New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        old_fval : float\n",
      "            Old function value ``f(x0)``.\n",
      "        new_slope : float or None\n",
      "            The local slope along the search direction at the\n",
      "            new value ``<myfprime(x_new), pk>``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses the line search algorithm to enforce strong Wolfe\n",
      "        conditions. See Wright and Nocedal, 'Numerical Optimization',\n",
      "        1999, pp. 59-61.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import line_search\n",
      "        \n",
      "        A objective function and its gradient are defined.\n",
      "        \n",
      "        >>> def obj_func(x):\n",
      "        ...     return (x[0])**2+(x[1])**2\n",
      "        >>> def obj_grad(x):\n",
      "        ...     return [2*x[0], 2*x[1]]\n",
      "        \n",
      "        We can find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        >>> start_point = np.array([1.8, 1.7])\n",
      "        >>> search_gradient = np.array([-1.0, -1.0])\n",
      "        >>> line_search(obj_func, obj_grad, start_point, search_gradient)\n",
      "        (1.0, 2, 1, 1.1300000000000001, 6.13, [1.6, 1.4])\n",
      "    \n",
      "    linear_sum_assignment(cost_matrix, maximize=False)\n",
      "        Solve the linear sum assignment problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cost_matrix : array\n",
      "            The cost matrix of the bipartite graph.\n",
      "        \n",
      "        maximize : bool (default: False)\n",
      "            Calculates a maximum weight matching if true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        row_ind, col_ind : array\n",
      "            An array of row indices and one of corresponding column indices giving\n",
      "            the optimal assignment. The cost of the assignment can be computed\n",
      "            as ``cost_matrix[row_ind, col_ind].sum()``. The row indices will be\n",
      "            sorted; in the case of a square cost matrix they will be equal to\n",
      "            ``numpy.arange(cost_matrix.shape[0])``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scipy.sparse.csgraph.min_weight_full_bipartite_matching : for sparse inputs\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        The linear sum assignment problem [1]_ is also known as minimum weight\n",
      "        matching in bipartite graphs. A problem instance is described by a matrix\n",
      "        C, where each C[i,j] is the cost of matching vertex i of the first partite\n",
      "        set (a \"worker\") and vertex j of the second set (a \"job\"). The goal is to\n",
      "        find a complete assignment of workers to jobs of minimal cost.\n",
      "        \n",
      "        Formally, let X be a boolean matrix where :math:`X[i,j] = 1` iff row i is\n",
      "        assigned to column j. Then the optimal assignment has cost\n",
      "        \n",
      "        .. math::\n",
      "            \\min \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
      "        \n",
      "        where, in the case where the matrix X is square, each row is assigned to\n",
      "        exactly one column, and each column to exactly one row.\n",
      "        \n",
      "        This function can also solve a generalization of the classic assignment\n",
      "        problem where the cost matrix is rectangular. If it has more rows than\n",
      "        columns, then not every row needs to be assigned to a column, and vice\n",
      "        versa.\n",
      "        \n",
      "        This implementation is a modified Jonker-Volgenant algorithm with no\n",
      "        initialization, described in ref. [2]_.\n",
      "        \n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] https://en.wikipedia.org/wiki/Assignment_problem\n",
      "        \n",
      "        .. [2] DF Crouse. On implementing 2D rectangular assignment algorithms.\n",
      "               *IEEE Transactions on Aerospace and Electronic Systems*,\n",
      "               52(4):1679-1696, August 2016, :doi:`10.1109/TAES.2016.140952`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
      "        >>> from scipy.optimize import linear_sum_assignment\n",
      "        >>> row_ind, col_ind = linear_sum_assignment(cost)\n",
      "        >>> col_ind\n",
      "        array([1, 0, 2])\n",
      "        >>> cost[row_ind, col_ind].sum()\n",
      "        5\n",
      "    \n",
      "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a scalar Jacobian approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            The Jacobian approximation is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='linearmixing'`` in particular.\n",
      "    \n",
      "    linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='interior-point', callback=None, options=None, x0=None)\n",
      "        Linear programming: minimize a linear objective function subject to linear\n",
      "        equality and inequality constraints.\n",
      "        \n",
      "        Linear programming solves problems of the following form:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_x \\ & c^T x \\\\\n",
      "            \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "            & A_{eq} x = b_{eq},\\\\\n",
      "            & l \\leq x \\leq u ,\n",
      "        \n",
      "        where :math:`x` is a vector of decision variables; :math:`c`,\n",
      "        :math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      "        :math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "        \n",
      "        Alternatively, that's:\n",
      "        \n",
      "        minimize::\n",
      "        \n",
      "            c @ x\n",
      "        \n",
      "        such that::\n",
      "        \n",
      "            A_ub @ x <= b_ub\n",
      "            A_eq @ x == b_eq\n",
      "            lb <= x <= ub\n",
      "        \n",
      "        Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "        ``bounds``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        c : 1-D array\n",
      "            The coefficients of the linear objective function to be minimized.\n",
      "        A_ub : 2-D array, optional\n",
      "            The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "            coefficients of a linear inequality constraint on ``x``.\n",
      "        b_ub : 1-D array, optional\n",
      "            The inequality constraint vector. Each element represents an\n",
      "            upper bound on the corresponding value of ``A_ub @ x``.\n",
      "        A_eq : 2-D array, optional\n",
      "            The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "            coefficients of a linear equality constraint on ``x``.\n",
      "        b_eq : 1-D array, optional\n",
      "            The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "            the corresponding element of ``b_eq``.\n",
      "        bounds : sequence, optional\n",
      "            A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the minimum and maximum values of that decision variable. Use ``None``\n",
      "            to indicate that there is no bound. By default, bounds are\n",
      "            ``(0, None)`` (all decision variables are non-negative).\n",
      "            If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "            ``max`` will serve as bounds for all decision variables.\n",
      "        method : str, optional\n",
      "            The algorithm used to solve the standard form problem.\n",
      "            :ref:`'highs-ds' <optimize.linprog-highs-ds>`,\n",
      "            :ref:`'highs-ipm' <optimize.linprog-highs-ipm>`,\n",
      "            :ref:`'highs' <optimize.linprog-highs>`,\n",
      "            :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "            :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "            :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "            are supported.\n",
      "        callback : callable, optional\n",
      "            If a callback function is provided, it will be called at least once per\n",
      "            iteration of the algorithm. The callback function must accept a single\n",
      "            `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "            x : 1-D array\n",
      "                The current solution vector.\n",
      "            fun : float\n",
      "                The current value of the objective function ``c @ x``.\n",
      "            success : bool\n",
      "                ``True`` when the algorithm has completed successfully.\n",
      "            slack : 1-D array\n",
      "                The (nominally positive) values of the slack,\n",
      "                ``b_ub - A_ub @ x``.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints,\n",
      "                ``b_eq - A_eq @ x``.\n",
      "            phase : int\n",
      "                The phase of the algorithm being executed.\n",
      "            status : int\n",
      "                An integer representing the status of the algorithm.\n",
      "        \n",
      "                ``0`` : Optimization proceeding nominally.\n",
      "        \n",
      "                ``1`` : Iteration limit reached.\n",
      "        \n",
      "                ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "                nit : int\n",
      "                    The current iteration number.\n",
      "                message : str\n",
      "                    A string descriptor of the algorithm status.\n",
      "        \n",
      "            Callback functions are not currently supported by the HiGHS methods.\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            options:\n",
      "        \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "                Default: see method-specific documentation.\n",
      "            disp : bool\n",
      "                Set to ``True`` to print convergence messages.\n",
      "                Default: ``False``.\n",
      "            presolve : bool\n",
      "                Set to ``False`` to disable automatic presolve.\n",
      "                Default: ``True``.\n",
      "        \n",
      "            All methods except the HiGHS solvers also accept:\n",
      "        \n",
      "            tol : float\n",
      "                A tolerance which determines when a residual is \"close enough\" to\n",
      "                zero to be considered exactly zero.\n",
      "            autoscale : bool\n",
      "                Set to ``True`` to automatically perform equilibration.\n",
      "                Consider using this option if the numerical values in the\n",
      "                constraints are separated by several orders of magnitude.\n",
      "                Default: ``False``.\n",
      "            rr : bool\n",
      "                Set to ``False`` to disable automatic redundancy removal.\n",
      "                Default: ``True``.\n",
      "            rr_method : string\n",
      "                Method used to identify and remove redundant rows from the\n",
      "                equality constraint matrix after presolve. For problems with\n",
      "                dense input, the available methods for redundancy removal are:\n",
      "        \n",
      "                \"SVD\":\n",
      "                    Repeatedly performs singular value decomposition on\n",
      "                    the matrix, detecting redundant rows based on nonzeros\n",
      "                    in the left singular vectors that correspond with\n",
      "                    zero singular values. May be fast when the matrix is\n",
      "                    nearly full rank.\n",
      "                \"pivot\":\n",
      "                    Uses the algorithm presented in [5]_ to identify\n",
      "                    redundant rows.\n",
      "                \"ID\":\n",
      "                    Uses a randomized interpolative decomposition.\n",
      "                    Identifies columns of the matrix transpose not used in\n",
      "                    a full-rank interpolative decomposition of the matrix.\n",
      "                None:\n",
      "                    Uses \"svd\" if the matrix is nearly full rank, that is,\n",
      "                    the difference between the matrix rank and the number\n",
      "                    of rows is less than five. If not, uses \"pivot\". The\n",
      "                    behavior of this default is subject to change without\n",
      "                    prior notice.\n",
      "        \n",
      "                Default: None.\n",
      "                For problems with sparse input, this option is ignored, and the\n",
      "                pivot-based algorithm presented in [5]_ is used.\n",
      "        \n",
      "            For method-specific options, see\n",
      "            :func:`show_options('linprog') <show_options>`.\n",
      "        \n",
      "        x0 : 1-D array, optional\n",
      "            Guess values of the decision variables, which will be refined by\n",
      "            the optimization algorithm. This argument is currently used only by the\n",
      "            'revised simplex' method, and can only be used if `x0` represents a\n",
      "            basic feasible solution.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "        \n",
      "            x : 1-D array\n",
      "                The values of the decision variables that minimizes the\n",
      "                objective function while satisfying the constraints.\n",
      "            fun : float\n",
      "                The optimal value of the objective function ``c @ x``.\n",
      "            slack : 1-D array\n",
      "                The (nominally positive) values of the slack variables,\n",
      "                ``b_ub - A_ub @ x``.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints,\n",
      "                ``b_eq - A_eq @ x``.\n",
      "            success : bool\n",
      "                ``True`` when the algorithm succeeds in finding an optimal\n",
      "                solution.\n",
      "            status : int\n",
      "                An integer representing the exit status of the algorithm.\n",
      "        \n",
      "                ``0`` : Optimization terminated successfully.\n",
      "        \n",
      "                ``1`` : Iteration limit reached.\n",
      "        \n",
      "                ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "            nit : int\n",
      "                The total number of iterations performed in all phases.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the algorithm.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        `'highs-ds'` and\n",
      "        `'highs-ipm'` are interfaces to the\n",
      "        HiGHS simplex and interior-point method solvers [13]_, respectively.\n",
      "        `'highs'` chooses between\n",
      "        the two automatically. These are the fastest linear\n",
      "        programming solvers in SciPy, especially for large, sparse problems;\n",
      "        which of these two is faster is problem-dependent.\n",
      "        `'interior-point'` is the default\n",
      "        as it was the fastest and most robust method before the recent\n",
      "        addition of the HiGHS solvers.\n",
      "        `'revised simplex'` is more\n",
      "        accurate than interior-point for the problems it solves.\n",
      "        `'simplex'` is the legacy method and is\n",
      "        included for backwards compatibility and educational purposes.\n",
      "        \n",
      "        Method *highs-ds* is a wrapper of the C++ high performance dual\n",
      "        revised simplex implementation (HSOL) [13]_, [14]_. Method *highs-ipm*\n",
      "        is a wrapper of a C++ implementation of an **i**\\ nterior-\\ **p**\\ oint\n",
      "        **m**\\ ethod [13]_; it features a crossover routine, so it is as accurate\n",
      "        as a simplex solver. Method *highs* chooses between the two automatically.\n",
      "        For new code involving `linprog`, we recommend explicitly choosing one of\n",
      "        these three method values.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        Method *interior-point* uses the primal-dual path following algorithm\n",
      "        as outlined in [4]_. This algorithm supports sparse constraint matrices and\n",
      "        is typically faster than the simplex methods, especially for large, sparse\n",
      "        problems. Note, however, that the solution returned may be slightly less\n",
      "        accurate than those of the simplex methods and will not, in general,\n",
      "        correspond with a vertex of the polytope defined by the constraints.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Method *revised simplex* uses the revised simplex method as described in\n",
      "        [9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "        its inverse, is efficiently maintained and used to solve the linear systems\n",
      "        at each iteration of the algorithm.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Method *simplex* uses a traditional, full-tableau implementation of\n",
      "        Dantzig's simplex algorithm [1]_, [2]_ (*not* the\n",
      "        Nelder-Mead simplex). This algorithm is included for backwards\n",
      "        compatibility and educational purposes.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Before applying *interior-point*, *revised simplex*, or *simplex*,\n",
      "        a presolve procedure based on [8]_ attempts\n",
      "        to identify trivial infeasibilities, trivial unboundedness, and potential\n",
      "        problem simplifications. Specifically, it checks for:\n",
      "        \n",
      "        - rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "        - columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "          variables;\n",
      "        - column singletons in ``A_eq``, representing fixed variables; and\n",
      "        - column singletons in ``A_ub``, representing simple bounds.\n",
      "        \n",
      "        If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "        and unbounded variable has negative cost) or infeasible (e.g., a row of\n",
      "        zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "        terminates with the appropriate status code. Note that presolve terminates\n",
      "        as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "        may be reported as unbounded when in reality the problem is infeasible\n",
      "        (but infeasibility has not been detected yet). Therefore, if it is\n",
      "        important to know whether the problem is actually infeasible, solve the\n",
      "        problem again with option ``presolve=False``.\n",
      "        \n",
      "        If neither infeasibility nor unboundedness are detected in a single pass\n",
      "        of the presolve, bounds are tightened where possible and fixed\n",
      "        variables are removed from the problem. Then, linearly dependent rows\n",
      "        of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "        infeasibility) to avoid numerical difficulties in the primary solve\n",
      "        routine. Note that rows that are nearly linearly dependent (within a\n",
      "        prescribed tolerance) may also be removed, which can change the optimal\n",
      "        solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "        your problem formulation and run with option ``rr=False`` or\n",
      "        ``presolve=False``.\n",
      "        \n",
      "        Several potential improvements can be made here: additional presolve\n",
      "        checks outlined in [8]_ should be implemented, the presolve routine should\n",
      "        be run multiple times (until no further simplifications can be made), and\n",
      "        more of the efficiency improvements from [5]_ should be implemented in the\n",
      "        redundancy removal routines.\n",
      "        \n",
      "        After presolve, the problem is transformed to standard form by converting\n",
      "        the (tightened) simple bounds to upper bound constraints, introducing\n",
      "        non-negative slack variables for inequality constraints, and expressing\n",
      "        unbounded variables as the difference between two non-negative variables.\n",
      "        Optionally, the problem is automatically scaled via equilibration [12]_.\n",
      "        The selected algorithm solves the standard form problem, and a\n",
      "        postprocessing routine converts the result to a solution to the original\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "               Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "               1963\n",
      "        .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "               Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "        .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "               Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "        .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "               optimizer for linear programming: an implementation of the\n",
      "               homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "               2000. 197-232.\n",
      "        .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "               large-scale linear programming.\" Optimization Methods and Software\n",
      "               6.3 (1995): 219-227.\n",
      "        .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "               Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "               March 2004. Available 2/25/2017 at\n",
      "               https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "        .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "               Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "               http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "        .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "               programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "        .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "               programming.\" Athena Scientific 1 (1997): 997.\n",
      "        .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "                methods for large scale linear programming. HEC/Universite de\n",
      "                Geneve, 1996.\n",
      "        .. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "                Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "        .. [12] Tomlin, J. A. \"On scaling linear programming problems.\"\n",
      "                Mathematical Programming Study 4 (1975): 146-166.\n",
      "        .. [13] Huangfu, Q., Galabova, I., Feldmeier, M., and Hall, J. A. J.\n",
      "                \"HiGHS - high performance software for linear optimization.\"\n",
      "                Accessed 4/16/2020 at https://www.maths.ed.ac.uk/hall/HiGHS/#guide\n",
      "        .. [14] Huangfu, Q. and Hall, J. A. J. \"Parallelizing the dual revised\n",
      "                simplex method.\" Mathematical Programming Computation, 10 (1),\n",
      "                119-142, 2018. DOI: 10.1007/s12532-017-0130-5\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the following problem:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_{x_0, x_1} \\ -x_0 + 4x_1 & \\\\\n",
      "            \\mbox{such that} \\ -3x_0 + x_1 & \\leq 6,\\\\\n",
      "            -x_0 - 2x_1 & \\geq -4,\\\\\n",
      "            x_1 & \\geq -3.\n",
      "        \n",
      "        The problem is not presented in the form accepted by `linprog`. This is\n",
      "        easily remedied by converting the \"greater than\" inequality\n",
      "        constraint to a \"less than\" inequality constraint by\n",
      "        multiplying both sides by a factor of :math:`-1`. Note also that the last\n",
      "        constraint is really the simple bound :math:`-3 \\leq x_1 \\leq \\infty`.\n",
      "        Finally, since there are no bounds on :math:`x_0`, we must explicitly\n",
      "        specify the bounds :math:`-\\infty \\leq x_0 \\leq \\infty`, as the\n",
      "        default is for variables to be non-negative. After collecting coeffecients\n",
      "        into arrays and tuples, the input for this problem is:\n",
      "        \n",
      "        >>> c = [-1, 4]\n",
      "        >>> A = [[-3, 1], [1, 2]]\n",
      "        >>> b = [6, 4]\n",
      "        >>> x0_bounds = (None, None)\n",
      "        >>> x1_bounds = (-3, None)\n",
      "        >>> from scipy.optimize import linprog\n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds])\n",
      "        \n",
      "        Note that the default method for `linprog` is 'interior-point', which is\n",
      "        approximate by nature.\n",
      "        \n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -21.99999984082494 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 6 # may vary\n",
      "           slack: array([3.89999997e+01, 8.46872439e-08] # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([ 9.99999989, -2.99999999]) # may vary\n",
      "        \n",
      "        If you need greater accuracy, try 'revised simplex'.\n",
      "        \n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds], method='revised simplex')\n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -22.0 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 1 # may vary\n",
      "           slack: array([39.,  0.]) # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([10., -3.]) # may vary\n",
      "    \n",
      "    linprog_verbose_callback(res)\n",
      "        A sample callback function demonstrating the linprog callback interface.\n",
      "        This callback produces detailed output to sys.stdout before each iteration\n",
      "        and after the final iteration of the simplex algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        res : A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "            x : 1-D array\n",
      "                The independent variable vector which optimizes the linear\n",
      "                programming problem.\n",
      "            fun : float\n",
      "                Value of the objective function.\n",
      "            success : bool\n",
      "                True if the algorithm succeeded in finding an optimal solution.\n",
      "            slack : 1-D array\n",
      "                The values of the slack variables. Each slack variable corresponds\n",
      "                to an inequality constraint. If the slack is zero, then the\n",
      "                corresponding constraint is active.\n",
      "            con : 1-D array\n",
      "                The (nominally zero) residuals of the equality constraints, that is,\n",
      "                ``b - A_eq @ x``\n",
      "            phase : int\n",
      "                The phase of the optimization being executed. In phase 1 a basic\n",
      "                feasible solution is sought and the T has an additional row\n",
      "                representing an alternate objective function.\n",
      "            status : int\n",
      "                An integer representing the exit status of the optimization::\n",
      "        \n",
      "                     0 : Optimization terminated successfully\n",
      "                     1 : Iteration limit reached\n",
      "                     2 : Problem appears to be infeasible\n",
      "                     3 : Problem appears to be unbounded\n",
      "                     4 : Serious numerical difficulties encountered\n",
      "        \n",
      "            nit : int\n",
      "                The number of iterations performed.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the optimization.\n",
      "    \n",
      "    lsq_linear(A, b, bounds=(-inf, inf), method='trf', tol=1e-10, lsq_solver=None, lsmr_tol=None, max_iter=None, verbose=0)\n",
      "        Solve a linear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given a m-by-n design matrix A and a target vector b with m elements,\n",
      "        `lsq_linear` solves the following optimization problem::\n",
      "        \n",
      "            minimize 0.5 * ||A x - b||**2\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        This optimization problem is convex, hence a found minimum (if iterations\n",
      "        have converged) is guaranteed to be global.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : array_like, sparse matrix of LinearOperator, shape (m, n)\n",
      "            Design matrix. Can be `scipy.sparse.linalg.LinearOperator`.\n",
      "        b : array_like, shape (m,)\n",
      "            Target vector.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must have shape (n,) or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : 'trf' or 'bvls', optional\n",
      "            Method to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm adapted for a linear\n",
      "                  least-squares problem. This is an interior-point-like method\n",
      "                  and the required number of iterations is weakly correlated with\n",
      "                  the number of variables.\n",
      "                * 'bvls' : Bounded-variable least-squares algorithm. This is\n",
      "                  an active set method, which requires the number of iterations\n",
      "                  comparable to the number of variables. Can't be used when `A` is\n",
      "                  sparse or LinearOperator.\n",
      "        \n",
      "            Default is 'trf'.\n",
      "        tol : float, optional\n",
      "            Tolerance parameter. The algorithm terminates if a relative change\n",
      "            of the cost function is less than `tol` on the last iteration.\n",
      "            Additionally, the first-order optimality measure is considered:\n",
      "        \n",
      "                * ``method='trf'`` terminates if the uniform norm of the gradient,\n",
      "                  scaled to account for the presence of the bounds, is less than\n",
      "                  `tol`.\n",
      "                * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n",
      "                  are satisfied within `tol` tolerance.\n",
      "        \n",
      "        lsq_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method of solving unbounded least-squares problems throughout\n",
      "            iterations:\n",
      "        \n",
      "                * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n",
      "                  used when `A` is sparse or LinearOperator.\n",
      "                * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n",
      "                  which requires only matrix-vector product evaluations. Can't\n",
      "                  be used with ``method='bvls'``.\n",
      "        \n",
      "            If None (default), the solver is chosen based on type of `A`.\n",
      "        lsmr_tol : None, float or 'auto', optional\n",
      "            Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n",
      "            If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n",
      "            tolerance will be adjusted based on the optimality of the current\n",
      "            iterate, which can speed up the optimization process, but is not always\n",
      "            reliable.\n",
      "        max_iter : None or int, optional\n",
      "            Maximum number of iterations before termination. If None (default), it\n",
      "            is set to 100 for ``method='trf'`` or to the number of variables for\n",
      "            ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 : work silently (default).\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        OptimizeResult with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. The exact meaning depends on `method`,\n",
      "            refer to the description of `tol` parameter.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for the `trf` method as it generates a\n",
      "            sequence of strictly feasible iterates and active_mask is determined\n",
      "            within a tolerance threshold.\n",
      "        nit : int\n",
      "            Number of iterations. Zero if the unconstrained solution is optimal.\n",
      "        status : int\n",
      "            Reason for algorithm termination:\n",
      "        \n",
      "                * -1 : the algorithm was not able to make progress on the last\n",
      "                  iteration.\n",
      "                *  0 : the maximum number of iterations is exceeded.\n",
      "                *  1 : the first-order optimality measure is less than `tol`.\n",
      "                *  2 : the relative change of the cost function is less than `tol`.\n",
      "                *  3 : the unconstrained solution is optimal.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nnls : Linear least squares with non-negativity constraint.\n",
      "        least_squares : Nonlinear least squares with bounds on the variables.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm first computes the unconstrained least-squares solution by\n",
      "        `numpy.linalg.lstsq` or `scipy.sparse.linalg.lsmr` depending on\n",
      "        `lsq_solver`. This solution is returned as optimal if it lies within the\n",
      "        bounds.\n",
      "        \n",
      "        Method 'trf' runs the adaptation of the algorithm described in [STIR]_ for\n",
      "        a linear least-squares problem. The iterations are essentially the same as\n",
      "        in the nonlinear least-squares algorithm, but as the quadratic function\n",
      "        model is always accurate, we don't need to track or modify the radius of\n",
      "        a trust region. The line search (backtracking) is used as a safety net\n",
      "        when a selected step does not decrease the cost function. Read more\n",
      "        detailed description of the algorithm in `scipy.optimize.least_squares`.\n",
      "        \n",
      "        Method 'bvls' runs a Python implementation of the algorithm described in\n",
      "        [BVLS]_. The algorithm maintains active and free sets of variables, on\n",
      "        each iteration chooses a new variable to move from the active set to the\n",
      "        free set and then solves the unconstrained least-squares problem on free\n",
      "        variables. This algorithm is guaranteed to give an accurate solution\n",
      "        eventually, but may require up to n iterations for a problem with n\n",
      "        variables. Additionally, an ad-hoc initialization procedure is\n",
      "        implemented, that determines which variables to set free or active\n",
      "        initially. It takes some number of iterations before actual BVLS starts,\n",
      "        but can significantly reduce the number of further iterations.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n",
      "                  an Algorithm and Applications\", Computational Statistics, 10,\n",
      "                  129-141, 1995.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example, a problem with a large sparse matrix and bounds on the\n",
      "        variables is solved.\n",
      "        \n",
      "        >>> from scipy.sparse import rand\n",
      "        >>> from scipy.optimize import lsq_linear\n",
      "        >>> rng = np.random.default_rng()\n",
      "        ...\n",
      "        >>> m = 20000\n",
      "        >>> n = 10000\n",
      "        ...\n",
      "        >>> A = rand(m, n, density=1e-4, random_state=rng)\n",
      "        >>> b = rng.standard_normal(m)\n",
      "        ...\n",
      "        >>> lb = rng.standard_normal(n)\n",
      "        >>> ub = lb + 1\n",
      "        ...\n",
      "        >>> res = lsq_linear(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1)\n",
      "        # may vary\n",
      "        The relative change of the cost function is less than `tol`.\n",
      "        Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n",
      "        first-order optimality 4.66e-08.\n",
      "    \n",
      "    minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "        Minimization of scalar function of one or more variables.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            The objective function to be minimized.\n",
      "        \n",
      "                ``fun(x, *args) -> float``\n",
      "        \n",
      "            where ``x`` is an 1-D array with shape (n,) and ``args``\n",
      "            is a tuple of the fixed parameters needed to completely\n",
      "            specify the function.\n",
      "        x0 : ndarray, shape (n,)\n",
      "            Initial guess. Array of real elements of size (n,),\n",
      "            where 'n' is the number of independent variables.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its\n",
      "            derivatives (`fun`, `jac` and `hess` functions).\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "                - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "                - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "                - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "                - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "                - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "                - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "                - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "                - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "                - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "                - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "                - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "                - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "                - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "                - custom - a callable object (added in version 0.14.0),\n",
      "                  see below for description.\n",
      "        \n",
      "            If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "            depending if the problem has constraints or bounds.\n",
      "        jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "            Method for computing the gradient vector. Only for CG, BFGS,\n",
      "            Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "            trust-exact and trust-constr.\n",
      "            If it is a callable, it should be a function that returns the gradient\n",
      "            vector:\n",
      "        \n",
      "                ``jac(x, *args) -> array_like, shape (n,)``\n",
      "        \n",
      "            where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "            the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "            assumed to return a tuple ``(f, g)`` containing the objective\n",
      "            function and the gradient.\n",
      "            Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "            'trust-krylov' require that either a callable be supplied, or that\n",
      "            `fun` return the objective and gradient.\n",
      "            If None or False, the gradient will be estimated using 2-point finite\n",
      "            difference estimation with an absolute step size.\n",
      "            Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "            to select a finite difference scheme for numerical estimation of the\n",
      "            gradient with a relative step size. These finite difference schemes\n",
      "            obey any specified `bounds`.\n",
      "        hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "            Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "            trust-ncg, trust-krylov, trust-exact and trust-constr. If it is\n",
      "            callable, it should return the Hessian matrix:\n",
      "        \n",
      "                ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "        \n",
      "            where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "            parameters. LinearOperator and sparse matrix returns are only allowed\n",
      "            for 'trust-constr' method. Alternatively, the keywords\n",
      "            {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "            for numerical estimation. Or, objects implementing the\n",
      "            `HessianUpdateStrategy` interface can be used to approximate\n",
      "            the Hessian. Available quasi-Newton methods implementing\n",
      "            this interface are:\n",
      "        \n",
      "                - `BFGS`;\n",
      "                - `SR1`.\n",
      "        \n",
      "            Whenever the gradient is estimated via finite-differences,\n",
      "            the Hessian cannot be estimated with options\n",
      "            {'2-point', '3-point', 'cs'} and needs to be\n",
      "            estimated using one of the quasi-Newton strategies.\n",
      "            'trust-exact' cannot use a finite-difference scheme, and must be used\n",
      "            with a callable returning an (n, n) array.\n",
      "        hessp : callable, optional\n",
      "            Hessian of objective function times an arbitrary vector p. Only for\n",
      "            Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "            provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "            Hessian times an arbitrary vector:\n",
      "        \n",
      "                ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "        \n",
      "            where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "            dimension (n,) and `args` is a tuple with the fixed\n",
      "            parameters.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds on variables for Nelder-Mead, L-BFGS-B, TNC, SLSQP, Powell, and\n",
      "            trust-constr methods. There are two ways to specify the bounds:\n",
      "        \n",
      "                1. Instance of `Bounds` class.\n",
      "                2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "                   is used to specify no bound.\n",
      "        \n",
      "        constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "            Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "        \n",
      "            Constraints for 'trust-constr' are defined as a single object or a\n",
      "            list of objects specifying constraints to the optimization problem.\n",
      "            Available constraints are:\n",
      "        \n",
      "                - `LinearConstraint`\n",
      "                - `NonlinearConstraint`\n",
      "        \n",
      "            Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "            Each dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. When `tol` is specified, the selected\n",
      "            minimization algorithm sets some relevant solver-specific tolerance(s)\n",
      "            equal to `tol`. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            generic options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform. Depending on the\n",
      "                    method each iteration may use several function evaluations.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            For method-specific options, see :func:`show_options()`.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration. For 'trust-constr' it is a callable with\n",
      "            the signature:\n",
      "        \n",
      "                ``callback(xk, OptimizeResult state) -> bool``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector. and ``state``\n",
      "            is an `OptimizeResult` object, with the same fields\n",
      "            as the ones from the return. If callback returns True\n",
      "            the algorithm execution is terminated.\n",
      "            For all the other methods, the signature is:\n",
      "        \n",
      "                ``callback(xk)``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar : Interface to minimization algorithms for scalar\n",
      "            univariate functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *BFGS*.\n",
      "        \n",
      "        **Unconstrained minimization**\n",
      "        \n",
      "        Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "        gradient algorithm by Polak and Ribiere, a variant of the\n",
      "        Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "        first derivatives are used.\n",
      "        \n",
      "        Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "        method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "        pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "        performance even for non-smooth optimizations. This method also\n",
      "        returns an approximation of the Hessian inverse, stored as\n",
      "        `hess_inv` in the OptimizeResult object.\n",
      "        \n",
      "        Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "        Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "        Newton method). It uses a CG method to the compute the search\n",
      "        direction. See also *TNC* method for a box-constrained\n",
      "        minimization with a similar algorithm. Suitable for large-scale\n",
      "        problems.\n",
      "        \n",
      "        Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "        trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "        algorithm requires the gradient and Hessian; furthermore the\n",
      "        Hessian is required to be positive definite.\n",
      "        \n",
      "        Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "        Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "        unconstrained minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "        the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "        minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        On indefinite problems it requires usually less iterations than the\n",
      "        `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "        is a trust-region method for unconstrained minimization in which\n",
      "        quadratic subproblems are solved almost exactly [13]_. This\n",
      "        algorithm requires the gradient and the Hessian (which is\n",
      "        *not* required to be positive definite). It is, in many\n",
      "        situations, the Newton method to converge in fewer iteraction\n",
      "        and the most recommended for small and medium-size problems.\n",
      "        \n",
      "        **Bound-Constrained minimization**\n",
      "        \n",
      "        Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "        Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "        applications. However, if numerical computation of derivative can be\n",
      "        trusted, other algorithms using the first and/or second derivatives\n",
      "        information might be preferred for their better performance in\n",
      "        general.\n",
      "        \n",
      "        Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "        algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "        \n",
      "        Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "        of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "        method. It performs sequential one-dimensional minimizations along\n",
      "        each vector of the directions set (`direc` field in `options` and\n",
      "        `info`), which is updated at each iteration of the main\n",
      "        minimization loop. The function need not be differentiable, and no\n",
      "        derivatives are taken. If bounds are not provided, then an\n",
      "        unbounded line search will be used. If bounds are provided and\n",
      "        the initial guess is within the bounds, then every function\n",
      "        evaluation throughout the minimization procedure will be within\n",
      "        the bounds. If bounds are provided, the initial guess is outside\n",
      "        the bounds, and `direc` is full rank (default has full rank), then\n",
      "        some function evaluations during the first iteration may be\n",
      "        outside the bounds, but every function evaluation after the first\n",
      "        iteration will be within the bounds. If `direc` is not full rank,\n",
      "        then some parameters may not be optimized and the solution is not\n",
      "        guaranteed to be within the bounds.\n",
      "        \n",
      "        Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "        algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "        to bounds. This algorithm uses gradient information; it is also\n",
      "        called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "        method described above as it wraps a C implementation and allows\n",
      "        each variable to be given upper and lower bounds.\n",
      "        \n",
      "        **Constrained Minimization**\n",
      "        \n",
      "        Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "        Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "        [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "        approximations to the objective function and each constraint. The\n",
      "        method wraps a FORTRAN implementation of the algorithm. The\n",
      "        constraints functions 'fun' may return either a single number\n",
      "        or an array or list of numbers.\n",
      "        \n",
      "        Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "        Least SQuares Programming to minimize a function of several\n",
      "        variables with any combination of bounds, equality and inequality\n",
      "        constraints. The method wraps the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft [12]_. Note that the\n",
      "        wrapper handles infinite values in bounds by converting them into\n",
      "        large floating values.\n",
      "        \n",
      "        Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "        trust-region algorithm for constrained optimization. It swiches\n",
      "        between two implementations depending on the problem definition.\n",
      "        It is the most versatile constrained minimization algorithm\n",
      "        implemented in SciPy and the most appropriate for large-scale problems.\n",
      "        For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "        Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "        inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "        interior point  method described in [16]_. This interior point algorithm,\n",
      "        in turn, solves inequality constraints by introducing slack variables\n",
      "        and solving a sequence of equality-constrained barrier problems\n",
      "        for progressively smaller values of the barrier parameter.\n",
      "        The previously described equality constrained SQP method is\n",
      "        used to solve the subproblems with increasing levels of accuracy\n",
      "        as the iterate gets closer to a solution.\n",
      "        \n",
      "        **Finite-Difference Options**\n",
      "        \n",
      "        For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "        the gradient and the Hessian may be approximated using\n",
      "        three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "        The scheme 'cs' is, potentially, the most accurate but it\n",
      "        requires the function to correctly handles complex inputs and to\n",
      "        be differentiable in the complex plane. The scheme '3-point' is more\n",
      "        accurate than '2-point' but requires twice as many operations.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "        or a different library.  You can simply pass a callable as the ``method``\n",
      "        parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "        `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "        `fun` returns just the function values and `jac` is converted to a function\n",
      "        returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "        object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "            Minimization. The Computer Journal 7: 308-13.\n",
      "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "            191-208.\n",
      "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "           a function of several variables without calculating derivatives. The\n",
      "           Computer Journal 7: 155-162.\n",
      "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "           Numerical Recipes (any edition), Cambridge University Press.\n",
      "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "           Springer New York.\n",
      "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "           550-560.\n",
      "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "        .. [9] Powell, M J D. A direct search optimization method that models\n",
      "           the objective and constraint functions by linear interpolation.\n",
      "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "           calculations. 1998. Acta Numerica 7: 287-336.\n",
      "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "           2007/NA03\n",
      "        .. [12] Kraft, D. A software package for sequential quadratic\n",
      "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "        .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "           Trust region methods. 2000. Siam. pp. 169-200.\n",
      "        .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "           implementation of the GLTR method for iterative solution of\n",
      "           the trust region problem\", :arxiv:`1611.04718`\n",
      "        .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "           Trust-Region Subproblem using the Lanczos Method\",\n",
      "           SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "        .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "            An interior point algorithm for large-scale nonlinear  programming.\n",
      "            SIAM Journal on Optimization 9.4: 877-900.\n",
      "        .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "            implementation of an algorithm for large-scale equality constrained\n",
      "            optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function (and its respective derivatives) is implemented in `rosen`\n",
      "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "        \n",
      "        A simple application of the *Nelder-Mead* method is:\n",
      "        \n",
      "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "        >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        \n",
      "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "        options:\n",
      "        \n",
      "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "        ...                options={'gtol': 1e-6, 'disp': True})\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 26\n",
      "                 Function evaluations: 31\n",
      "                 Gradient evaluations: 31\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        >>> print(res.message)\n",
      "        Optimization terminated successfully.\n",
      "        >>> res.hess_inv\n",
      "        array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "               [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "               [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "               [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "               [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "        \n",
      "        \n",
      "        Next, consider a minimization problem with several constraints (namely\n",
      "        Example 16.4 from [5]_). The objective function is:\n",
      "        \n",
      "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "        \n",
      "        There are three constraints defined as:\n",
      "        \n",
      "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "        \n",
      "        And variables must be positive, hence the following bounds:\n",
      "        \n",
      "        >>> bnds = ((0, None), (0, None))\n",
      "        \n",
      "        The optimization problem is solved using the SLSQP method as:\n",
      "        \n",
      "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "        ...                constraints=cons)\n",
      "        \n",
      "        It should converge to the theoretical solution (1.4 ,1.7).\n",
      "    \n",
      "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
      "        Minimization of scalar function of one variable.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Objective function.\n",
      "            Scalar function, must return a scalar.\n",
      "        bracket : sequence, optional\n",
      "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
      "            interval and can either have three items ``(a, b, c)`` so that\n",
      "            ``a < b < c`` and ``fun(b) < fun(a), fun(c)`` or two items ``a`` and\n",
      "            ``c`` which are assumed to be a starting interval for a downhill\n",
      "            bracket search (see `bracket`); it doesn't always mean that the\n",
      "            obtained solution will satisfy ``a <= x <= c``.\n",
      "        bounds : sequence, optional\n",
      "            For method 'bounded', `bounds` is mandatory and must have two items\n",
      "            corresponding to the optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function.\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of:\n",
      "        \n",
      "                - 'Brent'     :ref:`(see here) <optimize.minimize_scalar-brent>`\n",
      "                - 'Bounded'   :ref:`(see here) <optimize.minimize_scalar-bounded>`\n",
      "                - 'Golden'    :ref:`(see here) <optimize.minimize_scalar-golden>`\n",
      "                - custom - a callable object (added in version 0.14.0), see below\n",
      "        \n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options.\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            See :func:`show_options()` for solver-specific options.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize : Interface to minimization algorithms for scalar multivariate\n",
      "            functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *Brent*.\n",
      "        \n",
      "        Method :ref:`Brent <optimize.minimize_scalar-brent>` uses Brent's\n",
      "        algorithm to find a local minimum.  The algorithm uses inverse\n",
      "        parabolic interpolation when possible to speed up convergence of\n",
      "        the golden section method.\n",
      "        \n",
      "        Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
      "        golden section search technique. It uses analog of the bisection\n",
      "        method to decrease the bracketed interval. It is usually\n",
      "        preferable to use the *Brent* method.\n",
      "        \n",
      "        Method :ref:`Bounded <optimize.minimize_scalar-bounded>` can\n",
      "        perform bounded minimization. It uses the Brent method to find a\n",
      "        local minimum in the interval x1 < xopt < x2.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using some library frontend to minimize_scalar. You can simply\n",
      "        pass a callable as the ``method`` parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `bracket`, `tol`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  The method\n",
      "        shall return an `OptimizeResult` object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method. You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the problem of minimizing the following function.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x - 2) * x * (x + 2)**2\n",
      "        \n",
      "        Using the *Brent* method, we find the local minimum as:\n",
      "        \n",
      "        >>> from scipy.optimize import minimize_scalar\n",
      "        >>> res = minimize_scalar(f)\n",
      "        >>> res.x\n",
      "        1.28077640403\n",
      "        \n",
      "        Using the *Bounded* method, we find a local minimum with specified\n",
      "        bounds as:\n",
      "        \n",
      "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
      "        >>> res.x\n",
      "        -2.0000002026\n",
      "    \n",
      "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None, x1=None, rtol=0.0, full_output=False, disp=True)\n",
      "        Find a zero of a real or complex function using the Newton-Raphson\n",
      "        (or secant or Halley's) method.\n",
      "        \n",
      "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
      "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
      "        is provided, otherwise the secant method is used. If the second order\n",
      "        derivative `fprime2` of `func` is also provided, then Halley's method is\n",
      "        used.\n",
      "        \n",
      "        If `x0` is a sequence with more than one item, then `newton` returns an\n",
      "        array, and `func` must be vectorized and return a sequence or array of the\n",
      "        same shape as its first argument. If `fprime` or `fprime2` is given, then\n",
      "        its return must also have the same shape.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The function whose zero is wanted. It must be a function of a\n",
      "            single variable of the form ``f(x,a,b,c...)``, where ``a,b,c...``\n",
      "            are extra arguments that can be passed in the `args` parameter.\n",
      "        x0 : float, sequence, or ndarray\n",
      "            An initial estimate of the zero that should be somewhere near the\n",
      "            actual zero. If not scalar, then `func` must be vectorized and return\n",
      "            a sequence or array of the same shape as its first argument.\n",
      "        fprime : callable, optional\n",
      "            The derivative of the function when available and convenient. If it\n",
      "            is None (default), then the secant method is used.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to be used in the function call.\n",
      "        tol : float, optional\n",
      "            The allowable error of the zero value. If `func` is complex-valued,\n",
      "            a larger `tol` is recommended as both the real and imaginary parts\n",
      "            of `x` contribute to ``|x - x0|``.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        fprime2 : callable, optional\n",
      "            The second order derivative of the function when available and\n",
      "            convenient. If it is None (default), then the normal Newton-Raphson\n",
      "            or the secant method is used. If it is not None, then Halley's method\n",
      "            is used.\n",
      "        x1 : float, optional\n",
      "            Another estimate of the zero that should be somewhere near the\n",
      "            actual zero. Used if `fprime` is not provided.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False (default), the root is returned.\n",
      "            If True and `x0` is scalar, the return value is ``(x, r)``, where ``x``\n",
      "            is the root and ``r`` is a `RootResults` object.\n",
      "            If True and `x0` is non-scalar, the return value is ``(x, converged,\n",
      "            zero_der)`` (see Returns section for details).\n",
      "        disp : bool, optional\n",
      "            If True, raise a RuntimeError if the algorithm didn't converge, with\n",
      "            the error message containing the number of iterations and current\n",
      "            function value. Otherwise, the convergence status is recorded in a\n",
      "            `RootResults` return object.\n",
      "            Ignored if `x0` is not scalar.\n",
      "            *Note: this has little to do with displaying, however,\n",
      "            the `disp` keyword cannot be renamed for backwards compatibility.*\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        root : float, sequence, or ndarray\n",
      "            Estimated location where function is zero.\n",
      "        r : `RootResults`, optional\n",
      "            Present if ``full_output=True`` and `x0` is scalar.\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        converged : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements converged successfully.\n",
      "        zero_der : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements had a zero derivative.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect\n",
      "        fsolve : find zeros in N dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The convergence rate of the Newton-Raphson method is quadratic,\n",
      "        the Halley method is cubic, and the secant method is\n",
      "        sub-quadratic. This means that if the function is well-behaved\n",
      "        the actual error in the estimated zero after the nth iteration\n",
      "        is approximately the square (cube for Halley) of the error\n",
      "        after the (n-1)th step. However, the stopping criterion used\n",
      "        here is the step size and there is no guarantee that a zero\n",
      "        has been found. Consequently, the result should be verified.\n",
      "        Safer algorithms are brentq, brenth, ridder, and bisect,\n",
      "        but they all require that the root first be bracketed in an\n",
      "        interval where the function changes sign. The brentq algorithm\n",
      "        is recommended for general use in one dimensional problems\n",
      "        when such an interval has been found.\n",
      "        \n",
      "        When `newton` is used with arrays, it is best suited for the following\n",
      "        types of problems:\n",
      "        \n",
      "        * The initial guesses, `x0`, are all relatively the same distance from\n",
      "          the roots.\n",
      "        * Some or all of the extra arguments, `args`, are also arrays so that a\n",
      "          class of similar problems can be solved together.\n",
      "        * The size of the initial guesses, `x0`, is larger than O(100) elements.\n",
      "          Otherwise, a naive loop may perform as well or better than a vector.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        ``fprime`` is not provided, use the secant method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        >>> root = optimize.newton(f, 1.5, fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        \n",
      "        Only ``fprime`` is provided, use the Newton-Raphson method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        Both ``fprime2`` and ``fprime`` are provided, use Halley's method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2,\n",
      "        ...                        fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        When we want to find zeros for a set of related starting values and/or\n",
      "        function parameters, we can provide both of those as an array of inputs:\n",
      "        \n",
      "        >>> f = lambda x, a: x**3 - a\n",
      "        >>> fder = lambda x, a: 3 * x**2\n",
      "        >>> rng = np.random.default_rng()\n",
      "        >>> x = rng.standard_normal(100)\n",
      "        >>> a = np.arange(-50, 50)\n",
      "        >>> vec_res = optimize.newton(f, x, fprime=fder, args=(a, ), maxiter=200)\n",
      "        \n",
      "        The above is the equivalent of solving for each value in ``(x, a)``\n",
      "        separately in a for-loop, just faster:\n",
      "        \n",
      "        >>> loop_res = [optimize.newton(f, x0, fprime=fder, args=(a0,))\n",
      "        ...             for x0, a0 in zip(x, a)]\n",
      "        >>> np.allclose(vec_res, loop_res)\n",
      "        True\n",
      "        \n",
      "        Plot the results found for all values of ``a``:\n",
      "        \n",
      "        >>> analytical_result = np.sign(a) * np.abs(a)**(1/3)\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(a, analytical_result, 'o')\n",
      "        >>> ax.plot(a, vec_res, '.')\n",
      "        >>> ax.set_xlabel('$a$')\n",
      "        >>> ax.set_ylabel('$x$ where $f(x, a)=0$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
      "        \n",
      "        This method is suitable for solving large-scale problems.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        rdiff : float, optional\n",
      "            Relative step size to use in numerical differentiation.\n",
      "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
      "            Krylov method to use to approximate the Jacobian.\n",
      "            Can be a string, or a function implementing the same interface as\n",
      "            the iterative solvers in `scipy.sparse.linalg`.\n",
      "        \n",
      "            The default is `scipy.sparse.linalg.lgmres`.\n",
      "        inner_maxiter : int, optional\n",
      "            Parameter to pass to the \"inner\" Krylov solver: maximum number of\n",
      "            iterations. Iteration will stop after maxiter steps even if the\n",
      "            specified tolerance has not been achieved.\n",
      "        inner_M : LinearOperator or InverseJacobian\n",
      "            Preconditioner for the inner Krylov iteration.\n",
      "            Note that you can use also inverse Jacobians as (adaptive)\n",
      "            preconditioners. For example,\n",
      "        \n",
      "            >>> from scipy.optimize.nonlin import BroydenFirst, KrylovJacobian\n",
      "            >>> from scipy.optimize.nonlin import InverseJacobian\n",
      "            >>> jac = BroydenFirst()\n",
      "            >>> kjac = KrylovJacobian(inner_M=InverseJacobian(jac))\n",
      "        \n",
      "            If the preconditioner has a method named 'update', it will be called\n",
      "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
      "            the current point, and ``f`` the current function value.\n",
      "        outer_k : int, optional\n",
      "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
      "            See `scipy.sparse.linalg.lgmres` for details.\n",
      "        inner_kwargs : kwargs\n",
      "            Keyword parameters for the \"inner\" Krylov solver\n",
      "            (defined with `method`). Parameter names must start with\n",
      "            the `inner_` prefix which will be stripped before passing on\n",
      "            the inner method. See, e.g., `scipy.sparse.linalg.gmres` for details.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='krylov'`` in particular.\n",
      "        scipy.sparse.linalg.gmres\n",
      "        scipy.sparse.linalg.lgmres\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements a Newton-Krylov solver. The basic idea is\n",
      "        to compute the inverse of the Jacobian with an iterative Krylov\n",
      "        method. These methods require only evaluating the Jacobian-vector\n",
      "        products, which are conveniently approximated by a finite difference:\n",
      "        \n",
      "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
      "        \n",
      "        Due to the use of iterative matrix inverses, these methods can\n",
      "        deal with large nonlinear problems.\n",
      "        \n",
      "        SciPy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
      "        solvers to choose from. The default here is `lgmres`, which is a\n",
      "        variant of restarted GMRES iteration that reuses some of the\n",
      "        information obtained in the previous Newton steps to invert\n",
      "        Jacobians in subsequent steps.\n",
      "        \n",
      "        For a review on Newton-Krylov methods, see for example [1]_,\n",
      "        and for the LGMRES sparse inverse method, see [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2004).\n",
      "               :doi:`10.1016/j.jcp.2003.08.010`\n",
      "        .. [2] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
      "               SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
      "               :doi:`10.1137/S0895479803422014`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0] + 0.5 * x[1] - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0]) ** 2]\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.newton_krylov(fun, [0, 0])\n",
      "        >>> sol\n",
      "        array([0.66731771, 0.66536458])\n",
      "    \n",
      "    nnls(A, b, maxiter=None)\n",
      "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
      "        for a FORTRAN non-negative least squares solver.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : ndarray\n",
      "            Matrix ``A`` as shown above.\n",
      "        b : ndarray\n",
      "            Right-hand side vector.\n",
      "        maxiter: int, optional\n",
      "            Maximum number of iterations, optional.\n",
      "            Default is ``3 * A.shape[1]``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Solution vector.\n",
      "        rnorm : float\n",
      "            The residual, ``|| Ax-b ||_2``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lsq_linear : Linear least squares with bounds on the variables\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The FORTRAN code was published in the book below. The algorithm\n",
      "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
      "        conditions for the non-negative least squares problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
      "        \n",
      "         Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import nnls\n",
      "        ...\n",
      "        >>> A = np.array([[1, 0], [1, 0], [0, 1]])\n",
      "        >>> b = np.array([2, 1, 1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([1.5, 1. ]), 0.7071067811865475)\n",
      "        \n",
      "        >>> b = np.array([-1, -1, -1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([0., 0.]), 1.7320508075688772)\n",
      "    \n",
      "    quadratic_assignment(A, B, method='faq', options=None)\n",
      "        Approximates solution to the quadratic assignment problem and\n",
      "        the graph matching problem.\n",
      "        \n",
      "        Quadratic assignment solves problems of the following form:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_P & \\ {\\ \\text{trace}(A^T P B P^T)}\\\\\n",
      "            \\mbox{s.t. } & {P \\ \\epsilon \\ \\mathcal{P}}\\\\\n",
      "        \n",
      "        where :math:`\\mathcal{P}` is the set of all permutation matrices,\n",
      "        and :math:`A` and :math:`B` are square matrices.\n",
      "        \n",
      "        Graph matching tries to *maximize* the same objective function.\n",
      "        This algorithm can be thought of as finding the alignment of the\n",
      "        nodes of two graphs that minimizes the number of induced edge\n",
      "        disagreements, or, in the case of weighted graphs, the sum of squared\n",
      "        edge weight differences.\n",
      "        \n",
      "        Note that the quadratic assignment problem is NP-hard. The results given\n",
      "        here are approximations and are not guaranteed to be optimal.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : 2-D array, square\n",
      "            The square matrix :math:`A` in the objective function above.\n",
      "        \n",
      "        B : 2-D array, square\n",
      "            The square matrix :math:`B` in the objective function above.\n",
      "        \n",
      "        method :  str in {'faq', '2opt'} (default: 'faq')\n",
      "            The algorithm used to solve the problem.\n",
      "            :ref:`'faq' <optimize.qap-faq>` (default) and\n",
      "            :ref:`'2opt' <optimize.qap-2opt>` are available.\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All solvers support the following:\n",
      "        \n",
      "            maximize : bool (default: False)\n",
      "                Maximizes the objective function if ``True``.\n",
      "        \n",
      "            partial_match : 2-D array of integers, optional (default: None)\n",
      "                Fixes part of the matching. Also known as a \"seed\" [2]_.\n",
      "        \n",
      "                Each row of `partial_match` specifies a pair of matched nodes:\n",
      "                node ``partial_match[i, 0]`` of `A` is matched to node\n",
      "                ``partial_match[i, 1]`` of `B`. The array has shape ``(m, 2)``,\n",
      "                where ``m`` is not greater than the number of nodes, :math:`n`.\n",
      "        \n",
      "            rng : {None, int, `numpy.random.Generator`,\n",
      "                   `numpy.random.RandomState`}, optional\n",
      "        \n",
      "                If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      "                singleton is used.\n",
      "                If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "                seeded with `seed`.\n",
      "                If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      "                that instance is used.\n",
      "        \n",
      "            For method-specific options, see\n",
      "            :func:`show_options('quadratic_assignment') <show_options>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            `OptimizeResult` containing the following fields.\n",
      "        \n",
      "            col_ind : 1-D array\n",
      "                Column indices corresponding to the best permutation found of the\n",
      "                nodes of `B`.\n",
      "            fun : float\n",
      "                The objective value of the solution.\n",
      "            nit : int\n",
      "                The number of iterations performed during optimization.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The default method :ref:`'faq' <optimize.qap-faq>` uses the Fast\n",
      "        Approximate QAP algorithm [1]_; it typically offers the best combination of\n",
      "        speed and accuracy.\n",
      "        Method :ref:`'2opt' <optimize.qap-2opt>` can be computationally expensive,\n",
      "        but may be a useful alternative, or it can be used to refine the solution\n",
      "        returned by another method.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J.T. Vogelstein, J.M. Conroy, V. Lyzinski, L.J. Podrazik,\n",
      "               S.G. Kratzer, E.T. Harley, D.E. Fishkind, R.J. Vogelstein, and\n",
      "               C.E. Priebe, \"Fast approximate quadratic programming for graph\n",
      "               matching,\" PLOS one, vol. 10, no. 4, p. e0121002, 2015,\n",
      "               :doi:`10.1371/journal.pone.0121002`\n",
      "        \n",
      "        .. [2] D. Fishkind, S. Adali, H. Patsolic, L. Meng, D. Singh, V. Lyzinski,\n",
      "               C. Priebe, \"Seeded graph matching\", Pattern Recognit. 87 (2019):\n",
      "               203-215, :doi:`10.1016/j.patcog.2018.09.014`\n",
      "        \n",
      "        .. [3] \"2-opt,\" Wikipedia.\n",
      "               https://en.wikipedia.org/wiki/2-opt\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import quadratic_assignment\n",
      "        >>> A = np.array([[0, 80, 150, 170], [80, 0, 130, 100],\n",
      "        ...               [150, 130, 0, 120], [170, 100, 120, 0]])\n",
      "        >>> B = np.array([[0, 5, 2, 7], [0, 0, 3, 8],\n",
      "        ...               [0, 0, 0, 3], [0, 0, 0, 0]])\n",
      "        >>> res = quadratic_assignment(A, B)\n",
      "        >>> print(res)\n",
      "         col_ind: array([0, 3, 2, 1])\n",
      "             fun: 3260\n",
      "             nit: 9\n",
      "        \n",
      "        The see the relationship between the returned ``col_ind`` and ``fun``,\n",
      "        use ``col_ind`` to form the best permutation matrix found, then evaluate\n",
      "        the objective function :math:`f(P) = trace(A^T P B P^T )`.\n",
      "        \n",
      "        >>> perm = res['col_ind']\n",
      "        >>> P = np.eye(len(A), dtype=int)[perm]\n",
      "        >>> fun = np.trace(A.T @ P @ B @ P.T)\n",
      "        >>> print(fun)\n",
      "        3260\n",
      "        \n",
      "        Alternatively, to avoid constructing the permutation matrix explicitly,\n",
      "        directly permute the rows and columns of the distance matrix.\n",
      "        \n",
      "        >>> fun = np.trace(A.T @ B[perm][:, perm])\n",
      "        >>> print(fun)\n",
      "        3260\n",
      "        \n",
      "        Although not guaranteed in general, ``quadratic_assignment`` happens to\n",
      "        have found the globally optimal solution.\n",
      "        \n",
      "        >>> from itertools import permutations\n",
      "        >>> perm_opt, fun_opt = None, np.inf\n",
      "        >>> for perm in permutations([0, 1, 2, 3]):\n",
      "        ...     perm = np.array(perm)\n",
      "        ...     fun = np.trace(A.T @ B[perm][:, perm])\n",
      "        ...     if fun < fun_opt:\n",
      "        ...         fun_opt, perm_opt = fun, perm\n",
      "        >>> print(np.array_equal(perm_opt, res['col_ind']))\n",
      "        True\n",
      "        \n",
      "        Here is an example for which the default method,\n",
      "        :ref:`'faq' <optimize.qap-faq>`, does not find the global optimum.\n",
      "        \n",
      "        >>> A = np.array([[0, 5, 8, 6], [5, 0, 5, 1],\n",
      "        ...               [8, 5, 0, 2], [6, 1, 2, 0]])\n",
      "        >>> B = np.array([[0, 1, 8, 4], [1, 0, 5, 2],\n",
      "        ...               [8, 5, 0, 5], [4, 2, 5, 0]])\n",
      "        >>> res = quadratic_assignment(A, B)\n",
      "        >>> print(res)\n",
      "         col_ind: array([1, 0, 3, 2])\n",
      "             fun: 178\n",
      "             nit: 13\n",
      "        \n",
      "        If accuracy is important, consider using  :ref:`'2opt' <optimize.qap-2opt>`\n",
      "        to refine the solution.\n",
      "        \n",
      "        >>> guess = np.array([np.arange(len(A)), res.col_ind]).T\n",
      "        >>> res = quadratic_assignment(A, B, method=\"2opt\",\n",
      "        ...                            options = {'partial_guess': guess})\n",
      "        >>> print(res)\n",
      "         col_ind: array([1, 2, 3, 0])\n",
      "             fun: 176\n",
      "             nit: 17\n",
      "    \n",
      "    ridder(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in an interval using Ridder's method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number. f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            Containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.\n",
      "            In particular, ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton : 1-D root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
      "        generally as fast as the Brent routines. [Ridders1979]_ provides the\n",
      "        classic description and source of the algorithm. A description can also be\n",
      "        found in any recent edition of Numerical Recipes.\n",
      "        \n",
      "        The routine used here diverges slightly from standard presentations in\n",
      "        order to be a bit more careful of tolerance.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ridders1979]\n",
      "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
      "           Single Root of a Real Continuous Function.\"\n",
      "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.ridder(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.ridder(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "    \n",
      "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
      "        Find a root of a vector function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            A vector function to find a root of.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its Jacobian.\n",
      "        method : str, optional\n",
      "            Type of solver. Should be one of\n",
      "        \n",
      "                - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "                - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "                - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "                - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "                - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "                - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "                - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "                - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "                - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "                - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "        \n",
      "        jac : bool or callable, optional\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            value of Jacobian along with the objective function. If False, the\n",
      "            Jacobian will be estimated numerically.\n",
      "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "            this case, it must accept the same arguments as `fun`.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., `xtol` or `maxiter`, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : OptimizeResult\n",
      "            The solution represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the algorithm exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *hybr*.\n",
      "        \n",
      "        Method *hybr* uses a modification of the Powell hybrid method as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *lm* solves the system of nonlinear equations in a least squares\n",
      "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "        \n",
      "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "        with backtracking or full line searches [2]_. Each method corresponds\n",
      "        to a particular Jacobian approximations. See `nonlin` for details.\n",
      "        \n",
      "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "          known as Broyden's good method.\n",
      "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "          is known as Broyden's bad method.\n",
      "        - Method *anderson* uses (extended) Anderson mixing.\n",
      "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "          is suitable for large-scale problem.\n",
      "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "          approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            The algorithms implemented for methods *diagbroyden*,\n",
      "            *linearmixing* and *excitingmixing* may be useful for specific\n",
      "            problems, but whether they will work may depend strongly on the\n",
      "            problem.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "           1980. User Guide for MINPACK-1.\n",
      "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "           Equations. Society for Industrial and Applied Mathematics.\n",
      "           <https://archive.siam.org/books/kelley/fr16/>\n",
      "        .. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations and its\n",
      "        jacobian.\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        >>> def jac(x):\n",
      "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "        ...                       -1.5 * (x[0] - x[1])**2],\n",
      "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
      "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      "        >>> sol.x\n",
      "        array([ 0.8411639,  0.1588361])\n",
      "    \n",
      "    root_scalar(f, args=(), method=None, bracket=None, fprime=None, fprime2=None, x0=None, x1=None, xtol=None, rtol=None, maxiter=None, options=None)\n",
      "        Find a root of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            A function to find a root of.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its derivative(s).\n",
      "        method : str, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'bisect'    :ref:`(see here) <optimize.root_scalar-bisect>`\n",
      "                - 'brentq'    :ref:`(see here) <optimize.root_scalar-brentq>`\n",
      "                - 'brenth'    :ref:`(see here) <optimize.root_scalar-brenth>`\n",
      "                - 'ridder'    :ref:`(see here) <optimize.root_scalar-ridder>`\n",
      "                - 'toms748'    :ref:`(see here) <optimize.root_scalar-toms748>`\n",
      "                - 'newton'    :ref:`(see here) <optimize.root_scalar-newton>`\n",
      "                - 'secant'    :ref:`(see here) <optimize.root_scalar-secant>`\n",
      "                - 'halley'    :ref:`(see here) <optimize.root_scalar-halley>`\n",
      "        \n",
      "        bracket: A sequence of 2 floats, optional\n",
      "            An interval bracketing a root.  `f(x, *args)` must have different\n",
      "            signs at the two endpoints.\n",
      "        x0 : float, optional\n",
      "            Initial guess.\n",
      "        x1 : float, optional\n",
      "            A second guess.\n",
      "        fprime : bool or callable, optional\n",
      "            If `fprime` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the derivative.\n",
      "            `fprime` can also be a callable returning the derivative of `f`. In\n",
      "            this case, it must accept the same arguments as `f`.\n",
      "        fprime2 : bool or callable, optional\n",
      "            If `fprime2` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the\n",
      "            first and second derivatives.\n",
      "            `fprime2` can also be a callable returning the second derivative of `f`.\n",
      "            In this case, it must accept the same arguments as `f`.\n",
      "        xtol : float, optional\n",
      "            Tolerance (absolute) for termination.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g., ``k``, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : RootResults\n",
      "            The solution represented as a ``RootResults`` object.\n",
      "            Important attributes are: ``root`` the solution , ``converged`` a\n",
      "            boolean flag indicating if the algorithm exited successfully and\n",
      "            ``flag`` which describes the cause of the termination. See\n",
      "            `RootResults` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        root : Find a root of a vector function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        The default is to use the best method available for the situation\n",
      "        presented.\n",
      "        If a bracket is provided, it may use one of the bracketing methods.\n",
      "        If a derivative and an initial value are specified, it may\n",
      "        select one of the derivative-based methods.\n",
      "        If no method is judged applicable, it will raise an Exception.\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Find the root of a simple cubic\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> def fprime(x):\n",
      "        ...     return 3*x**2\n",
      "        \n",
      "        The `brentq` method takes as input a bracket\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, bracket=[0, 3], method='brentq')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 10, 11)\n",
      "        \n",
      "        The `newton` method takes as input a single point and uses the derivative(s)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, x0=0.2, fprime=fprime, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 22)\n",
      "        \n",
      "        The function can provide the value and derivative(s) in a single call.\n",
      "        \n",
      "        >>> def f_p_pp(x):\n",
      "        ...     return (x**3 - 1), 3*x**2, 6*x\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 11)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, fprime2=True, method='halley')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 7, 8)\n",
      "    \n",
      "    rosen(x)\n",
      "        The Rosenbrock function.\n",
      "        \n",
      "        The function computed is::\n",
      "        \n",
      "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Rosenbrock function is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            The value of the Rosenbrock function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen_der, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen\n",
      "        >>> X = 0.1 * np.arange(10)\n",
      "        >>> rosen(X)\n",
      "        76.56\n",
      "        \n",
      "        For higher-dimensional input ``rosen`` broadcasts.\n",
      "        In the following example, we use this to plot a 2D landscape.\n",
      "        Note that ``rosen_hess`` does not broadcast in this manner.\n",
      "        \n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from mpl_toolkits.mplot3d import Axes3D\n",
      "        >>> x = np.linspace(-1, 1, 50)\n",
      "        >>> X, Y = np.meshgrid(x, x)\n",
      "        >>> ax = plt.subplot(111, projection='3d')\n",
      "        >>> ax.plot_surface(X, Y, rosen([X, Y]))\n",
      "        >>> plt.show()\n",
      "    \n",
      "    rosen_der(x)\n",
      "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the derivative is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_der : (N,) ndarray\n",
      "            The gradient of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_der\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> rosen_der(X)\n",
      "        array([ -2. ,  10.6,  15.6,  13.4,   6.4,  -3. , -12.4, -19.4,  62. ])\n",
      "    \n",
      "    rosen_hess(x)\n",
      "        The Hessian matrix of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess\n",
      "        >>> X = 0.1 * np.arange(4)\n",
      "        >>> rosen_hess(X)\n",
      "        array([[-38.,   0.,   0.,   0.],\n",
      "               [  0., 134., -40.,   0.],\n",
      "               [  0., -40., 130., -80.],\n",
      "               [  0.,   0., -80., 200.]])\n",
      "    \n",
      "    rosen_hess_prod(x, p)\n",
      "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        p : array_like\n",
      "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess_prod : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
      "            by the vector `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess_prod\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> p = 0.5 * np.arange(9)\n",
      "        >>> rosen_hess_prod(X, p)\n",
      "        array([  -0.,   27.,  -10.,  -95., -192., -265., -278., -195., -180.])\n",
      "    \n",
      "    shgo(func, bounds, args=(), constraints=None, n=None, iters=1, callback=None, minimizer_kwargs=None, options=None, sampling_method='simplicial')\n",
      "        Finds the global minimum of a function using SHG optimization.\n",
      "        \n",
      "        SHGO stands for \"simplicial homology global optimization\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining the lower and upper bounds for the optimizing argument of\n",
      "            `func`. It is required to have ``len(bounds) == len(x)``.\n",
      "            ``len(bounds)`` is used to determine the number of parameters in ``x``.\n",
      "            Use ``None`` for one of min or max when there is no bound in that\n",
      "            direction. By default bounds are ``(None, None)``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        constraints : dict or sequence of dict, optional\n",
      "            Constraints definition.\n",
      "            Function(s) ``R**n`` in the form::\n",
      "        \n",
      "                g(x) >= 0 applied as g : R^n -> R^m\n",
      "                h(x) == 0 applied as h : R^n -> R^p\n",
      "        \n",
      "            Each constraint is defined in a dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "               Only the COBYLA and SLSQP local minimize methods currently\n",
      "               support constraint arguments. If the ``constraints`` sequence\n",
      "               used in the local optimization problem is not defined in\n",
      "               ``minimizer_kwargs`` and a constrained method is used then the\n",
      "               global ``constraints`` will be used.\n",
      "               (Defining a ``constraints`` sequence in ``minimizer_kwargs``\n",
      "               means that ``constraints`` will not be added so if equality\n",
      "               constraints and so forth need to be added then the inequality\n",
      "               functions in ``constraints`` need to be added to\n",
      "               ``minimizer_kwargs`` too).\n",
      "        \n",
      "        n : int, optional\n",
      "            Number of sampling points used in the construction of the simplicial\n",
      "            complex. Note that this argument is only used for ``sobol`` and other\n",
      "            arbitrary `sampling_methods`. In case of ``sobol``, it must be a\n",
      "            power of 2: ``n=2**m``, and the argument will automatically be\n",
      "            converted to the next higher power of 2. Default is 100 for\n",
      "            ``sampling_method='simplicial'`` and 128 for\n",
      "            ``sampling_method='sobol'``.\n",
      "        iters : int, optional\n",
      "            Number of iterations used in the construction of the simplicial\n",
      "            complex. Default is 1.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the minimizer\n",
      "            ``scipy.optimize.minimize`` Some important options could be:\n",
      "        \n",
      "                * method : str\n",
      "                    The minimization method, the default is ``SLSQP``.\n",
      "                * args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "                * options : dict, optional\n",
      "                    Note that by default the tolerance is specified as\n",
      "                    ``{ftol: 1e-12}``\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. Many of the options specified for the\n",
      "            global routine are also passed to the scipy.optimize.minimize routine.\n",
      "            The options that are also passed to the local routine are marked with\n",
      "            \"(L)\".\n",
      "        \n",
      "            Stopping criteria, the algorithm will terminate if any of the specified\n",
      "            criteria are met. However, the default algorithm does not require any to\n",
      "            be specified:\n",
      "        \n",
      "            * maxfev : int (L)\n",
      "                Maximum number of function evaluations in the feasible domain.\n",
      "                (Note only methods that support this option will terminate\n",
      "                the routine at precisely exact specified value. Otherwise the\n",
      "                criterion will only terminate during a global iteration)\n",
      "            * f_min\n",
      "                Specify the minimum objective function value, if it is known.\n",
      "            * f_tol : float\n",
      "                Precision goal for the value of f in the stopping\n",
      "                criterion. Note that the global routine will also\n",
      "                terminate if a sampling point in the global routine is\n",
      "                within this tolerance.\n",
      "            * maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            * maxev : int\n",
      "                Maximum number of sampling evaluations to perform (includes\n",
      "                searching in infeasible points).\n",
      "            * maxtime : float\n",
      "                Maximum processing runtime allowed\n",
      "            * minhgrd : int\n",
      "                Minimum homology group rank differential. The homology group of the\n",
      "                objective function is calculated (approximately) during every\n",
      "                iteration. The rank of this group has a one-to-one correspondence\n",
      "                with the number of locally convex subdomains in the objective\n",
      "                function (after adequate sampling points each of these subdomains\n",
      "                contain a unique global minimum). If the difference in the hgr is 0\n",
      "                between iterations for ``maxhgrd`` specified iterations the\n",
      "                algorithm will terminate.\n",
      "        \n",
      "            Objective function knowledge:\n",
      "        \n",
      "            * symmetry : bool\n",
      "                Specify True if the objective function contains symmetric variables.\n",
      "                The search space (and therefore performance) is decreased by O(n!).\n",
      "        \n",
      "            * jac : bool or callable, optional\n",
      "                Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "                Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If ``jac`` is a\n",
      "                boolean and is True, ``fun`` is assumed to return the gradient along\n",
      "                with the objective function. If False, the gradient will be\n",
      "                estimated numerically. ``jac`` can also be a callable returning the\n",
      "                gradient of the objective. In this case, it must accept the same\n",
      "                arguments as ``fun``. (Passed to `scipy.optimize.minmize` automatically)\n",
      "        \n",
      "            * hess, hessp : callable, optional\n",
      "                Hessian (matrix of second-order derivatives) of objective function\n",
      "                or Hessian of objective function times an arbitrary vector p.\n",
      "                Only for Newton-CG, dogleg, trust-ncg. Only one of ``hessp`` or\n",
      "                ``hess`` needs to be given. If ``hess`` is provided, then\n",
      "                ``hessp`` will be ignored. If neither ``hess`` nor ``hessp`` is\n",
      "                provided, then the Hessian product will be approximated using\n",
      "                finite differences on ``jac``. ``hessp`` must compute the Hessian\n",
      "                times an arbitrary vector. (Passed to `scipy.optimize.minmize`\n",
      "                automatically)\n",
      "        \n",
      "            Algorithm settings:\n",
      "        \n",
      "            * minimize_every_iter : bool\n",
      "                If True then promising global sampling points will be passed to a\n",
      "                local minimization routine every iteration. If False then only the\n",
      "                final minimizer pool will be run. Defaults to False.\n",
      "            * local_iter : int\n",
      "                Only evaluate a few of the best minimizer pool candidates every\n",
      "                iteration. If False all potential points are passed to the local\n",
      "                minimization routine.\n",
      "            * infty_constraints: bool\n",
      "                If True then any sampling points generated which are outside will\n",
      "                the feasible domain will be saved and given an objective function\n",
      "                value of ``inf``. If False then these points will be discarded.\n",
      "                Using this functionality could lead to higher performance with\n",
      "                respect to function evaluations before the global minimum is found,\n",
      "                specifying False will use less memory at the cost of a slight\n",
      "                decrease in performance. Defaults to True.\n",
      "        \n",
      "            Feedback:\n",
      "        \n",
      "            * disp : bool (L)\n",
      "                Set to True to print convergence messages.\n",
      "        \n",
      "        sampling_method : str or function, optional\n",
      "            Current built in sampling method options are ``halton``, ``sobol`` and\n",
      "            ``simplicial``. The default ``simplicial`` provides\n",
      "            the theoretical guarantee of convergence to the global minimum in finite\n",
      "            time. ``halton`` and ``sobol`` method are faster in terms of sampling\n",
      "            point generation at the cost of the loss of\n",
      "            guaranteed convergence. It is more appropriate for most \"easier\"\n",
      "            problems where the convergence is relatively fast.\n",
      "            User defined sampling functions must accept two arguments of ``n``\n",
      "            sampling points of dimension ``dim`` per call and output an array of\n",
      "            sampling points with shape `n x dim`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are:\n",
      "            ``x`` the solution array corresponding to the global minimum,\n",
      "            ``fun`` the function output at the global solution,\n",
      "            ``xl`` an ordered list of local minima solutions,\n",
      "            ``funl`` the function output at the corresponding local solutions,\n",
      "            ``success`` a Boolean flag indicating if the optimizer exited\n",
      "            successfully,\n",
      "            ``message`` which describes the cause of the termination,\n",
      "            ``nfev`` the total number of objective function evaluations including\n",
      "            the sampling calls,\n",
      "            ``nlfev`` the total number of objective function evaluations\n",
      "            culminating from all local search optimizations,\n",
      "            ``nit`` number of iterations performed by the global routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Global optimization using simplicial homology global optimization [1]_.\n",
      "        Appropriate for solving general purpose NLP and blackbox optimization\n",
      "        problems to global optimality (low-dimensional problems).\n",
      "        \n",
      "        In general, the optimization problems are of the form::\n",
      "        \n",
      "            minimize f(x) subject to\n",
      "        \n",
      "            g_i(x) >= 0,  i = 1,...,m\n",
      "            h_j(x)  = 0,  j = 1,...,p\n",
      "        \n",
      "        where x is a vector of one or more variables. ``f(x)`` is the objective\n",
      "        function ``R^n -> R``, ``g_i(x)`` are the inequality constraints, and\n",
      "        ``h_j(x)`` are the equality constraints.\n",
      "        \n",
      "        Optionally, the lower and upper bounds for each element in x can also be\n",
      "        specified using the `bounds` argument.\n",
      "        \n",
      "        While most of the theoretical advantages of SHGO are only proven for when\n",
      "        ``f(x)`` is a Lipschitz smooth function, the algorithm is also proven to\n",
      "        converge to the global optimum for the more general case where ``f(x)`` is\n",
      "        non-continuous, non-convex and non-smooth, if the default sampling method\n",
      "        is used [1]_.\n",
      "        \n",
      "        The local search method may be specified using the ``minimizer_kwargs``\n",
      "        parameter which is passed on to ``scipy.optimize.minimize``. By default,\n",
      "        the ``SLSQP`` method is used. In general, it is recommended to use the\n",
      "        ``SLSQP`` or ``COBYLA`` local minimization if inequality constraints\n",
      "        are defined for the problem since the other methods do not use constraints.\n",
      "        \n",
      "        The ``halton`` and ``sobol`` method points are generated using\n",
      "        `scipy.stats.qmc`. Any other QMC method could be used.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Endres, SC, Sandrock, C, Focke, WW (2018) \"A simplicial homology\n",
      "               algorithm for lipschitz optimisation\", Journal of Global Optimization.\n",
      "        .. [2] Joe, SW and Kuo, FY (2008) \"Constructing Sobol' sequences with\n",
      "               better  two-dimensional projections\", SIAM J. Sci. Comput. 30,\n",
      "               2635-2654.\n",
      "        .. [3] Hoch, W and Schittkowski, K (1981) \"Test examples for nonlinear\n",
      "               programming codes\", Lecture Notes in Economics and Mathematical\n",
      "               Systems, 187. Springer-Verlag, New York.\n",
      "               http://www.ai7.uni-bayreuth.de/test_problem_coll.pdf\n",
      "        .. [4] Wales, DJ (2015) \"Perspective: Insight into reaction coordinates and\n",
      "               dynamics from the potential energy landscape\",\n",
      "               Journal of Chemical Physics, 142(13), 2015.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First consider the problem of minimizing the Rosenbrock function, `rosen`:\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, shgo\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 2.920392374190081e-18)\n",
      "        \n",
      "        Note that bounds determine the dimensionality of the objective\n",
      "        function and is therefore a required input, however you can specify\n",
      "        empty bounds using ``None`` or objects like ``np.inf`` which will be\n",
      "        converted to large float numbers.\n",
      "        \n",
      "        >>> bounds = [(None, None), ]*4\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x\n",
      "        array([0.99999851, 0.99999704, 0.99999411, 0.9999882 ])\n",
      "        \n",
      "        Next, we consider the Eggholder function, a problem with several local\n",
      "        minima and one global minimum. We will demonstrate the use of arguments and\n",
      "        the capabilities of `shgo`.\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization)\n",
      "        \n",
      "        >>> def eggholder(x):\n",
      "        ...     return (-(x[1] + 47.0)\n",
      "        ...             * np.sin(np.sqrt(abs(x[0]/2.0 + (x[1] + 47.0))))\n",
      "        ...             - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47.0))))\n",
      "        ...             )\n",
      "        ...\n",
      "        >>> bounds = [(-512, 512), (-512, 512)]\n",
      "        \n",
      "        `shgo` has built-in low discrepancy sampling sequences. First, we will\n",
      "        input 64 initial sampling points of the *Sobol'* sequence:\n",
      "        \n",
      "        >>> result = shgo(eggholder, bounds, n=64, sampling_method='sobol')\n",
      "        >>> result.x, result.fun\n",
      "        (array([512.        , 404.23180824]), -959.6406627208397)\n",
      "        \n",
      "        `shgo` also has a return for any other local minima that was found, these\n",
      "        can be called using:\n",
      "        \n",
      "        >>> result.xl\n",
      "        array([[ 512.        ,  404.23180824],\n",
      "               [ 283.0759062 , -487.12565635],\n",
      "               [-294.66820039, -462.01964031],\n",
      "               [-105.87688911,  423.15323845],\n",
      "               [-242.97926   ,  274.38030925],\n",
      "               [-506.25823477,    6.3131022 ],\n",
      "               [-408.71980731, -156.10116949],\n",
      "               [ 150.23207937,  301.31376595],\n",
      "               [  91.00920901, -391.283763  ],\n",
      "               [ 202.89662724, -269.38043241],\n",
      "               [ 361.66623976, -106.96493868],\n",
      "               [-219.40612786, -244.06020508]])\n",
      "        \n",
      "        >>> result.funl\n",
      "        array([-959.64066272, -718.16745962, -704.80659592, -565.99778097,\n",
      "               -559.78685655, -557.36868733, -507.87385942, -493.9605115 ,\n",
      "               -426.48799655, -421.15571437, -419.31194957, -410.98477763])\n",
      "        \n",
      "        These results are useful in applications where there are many global minima\n",
      "        and the values of other global minima are desired or where the local minima\n",
      "        can provide insight into the system (for example morphologies\n",
      "        in physical chemistry [4]_).\n",
      "        \n",
      "        If we want to find a larger number of local minima, we can increase the\n",
      "        number of sampling points or the number of iterations. We'll increase the\n",
      "        number of sampling points to 64 and the number of iterations from the\n",
      "        default of 1 to 3. Using ``simplicial`` this would have given us\n",
      "        64 x 3 = 192 initial sampling points.\n",
      "        \n",
      "        >>> result_2 = shgo(eggholder, bounds, n=64, iters=3, sampling_method='sobol')\n",
      "        >>> len(result.xl), len(result_2.xl)\n",
      "        (12, 20)\n",
      "        \n",
      "        Note the difference between, e.g., ``n=192, iters=1`` and ``n=64,\n",
      "        iters=3``.\n",
      "        In the first case the promising points contained in the minimiser pool\n",
      "        are processed only once. In the latter case it is processed every 64\n",
      "        sampling points for a total of 3 times.\n",
      "        \n",
      "        To demonstrate solving problems with non-linear constraints consider the\n",
      "        following example from Hock and Schittkowski problem 73 (cattle-feed) [3]_::\n",
      "        \n",
      "            minimize: f = 24.55 * x_1 + 26.75 * x_2 + 39 * x_3 + 40.50 * x_4\n",
      "        \n",
      "            subject to: 2.3 * x_1 + 5.6 * x_2 + 11.1 * x_3 + 1.3 * x_4 - 5     >= 0,\n",
      "        \n",
      "                        12 * x_1 + 11.9 * x_2 + 41.8 * x_3 + 52.1 * x_4 - 21\n",
      "                            -1.645 * sqrt(0.28 * x_1**2 + 0.19 * x_2**2 +\n",
      "                                          20.5 * x_3**2 + 0.62 * x_4**2)       >= 0,\n",
      "        \n",
      "                        x_1 + x_2 + x_3 + x_4 - 1                              == 0,\n",
      "        \n",
      "                        1 >= x_i >= 0 for all i\n",
      "        \n",
      "        The approximate answer given in [3]_ is::\n",
      "        \n",
      "            f([0.6355216, -0.12e-11, 0.3127019, 0.05177655]) = 29.894378\n",
      "        \n",
      "        >>> def f(x):  # (cattle-feed)\n",
      "        ...     return 24.55*x[0] + 26.75*x[1] + 39*x[2] + 40.50*x[3]\n",
      "        ...\n",
      "        >>> def g1(x):\n",
      "        ...     return 2.3*x[0] + 5.6*x[1] + 11.1*x[2] + 1.3*x[3] - 5  # >=0\n",
      "        ...\n",
      "        >>> def g2(x):\n",
      "        ...     return (12*x[0] + 11.9*x[1] +41.8*x[2] + 52.1*x[3] - 21\n",
      "        ...             - 1.645 * np.sqrt(0.28*x[0]**2 + 0.19*x[1]**2\n",
      "        ...                             + 20.5*x[2]**2 + 0.62*x[3]**2)\n",
      "        ...             ) # >=0\n",
      "        ...\n",
      "        >>> def h1(x):\n",
      "        ...     return x[0] + x[1] + x[2] + x[3] - 1  # == 0\n",
      "        ...\n",
      "        >>> cons = ({'type': 'ineq', 'fun': g1},\n",
      "        ...         {'type': 'ineq', 'fun': g2},\n",
      "        ...         {'type': 'eq', 'fun': h1})\n",
      "        >>> bounds = [(0, 1.0),]*4\n",
      "        >>> res = shgo(f, bounds, iters=3, constraints=cons)\n",
      "        >>> res\n",
      "             fun: 29.894378159142136\n",
      "            funl: array([29.89437816])\n",
      "         message: 'Optimization terminated successfully.'\n",
      "            nfev: 114\n",
      "             nit: 3\n",
      "           nlfev: 35\n",
      "           nlhev: 0\n",
      "           nljev: 5\n",
      "         success: True\n",
      "               x: array([6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02])\n",
      "              xl: array([[6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02]])\n",
      "        \n",
      "        >>> g1(res.x), g2(res.x), h1(res.x)\n",
      "        (-5.062616992290714e-14, -2.9594104944408173e-12, 0.0)\n",
      "    \n",
      "    show_options(solver=None, method=None, disp=True)\n",
      "        Show documentation for additional options of optimization solvers.\n",
      "        \n",
      "        These are method-specific options that can be supplied through the\n",
      "        ``options`` dict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        solver : str\n",
      "            Type of optimization solver. One of 'minimize', 'minimize_scalar',\n",
      "            'root', 'root_scalar', 'linprog', or 'quadratic_assignment'.\n",
      "        method : str, optional\n",
      "            If not given, shows all methods of the specified solver. Otherwise,\n",
      "            show only the options for the specified method. Valid values\n",
      "            corresponds to methods' names of respective solver (e.g., 'BFGS' for\n",
      "            'minimize').\n",
      "        disp : bool, optional\n",
      "            Whether to print the result rather than returning it.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            Either None (for disp=True) or the text string (disp=False)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The solver-specific methods are:\n",
      "        \n",
      "        `scipy.optimize.minimize`\n",
      "        \n",
      "        - :ref:`Nelder-Mead <optimize.minimize-neldermead>`\n",
      "        - :ref:`Powell      <optimize.minimize-powell>`\n",
      "        - :ref:`CG          <optimize.minimize-cg>`\n",
      "        - :ref:`BFGS        <optimize.minimize-bfgs>`\n",
      "        - :ref:`Newton-CG   <optimize.minimize-newtoncg>`\n",
      "        - :ref:`L-BFGS-B    <optimize.minimize-lbfgsb>`\n",
      "        - :ref:`TNC         <optimize.minimize-tnc>`\n",
      "        - :ref:`COBYLA      <optimize.minimize-cobyla>`\n",
      "        - :ref:`SLSQP       <optimize.minimize-slsqp>`\n",
      "        - :ref:`dogleg      <optimize.minimize-dogleg>`\n",
      "        - :ref:`trust-ncg   <optimize.minimize-trustncg>`\n",
      "        \n",
      "        `scipy.optimize.root`\n",
      "        \n",
      "        - :ref:`hybr              <optimize.root-hybr>`\n",
      "        - :ref:`lm                <optimize.root-lm>`\n",
      "        - :ref:`broyden1          <optimize.root-broyden1>`\n",
      "        - :ref:`broyden2          <optimize.root-broyden2>`\n",
      "        - :ref:`anderson          <optimize.root-anderson>`\n",
      "        - :ref:`linearmixing      <optimize.root-linearmixing>`\n",
      "        - :ref:`diagbroyden       <optimize.root-diagbroyden>`\n",
      "        - :ref:`excitingmixing    <optimize.root-excitingmixing>`\n",
      "        - :ref:`krylov            <optimize.root-krylov>`\n",
      "        - :ref:`df-sane           <optimize.root-dfsane>`\n",
      "        \n",
      "        `scipy.optimize.minimize_scalar`\n",
      "        \n",
      "        - :ref:`brent       <optimize.minimize_scalar-brent>`\n",
      "        - :ref:`golden      <optimize.minimize_scalar-golden>`\n",
      "        - :ref:`bounded     <optimize.minimize_scalar-bounded>`\n",
      "        \n",
      "        `scipy.optimize.root_scalar`\n",
      "        \n",
      "        - :ref:`bisect  <optimize.root_scalar-bisect>`\n",
      "        - :ref:`brentq  <optimize.root_scalar-brentq>`\n",
      "        - :ref:`brenth  <optimize.root_scalar-brenth>`\n",
      "        - :ref:`ridder  <optimize.root_scalar-ridder>`\n",
      "        - :ref:`toms748 <optimize.root_scalar-toms748>`\n",
      "        - :ref:`newton  <optimize.root_scalar-newton>`\n",
      "        - :ref:`secant  <optimize.root_scalar-secant>`\n",
      "        - :ref:`halley  <optimize.root_scalar-halley>`\n",
      "        \n",
      "        `scipy.optimize.linprog`\n",
      "        \n",
      "        - :ref:`simplex           <optimize.linprog-simplex>`\n",
      "        - :ref:`interior-point    <optimize.linprog-interior-point>`\n",
      "        - :ref:`revised simplex   <optimize.linprog-revised_simplex>`\n",
      "        - :ref:`highs             <optimize.linprog-highs>`\n",
      "        - :ref:`highs-ds          <optimize.linprog-highs-ds>`\n",
      "        - :ref:`highs-ipm         <optimize.linprog-highs-ipm>`\n",
      "        \n",
      "        `scipy.optimize.quadratic_assignment`\n",
      "        \n",
      "        - :ref:`faq             <optimize.qap-faq>`\n",
      "        - :ref:`2opt            <optimize.qap-2opt>`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We can print documentations of a solver in stdout:\n",
      "        \n",
      "        >>> from scipy.optimize import show_options\n",
      "        >>> show_options(solver=\"minimize\")\n",
      "        ...\n",
      "        \n",
      "        Specifying a method is possible:\n",
      "        \n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\")\n",
      "        ...\n",
      "        \n",
      "        We can also get the documentations as a string:\n",
      "        \n",
      "        >>> show_options(solver=\"minimize\", method=\"Nelder-Mead\", disp=False)\n",
      "        Minimization of scalar function of one or more variables using the ...\n",
      "    \n",
      "    toms748(f, a, b, args=(), k=1, xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a zero using TOMS Algorithm 748 method.\n",
      "        \n",
      "        Implements the Algorithm 748 method of Alefeld, Potro and Shi to find a\n",
      "        zero of the function `f` on the interval `[a , b]`, where `f(a)` and\n",
      "        `f(b)` must have opposite signs.\n",
      "        \n",
      "        It uses a mixture of inverse cubic interpolation and\n",
      "        \"Newton-quadratic\" steps. [APS1995].\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a scalar. The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)`\n",
      "            have opposite signs.\n",
      "        a : scalar,\n",
      "            lower boundary of the search interval\n",
      "        b : scalar,\n",
      "            upper boundary of the search interval\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``f(x, *args)``.\n",
      "        k : int, optional\n",
      "            The number of Newton quadratic steps to perform each\n",
      "            iteration. ``k>=1``.\n",
      "        xtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root.\n",
      "        maxiter : int, optional\n",
      "            If convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised. Must be >= 0.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned. If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise, the convergence status is recorded in the `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Approximate Zero of `f`\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence. In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect, newton\n",
      "        fsolve : find zeroes in N dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.\n",
      "        Algorithm 748 with ``k=2`` is asymptotically the most efficient\n",
      "        algorithm known for finding roots of a four times continuously\n",
      "        differentiable function.\n",
      "        In contrast with Brent's algorithm, which may only decrease the length of\n",
      "        the enclosing bracket on the last step, Algorithm 748 decreases it each\n",
      "        iteration with the same asymptotic efficiency as it finds the root.\n",
      "        \n",
      "        For easy statement of efficiency indices, assume that `f` has 4\n",
      "        continuouous deriviatives.\n",
      "        For ``k=1``, the convergence order is at least 2.7, and with about\n",
      "        asymptotically 2 function evaluations per iteration, the efficiency\n",
      "        index is approximately 1.65.\n",
      "        For ``k=2``, the order is about 4.6 with asymptotically 3 function\n",
      "        evaluations per iteration, and the efficiency index 1.66.\n",
      "        For higher values of `k`, the efficiency index approaches\n",
      "        the kth root of ``(3k-2)``, hence ``k=1`` or ``k=2`` are\n",
      "        usually appropriate.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [APS1995]\n",
      "           Alefeld, G. E. and Potra, F. A. and Shi, Yixun,\n",
      "           *Algorithm 748: Enclosing Zeros of Continuous Functions*,\n",
      "           ACM Trans. Math. Softw. Volume 221(1995)\n",
      "           doi = {10.1145/210089.210111}\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> root, results = optimize.toms748(f, 0, 2, full_output=True)\n",
      "        >>> root\n",
      "        1.0\n",
      "        >>> results\n",
      "              converged: True\n",
      "                   flag: 'converged'\n",
      "         function_calls: 11\n",
      "             iterations: 5\n",
      "                   root: 1.0\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BFGS', 'Bounds', 'HessianUpdateStrategy', 'LbfgsInvHessPro...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\amant\\anaconda3\\lib\\site-packages\\scipy\\optimize\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a6381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14034f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Akima1DInterpolator',\n",
       " 'BPoly',\n",
       " 'BSpline',\n",
       " 'BarycentricInterpolator',\n",
       " 'BivariateSpline',\n",
       " 'CloughTocher2DInterpolator',\n",
       " 'CubicHermiteSpline',\n",
       " 'CubicSpline',\n",
       " 'InterpolatedUnivariateSpline',\n",
       " 'KroghInterpolator',\n",
       " 'LSQBivariateSpline',\n",
       " 'LSQSphereBivariateSpline',\n",
       " 'LSQUnivariateSpline',\n",
       " 'LinearNDInterpolator',\n",
       " 'NdPPoly',\n",
       " 'NearestNDInterpolator',\n",
       " 'PPoly',\n",
       " 'PchipInterpolator',\n",
       " 'RBFInterpolator',\n",
       " 'Rbf',\n",
       " 'RectBivariateSpline',\n",
       " 'RectSphereBivariateSpline',\n",
       " 'RegularGridInterpolator',\n",
       " 'SmoothBivariateSpline',\n",
       " 'SmoothSphereBivariateSpline',\n",
       " 'UnivariateSpline',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_bspl',\n",
       " '_bsplines',\n",
       " '_cubic',\n",
       " '_fitpack',\n",
       " '_fitpack_impl',\n",
       " '_pade',\n",
       " '_ppoly',\n",
       " '_rbfinterp',\n",
       " '_rbfinterp_pythran',\n",
       " 'approximate_taylor_polynomial',\n",
       " 'barycentric_interpolate',\n",
       " 'bisplev',\n",
       " 'bisplrep',\n",
       " 'dfitpack',\n",
       " 'fitpack',\n",
       " 'fitpack2',\n",
       " 'griddata',\n",
       " 'insert',\n",
       " 'interp1d',\n",
       " 'interp2d',\n",
       " 'interpn',\n",
       " 'interpnd',\n",
       " 'interpolate',\n",
       " 'krogh_interpolate',\n",
       " 'lagrange',\n",
       " 'make_interp_spline',\n",
       " 'make_lsq_spline',\n",
       " 'ndgriddata',\n",
       " 'pade',\n",
       " 'pchip',\n",
       " 'pchip_interpolate',\n",
       " 'polyint',\n",
       " 'rbf',\n",
       " 'spalde',\n",
       " 'splantider',\n",
       " 'splder',\n",
       " 'splev',\n",
       " 'splint',\n",
       " 'splprep',\n",
       " 'splrep',\n",
       " 'sproot',\n",
       " 'test']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(interpolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab6859a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54366c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__closure__',\n",
       " '__code__',\n",
       " '__defaults__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__globals__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__kwdefaults__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__wrapped__']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d630e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\amant\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\amant\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\amant\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\amant\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\amant\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e4f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\amant\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\amant\\anaconda3\\lib\\site-packages (from scipy) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e723e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d106653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Construct a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It returns a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimator : object\n",
      "            The deep copy of the input, an estimator if input is an estimator.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        return different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "    \n",
      "    config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Context manager for global scikit-learn configuration.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. If None, the existing value won't change.\n",
      "            The default value is False.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. If None, the existing value won't change.\n",
      "            The default value is 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. If None, the existing value won't change.\n",
      "            The default value is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. If None, the existing value won't change.\n",
      "            The default value is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    1.0.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\amant\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f64d2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__SKLEARN_SETUP__',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__check_build',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_config',\n",
       " '_distributor_init',\n",
       " 'base',\n",
       " 'clone',\n",
       " 'config_context',\n",
       " 'exceptions',\n",
       " 'externals',\n",
       " 'get_config',\n",
       " 'logger',\n",
       " 'logging',\n",
       " 'os',\n",
       " 'random',\n",
       " 'set_config',\n",
       " 'setup_module',\n",
       " 'show_versions',\n",
       " 'sys',\n",
       " 'utils']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12851831",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (78741759.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [22]\u001b[1;36m\u001b[0m\n\u001b[1;33m    from scipy.constants\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from scipy.constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc3b7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc113b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3eb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
